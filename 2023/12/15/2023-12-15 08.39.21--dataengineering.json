{"kind": "Listing", "data": {"after": "t3_18i5ovj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys!\n\nI am making a small group of people learning data engineering where we get on a call together every other week and talk about tools we're learning and other DE-related things. This will be good for everyone in the group to get better at DE and help each other out when needed. \n\nThanks, and happy learning to everyone!\n\nEdit: If more of you are interested consider making small groups with each other.\n\nEdit, again: If you are still interested please reach out to other people who want to make groups.", "author_fullname": "t2_3arqiq34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Group of Data Engineering Learners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i9w4x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702601650.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702564301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys!&lt;/p&gt;\n\n&lt;p&gt;I am making a small group of people learning data engineering where we get on a call together every other week and talk about tools we&amp;#39;re learning and other DE-related things. This will be good for everyone in the group to get better at DE and help each other out when needed. &lt;/p&gt;\n\n&lt;p&gt;Thanks, and happy learning to everyone!&lt;/p&gt;\n\n&lt;p&gt;Edit: If more of you are interested consider making small groups with each other.&lt;/p&gt;\n\n&lt;p&gt;Edit, again: If you are still interested please reach out to other people who want to make groups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18i9w4x", "is_robot_indexable": true, "report_reasons": null, "author": "RepresentativePen297", "discussion_type": null, "num_comments": 134, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i9w4x/small_group_of_data_engineering_learners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i9w4x/small_group_of_data_engineering_learners/", "subreddit_subscribers": 146037, "created_utc": 1702564301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "People who\u2019ve been in this game since when the hype was really getting started in the beginning of the last decade, what was it like working in the first companies that tried to build big data pipelines? What was the stack you worked on? I know that Hadoop and NoSQL were very trendy at the time, but can you share some more typical technologies that you ran into a lot? Are some of them supplanted by now or are they mostly still going strong?", "author_fullname": "t2_1xobc52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What was data engineering like circa 2011-2013?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ilbto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702594923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who\u2019ve been in this game since when the hype was really getting started in the beginning of the last decade, what was it like working in the first companies that tried to build big data pipelines? What was the stack you worked on? I know that Hadoop and NoSQL were very trendy at the time, but can you share some more typical technologies that you ran into a lot? Are some of them supplanted by now or are they mostly still going strong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ilbto", "is_robot_indexable": true, "report_reasons": null, "author": "pimmen89", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ilbto/what_was_data_engineering_like_circa_20112013/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ilbto/what_was_data_engineering_like_circa_20112013/", "subreddit_subscribers": 146037, "created_utc": 1702594923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a proprietary Excel .VBA that uses a highly complex mathematical function using 6 values to generate a number.  E.g.,:\n\n=PropietaryFormula(A1,B1,C1,D1,E1)*F1\n\nI don't have access to the VBA source code and a can't reverse engineer the math function.  I want to get away from using Excel and be able to fetch the value with an HTTP call (Azure function) by sending the 6 inputs in the HTTP request.   To generate all possible values using these inputs, the end result is around 600 billion unique combinations.\n\nI'm able to use Power Automate Desktop to open Excel, populate the inputs, and generate the needed value using the function.  I think I can do this for about 100,000 rows for each Excel file to stay within the memory limits on my desktop.  From there is where I'm wondering what would be the easiest way to get this into a data warehouse.  I'm thinking I could upload these 100s of thousands of Excel files to Azure ADL2 storage and use Synapse Analytics or Databricks to push them into a database, but I'm hoping someone out there may have a much better, faster, and cheaper idea.\n\nThanks!", "author_fullname": "t2_9lsmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you populate 600 billion rows in a structured database where the values are generated from Excel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18il8i8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702594692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a proprietary Excel .VBA that uses a highly complex mathematical function using 6 values to generate a number.  E.g.,:&lt;/p&gt;\n\n&lt;p&gt;=PropietaryFormula(A1,B1,C1,D1,E1)*F1&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have access to the VBA source code and a can&amp;#39;t reverse engineer the math function.  I want to get away from using Excel and be able to fetch the value with an HTTP call (Azure function) by sending the 6 inputs in the HTTP request.   To generate all possible values using these inputs, the end result is around 600 billion unique combinations.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m able to use Power Automate Desktop to open Excel, populate the inputs, and generate the needed value using the function.  I think I can do this for about 100,000 rows for each Excel file to stay within the memory limits on my desktop.  From there is where I&amp;#39;m wondering what would be the easiest way to get this into a data warehouse.  I&amp;#39;m thinking I could upload these 100s of thousands of Excel files to Azure ADL2 storage and use Synapse Analytics or Databricks to push them into a database, but I&amp;#39;m hoping someone out there may have a much better, faster, and cheaper idea.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18il8i8", "is_robot_indexable": true, "report_reasons": null, "author": "dantasticdotorg", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18il8i8/how_would_you_populate_600_billion_rows_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18il8i8/how_would_you_populate_600_billion_rows_in_a/", "subreddit_subscribers": 146037, "created_utc": 1702594692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was looking into Azure documentation for clustered columnstore indexes and I noticed the following:  \n\" Clustered columnstore indexes work on segments of 1,048,576 rows. As Azure Synapse Analytics has 60 nodes per distribution, the minimum recommended number of rows for a clustered columnstore index is 60,000,000. \"\n\nwhy is it that the don\u00b4t close it to just 1million? also, why is this the exact same limit of rows on excel?\n\nIs there someting I'm missing about this number?", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why 1,048,576 rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18id3ea", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702573133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking into Azure documentation for clustered columnstore indexes and I noticed the following:&lt;br/&gt;\n&amp;quot; Clustered columnstore indexes work on segments of 1,048,576 rows. As Azure Synapse Analytics has 60 nodes per distribution, the minimum recommended number of rows for a clustered columnstore index is 60,000,000. &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;why is it that the don\u00b4t close it to just 1million? also, why is this the exact same limit of rows on excel?&lt;/p&gt;\n\n&lt;p&gt;Is there someting I&amp;#39;m missing about this number?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18id3ea", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18id3ea/why_1048576_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18id3ea/why_1048576_rows/", "subreddit_subscribers": 146037, "created_utc": 1702573133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Heres an architectural journey that I experienced on working with a data product on AWS Serverless architecture then  to migrate it back to servers using celery and Kubernetes and creating an entire orchestration workflow that would give  more control in architecture, feature developments, cost reductions and turn around time reductions. What we ended up was with creating our own AWS lambda and stepfunctions and avoid vendor lockins\n\n[https://medium.com/@rajani.param1/back-to-the-future-the-future-might-not-just-be-serverless-after-all-b8165d6e84c2](https://medium.com/@rajani.param1/back-to-the-future-the-future-might-not-just-be-serverless-after-all-b8165d6e84c2)\n\n  \nFeel free to share down your thoughts and opinions on this .  \n", "author_fullname": "t2_5fi8z4gf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To Serverless or not to Serverless that is the question..?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18icwuf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702577435.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702572640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heres an architectural journey that I experienced on working with a data product on AWS Serverless architecture then  to migrate it back to servers using celery and Kubernetes and creating an entire orchestration workflow that would give  more control in architecture, feature developments, cost reductions and turn around time reductions. What we ended up was with creating our own AWS lambda and stepfunctions and avoid vendor lockins&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@rajani.param1/back-to-the-future-the-future-might-not-just-be-serverless-after-all-b8165d6e84c2\"&gt;https://medium.com/@rajani.param1/back-to-the-future-the-future-might-not-just-be-serverless-after-all-b8165d6e84c2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feel free to share down your thoughts and opinions on this .  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?auto=webp&amp;s=67d661201a964c66dc3ea00e4111958c4811fe64", "width": 1012, "height": 969}, "resolutions": [{"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=961c02fbc890e97e99c12d9c30e2f3de2d710276", "width": 108, "height": 103}, {"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb182e35d0e3cc78a4e456b2f3410fd275f7ade3", "width": 216, "height": 206}, {"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c0cbb4a8db6a2a5468b4d2ef7efe2e5d3a21de4", "width": 320, "height": 306}, {"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=83b51bc6a41e93746a01b5f547914ad8d1726684", "width": 640, "height": 612}, {"url": "https://external-preview.redd.it/NgGer2EJ-QrPfxGEhcsFKF5-gy4CdGbR9hNIesltjEc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=58a3a85dd698d423e21144f798dbb70c58014e9e", "width": 960, "height": 919}], "variants": {}, "id": "mVLxJRsuAEGjFpeAi_DoJiW5UQVt-Y4U2Q6WWdTEZwg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18icwuf", "is_robot_indexable": true, "report_reasons": null, "author": "Drphysics5", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18icwuf/to_serverless_or_not_to_serverless_that_is_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18icwuf/to_serverless_or_not_to_serverless_that_is_the/", "subreddit_subscribers": 146037, "created_utc": 1702572640.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our org (~ 7000 employees) is looking to get into DataBricks. Management has expressed their belief that a dedicated application admin position should not be necessary since DataBricks handles all the infrastructure for you.\n\nI\u2019m of the belief a mission critical system should have an owner to handle troubleshooting, permissioning, policy rules, etc, and that making it an analyst\u2019s part time job is asking for trouble.\n\nThoughts? Data is probably in the 10 tb range but with a desire to grow into high volume streaming use cases.", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks admin?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18idt4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702575002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our org (~ 7000 employees) is looking to get into DataBricks. Management has expressed their belief that a dedicated application admin position should not be necessary since DataBricks handles all the infrastructure for you.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m of the belief a mission critical system should have an owner to handle troubleshooting, permissioning, policy rules, etc, and that making it an analyst\u2019s part time job is asking for trouble.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Data is probably in the 10 tb range but with a desire to grow into high volume streaming use cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18idt4y", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18idt4y/databricks_admin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18idt4y/databricks_admin/", "subreddit_subscribers": 146037, "created_utc": 1702575002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started working with a small business. Until now, they have been using Microsoft SharePoint to keep their system-generated Excel files every day and then access them with other programs like Power BI. Still, now they are thinking about some more reliable solution to have a database to keep the data  they were thinking about Microsoft access. Still, I am thinking about what is the best thing these days for a small business to keep their data just a few different tables with a few relationships or even none of them.", "author_fullname": "t2_q9hhtqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help with a database choice for small business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i43ue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702542541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working with a small business. Until now, they have been using Microsoft SharePoint to keep their system-generated Excel files every day and then access them with other programs like Power BI. Still, now they are thinking about some more reliable solution to have a database to keep the data  they were thinking about Microsoft access. Still, I am thinking about what is the best thing these days for a small business to keep their data just a few different tables with a few relationships or even none of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18i43ue", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-Nyan", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i43ue/help_with_a_database_choice_for_small_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i43ue/help_with_a_database_choice_for_small_business/", "subreddit_subscribers": 146037, "created_utc": 1702542541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_pxof4neu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing LaunchFlow: The Developer Platform Built for Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ie2zo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702575712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "launchflow.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.launchflow.com/blog/introducing-launchflow", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ie2zo", "is_robot_indexable": true, "report_reasons": null, "author": "josh_flow", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ie2zo/introducing_launchflow_the_developer_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.launchflow.com/blog/introducing-launchflow", "subreddit_subscribers": 146037, "created_utc": 1702575712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019ve been trying to solve for this problem for a couple of years now. Trying to make a generic platform/product for data quality that would work for multiple data personas within the company. We knew this was going to be a hard one to solve.. and we\u2019re yet to hit that breakthrough. Curious to know what other data folks are doing and how they are solving for data quality.", "author_fullname": "t2_h5ll08of", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iqbbp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702609777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019ve been trying to solve for this problem for a couple of years now. Trying to make a generic platform/product for data quality that would work for multiple data personas within the company. We knew this was going to be a hard one to solve.. and we\u2019re yet to hit that breakthrough. Curious to know what other data folks are doing and how they are solving for data quality.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18iqbbp", "is_robot_indexable": true, "report_reasons": null, "author": "Lucky-Front7675", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18iqbbp/data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18iqbbp/data_quality/", "subreddit_subscribers": 146037, "created_utc": 1702609777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fj8n8tp3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Database Engines use Functional Dependency Analysis to Improve Join Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18ih5ww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ROhIT7U9rbH6v9fxxr0BjA8yGwUvHK83afHPMnkHcMQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702583902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dolthub.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dolthub.com/blog/2023-12-13-functional-dependency-analysis/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?auto=webp&amp;s=399f65a130fe7e276dcf2f712ab6c84acaefd398", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ac95291e9bfb7eaa406f2ed4b8db927aa863b6e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3bdab7fa719123ca2fdd9b7bd5358504b0375cb9", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abf32a74a7c9b106a3f8356c095900cceb64406", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e03f01c2a5f37cac07548ca5fc2c1003396eb99", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a289f58fc92cb2f6ca74962bef71ee39d1a7adb0", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/43U8RAV7umjnXMZG0cLdxdRUI3nPqYqJ1jvZnT6AgG8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21bc571e2e3faa66af1e41315b3c9ebac4fb0473", "width": 1080, "height": 564}], "variants": {}, "id": "GVDVzKAiIGM2SDFASEWM2e-sTg22Qu_GDhOd_tP5EmU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ih5ww", "is_robot_indexable": true, "report_reasons": null, "author": "nick_at_dolt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ih5ww/how_database_engines_use_functional_dependency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dolthub.com/blog/2023-12-13-functional-dependency-analysis/", "subreddit_subscribers": 146037, "created_utc": 1702583902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an owner of a small retail business who is teaching myself DE/BI concepts via Coursera, ChatGPT, YouTube, podcasts, etc. Our business sells fairly niche goods online and also has a small consignment-based popup inside another retail business. Both my business and our pop-up host each use our own Shopify accounts for sales/inventory/product tracking. \n\nMy business goal is to have a single source of truth for product sales, by aggregating both Shopify accounts sales and product records into a database of my own. Currently, doing any sort of meaningful analysis involves manually exporting a csv from each store and joining the data in spreadsheets (or SQL if I\u2019m feeling saucy and the data is bigger than sheets can take.)\n\nTrying to solve for this has led me to DE-related online courses, which are really interesting and getting me feeling closer to a solution. I had an initial, fleeting success making a PostgreSQL database through [Render.com](https://Render.com) and setting up pipelines in [Make.com](https://Make.com) to catch incoming sales from both Shopify accounts, and load them into a \u2018master\\_sales\u2019 table, that I could then query to answer questions about our total product sales over varying time periods. This ended up becoming a mess, though, after a couple of failed pipeline executions, and numerous missing records and duplicate records. So, I went back to the drawing board. Currently, I've now got a bash script that can query Shopify REST API. I've also got some good grounding in Pandas for data transformation. Overall, I want to get to the point where I have some checks in place. \n\nI\u2019d love to hear from more experienced people in this field about what sort of approach you might apply to this problem. None of the data is that big (yet, right now accumulating about +100MB/month), nor does it need to be updated more frequently than daily (at best). I\u2019m looking at options such as setting up a dedicated linux server at home running cron jobs, or digging into AWS/Azure (because cloud computing is super interesting). How would you approach this business problem?\n\nSince starting this business, I\u2019ve definitely unlocked a part of myself that is continually excited about solving the computer-technical/data/systems design opportunities that might improve my day-to-day experience operating things. I've always been into computers as a hobby, but never for work. My work experience is in bartending and retail.\n\nAlso, I have another question: My small business is very small, and can't pay me that well. I really love all these things I\u2019m learning and data-oriented jobs pay alot better than I'm currently doing. Given that I don\u2019t have any tech industry experience, how might one approach a career change, from service industry and small retail business ownership, into Data Engineering? I\u2019m really happy that I have potential projects on my plate to help me study, as well as add potential fodder to a portfolio. I feel like if a career change is in the books, I should maybe aim for a Junior IT/Data Analytics position as an entry point and try to advance from there? Does anyone have a similar transition experience? ", "author_fullname": "t2_miufv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small business owner, new to, and interested in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18itcsw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702620192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an owner of a small retail business who is teaching myself DE/BI concepts via Coursera, ChatGPT, YouTube, podcasts, etc. Our business sells fairly niche goods online and also has a small consignment-based popup inside another retail business. Both my business and our pop-up host each use our own Shopify accounts for sales/inventory/product tracking. &lt;/p&gt;\n\n&lt;p&gt;My business goal is to have a single source of truth for product sales, by aggregating both Shopify accounts sales and product records into a database of my own. Currently, doing any sort of meaningful analysis involves manually exporting a csv from each store and joining the data in spreadsheets (or SQL if I\u2019m feeling saucy and the data is bigger than sheets can take.)&lt;/p&gt;\n\n&lt;p&gt;Trying to solve for this has led me to DE-related online courses, which are really interesting and getting me feeling closer to a solution. I had an initial, fleeting success making a PostgreSQL database through &lt;a href=\"https://Render.com\"&gt;Render.com&lt;/a&gt; and setting up pipelines in &lt;a href=\"https://Make.com\"&gt;Make.com&lt;/a&gt; to catch incoming sales from both Shopify accounts, and load them into a \u2018master_sales\u2019 table, that I could then query to answer questions about our total product sales over varying time periods. This ended up becoming a mess, though, after a couple of failed pipeline executions, and numerous missing records and duplicate records. So, I went back to the drawing board. Currently, I&amp;#39;ve now got a bash script that can query Shopify REST API. I&amp;#39;ve also got some good grounding in Pandas for data transformation. Overall, I want to get to the point where I have some checks in place. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d love to hear from more experienced people in this field about what sort of approach you might apply to this problem. None of the data is that big (yet, right now accumulating about +100MB/month), nor does it need to be updated more frequently than daily (at best). I\u2019m looking at options such as setting up a dedicated linux server at home running cron jobs, or digging into AWS/Azure (because cloud computing is super interesting). How would you approach this business problem?&lt;/p&gt;\n\n&lt;p&gt;Since starting this business, I\u2019ve definitely unlocked a part of myself that is continually excited about solving the computer-technical/data/systems design opportunities that might improve my day-to-day experience operating things. I&amp;#39;ve always been into computers as a hobby, but never for work. My work experience is in bartending and retail.&lt;/p&gt;\n\n&lt;p&gt;Also, I have another question: My small business is very small, and can&amp;#39;t pay me that well. I really love all these things I\u2019m learning and data-oriented jobs pay alot better than I&amp;#39;m currently doing. Given that I don\u2019t have any tech industry experience, how might one approach a career change, from service industry and small retail business ownership, into Data Engineering? I\u2019m really happy that I have potential projects on my plate to help me study, as well as add potential fodder to a portfolio. I feel like if a career change is in the books, I should maybe aim for a Junior IT/Data Analytics position as an entry point and try to advance from there? Does anyone have a similar transition experience? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?auto=webp&amp;s=afa45699d6fb3f760fb20e2d9d6f44228063d633", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0488541fa0dd55375440acdececcdee031a940d5", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba3bfd8369fa2c98c582333a477292123d7ffdb8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad745860ee3dc44a3840afefd8869e8efd01055c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fe4ec78918e70f6bbec642e50841b7e4c7f4ad1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16765ff17f5a2f4e8135f14db9cb079337aa67fb", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0991e51843b0a32b2501554aa0168f72ea7e8116", "width": 1080, "height": 607}], "variants": {}, "id": "w1lzwcy17wjJvdZBaFtVGF670_Y4Sib7sWLEw0rNHKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18itcsw", "is_robot_indexable": true, "report_reasons": null, "author": "dondelamort", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18itcsw/small_business_owner_new_to_and_interested_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18itcsw/small_business_owner_new_to_and_interested_in/", "subreddit_subscribers": 146037, "created_utc": 1702620192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have always been wondering if luster based frameworks like Spark and Flink would make sense for stateless streaming application that also don't needny multi row aggregation (group by, avgs, sums, etc). I mean cases where all we need is to work record by record without needing to aggregate them nor to hold state.\n\nIf, for example, the deployment is done in a k8s cluster, we can write \"standard\" code and scale horizontally to handle more records (parallelism) also eventually opening threads for get more performance (concurrency).\n\nWould that be more perfomant and easy to deploy for such a case, since we avoid the cost of abstraction where we don't need it?\n\nI think that in other words I'm asking what is the purpose of Spark streaming and Flink when records/rows don't have to be aware of each other...\n\n  \nTwo use cases I'm think about is consuming from a Kafka topic and reading new files from S3 where the logic is row-by-row/file-by-file as stated. \n\nAppreciate opinions.\n\nThanks.", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any reason to use Spark or Flink for stateless non aggregative streaming application?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ihmuo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702585389.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702585146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have always been wondering if luster based frameworks like Spark and Flink would make sense for stateless streaming application that also don&amp;#39;t needny multi row aggregation (group by, avgs, sums, etc). I mean cases where all we need is to work record by record without needing to aggregate them nor to hold state.&lt;/p&gt;\n\n&lt;p&gt;If, for example, the deployment is done in a k8s cluster, we can write &amp;quot;standard&amp;quot; code and scale horizontally to handle more records (parallelism) also eventually opening threads for get more performance (concurrency).&lt;/p&gt;\n\n&lt;p&gt;Would that be more perfomant and easy to deploy for such a case, since we avoid the cost of abstraction where we don&amp;#39;t need it?&lt;/p&gt;\n\n&lt;p&gt;I think that in other words I&amp;#39;m asking what is the purpose of Spark streaming and Flink when records/rows don&amp;#39;t have to be aware of each other...&lt;/p&gt;\n\n&lt;p&gt;Two use cases I&amp;#39;m think about is consuming from a Kafka topic and reading new files from S3 where the logic is row-by-row/file-by-file as stated. &lt;/p&gt;\n\n&lt;p&gt;Appreciate opinions.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ihmuo", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ihmuo/is_there_any_reason_to_use_spark_or_flink_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ihmuo/is_there_any_reason_to_use_spark_or_flink_for/", "subreddit_subscribers": 146037, "created_utc": 1702585146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, looking for some advice on my situation. \n\nI have been at my company for 1.5 yrs, having joined in a rotational program where there were 2 rotations, 6 months long each and this is my 3rd team now, doing data engineering(?) work for the first time. In one of my roles during the rotational program, I did primarily data analyst stuff but it was a bs role because I had only 1 project my entire time on the team and that was to create a basic tableau and power bi dashboard. That\u2019s literally it. \n\nI am not entirely sure if I am even a data engineer proper on this team now, because I am mostly doing analyst work and basic level sql queries. I haven\u2019t touched Python or done any ETL work. \n\nRecently, I was having a check in with my manager and he was expressing discontent about how I\u2019m not able to do my work independently yet and haven\u2019t been staying up to date with all updates coming through via email and teams chats. I completely get where he is coming from and I too am frustrated that I can\u2019t really work independently and he gave me some tips but they weren\u2019t really helpful, just the same old \u201clook at the documentation\u201d but the documentation isn\u2019t that much clearer either.\n\nHowever, on the other side, I feel like there wasn\u2019t really any chance to learn when I joined this team and was kind of thrown into this work without any relevant experience and structured trainings. His excuse for this is \u201ceveryone is learning\u201d and that this is new territory for everyone, but if that is the case, I feel like me not being at the same level as my colleagues in working independently should be okay because I\u2019ve never done this type of work before and it isn\u2019t like I\u2019m not trying, because I really am. \n\nIn addition, it can be hard to keep track of all the messages, emails, etc because there is so much back and forth and I get pulled into long meetings, leaving little time for me to do my work. I am still learning how to task switch especially because my previous teams were never this crazy busy. He also said in my check in that things will only get more busy from here onward into the new year. I am really feeling down on myself because I thought I was doing well despite the circumstances, but this was a ding on my confidence in myself. \n\nI am not sure if I can stay on this team long term if this is how things will be with the project ramping up in 2024. \n\nHas anyone else had this experience? Any advice? Should I just find a new role? I don\u2019t want to quit because things are getting hard but I also feel like this isn\u2019t an environment for learning and the longer I stay on this team doing lower level analyst work, the more I am putting myself behind in the data field and doing actual relevant work.", "author_fullname": "t2_tna1k085", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feel like I still don\u2019t know what I\u2019m doing 6 months into my role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ioz9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, looking for some advice on my situation. &lt;/p&gt;\n\n&lt;p&gt;I have been at my company for 1.5 yrs, having joined in a rotational program where there were 2 rotations, 6 months long each and this is my 3rd team now, doing data engineering(?) work for the first time. In one of my roles during the rotational program, I did primarily data analyst stuff but it was a bs role because I had only 1 project my entire time on the team and that was to create a basic tableau and power bi dashboard. That\u2019s literally it. &lt;/p&gt;\n\n&lt;p&gt;I am not entirely sure if I am even a data engineer proper on this team now, because I am mostly doing analyst work and basic level sql queries. I haven\u2019t touched Python or done any ETL work. &lt;/p&gt;\n\n&lt;p&gt;Recently, I was having a check in with my manager and he was expressing discontent about how I\u2019m not able to do my work independently yet and haven\u2019t been staying up to date with all updates coming through via email and teams chats. I completely get where he is coming from and I too am frustrated that I can\u2019t really work independently and he gave me some tips but they weren\u2019t really helpful, just the same old \u201clook at the documentation\u201d but the documentation isn\u2019t that much clearer either.&lt;/p&gt;\n\n&lt;p&gt;However, on the other side, I feel like there wasn\u2019t really any chance to learn when I joined this team and was kind of thrown into this work without any relevant experience and structured trainings. His excuse for this is \u201ceveryone is learning\u201d and that this is new territory for everyone, but if that is the case, I feel like me not being at the same level as my colleagues in working independently should be okay because I\u2019ve never done this type of work before and it isn\u2019t like I\u2019m not trying, because I really am. &lt;/p&gt;\n\n&lt;p&gt;In addition, it can be hard to keep track of all the messages, emails, etc because there is so much back and forth and I get pulled into long meetings, leaving little time for me to do my work. I am still learning how to task switch especially because my previous teams were never this crazy busy. He also said in my check in that things will only get more busy from here onward into the new year. I am really feeling down on myself because I thought I was doing well despite the circumstances, but this was a ding on my confidence in myself. &lt;/p&gt;\n\n&lt;p&gt;I am not sure if I can stay on this team long term if this is how things will be with the project ramping up in 2024. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else had this experience? Any advice? Should I just find a new role? I don\u2019t want to quit because things are getting hard but I also feel like this isn\u2019t an environment for learning and the longer I stay on this team doing lower level analyst work, the more I am putting myself behind in the data field and doing actual relevant work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ioz9p", "is_robot_indexable": true, "report_reasons": null, "author": "miserablywinning", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ioz9p/feel_like_i_still_dont_know_what_im_doing_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ioz9p/feel_like_i_still_dont_know_what_im_doing_6/", "subreddit_subscribers": 146037, "created_utc": 1702605589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting a new job soon in which I'll be migrating the current data set up to Databricks. The Databricks instance will be created when I join the company.\n\nWhat should I be thinking about/researching prior to this?\n\nE.g. Data modelling, s3 bucket naming conventions, etc.", "author_fullname": "t2_17e8p1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to plan before migrating to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i84pw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702558883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting a new job soon in which I&amp;#39;ll be migrating the current data set up to Databricks. The Databricks instance will be created when I join the company.&lt;/p&gt;\n\n&lt;p&gt;What should I be thinking about/researching prior to this?&lt;/p&gt;\n\n&lt;p&gt;E.g. Data modelling, s3 bucket naming conventions, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18i84pw", "is_robot_indexable": true, "report_reasons": null, "author": "RatBapple", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i84pw/what_to_plan_before_migrating_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i84pw/what_to_plan_before_migrating_to_databricks/", "subreddit_subscribers": 146037, "created_utc": 1702558883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here used Nessie's branching to trigger a data quality check? That is, whenever a merge is performed on a nessie branch, a GX/Soda/MC job is triggered on the branch for a Write-Audit-Publish workflow?\n\nIs it something to do with Nessie hooks? ([https://projectnessie.org/nessie\\_provider/](https://projectnessie.org/nessie_provider/)). I am also curious to know how people run their data quality checks on Iceberg tables.  \n", "author_fullname": "t2_abpkolp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks on Nessie branch merges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i6617", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702551507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here used Nessie&amp;#39;s branching to trigger a data quality check? That is, whenever a merge is performed on a nessie branch, a GX/Soda/MC job is triggered on the branch for a Write-Audit-Publish workflow?&lt;/p&gt;\n\n&lt;p&gt;Is it something to do with Nessie hooks? (&lt;a href=\"https://projectnessie.org/nessie_provider/\"&gt;https://projectnessie.org/nessie_provider/&lt;/a&gt;). I am also curious to know how people run their data quality checks on Iceberg tables.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18i6617", "is_robot_indexable": true, "report_reasons": null, "author": "bytesapart", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i6617/data_quality_checks_on_nessie_branch_merges/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i6617/data_quality_checks_on_nessie_branch_merges/", "subreddit_subscribers": 146037, "created_utc": 1702551507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was approached by a recruiter for a Data Engineer III contract remote (USA) position at a FAANG company, who informed me that the pay rate was $75/hr. I would have expected a higher rate from a FAANG company, or am I being unrealistic? I've seen many posts where individuals mention working on contract for FAANG, and I wanted to understand the actual figures before I negotiate.\n\nAdditionally, they're asking me to agree to and acknowledge the pay rate via email before any process begins. Can I still negotiate the rate later if I acknowledge do the interviews and end up receiving an offer?\n\nEdit: It's a shady contracting firm run from India that's employing me on their W2 ( No benefits, paid hourly) while I work for the end client.", "author_fullname": "t2_8b8jp1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer III Pay Rate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i60w7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702574651.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702550905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was approached by a recruiter for a Data Engineer III contract remote (USA) position at a FAANG company, who informed me that the pay rate was $75/hr. I would have expected a higher rate from a FAANG company, or am I being unrealistic? I&amp;#39;ve seen many posts where individuals mention working on contract for FAANG, and I wanted to understand the actual figures before I negotiate.&lt;/p&gt;\n\n&lt;p&gt;Additionally, they&amp;#39;re asking me to agree to and acknowledge the pay rate via email before any process begins. Can I still negotiate the rate later if I acknowledge do the interviews and end up receiving an offer?&lt;/p&gt;\n\n&lt;p&gt;Edit: It&amp;#39;s a shady contracting firm run from India that&amp;#39;s employing me on their W2 ( No benefits, paid hourly) while I work for the end client.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18i60w7", "is_robot_indexable": true, "report_reasons": null, "author": "EstablishmentTop3908", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i60w7/data_engineer_iii_pay_rate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i60w7/data_engineer_iii_pay_rate/", "subreddit_subscribers": 146037, "created_utc": 1702550905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I\u2019m in over my head at a startup and have to lead the construction of analytics infrastructure that will support customer-facing embedded dashboards along with downloadable reports. \n\nThe volume of data we\u2019re dealing with is nothing crazy, probably gonna be around 20TB. And we aim to support around 5000+ users (though starting much smaller) who will regularly view a few embedded dashboards and also download reports somewhat regularly. The dashboards will need some basic drill-down and date range picking. The data is standard aggregated data that is well-suited for a DWH.\n\nOne of my main questions though is if there are special considerations when the use case is for customer-facing analytics? Will this many users interacting with Snowflake kill us with costs? We are thinking of powering the dashboards/reports with QuickSight or similar. And from the little I know so far, there is some caching that will help reduce the query load on Snowflake. Will that be sufficient?\n\nIs there a better store for aggregated relational data that\u2019s used for customer-facing analytics? Will we survive with Snowflake?\n\nHow do we handle Snowflake users for this? Do we have to provision a user for each customer? I know nothing about it.\n\nWe\u2019re entirely in AWS. I\u2019m thinking we will use DMS CDC to S3 and then 24 hour batch copy the data to Snowflake. Don\u2019t know the best tool for scheduled batch copy, any recommendations? All I know is maybe I could use Airflow or just a simple cron job. From there I will use DBT to populate a handful of star schemas that I would connect to QuickSight or similar. \n\nI would honestly be very grateful for any wisdom on any part of this setup because I\u2019m largely on my own right now to figure this out and get it built.", "author_fullname": "t2_ijoifzps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake for customer-facing analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18is4a1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702616862.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702615758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I\u2019m in over my head at a startup and have to lead the construction of analytics infrastructure that will support customer-facing embedded dashboards along with downloadable reports. &lt;/p&gt;\n\n&lt;p&gt;The volume of data we\u2019re dealing with is nothing crazy, probably gonna be around 20TB. And we aim to support around 5000+ users (though starting much smaller) who will regularly view a few embedded dashboards and also download reports somewhat regularly. The dashboards will need some basic drill-down and date range picking. The data is standard aggregated data that is well-suited for a DWH.&lt;/p&gt;\n\n&lt;p&gt;One of my main questions though is if there are special considerations when the use case is for customer-facing analytics? Will this many users interacting with Snowflake kill us with costs? We are thinking of powering the dashboards/reports with QuickSight or similar. And from the little I know so far, there is some caching that will help reduce the query load on Snowflake. Will that be sufficient?&lt;/p&gt;\n\n&lt;p&gt;Is there a better store for aggregated relational data that\u2019s used for customer-facing analytics? Will we survive with Snowflake?&lt;/p&gt;\n\n&lt;p&gt;How do we handle Snowflake users for this? Do we have to provision a user for each customer? I know nothing about it.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re entirely in AWS. I\u2019m thinking we will use DMS CDC to S3 and then 24 hour batch copy the data to Snowflake. Don\u2019t know the best tool for scheduled batch copy, any recommendations? All I know is maybe I could use Airflow or just a simple cron job. From there I will use DBT to populate a handful of star schemas that I would connect to QuickSight or similar. &lt;/p&gt;\n\n&lt;p&gt;I would honestly be very grateful for any wisdom on any part of this setup because I\u2019m largely on my own right now to figure this out and get it built.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18is4a1", "is_robot_indexable": true, "report_reasons": null, "author": "DataDude999", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18is4a1/snowflake_for_customerfacing_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18is4a1/snowflake_for_customerfacing_analytics/", "subreddit_subscribers": 146037, "created_utc": 1702615758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI'm responsible for building out a financial reporting system at my company and I could use some advice on my approach.\n\n**A little background:** We offer coaching courses that will be split amongst various systems, but all purchased through deals with customers (companies), which are converted to credits and consumed through an atomic product model. We are in the process of transitioning to a credit system, so we will need to convert our existing deals into credits for the future system (deferred credits).\n\nEssentially I need to tie in various data sources to build a table to track at a deal level the amount of credits that have been assigned, unassigned and consumed. One of the challenges is that credit values can vary by deal and therefore need to be tracked at a deal level. Also, they need to burn down to 0 by oldest deal, first in first out. This might add a bit of annoyance dealing with remainders.\n\nHeres an example of roughly what im envisioning:\n\n|Company ID|deal id|trans dt|scheduled dt|completed dt|credit category|$ per credit|\\# of Credits|product|Product Cost|Credits Remaining|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|13|2|2/23/24|||unassigned|20|1000|A|300|1000|\n|13|2|2/23/24|3/01/24||assigned|20|1000|B|400|1000|\n|13|1|1/15/24|1/23/24|1/23/24|consumed|17.50|1500|B|400|1100|\n|13|1|1/15/24|||unassigned|17.50|1500|A|300|1100|\n|13|999|1/1/24|||deferred|23|3000|||0|\n\n&amp;#x200B;\n\nIts unclear to me what the best approach is for working through the many transformations involved here. I could build out a bunch of additional fields and apply calculations to the table at various stages of my pipeline with SQL, but im wondering if theres a more elegant solution through breaking it out into various tables and processes, using python or recursive functions etc.\n\nOne last caveat is that ideally this system will be able to track rate of spend of customers, which Id imagine would most simply be tracked through snapshot tables.\n\nSorry this was a lot, but any help is greatly appreciated!", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial Reporting Problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ip1ya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702612993.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m responsible for building out a financial reporting system at my company and I could use some advice on my approach.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A little background:&lt;/strong&gt; We offer coaching courses that will be split amongst various systems, but all purchased through deals with customers (companies), which are converted to credits and consumed through an atomic product model. We are in the process of transitioning to a credit system, so we will need to convert our existing deals into credits for the future system (deferred credits).&lt;/p&gt;\n\n&lt;p&gt;Essentially I need to tie in various data sources to build a table to track at a deal level the amount of credits that have been assigned, unassigned and consumed. One of the challenges is that credit values can vary by deal and therefore need to be tracked at a deal level. Also, they need to burn down to 0 by oldest deal, first in first out. This might add a bit of annoyance dealing with remainders.&lt;/p&gt;\n\n&lt;p&gt;Heres an example of roughly what im envisioning:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Company ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;deal id&lt;/th&gt;\n&lt;th align=\"left\"&gt;trans dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;scheduled dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;completed dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;credit category&lt;/th&gt;\n&lt;th align=\"left\"&gt;$ per credit&lt;/th&gt;\n&lt;th align=\"left\"&gt;# of Credits&lt;/th&gt;\n&lt;th align=\"left\"&gt;product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Product Cost&lt;/th&gt;\n&lt;th align=\"left\"&gt;Credits Remaining&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;unassigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;300&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/01/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;assigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/15/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;consumed&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.50&lt;/td&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400&lt;/td&gt;\n&lt;td align=\"left\"&gt;1100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/15/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;unassigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.50&lt;/td&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;300&lt;/td&gt;\n&lt;td align=\"left\"&gt;1100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;999&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/1/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;deferred&lt;/td&gt;\n&lt;td align=\"left\"&gt;23&lt;/td&gt;\n&lt;td align=\"left\"&gt;3000&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Its unclear to me what the best approach is for working through the many transformations involved here. I could build out a bunch of additional fields and apply calculations to the table at various stages of my pipeline with SQL, but im wondering if theres a more elegant solution through breaking it out into various tables and processes, using python or recursive functions etc.&lt;/p&gt;\n\n&lt;p&gt;One last caveat is that ideally this system will be able to track rate of spend of customers, which Id imagine would most simply be tracked through snapshot tables.&lt;/p&gt;\n\n&lt;p&gt;Sorry this was a lot, but any help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ip1ya", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ip1ya/financial_reporting_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ip1ya/financial_reporting_problem/", "subreddit_subscribers": 146037, "created_utc": 1702605806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Experts,\n\nI'm currently exploring optimal solutions for our project, aiming to identify the most suitable framework or service for our data processing. We operate real-time dashboards that showcase alarms every two minutes, and our goal is to achieve reporting intervals closer to 10 seconds or less for new alarms.\n\nGiven the cost considerations highlighted by our product manager, we're seeking a solution that is efficient without being overly expensive. Our typical data size is approximately 50k records per job. I'd appreciate your recommendations and insights. If you're involved in a similar project, what framework or service are you utilizing? \n \nBTW- this is timeseries data\n\nThank you!", "author_fullname": "t2_6d76dzgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should I use for real time job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ic0pw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702570231.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Experts,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently exploring optimal solutions for our project, aiming to identify the most suitable framework or service for our data processing. We operate real-time dashboards that showcase alarms every two minutes, and our goal is to achieve reporting intervals closer to 10 seconds or less for new alarms.&lt;/p&gt;\n\n&lt;p&gt;Given the cost considerations highlighted by our product manager, we&amp;#39;re seeking a solution that is efficient without being overly expensive. Our typical data size is approximately 50k records per job. I&amp;#39;d appreciate your recommendations and insights. If you&amp;#39;re involved in a similar project, what framework or service are you utilizing? &lt;/p&gt;\n\n&lt;p&gt;BTW- this is timeseries data&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ic0pw", "is_robot_indexable": true, "report_reasons": null, "author": "Friendly-Radio-6312", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ic0pw/what_should_i_use_for_real_time_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ic0pw/what_should_i_use_for_real_time_job/", "subreddit_subscribers": 146037, "created_utc": 1702570231.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, post this here as well, because I spent some time gathering all the new releases of Snowflake (and competitors) in 2023 and mapping features against comparing platforms (and creating the pictures). Hopefully this usable for you as well\n\nhttps://preview.redd.it/h9uh1s2dj86c1.png?width=1219&amp;format=png&amp;auto=webp&amp;s=00efc02808eb614032adfb04a9e72df769d08cc1\n\n[https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition](https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition)", "author_fullname": "t2_sxd6cnuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The State of Cloud Data Warehouses - 2023 Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"h9uh1s2dj86c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5302f49fa021390b5f4b94e6b5a00034a0806fa"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64fa7f821c9f94695adc2357243c1e72b8158f87"}, {"y": 161, "x": 320, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6648a02213aef0fe21833b9515edf452b4a8c9a8"}, {"y": 323, "x": 640, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fee02f395661e8a2c854e848b45df4378edae569"}, {"y": 485, "x": 960, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c1526461308512b4615a14d18b6375040c30395"}, {"y": 545, "x": 1080, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=614574dfd1538ff65c048b5650d468401f3f96be"}], "s": {"y": 616, "x": 1219, "u": "https://preview.redd.it/h9uh1s2dj86c1.png?width=1219&amp;format=png&amp;auto=webp&amp;s=00efc02808eb614032adfb04a9e72df769d08cc1"}, "id": "h9uh1s2dj86c1"}}, "name": "t3_18i5i5m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gEblvS_CjO-pSV2N3I12DjaRfrDFrrjQkAR0uOrbfpI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702548749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, post this here as well, because I spent some time gathering all the new releases of Snowflake (and competitors) in 2023 and mapping features against comparing platforms (and creating the pictures). Hopefully this usable for you as well&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h9uh1s2dj86c1.png?width=1219&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00efc02808eb614032adfb04a9e72df769d08cc1\"&gt;https://preview.redd.it/h9uh1s2dj86c1.png?width=1219&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00efc02808eb614032adfb04a9e72df769d08cc1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition\"&gt;https://www.recordlydata.com/blog/the-state-of-cloud-data-warehouses-2023-edition&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18i5i5m", "is_robot_indexable": true, "report_reasons": null, "author": "Recordly_MHeino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i5i5m/the_state_of_cloud_data_warehouses_2023_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i5i5m/the_state_of_cloud_data_warehouses_2023_edition/", "subreddit_subscribers": 146037, "created_utc": 1702548749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.bytebase.com/changelog/bytebase-2-12-0/](https://www.bytebase.com/changelog/bytebase-2-12-0/)", "author_fullname": "t2_gxesw7ji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bytebase v2.12.0 released, Database DevOps &amp; CI/CD tool for engineering teams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i5hdu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702548661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.bytebase.com/changelog/bytebase-2-12-0/\"&gt;https://www.bytebase.com/changelog/bytebase-2-12-0/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?auto=webp&amp;s=5485407d9406aef94dbc618fc9bd92277d79553c", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c9538a4c1085c8724b674264582b7ee71129e2b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa17aeed2d215a80b66b54b5df5e640d95141375", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0127f128a25d80af9f1433bdab92cae115c3c8c1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa7d61d00774d86ca0256598f6b54ccad18e04e1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0abdac1d7890e4a0f3902a6cb512c29d80241378", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/eRDIsmJXd2QMgaOjPsdH8_fTY86ICVixMhhsYkJftr8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bbc8c5b2db4e4f1b58c0654325ef6f40993c3c7", "width": 1080, "height": 607}], "variants": {}, "id": "p7SDgKrdHb2CMLnOC8NrxjbTLNsmuMWWU9jl0_vAhaI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18i5hdu", "is_robot_indexable": true, "report_reasons": null, "author": "Adela_freedom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i5hdu/bytebase_v2120_released_database_devops_cicd_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i5hdu/bytebase_v2120_released_database_devops_cicd_tool/", "subreddit_subscribers": 146037, "created_utc": 1702548661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Hello Data Engineers!\n\nI have a requirement to stream data from **Aurora Postgres RDS** to **Redshift** using AWS services. Currently, my approach involves creating a **DMS** (Database Migration Service) task to migrate data to Kinesis, and then pulling data from **Kinesis** to Redshift by creating an external schema. I'm wondering if there are alternative methods for achieving this.\n\nAdditionally, I need to perform a full load initially, followed by capturing and migrating only the changed data. Are there other recommended approaches for this scenario?\n\nI appreciate any insights or suggestions.", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion on Postgres RDS to Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18it9jp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702619891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Hello Data Engineers!&lt;/h1&gt;\n\n&lt;p&gt;I have a requirement to stream data from &lt;strong&gt;Aurora Postgres RDS&lt;/strong&gt; to &lt;strong&gt;Redshift&lt;/strong&gt; using AWS services. Currently, my approach involves creating a &lt;strong&gt;DMS&lt;/strong&gt; (Database Migration Service) task to migrate data to Kinesis, and then pulling data from &lt;strong&gt;Kinesis&lt;/strong&gt; to Redshift by creating an external schema. I&amp;#39;m wondering if there are alternative methods for achieving this.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I need to perform a full load initially, followed by capturing and migrating only the changed data. Are there other recommended approaches for this scenario?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any insights or suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18it9jp", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18it9jp/discussion_on_postgres_rds_to_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18it9jp/discussion_on_postgres_rds_to_redshift/", "subreddit_subscribers": 146037, "created_utc": 1702619891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a1tanc569", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask HN: Is there an open source database like this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ilc2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702594940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atwong.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atwong.medium.com/ask-hn-is-there-an-open-source-database-like-this-39ba6ceaec1e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ilc2m", "is_robot_indexable": true, "report_reasons": null, "author": "albertstarrocks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ilc2m/ask_hn_is_there_an_open_source_database_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atwong.medium.com/ask-hn-is-there-an-open-source-database-like-this-39ba6ceaec1e", "subreddit_subscribers": 146037, "created_utc": 1702594940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it's public sector to emphasise that both the people there and processes are quite old school. \n\nSo, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.\n\nThe person who developed this workflow 10 years or so ago, which hasn't changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!", "author_fullname": "t2_ppxsc3cy3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data warehousing for public sectory company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i76gi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702555414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was hoping to get some advice on the current methdology used for data warehousing in the public sector company I work for. I mention it&amp;#39;s public sector to emphasise that both the people there and processes are quite old school. &lt;/p&gt;\n\n&lt;p&gt;So, currently the data warehouse is an on-premise SQL Server solution. The enterprise application we use, among a few other things, gets warehoused nightly via PowerShell scripts that dynamically generate stored procedures from the databases and tables identified from the various SQL servers from the applications and such. These stored procedures then run on a nightly basis via an SSIS package that loops through these and creates batches of tables to warehouse for each CPU thread.&lt;/p&gt;\n\n&lt;p&gt;The person who developed this workflow 10 years or so ago, which hasn&amp;#39;t changed since, has now left. The data warehouse is primarily used for reporting in SSRS &amp;amp; Power BI and archiving of data for auditing purposes. Myself and IT are wanting to potentially update our data warehouse methodology to hopefully improve and simplify it. My questions are: is the current workflow an effective data warehousing solution? Would using a cloud platform such as Azure SQL simplify things for reasons such as not needing to worry about scalability? What solutions can be recommended to transform and improve this workflow to warehouse our systems that mainly use on-premise SQL servers? I should also mention the company is very Microsoft orientated. Many thanks in advance for assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18i76gi", "is_robot_indexable": true, "report_reasons": null, "author": "strictlydatabusiness", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i76gi/data_warehousing_for_public_sectory_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i76gi/data_warehousing_for_public_sectory_company/", "subreddit_subscribers": 146037, "created_utc": 1702555414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nI am currently working for a rather small consultancy company (5 FTEs + some working students). \n\nI am seeking some guidance/ feedback on some questions that are going through my mind and our current setup as a company. First, here's some information that should give you an overview about the situation.\n\n**Company background**\n\n* We're a consultancy that has developed a platform for our clients\n* Within this platform various tableau dashboard can be viewed, which we're embedding there.\n* Anything data engineering related we're doing involves some sort of ETL process and displaying the extracted data in a dashboard to our clients.\n\n**Team-Setup**\n\n* There are mostly non-IT people working here\n* There is one DevOps Guy that is mainly in charge for setting up infrastructure or, together with myself, fixing things if things go nuts. He's not working full time and is rather working on demand.\n* I am the only data engineer working here full time. When I started working here I mainly focused on building dashboards but as things are growing, I am focusing more on data engineering topics.\n* There is one other working student that has a data-background. He's mostly building dashboards or implementing some smaller ETL pipelines.\n* My Boss is thankfully supporting the topics I am working on, in case that matters for your feedback.\n\n**Tech-Stack**\n\n* On-Prem Postgres Cluster running on K8s as transactional database for our platform.\n* On-Prem Tableau Server for dashboards\n* Tableau Prep (UI-based ETL Tool)/Python for ETL processes\n* Prefect for Orchestration\n* Gitlab CI/CD\n\n**Current Challenges:**\n\n* Database / Data Warehouse: Currently everything is stored in the Postgres Database. So all the transactional data goes into the postgres (which is fine) but also the dashboard related data, which I know should not be stored there. As the number of projects we're doing, so is the database and everything starts to get more and more messy. I feel like I can still oversee everything but on the other hand this also feels like a ticking time bomb. Ideally I am thinking about setting up a data warehouse which we'll then use for all analytic related topics such as our dashboards. At the same time I feel like this is such a huge task that I am also underestimating. \n* We're not following any \"fixed\" guideline when it comes to how we're modeling/storing our dashboard related data in the postgres. I am currently making myself familiar with dimensional modeling (kimball) as I feel like we need a way to approach this, sometimes messy, situation on how we're storing our data.\n* Another thing on my list is DBT. We're currently using materialized views for some of our dashboards, mainly to shift aggregation away from tableau into the database. While this works fine, I feel like using DBT could improve our data quality and eg improve re usability among other things.\n\n**My questions to you:** \n\n* Has someone worked in a similar situation? How did you decide on \"what to do next\"? Quitting is not an option as I my pay is quite good for the YOE I have currently.\n* What topic would you start with? Is there any specific \"order\" that makes most sense and why?\n* Any other thoughts/feedback are of course appreciated.\n\n  \nTLDR; We're a small company and I am not sure what topic to tackle first to improve our data architecture.   \n\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_u88hoerb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance/ Feedback on data architecture and next steps planned", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18i5ovj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702549525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I am currently working for a rather small consultancy company (5 FTEs + some working students). &lt;/p&gt;\n\n&lt;p&gt;I am seeking some guidance/ feedback on some questions that are going through my mind and our current setup as a company. First, here&amp;#39;s some information that should give you an overview about the situation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Company background&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We&amp;#39;re a consultancy that has developed a platform for our clients&lt;/li&gt;\n&lt;li&gt;Within this platform various tableau dashboard can be viewed, which we&amp;#39;re embedding there.&lt;/li&gt;\n&lt;li&gt;Anything data engineering related we&amp;#39;re doing involves some sort of ETL process and displaying the extracted data in a dashboard to our clients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Team-Setup&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;There are mostly non-IT people working here&lt;/li&gt;\n&lt;li&gt;There is one DevOps Guy that is mainly in charge for setting up infrastructure or, together with myself, fixing things if things go nuts. He&amp;#39;s not working full time and is rather working on demand.&lt;/li&gt;\n&lt;li&gt;I am the only data engineer working here full time. When I started working here I mainly focused on building dashboards but as things are growing, I am focusing more on data engineering topics.&lt;/li&gt;\n&lt;li&gt;There is one other working student that has a data-background. He&amp;#39;s mostly building dashboards or implementing some smaller ETL pipelines.&lt;/li&gt;\n&lt;li&gt;My Boss is thankfully supporting the topics I am working on, in case that matters for your feedback.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Tech-Stack&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On-Prem Postgres Cluster running on K8s as transactional database for our platform.&lt;/li&gt;\n&lt;li&gt;On-Prem Tableau Server for dashboards&lt;/li&gt;\n&lt;li&gt;Tableau Prep (UI-based ETL Tool)/Python for ETL processes&lt;/li&gt;\n&lt;li&gt;Prefect for Orchestration&lt;/li&gt;\n&lt;li&gt;Gitlab CI/CD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Challenges:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Database / Data Warehouse: Currently everything is stored in the Postgres Database. So all the transactional data goes into the postgres (which is fine) but also the dashboard related data, which I know should not be stored there. As the number of projects we&amp;#39;re doing, so is the database and everything starts to get more and more messy. I feel like I can still oversee everything but on the other hand this also feels like a ticking time bomb. Ideally I am thinking about setting up a data warehouse which we&amp;#39;ll then use for all analytic related topics such as our dashboards. At the same time I feel like this is such a huge task that I am also underestimating. &lt;/li&gt;\n&lt;li&gt;We&amp;#39;re not following any &amp;quot;fixed&amp;quot; guideline when it comes to how we&amp;#39;re modeling/storing our dashboard related data in the postgres. I am currently making myself familiar with dimensional modeling (kimball) as I feel like we need a way to approach this, sometimes messy, situation on how we&amp;#39;re storing our data.&lt;/li&gt;\n&lt;li&gt;Another thing on my list is DBT. We&amp;#39;re currently using materialized views for some of our dashboards, mainly to shift aggregation away from tableau into the database. While this works fine, I feel like using DBT could improve our data quality and eg improve re usability among other things.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My questions to you:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Has someone worked in a similar situation? How did you decide on &amp;quot;what to do next&amp;quot;? Quitting is not an option as I my pay is quite good for the YOE I have currently.&lt;/li&gt;\n&lt;li&gt;What topic would you start with? Is there any specific &amp;quot;order&amp;quot; that makes most sense and why?&lt;/li&gt;\n&lt;li&gt;Any other thoughts/feedback are of course appreciated.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;TLDR; We&amp;#39;re a small company and I am not sure what topic to tackle first to improve our data architecture.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18i5ovj", "is_robot_indexable": true, "report_reasons": null, "author": "heggbert", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18i5ovj/guidance_feedback_on_data_architecture_and_next/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18i5ovj/guidance_feedback_on_data_architecture_and_next/", "subreddit_subscribers": 146037, "created_utc": 1702549525.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}