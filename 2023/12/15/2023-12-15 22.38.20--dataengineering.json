{"kind": "Listing", "data": {"after": "t3_18iy492", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A collection of videos shared by Netflix from their [Data Engineering Summit](https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102)\n\n* [The Netflix Data Engineering Stack](https://youtu.be/QxaOlmv79ls)\n* [Data Processing Patterns](https://www.youtube.com/watch?v=vuyjK2TFZNk&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=3)\n* [Streaming SQL on Data Mesh using Apache Flink](https://www.youtube.com/watch?v=TwcWvwU7B64&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=4)\n* [Building Reliable Data Pipelines](https://www.youtube.com/watch?v=uWmJxbhI304&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=5)\n* [Knowledge Management \u2014 Leveraging Institutional Data](https://www.youtube.com/watch?v=F4N8AmScZ-w&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=6)\n* [Psyberg, An Incremental ETL Framework Using Iceberg](https://www.youtube.com/watch?v=jRckeOedtx0&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=8)\n* [Start/Stop/Continue for optimizing complex ETL jobs](https://www.youtube.com/watch?v=Dr8LMn-nJGc&amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;index=9)\n* [Media Data for ML Studio Creative Production](https://youtu.be/1gGi3NBZk7M)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Netflix does Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ix6hd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 278, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 278, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702636326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A collection of videos shared by Netflix from their &lt;a href=\"https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102\"&gt;Data Engineering Summit&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://youtu.be/QxaOlmv79ls\"&gt;The Netflix Data Engineering Stack&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=vuyjK2TFZNk&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=3\"&gt;Data Processing Patterns&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=TwcWvwU7B64&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=4\"&gt;Streaming SQL on Data Mesh using Apache Flink&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=uWmJxbhI304&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=5\"&gt;Building Reliable Data Pipelines&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=F4N8AmScZ-w&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=6\"&gt;Knowledge Management \u2014 Leveraging Institutional Data&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=jRckeOedtx0&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=8\"&gt;Psyberg, An Incremental ETL Framework Using Iceberg&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=Dr8LMn-nJGc&amp;amp;list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&amp;amp;index=9\"&gt;Start/Stop/Continue for optimizing complex ETL jobs&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://youtu.be/1gGi3NBZk7M\"&gt;Media Data for ML Studio Creative Production&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?auto=webp&amp;s=5c559c3b17160025d56491777d567987248931ef", "width": 1111, "height": 571}, "resolutions": [{"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da24254e4bb038691cfdd4c156a5d5244c08c4bf", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=052f735a1ffa64a0e1d6d51c639439521a6f2004", "width": 216, "height": 111}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d0fc2cebd67c1385fae0b1324dc39348b3217f8", "width": 320, "height": 164}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc32a3e086f0217c8aa2eab757773f562dea42d3", "width": 640, "height": 328}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=756a90ecc71cbca89abb9212b906435a9bd267bd", "width": 960, "height": 493}, {"url": "https://external-preview.redd.it/W5u6SrlwaxjlbNE6tqal8ItkeMrk5nRA-wIEcfOuEwY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b00bb639fe14e1acfb87b0939a9785a7a7e3c46", "width": 1080, "height": 555}], "variants": {}, "id": "TMo7-NZlAEt-Oth2MSgWHW_-6WCPBJ0hDscjjgNPtxU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ix6hd", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ix6hd/how_netflix_does_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ix6hd/how_netflix_does_data_engineering/", "subreddit_subscribers": 146168, "created_utc": 1702636326.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_j7j4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"We have so many challenging projects!\" ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_18j0ygk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qgx75hdxug6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 714, "scrubber_media_url": "https://v.redd.it/qgx75hdxug6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/qgx75hdxug6c1/DASHPlaylist.mpd?a=1705271900%2CYTQ0YTJmOWY1YmU3NDZjYzNlYjQ5YmQyMTBhZTdlNTk2YjNiYzZmYmJhMWY0NjYxMjQxYzdkZmUwMjE0NzM5Yw%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/qgx75hdxug6c1/HLSPlaylist.m3u8?a=1705271900%2CY2U4NzQ4MTM1OGUxOTViYjUxMjAwNmI3YWYxZjgxNDBmZTg5ZjE1NDdlNjkyYzdmNTg3NTYzNGQ4ZGM2MzVhYw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=227036178b5d344290414f523618603d36a0186e", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702649432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/qgx75hdxug6c1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?format=pjpg&amp;auto=webp&amp;s=69523afcc11a24b54037c8b3cb719b6ec403c640", "width": 983, "height": 1764}, "resolutions": [{"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2028b47397af20b16906282fe6cc7cfc2fed329f", "width": 108, "height": 193}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06eca739f46c7645390bc07092b7143ccada48ee", "width": 216, "height": 387}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b9500c3eb62e1d6c22ea1e7a14f358c8d892b60e", "width": 320, "height": 574}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0aa541b0d69aff973a1d9be1d34c4d7bd12bd9fb", "width": 640, "height": 1148}, {"url": "https://external-preview.redd.it/OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0478df9f02dcc92699a762226bf241906adaf69", "width": 960, "height": 1722}], "variants": {}, "id": "OGtlbWxzNnh1ZzZjMcBfrwjWG25NCSrUTiYrjVq63eOOr7vAciQk4a_yV0yQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18j0ygk", "is_robot_indexable": true, "report_reasons": null, "author": "Marawishka", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j0ygk/we_have_so_many_challenging_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/qgx75hdxug6c1", "subreddit_subscribers": 146168, "created_utc": 1702649432.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/qgx75hdxug6c1/DASH_720.mp4?source=fallback", "has_audio": true, "height": 1280, "width": 714, "scrubber_media_url": "https://v.redd.it/qgx75hdxug6c1/DASH_96.mp4", "dash_url": "https://v.redd.it/qgx75hdxug6c1/DASHPlaylist.mpd?a=1705271900%2CYTQ0YTJmOWY1YmU3NDZjYzNlYjQ5YmQyMTBhZTdlNTk2YjNiYzZmYmJhMWY0NjYxMjQxYzdkZmUwMjE0NzM5Yw%3D%3D&amp;v=1&amp;f=sd", "duration": 6, "hls_url": "https://v.redd.it/qgx75hdxug6c1/HLSPlaylist.m3u8?a=1705271900%2CY2U4NzQ4MTM1OGUxOTViYjUxMjAwNmI3YWYxZjgxNDBmZTg5ZjE1NDdlNjkyYzdmNTg3NTYzNGQ4ZGM2MzVhYw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "People who\u2019ve been in this game since when the hype was really getting started in the beginning of the last decade, what was it like working in the first companies that tried to build big data pipelines? What was the stack you worked on? I know that Hadoop and NoSQL were very trendy at the time, but can you share some more typical technologies that you ran into a lot? Are some of them supplanted by now or are they mostly still going strong?", "author_fullname": "t2_1xobc52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What was data engineering like circa 2011-2013?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ilbto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702594923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who\u2019ve been in this game since when the hype was really getting started in the beginning of the last decade, what was it like working in the first companies that tried to build big data pipelines? What was the stack you worked on? I know that Hadoop and NoSQL were very trendy at the time, but can you share some more typical technologies that you ran into a lot? Are some of them supplanted by now or are they mostly still going strong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ilbto", "is_robot_indexable": true, "report_reasons": null, "author": "pimmen89", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ilbto/what_was_data_engineering_like_circa_20112013/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ilbto/what_was_data_engineering_like_circa_20112013/", "subreddit_subscribers": 146168, "created_utc": 1702594923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a proprietary Excel .VBA that uses a highly complex mathematical function using 6 values to generate a number.  E.g.,:\n\n=PropietaryFormula(A1,B1,C1,D1,E1)*F1\n\nI don't have access to the VBA source code and a can't reverse engineer the math function.  I want to get away from using Excel and be able to fetch the value with an HTTP call (Azure function) by sending the 6 inputs in the HTTP request.   To generate all possible values using these inputs, the end result is around 600 billion unique combinations.\n\nI'm able to use Power Automate Desktop to open Excel, populate the inputs, and generate the needed value using the function.  I think I can do this for about 100,000 rows for each Excel file to stay within the memory limits on my desktop.  From there is where I'm wondering what would be the easiest way to get this into a data warehouse.  I'm thinking I could upload these 100s of thousands of Excel files to Azure ADL2 storage and use Synapse Analytics or Databricks to push them into a database, but I'm hoping someone out there may have a much better, faster, and cheaper idea.\n\nThanks!\n\n** UPDATE:  After some further analysis, I think I can get the number of rows required down to 6 billion, which may make things more palatable.  I appreciate all of the comments so far!", "author_fullname": "t2_9lsmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you populate 600 billion rows in a structured database where the values are generated from Excel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18il8i8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702649061.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702594692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a proprietary Excel .VBA that uses a highly complex mathematical function using 6 values to generate a number.  E.g.,:&lt;/p&gt;\n\n&lt;p&gt;=PropietaryFormula(A1,B1,C1,D1,E1)*F1&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have access to the VBA source code and a can&amp;#39;t reverse engineer the math function.  I want to get away from using Excel and be able to fetch the value with an HTTP call (Azure function) by sending the 6 inputs in the HTTP request.   To generate all possible values using these inputs, the end result is around 600 billion unique combinations.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m able to use Power Automate Desktop to open Excel, populate the inputs, and generate the needed value using the function.  I think I can do this for about 100,000 rows for each Excel file to stay within the memory limits on my desktop.  From there is where I&amp;#39;m wondering what would be the easiest way to get this into a data warehouse.  I&amp;#39;m thinking I could upload these 100s of thousands of Excel files to Azure ADL2 storage and use Synapse Analytics or Databricks to push them into a database, but I&amp;#39;m hoping someone out there may have a much better, faster, and cheaper idea.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;** UPDATE:  After some further analysis, I think I can get the number of rows required down to 6 billion, which may make things more palatable.  I appreciate all of the comments so far!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18il8i8", "is_robot_indexable": true, "report_reasons": null, "author": "dantasticdotorg", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18il8i8/how_would_you_populate_600_billion_rows_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18il8i8/how_would_you_populate_600_billion_rows_in_a/", "subreddit_subscribers": 146168, "created_utc": 1702594692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It was on this day that Data Engineering was born", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_18j25eu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3IwlOr3WzZlLtJ6JCuLBD2j46D0QTOyxN8FxgikyptM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702652766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ad8gxeos4h6c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?auto=webp&amp;s=25faaf445c6d3315d4710fee34a974144be3b0b2", "width": 1792, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f3d28a2ca30fe5246337857d3e6ba043cb2b831", "width": 108, "height": 61}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=459573272534a9db2d52e6eb1cb2abf827ee3cee", "width": 216, "height": 123}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571d0c21cb3a0ac43702e07e8f079c333f9ee893", "width": 320, "height": 182}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=593ff544d465402452d7309ad4cae72d0cc90dd7", "width": 640, "height": 365}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7529979eb3706e099419035507d99d13912efe9", "width": 960, "height": 548}, {"url": "https://preview.redd.it/ad8gxeos4h6c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67155114bf2f671a7a7b6e45437bec502a16f2cf", "width": 1080, "height": 617}], "variants": {}, "id": "AEf51p2KV45AN5i0u4jdzl_w_LWvhR69DlaAlf1CvCc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18j25eu", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j25eu/it_was_on_this_day_that_data_engineering_was_born/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ad8gxeos4h6c1.png", "subreddit_subscribers": 146168, "created_utc": 1702652766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, looking for some advice on my situation. \n\nI have been at my company for 1.5 yrs, having joined in a rotational program where there were 2 rotations, 6 months long each and this is my 3rd team now, doing data engineering(?) work for the first time. In one of my roles during the rotational program, I did primarily data analyst stuff but it was a bs role because I had only 1 project my entire time on the team and that was to create a basic tableau and power bi dashboard. That\u2019s literally it. \n\nI am not entirely sure if I am even a data engineer proper on this team now, because I am mostly doing analyst work and basic level sql queries. I haven\u2019t touched Python or done any ETL work. \n\nRecently, I was having a check in with my manager and he was expressing discontent about how I\u2019m not able to do my work independently yet and haven\u2019t been staying up to date with all updates coming through via email and teams chats. I completely get where he is coming from and I too am frustrated that I can\u2019t really work independently and he gave me some tips but they weren\u2019t really helpful, just the same old \u201clook at the documentation\u201d but the documentation isn\u2019t that much clearer either.\n\nHowever, on the other side, I feel like there wasn\u2019t really any chance to learn when I joined this team and was kind of thrown into this work without any relevant experience and structured trainings. His excuse for this is \u201ceveryone is learning\u201d and that this is new territory for everyone, but if that is the case, I feel like me not being at the same level as my colleagues in working independently should be okay because I\u2019ve never done this type of work before and it isn\u2019t like I\u2019m not trying, because I really am. \n\nIn addition, it can be hard to keep track of all the messages, emails, etc because there is so much back and forth and I get pulled into long meetings, leaving little time for me to do my work. I am still learning how to task switch especially because my previous teams were never this crazy busy. He also said in my check in that things will only get more busy from here onward into the new year. I am really feeling down on myself because I thought I was doing well despite the circumstances, but this was a ding on my confidence in myself. \n\nI am not sure if I can stay on this team long term if this is how things will be with the project ramping up in 2024. \n\nHas anyone else had this experience? Any advice? Should I just find a new role? I don\u2019t want to quit because things are getting hard but I also feel like this isn\u2019t an environment for learning and the longer I stay on this team doing lower level analyst work, the more I am putting myself behind in the data field and doing actual relevant work.", "author_fullname": "t2_tna1k085", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feel like I still don\u2019t know what I\u2019m doing 6 months into my role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ioz9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, looking for some advice on my situation. &lt;/p&gt;\n\n&lt;p&gt;I have been at my company for 1.5 yrs, having joined in a rotational program where there were 2 rotations, 6 months long each and this is my 3rd team now, doing data engineering(?) work for the first time. In one of my roles during the rotational program, I did primarily data analyst stuff but it was a bs role because I had only 1 project my entire time on the team and that was to create a basic tableau and power bi dashboard. That\u2019s literally it. &lt;/p&gt;\n\n&lt;p&gt;I am not entirely sure if I am even a data engineer proper on this team now, because I am mostly doing analyst work and basic level sql queries. I haven\u2019t touched Python or done any ETL work. &lt;/p&gt;\n\n&lt;p&gt;Recently, I was having a check in with my manager and he was expressing discontent about how I\u2019m not able to do my work independently yet and haven\u2019t been staying up to date with all updates coming through via email and teams chats. I completely get where he is coming from and I too am frustrated that I can\u2019t really work independently and he gave me some tips but they weren\u2019t really helpful, just the same old \u201clook at the documentation\u201d but the documentation isn\u2019t that much clearer either.&lt;/p&gt;\n\n&lt;p&gt;However, on the other side, I feel like there wasn\u2019t really any chance to learn when I joined this team and was kind of thrown into this work without any relevant experience and structured trainings. His excuse for this is \u201ceveryone is learning\u201d and that this is new territory for everyone, but if that is the case, I feel like me not being at the same level as my colleagues in working independently should be okay because I\u2019ve never done this type of work before and it isn\u2019t like I\u2019m not trying, because I really am. &lt;/p&gt;\n\n&lt;p&gt;In addition, it can be hard to keep track of all the messages, emails, etc because there is so much back and forth and I get pulled into long meetings, leaving little time for me to do my work. I am still learning how to task switch especially because my previous teams were never this crazy busy. He also said in my check in that things will only get more busy from here onward into the new year. I am really feeling down on myself because I thought I was doing well despite the circumstances, but this was a ding on my confidence in myself. &lt;/p&gt;\n\n&lt;p&gt;I am not sure if I can stay on this team long term if this is how things will be with the project ramping up in 2024. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else had this experience? Any advice? Should I just find a new role? I don\u2019t want to quit because things are getting hard but I also feel like this isn\u2019t an environment for learning and the longer I stay on this team doing lower level analyst work, the more I am putting myself behind in the data field and doing actual relevant work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ioz9p", "is_robot_indexable": true, "report_reasons": null, "author": "miserablywinning", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ioz9p/feel_like_i_still_dont_know_what_im_doing_6/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ioz9p/feel_like_i_still_dont_know_what_im_doing_6/", "subreddit_subscribers": 146168, "created_utc": 1702605589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019ve been trying to solve for this problem for a couple of years now. Trying to make a generic platform/product for data quality that would work for multiple data personas within the company. We knew this was going to be a hard one to solve.. and we\u2019re yet to hit that breakthrough. Curious to know what other data folks are doing and how they are solving for data quality.", "author_fullname": "t2_h5ll08of", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iqbbp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702609777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019ve been trying to solve for this problem for a couple of years now. Trying to make a generic platform/product for data quality that would work for multiple data personas within the company. We knew this was going to be a hard one to solve.. and we\u2019re yet to hit that breakthrough. Curious to know what other data folks are doing and how they are solving for data quality.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18iqbbp", "is_robot_indexable": true, "report_reasons": null, "author": "Lucky-Front7675", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18iqbbp/data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18iqbbp/data_quality/", "subreddit_subscribers": 146168, "created_utc": 1702609777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an owner of a small retail business who is teaching myself DE/BI concepts via Coursera, ChatGPT, YouTube, podcasts, etc. Our business sells fairly niche goods online and also has a small consignment-based popup inside another retail business. Both my business and our pop-up host each use our own Shopify accounts for sales/inventory/product tracking. \n\nMy business goal is to have a single source of truth for product sales, by aggregating both Shopify accounts sales and product records into a database of my own. Currently, doing any sort of meaningful analysis involves manually exporting a csv from each store and joining the data in spreadsheets (or SQL if I\u2019m feeling saucy and the data is bigger than sheets can take.)\n\nTrying to solve for this has led me to DE-related online courses, which are really interesting and getting me feeling closer to a solution. I had an initial, fleeting success making a PostgreSQL database through [Render.com](https://Render.com) and setting up pipelines in [Make.com](https://Make.com) to catch incoming sales from both Shopify accounts, and load them into a \u2018master\\_sales\u2019 table, that I could then query to answer questions about our total product sales over varying time periods. This ended up becoming a mess, though, after a couple of failed pipeline executions, and numerous missing records and duplicate records. So, I went back to the drawing board. Currently, I've now got a bash script that can query Shopify REST API. I've also got some good grounding in Pandas for data transformation. Overall, I want to get to the point where I have some checks in place. \n\nI\u2019d love to hear from more experienced people in this field about what sort of approach you might apply to this problem. None of the data is that big (yet, right now accumulating about +100MB/month), nor does it need to be updated more frequently than daily (at best). I\u2019m looking at options such as setting up a dedicated linux server at home running cron jobs, or digging into AWS/Azure (because cloud computing is super interesting). How would you approach this business problem?\n\nSince starting this business, I\u2019ve definitely unlocked a part of myself that is continually excited about solving the computer-technical/data/systems design opportunities that might improve my day-to-day experience operating things. I've always been into computers as a hobby, but never for work. My work experience is in bartending and retail.\n\nAlso, I have another question: My small business is very small, and can't pay me that well. I really love all these things I\u2019m learning and data-oriented jobs pay alot better than I'm currently doing. Given that I don\u2019t have any tech industry experience, how might one approach a career change, from service industry and small retail business ownership, into Data Engineering? I\u2019m really happy that I have potential projects on my plate to help me study, as well as add potential fodder to a portfolio. I feel like if a career change is in the books, I should maybe aim for a Junior IT/Data Analytics position as an entry point and try to advance from there? Does anyone have a similar transition experience? ", "author_fullname": "t2_miufv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small business owner, new to, and interested in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18itcsw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702620192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an owner of a small retail business who is teaching myself DE/BI concepts via Coursera, ChatGPT, YouTube, podcasts, etc. Our business sells fairly niche goods online and also has a small consignment-based popup inside another retail business. Both my business and our pop-up host each use our own Shopify accounts for sales/inventory/product tracking. &lt;/p&gt;\n\n&lt;p&gt;My business goal is to have a single source of truth for product sales, by aggregating both Shopify accounts sales and product records into a database of my own. Currently, doing any sort of meaningful analysis involves manually exporting a csv from each store and joining the data in spreadsheets (or SQL if I\u2019m feeling saucy and the data is bigger than sheets can take.)&lt;/p&gt;\n\n&lt;p&gt;Trying to solve for this has led me to DE-related online courses, which are really interesting and getting me feeling closer to a solution. I had an initial, fleeting success making a PostgreSQL database through &lt;a href=\"https://Render.com\"&gt;Render.com&lt;/a&gt; and setting up pipelines in &lt;a href=\"https://Make.com\"&gt;Make.com&lt;/a&gt; to catch incoming sales from both Shopify accounts, and load them into a \u2018master_sales\u2019 table, that I could then query to answer questions about our total product sales over varying time periods. This ended up becoming a mess, though, after a couple of failed pipeline executions, and numerous missing records and duplicate records. So, I went back to the drawing board. Currently, I&amp;#39;ve now got a bash script that can query Shopify REST API. I&amp;#39;ve also got some good grounding in Pandas for data transformation. Overall, I want to get to the point where I have some checks in place. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d love to hear from more experienced people in this field about what sort of approach you might apply to this problem. None of the data is that big (yet, right now accumulating about +100MB/month), nor does it need to be updated more frequently than daily (at best). I\u2019m looking at options such as setting up a dedicated linux server at home running cron jobs, or digging into AWS/Azure (because cloud computing is super interesting). How would you approach this business problem?&lt;/p&gt;\n\n&lt;p&gt;Since starting this business, I\u2019ve definitely unlocked a part of myself that is continually excited about solving the computer-technical/data/systems design opportunities that might improve my day-to-day experience operating things. I&amp;#39;ve always been into computers as a hobby, but never for work. My work experience is in bartending and retail.&lt;/p&gt;\n\n&lt;p&gt;Also, I have another question: My small business is very small, and can&amp;#39;t pay me that well. I really love all these things I\u2019m learning and data-oriented jobs pay alot better than I&amp;#39;m currently doing. Given that I don\u2019t have any tech industry experience, how might one approach a career change, from service industry and small retail business ownership, into Data Engineering? I\u2019m really happy that I have potential projects on my plate to help me study, as well as add potential fodder to a portfolio. I feel like if a career change is in the books, I should maybe aim for a Junior IT/Data Analytics position as an entry point and try to advance from there? Does anyone have a similar transition experience? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?auto=webp&amp;s=afa45699d6fb3f760fb20e2d9d6f44228063d633", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0488541fa0dd55375440acdececcdee031a940d5", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba3bfd8369fa2c98c582333a477292123d7ffdb8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad745860ee3dc44a3840afefd8869e8efd01055c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fe4ec78918e70f6bbec642e50841b7e4c7f4ad1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16765ff17f5a2f4e8135f14db9cb079337aa67fb", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/cDDyYwDVuCLRWiYzJf9UerbDnkKv5uUTV46K5o5NMjM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0991e51843b0a32b2501554aa0168f72ea7e8116", "width": 1080, "height": 607}], "variants": {}, "id": "w1lzwcy17wjJvdZBaFtVGF670_Y4Sib7sWLEw0rNHKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18itcsw", "is_robot_indexable": true, "report_reasons": null, "author": "dondelamort", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18itcsw/small_business_owner_new_to_and_interested_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18itcsw/small_business_owner_new_to_and_interested_in/", "subreddit_subscribers": 146168, "created_utc": 1702620192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everybody,\n\nThis is a bit of a self-promotion, and I don't usually do that (I have never done it here), but I figured many of you may find it helpful.\n\nFor context, I am a Head of data (&amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.\n\nWhat I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don't usually discuss.\n\nPlease go wild with any questions you may have.\n\nhttps://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcome=true", "author_fullname": "t2_1c6f704", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I interview data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ja9hq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;This is a bit of a self-promotion, and I don&amp;#39;t usually do that (I have never done it here), but I figured many of you may find it helpful.&lt;/p&gt;\n\n&lt;p&gt;For context, I am a Head of data (&amp;amp; analytics) engineering at a Fintech company and have interviewed hundreds of candidates.&lt;/p&gt;\n\n&lt;p&gt;What I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don&amp;#39;t usually discuss.&lt;/p&gt;\n\n&lt;p&gt;Please go wild with any questions you may have.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true\"&gt;https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcome=true&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?auto=webp&amp;s=d62d1df611eb5fd3b6c201a2ca1198764c34576e", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d233877d035936e4cdd051c8308d2ed8687ea4b4", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=658a8bd49e51df3ec03cb2ee2e11353625e67188", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6fa1c75aa9279837a1bc754209d43a4ecc494bf", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e81e57fe8694136946f0e0b90cd174e447289020", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c2d765d29649bb8542956c04e415c72a8794e9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/6iVs_YuK7Bti4JZ6J7Q-nFZPVA4rg-HVAQNazDj9V3E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b00fa2cb6c57cffff61869c14eb6b4e6cbb86d4", "width": 1080, "height": 720}], "variants": {}, "id": "5zTWOZ4g6_ZEJv5W5FoiMbIfg0hblAMLhiLqgJKuoFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ja9hq", "is_robot_indexable": true, "report_reasons": null, "author": "ivanovyordan", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/", "subreddit_subscribers": 146168, "created_utc": 1702674334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I\u2019m in over my head at a startup and have to lead the construction of analytics infrastructure that will support customer-facing embedded dashboards along with downloadable reports. \n\nThe volume of data we\u2019re dealing with is nothing crazy, probably gonna be around 20TB. And we aim to support around 5000+ users (though starting much smaller) who will regularly view a few embedded dashboards and also download reports somewhat regularly. The dashboards will need some basic drill-down and date range picking. The data is standard aggregated data that is well-suited for a DWH.\n\nOne of my main questions though is if there are special considerations when the use case is for customer-facing analytics? Will this many users interacting with Snowflake kill us with costs? We are thinking of powering the dashboards/reports with QuickSight or similar. And from the little I know so far, there is some caching that will help reduce the query load on Snowflake. Will that be sufficient?\n\nIs there a better store for aggregated relational data that\u2019s used for customer-facing analytics? Will we survive with Snowflake?\n\nHow do we handle Snowflake users for this? Do we have to provision a user for each customer? I know nothing about it.\n\nWe\u2019re entirely in AWS. I\u2019m thinking we will use DMS CDC to S3 and then 24 hour batch copy the data to Snowflake. Don\u2019t know the best tool for scheduled batch copy, any recommendations? All I know is maybe I could use Airflow or just a simple cron job. From there I will use DBT to populate a handful of star schemas that I would connect to QuickSight or similar. \n\nI would honestly be very grateful for any wisdom on any part of this setup because I\u2019m largely on my own right now to figure this out and get it built.", "author_fullname": "t2_ijoifzps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake for customer-facing analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18is4a1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702616862.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702615758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I\u2019m in over my head at a startup and have to lead the construction of analytics infrastructure that will support customer-facing embedded dashboards along with downloadable reports. &lt;/p&gt;\n\n&lt;p&gt;The volume of data we\u2019re dealing with is nothing crazy, probably gonna be around 20TB. And we aim to support around 5000+ users (though starting much smaller) who will regularly view a few embedded dashboards and also download reports somewhat regularly. The dashboards will need some basic drill-down and date range picking. The data is standard aggregated data that is well-suited for a DWH.&lt;/p&gt;\n\n&lt;p&gt;One of my main questions though is if there are special considerations when the use case is for customer-facing analytics? Will this many users interacting with Snowflake kill us with costs? We are thinking of powering the dashboards/reports with QuickSight or similar. And from the little I know so far, there is some caching that will help reduce the query load on Snowflake. Will that be sufficient?&lt;/p&gt;\n\n&lt;p&gt;Is there a better store for aggregated relational data that\u2019s used for customer-facing analytics? Will we survive with Snowflake?&lt;/p&gt;\n\n&lt;p&gt;How do we handle Snowflake users for this? Do we have to provision a user for each customer? I know nothing about it.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re entirely in AWS. I\u2019m thinking we will use DMS CDC to S3 and then 24 hour batch copy the data to Snowflake. Don\u2019t know the best tool for scheduled batch copy, any recommendations? All I know is maybe I could use Airflow or just a simple cron job. From there I will use DBT to populate a handful of star schemas that I would connect to QuickSight or similar. &lt;/p&gt;\n\n&lt;p&gt;I would honestly be very grateful for any wisdom on any part of this setup because I\u2019m largely on my own right now to figure this out and get it built.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18is4a1", "is_robot_indexable": true, "report_reasons": null, "author": "DataDude999", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18is4a1/snowflake_for_customerfacing_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18is4a1/snowflake_for_customerfacing_analytics/", "subreddit_subscribers": 146168, "created_utc": 1702615758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Hello Data Engineers!\n\nI have a requirement to stream data from **Aurora Postgres RDS** to **Redshift** using AWS services. Currently, my approach involves creating a **DMS** (Database Migration Service) task to migrate data to Kinesis, and then pulling data from **Kinesis** to Redshift by creating an external schema. I'm wondering if there are alternative methods for achieving this.\n\nAdditionally, I need to perform a full load initially, followed by capturing and migrating only the changed data. Are there other recommended approaches for this scenario?\n\nI appreciate any insights or suggestions.", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion on Postgres RDS to Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18it9jp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702619891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Hello Data Engineers!&lt;/h1&gt;\n\n&lt;p&gt;I have a requirement to stream data from &lt;strong&gt;Aurora Postgres RDS&lt;/strong&gt; to &lt;strong&gt;Redshift&lt;/strong&gt; using AWS services. Currently, my approach involves creating a &lt;strong&gt;DMS&lt;/strong&gt; (Database Migration Service) task to migrate data to Kinesis, and then pulling data from &lt;strong&gt;Kinesis&lt;/strong&gt; to Redshift by creating an external schema. I&amp;#39;m wondering if there are alternative methods for achieving this.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I need to perform a full load initially, followed by capturing and migrating only the changed data. Are there other recommended approaches for this scenario?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any insights or suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18it9jp", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18it9jp/discussion_on_postgres_rds_to_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18it9jp/discussion_on_postgres_rds_to_redshift/", "subreddit_subscribers": 146168, "created_utc": 1702619891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI'm responsible for building out a financial reporting system at my company and I could use some advice on my approach.\n\n**A little background:** We offer coaching courses that will be split amongst various systems, but all purchased through deals with customers (companies), which are converted to credits and consumed through an atomic product model. We are in the process of transitioning to a credit system, so we will need to convert our existing deals into credits for the future system (deferred credits).\n\nEssentially I need to tie in various data sources to build a table to track at a deal level the amount of credits that have been assigned, unassigned and consumed. One of the challenges is that credit values can vary by deal and therefore need to be tracked at a deal level. Also, they need to burn down to 0 by oldest deal, first in first out. This might add a bit of annoyance dealing with remainders.\n\nHeres an example of roughly what im envisioning:\n\n|Company ID|deal id|trans dt|scheduled dt|completed dt|credit category|$ per credit|\\# of Credits|product|Product Cost|Credits Remaining|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|13|2|2/23/24|||unassigned|20|1000|A|300|1000|\n|13|2|2/23/24|3/01/24||assigned|20|1000|B|400|1000|\n|13|1|1/15/24|1/23/24|1/23/24|consumed|17.50|1500|B|400|1100|\n|13|1|1/15/24|||unassigned|17.50|1500|A|300|1100|\n|13|999|1/1/24|||deferred|23|3000|||0|\n\n&amp;#x200B;\n\nIts unclear to me what the best approach is for working through the many transformations involved here. I could build out a bunch of additional fields and apply calculations to the table at various stages of my pipeline with SQL, but im wondering if theres a more elegant solution through breaking it out into various tables and processes, using python or recursive functions etc.\n\nOne last caveat is that ideally this system will be able to track rate of spend of customers, which Id imagine would most simply be tracked through snapshot tables.\n\nSorry this was a lot, but any help is greatly appreciated!", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial Reporting Problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ip1ya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702612993.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702605806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m responsible for building out a financial reporting system at my company and I could use some advice on my approach.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A little background:&lt;/strong&gt; We offer coaching courses that will be split amongst various systems, but all purchased through deals with customers (companies), which are converted to credits and consumed through an atomic product model. We are in the process of transitioning to a credit system, so we will need to convert our existing deals into credits for the future system (deferred credits).&lt;/p&gt;\n\n&lt;p&gt;Essentially I need to tie in various data sources to build a table to track at a deal level the amount of credits that have been assigned, unassigned and consumed. One of the challenges is that credit values can vary by deal and therefore need to be tracked at a deal level. Also, they need to burn down to 0 by oldest deal, first in first out. This might add a bit of annoyance dealing with remainders.&lt;/p&gt;\n\n&lt;p&gt;Heres an example of roughly what im envisioning:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Company ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;deal id&lt;/th&gt;\n&lt;th align=\"left\"&gt;trans dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;scheduled dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;completed dt&lt;/th&gt;\n&lt;th align=\"left\"&gt;credit category&lt;/th&gt;\n&lt;th align=\"left\"&gt;$ per credit&lt;/th&gt;\n&lt;th align=\"left\"&gt;# of Credits&lt;/th&gt;\n&lt;th align=\"left\"&gt;product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Product Cost&lt;/th&gt;\n&lt;th align=\"left\"&gt;Credits Remaining&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;unassigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;300&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;2/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;3/01/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;assigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400&lt;/td&gt;\n&lt;td align=\"left\"&gt;1000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/15/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/23/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;consumed&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.50&lt;/td&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;400&lt;/td&gt;\n&lt;td align=\"left\"&gt;1100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/15/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;unassigned&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.50&lt;/td&gt;\n&lt;td align=\"left\"&gt;1500&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;300&lt;/td&gt;\n&lt;td align=\"left\"&gt;1100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;999&lt;/td&gt;\n&lt;td align=\"left\"&gt;1/1/24&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;deferred&lt;/td&gt;\n&lt;td align=\"left\"&gt;23&lt;/td&gt;\n&lt;td align=\"left\"&gt;3000&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Its unclear to me what the best approach is for working through the many transformations involved here. I could build out a bunch of additional fields and apply calculations to the table at various stages of my pipeline with SQL, but im wondering if theres a more elegant solution through breaking it out into various tables and processes, using python or recursive functions etc.&lt;/p&gt;\n\n&lt;p&gt;One last caveat is that ideally this system will be able to track rate of spend of customers, which Id imagine would most simply be tracked through snapshot tables.&lt;/p&gt;\n\n&lt;p&gt;Sorry this was a lot, but any help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ip1ya", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ip1ya/financial_reporting_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ip1ya/financial_reporting_problem/", "subreddit_subscribers": 146168, "created_utc": 1702605806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Question:**\n\nIs it possible to do so? I want either of \n\n1. run with latest commited code\n2. straight up fail this pipeline without running this operation/anything\n\n**Clarification:**\n\nA colleague used a production notebook, the kind that runs automatically on schedules, as a scratchpad to showcase code to a junior member. Kinda junior move from him, if you ask me, but not the point I'm trying to make. At the end of this teaching session, he kept an extra cmd on the notebook, a very unfortunate one: it contained a `dbutils.fs.rm(CONTAINER_URI)` line. Long story short, this is going to be a long weekend babysitting some jobs.  \nOur pull review fail safes were bypassed by this user editing the production notebook. Nothing like that would pass by a peer's review. It wouldn't happen if uncommited code were somehow blocked from running in jobs in that cluster.\n\nA lot has to change at my current job, but this is priority 0, I want it implemented today. Anyone has done it before and can point me in the right direction? Thanks ", "author_fullname": "t2_lpkh5lbv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prevent Data Factory from running Databricks notebooks with uncommited code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j27to", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702652948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is it possible to do so? I want either of &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;run with latest commited code&lt;/li&gt;\n&lt;li&gt;straight up fail this pipeline without running this operation/anything&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Clarification:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A colleague used a production notebook, the kind that runs automatically on schedules, as a scratchpad to showcase code to a junior member. Kinda junior move from him, if you ask me, but not the point I&amp;#39;m trying to make. At the end of this teaching session, he kept an extra cmd on the notebook, a very unfortunate one: it contained a &lt;code&gt;dbutils.fs.rm(CONTAINER_URI)&lt;/code&gt; line. Long story short, this is going to be a long weekend babysitting some jobs.&lt;br/&gt;\nOur pull review fail safes were bypassed by this user editing the production notebook. Nothing like that would pass by a peer&amp;#39;s review. It wouldn&amp;#39;t happen if uncommited code were somehow blocked from running in jobs in that cluster.&lt;/p&gt;\n\n&lt;p&gt;A lot has to change at my current job, but this is priority 0, I want it implemented today. Anyone has done it before and can point me in the right direction? Thanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18j27to", "is_robot_indexable": true, "report_reasons": null, "author": "recruta54", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j27to/prevent_data_factory_from_running_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j27to/prevent_data_factory_from_running_databricks/", "subreddit_subscribers": 146168, "created_utc": 1702652948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there,\n\nI'm currently kinda having a hard time understanding the DevOps process for databases. Up until now, my experience has been limited to on-premises databases, where any modifications made in the development database were manually transferred to the production database.\n\nI'm now transitioning to setting up my DB in Azure, where I plan to establish both a development and production environment. I've delved into various tutorials on configuring build and release pipelines, as well as setting up database projects in Visual Studio Code. However, I'm still uncertain whether these processes exclusively apply to changes in the schema, views, procedures, etc., or if there should also be a component in the pipeline responsible for transferring the entire (new) data from the development (or QA) environment to production.\n\nThis entire process seems a bit like a overkill to me, especially since the data model I'm using is already well-established. The only potential changes I might make occasionally are related to views. Also the entire ETL process happens outside of Azure.\n\nI would greatly appreciate hearing examples of how you implement DevOps with SQL Databases. Additionally, any advice or recommendations for videos or literature would be highly appreciated.\n\nP.S.: I am no data engineer by training, just a data scientist/chemist, who is forced to do all this stuff with very limited support at work. So please excuse any stupid questions. \n\n&amp;#x200B;", "author_fullname": "t2_4ed3khrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure DevOps Pipelines for SQL Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j23or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702652637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently kinda having a hard time understanding the DevOps process for databases. Up until now, my experience has been limited to on-premises databases, where any modifications made in the development database were manually transferred to the production database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now transitioning to setting up my DB in Azure, where I plan to establish both a development and production environment. I&amp;#39;ve delved into various tutorials on configuring build and release pipelines, as well as setting up database projects in Visual Studio Code. However, I&amp;#39;m still uncertain whether these processes exclusively apply to changes in the schema, views, procedures, etc., or if there should also be a component in the pipeline responsible for transferring the entire (new) data from the development (or QA) environment to production.&lt;/p&gt;\n\n&lt;p&gt;This entire process seems a bit like a overkill to me, especially since the data model I&amp;#39;m using is already well-established. The only potential changes I might make occasionally are related to views. Also the entire ETL process happens outside of Azure.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate hearing examples of how you implement DevOps with SQL Databases. Additionally, any advice or recommendations for videos or literature would be highly appreciated.&lt;/p&gt;\n\n&lt;p&gt;P.S.: I am no data engineer by training, just a data scientist/chemist, who is forced to do all this stuff with very limited support at work. So please excuse any stupid questions. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18j23or", "is_robot_indexable": true, "report_reasons": null, "author": "stelo55", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j23or/azure_devops_pipelines_for_sql_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j23or/azure_devops_pipelines_for_sql_database/", "subreddit_subscribers": 146168, "created_utc": 1702652637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\n\ndlt founder here. one of the dlt users offers a code template and details about how to do event ingestion on the cheap on AWS. In this case he loads it to snowflake.\n\nThe setup Simon describes (code included) costs under 10 usd/1m events, which is 2 orders of magnitude cheaper than some popular SaaS tools.\n\nThere is a code template included. The advantage of using dlt here is that it can process any types of events and with schemas and data contracts we can also control the quality of the data ingested.\n\n[https://dlthub.com/docs/blog/dlt-aws-taktile-blog](https://dlthub.com/docs/blog/dlt-aws-taktile-blog)\n\nIf you are interested to see similar setups, please let me know what you are interested in. We are working on doing the same setup on GCP for ourselves.\n\nIf you want to try it with [data contracts](https://dlthub.com/devel/general-usage/schema-contracts), they are in early release available on devel branch, we are adding bad event sinks and merging them to prod this year.", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event ingestion with AWS lambda and dlt - a case study and code template", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ix7xx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702636501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;dlt founder here. one of the dlt users offers a code template and details about how to do event ingestion on the cheap on AWS. In this case he loads it to snowflake.&lt;/p&gt;\n\n&lt;p&gt;The setup Simon describes (code included) costs under 10 usd/1m events, which is 2 orders of magnitude cheaper than some popular SaaS tools.&lt;/p&gt;\n\n&lt;p&gt;There is a code template included. The advantage of using dlt here is that it can process any types of events and with schemas and data contracts we can also control the quality of the data ingested.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/blog/dlt-aws-taktile-blog\"&gt;https://dlthub.com/docs/blog/dlt-aws-taktile-blog&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you are interested to see similar setups, please let me know what you are interested in. We are working on doing the same setup on GCP for ourselves.&lt;/p&gt;\n\n&lt;p&gt;If you want to try it with &lt;a href=\"https://dlthub.com/devel/general-usage/schema-contracts\"&gt;data contracts&lt;/a&gt;, they are in early release available on devel branch, we are adding bad event sinks and merging them to prod this year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?auto=webp&amp;s=a05aa56b4257c1c7f5d1d1ca57689d52084ffa8b", "width": 4032, "height": 2268}, "resolutions": [{"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=914049f9e98130b163c4064aa552d154bb7312cc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a824fa9f8f01c716863222da0b9aa3b15696b6d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f268828bc2c5d6bf61569794231b6dbd40829e7", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbfe2e0052a20b7b1a62cba4399deca3b61d2e3f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cad4e1ebea2d81f5191f46bb9f3bbe122788888", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/N7zat4bST2M6S3HtnPLNjlP4_cRLAh9RdbLGr96Ek-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73ae6152911933676ecd83dd3364bc30b23093b8", "width": 1080, "height": 607}], "variants": {}, "id": "EPFgYwOLokSnjFQOr12db2YaP5RfXFTu7-zIA1it1LA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ix7xx", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ix7xx/event_ingestion_with_aws_lambda_and_dlt_a_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ix7xx/event_ingestion_with_aws_lambda_and_dlt_a_case/", "subreddit_subscribers": 146168, "created_utc": 1702636501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a1tanc569", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ask HN: Is there an open source database like this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ilc2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1702594940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atwong.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atwong.medium.com/ask-hn-is-there-an-open-source-database-like-this-39ba6ceaec1e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ilc2m", "is_robot_indexable": true, "report_reasons": null, "author": "albertstarrocks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ilc2m/ask_hn_is_there_an_open_source_database_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atwong.medium.com/ask-hn-is-there-an-open-source-database-like-this-39ba6ceaec1e", "subreddit_subscribers": 146168, "created_utc": 1702594940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.\n\nTIA ALL!", "author_fullname": "t2_8txv38ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BEST ETL TRANSFORMING PRACTICE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18jb8hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702677553.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may seem like a dumb question, I am working on pulling data from an  RDBMS,  with a Python script and placing it into a data warehouse for visualization purposes. My question is, in this instance is it best practice to make as many transformations in my  RDBMS via SQL, create the table extract and load it. Or should this be done only in my data ware house. The data in my in my table I would be creating would be me averaging data, also not sure how that would effect me when using PowerBI.&lt;/p&gt;\n\n&lt;p&gt;TIA ALL!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb8hr", "is_robot_indexable": true, "report_reasons": null, "author": "Fraiz24", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb8hr/best_etl_transforming_practice/", "subreddit_subscribers": 146168, "created_utc": 1702676932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? ", "author_fullname": "t2_2gjyr66m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for non dataengineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18jb2ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702676793.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702676517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, my company is currently storing a ton of data in databricks and i would like to do \u201cstuff\u201d with this data. Like analysis, maybe some graphs, maybe some code to get some specific metrics. Not being a dataengineer (but im a developer) what are some things i can learn in this space to join meetings and not be 100% lost. Is this one of pandas use cases? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18jb2ze", "is_robot_indexable": true, "report_reasons": null, "author": "SuperLucas2000", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18jb2ze/databricks_for_non_dataengineers/", "subreddit_subscribers": 146168, "created_utc": 1702676517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This week, I created a dbt model that pinpoints the NBA's top \"one-hit wonders.\"\n\n\"One hit wonder\" = Players who had 1 season that's dramatically better than the avg. of all their other seasons.\n\nTo find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player's contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.\n\nDisclaimer: PER isn't the end-all and be-all of player metrics, it points me in the right direction.\n\nTools used:\n\n\\- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python\n\n\\- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp; Snowflake (Production)\n\n\\- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime\n\n\\- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash\n\nIf you're curious, here's the repo:  \nhttps://github.com/jpooksy/NBA\\_Data\\_Modeling  \n\n\nhttps://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\n\n&amp;#x200B;", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing \"One hit wonder\" NBA Players", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 110, "top_awarded_type": null, "hide_score": true, "media_metadata": {"69709ezuwi6c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3190b528ffc53e48e3fc3191fcc8e57763ae961c"}, {"y": 170, "x": 216, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6f5c589ea0dc15a126060d837e4e4dc82803a0f"}, {"y": 252, "x": 320, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a0dc820f9448ed220606e4ffc0a0673867b069ba"}, {"y": 504, "x": 640, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=66ea372b61096ac6e87f289064776282d14b7eaa"}, {"y": 756, "x": 960, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd3fa91bf0d7504759d0c1e22337ecd4d71c9da7"}], "s": {"y": 771, "x": 978, "u": "https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;format=png&amp;auto=webp&amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590"}, "id": "69709ezuwi6c1"}}, "name": "t3_18ja96u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/73whFpo_IHCGK7dqQzOmjYuICSsjOFhke4-OP5fs-Yo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702674311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This week, I created a dbt model that pinpoints the NBA&amp;#39;s top &amp;quot;one-hit wonders.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;One hit wonder&amp;quot; = Players who had 1 season that&amp;#39;s dramatically better than the avg. of all their other seasons.&lt;/p&gt;\n\n&lt;p&gt;To find these players, I used a formula called Player Efficiency Rating (PER) across seasons. The PER formula condenses a player&amp;#39;s contributions into a single, comprehensive metric. By weighing 12 distinct stats, each with its unique importance, PER offers a all-in-one metric to identify a players performance.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: PER isn&amp;#39;t the end-all and be-all of player metrics, it points me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Tools used:&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc08\ud835\udc27\ud835\udc20\ud835\udc1e\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27: public NBA API + Python&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc2d\ud835\udc28\ud835\udc2b\ud835\udc1a\ud835\udc20\ud835\udc1e: DuckDB (development) &amp;amp; Snowflake (Production)&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc28\ud835\udc2b\ud835\udc26\ud835\udc1a\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\ud835\udc2c (dbt): Paradime&lt;/p&gt;\n\n&lt;p&gt;- \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc22\ud835\udc27\ud835\udc20 (\ud835\udc01\ud835\udc08) -Lightdash&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re curious, here&amp;#39;s the repo:&lt;br/&gt;\n&lt;a href=\"https://github.com/jpooksy/NBA%5C_Data%5C_Modeling\"&gt;https://github.com/jpooksy/NBA\\_Data\\_Modeling&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590\"&gt;https://preview.redd.it/69709ezuwi6c1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=edf602fcd04b4406d7385f1377476bda5a6bb590&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18ja96u", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ja96u/analyzing_one_hit_wonder_nba_players/", "subreddit_subscribers": 146168, "created_utc": 1702674311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am currently working for one year as a BI Developer, with almost ten years of previous experience as a Data Analyst (Excel, OLAP Cubes, Power Query, VBA). I work on a technology stack: SQL, SSIS (ETL EDW), SSAS, SSRS, PowerBI, DAX. Before taking on my current BI role, I learned the basics of Python on my own and played around a bit with Django and FastAPI. Due to a rather wide range of responsibilities, I'm considering my future career path; it's hard to be good with all this tools. I'm leaning towards data engineering, (analytics and reporting have bored me a bit). At my current job, we are considering Azure cloud, more specifically Microsoft Fabric solution (currently everything is on premise -EDW, multidimensional SSAS). Should I nevertheless stay in my current job, even though I don't know how many tasks regarding the future cloud I will be given and rather average paycheck. In the meantime, I'm also doing a bit of a project management role and getting in touch with an external client regarding the introduction of AI solutions to improve product management in the company store and warehouse. We had our first course with Azure DP-900 this week. Atm its hard to focus on BI job because we are drawning with adhoc tasks, 3 people doing BI tasks, reports for 2 companies with 100 physical stores, 15 ecommrece countries , 1 marketplace. So I'd like to ask you guys, what to learn and focus on?", "author_fullname": "t2_12zmul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI Devloper career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18j9vnl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702673297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am currently working for one year as a BI Developer, with almost ten years of previous experience as a Data Analyst (Excel, OLAP Cubes, Power Query, VBA). I work on a technology stack: SQL, SSIS (ETL EDW), SSAS, SSRS, PowerBI, DAX. Before taking on my current BI role, I learned the basics of Python on my own and played around a bit with Django and FastAPI. Due to a rather wide range of responsibilities, I&amp;#39;m considering my future career path; it&amp;#39;s hard to be good with all this tools. I&amp;#39;m leaning towards data engineering, (analytics and reporting have bored me a bit). At my current job, we are considering Azure cloud, more specifically Microsoft Fabric solution (currently everything is on premise -EDW, multidimensional SSAS). Should I nevertheless stay in my current job, even though I don&amp;#39;t know how many tasks regarding the future cloud I will be given and rather average paycheck. In the meantime, I&amp;#39;m also doing a bit of a project management role and getting in touch with an external client regarding the introduction of AI solutions to improve product management in the company store and warehouse. We had our first course with Azure DP-900 this week. Atm its hard to focus on BI job because we are drawning with adhoc tasks, 3 people doing BI tasks, reports for 2 companies with 100 physical stores, 15 ecommrece countries , 1 marketplace. So I&amp;#39;d like to ask you guys, what to learn and focus on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18j9vnl", "is_robot_indexable": true, "report_reasons": null, "author": "StarBuzz89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j9vnl/bi_devloper_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j9vnl/bi_devloper_career_advice/", "subreddit_subscribers": 146168, "created_utc": 1702673297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm heading into an interview next week for a Business Intelligence Engineer role requiring 2+ years experience. It's a technical revolving around SQL and data visualization.\n\nDoes anyone have insight into the types of questions asked on these interviews? I've reviewed the cheat sheet and SQL is honestly a weakness for me so I'd like to be as prepared as possible.", "author_fullname": "t2_1323iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon BIE Interview Process (SQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j8s9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702670328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m heading into an interview next week for a Business Intelligence Engineer role requiring 2+ years experience. It&amp;#39;s a technical revolving around SQL and data visualization.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have insight into the types of questions asked on these interviews? I&amp;#39;ve reviewed the cheat sheet and SQL is honestly a weakness for me so I&amp;#39;d like to be as prepared as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18j8s9p", "is_robot_indexable": true, "report_reasons": null, "author": "bopper1826", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j8s9p/amazon_bie_interview_process_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j8s9p/amazon_bie_interview_process_sql/", "subreddit_subscribers": 146168, "created_utc": 1702670328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some credits from my company that I didn't use for this year. Can I pass the dbt certification exam with 0 knowledge of **dbt cloud**? I have used **dbt core** extensively in my work.", "author_fullname": "t2_ncq2v4j0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much knowledge of dbt Cloud is required for the dbt certification exam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j8ar8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702668988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some credits from my company that I didn&amp;#39;t use for this year. Can I pass the dbt certification exam with 0 knowledge of &lt;strong&gt;dbt cloud&lt;/strong&gt;? I have used &lt;strong&gt;dbt core&lt;/strong&gt; extensively in my work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18j8ar8", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Buyer_4487", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j8ar8/how_much_knowledge_of_dbt_cloud_is_required_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j8ar8/how_much_knowledge_of_dbt_cloud_is_required_for/", "subreddit_subscribers": 146168, "created_utc": 1702668988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been applying and feel like I'm over qualified since going from coding to report making is an easy switch. My current role is basically a data engineer even though my title is software developer. I can write sql in my sleep, I understand relational databases, I understand and work with big data. I have created reports in everything from power Bi, tableau, ssrs, etc...But I think because my resume lacks any history of a data title I've been getting rejected.... Do I lie and fudge my title or what?", "author_fullname": "t2_5hg1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to pivot from software dev to data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j87m2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702668744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been applying and feel like I&amp;#39;m over qualified since going from coding to report making is an easy switch. My current role is basically a data engineer even though my title is software developer. I can write sql in my sleep, I understand relational databases, I understand and work with big data. I have created reports in everything from power Bi, tableau, ssrs, etc...But I think because my resume lacks any history of a data title I&amp;#39;ve been getting rejected.... Do I lie and fudge my title or what?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18j87m2", "is_robot_indexable": true, "report_reasons": null, "author": "skydreamer303", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j87m2/how_to_pivot_from_software_dev_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j87m2/how_to_pivot_from_software_dev_to_data_engineer/", "subreddit_subscribers": 146168, "created_utc": 1702668744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, this might be a basic question but I'm having difficulty filling a new derived column.\n\nI have a column of datatype date I want to populate, let's call this \"combodates\", this is a combination of a two columns, one is full of old dates called OGdates of datatype int, and one is half full of new dates called Newdates datatype date.\n\nI've been trying to build an expression in adf to populate this combodates column with old dates, unless there's a new date in which case populate it using that.\n\nSo far the best I've come up with is changing the datatype of the OGdates column through a derived column to date and putting it into an expression in another derived column:\n\niif(isNull(Newdates), OGdates, Newdates)\n\nI used a derived column to change the datatype because cast was absolutely not playing ball.\n\nThe script I have so far is filling the new combodates column only when there is data in the Newdates column. It's completely ignoring the request to fill the column with OGdates when the Newdates column has no data.\n\nI'm not sure what to do I've been wracking my brain, Google, forums and chatgpt for hours. \n\nFor context this is part of a data flow which upserts into a table if 1==1 \n\nThank you in advance.", "author_fullname": "t2_lovh9cgne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble with derived column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18j7386", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702665819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, this might be a basic question but I&amp;#39;m having difficulty filling a new derived column.&lt;/p&gt;\n\n&lt;p&gt;I have a column of datatype date I want to populate, let&amp;#39;s call this &amp;quot;combodates&amp;quot;, this is a combination of a two columns, one is full of old dates called OGdates of datatype int, and one is half full of new dates called Newdates datatype date.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to build an expression in adf to populate this combodates column with old dates, unless there&amp;#39;s a new date in which case populate it using that.&lt;/p&gt;\n\n&lt;p&gt;So far the best I&amp;#39;ve come up with is changing the datatype of the OGdates column through a derived column to date and putting it into an expression in another derived column:&lt;/p&gt;\n\n&lt;p&gt;iif(isNull(Newdates), OGdates, Newdates)&lt;/p&gt;\n\n&lt;p&gt;I used a derived column to change the datatype because cast was absolutely not playing ball.&lt;/p&gt;\n\n&lt;p&gt;The script I have so far is filling the new combodates column only when there is data in the Newdates column. It&amp;#39;s completely ignoring the request to fill the column with OGdates when the Newdates column has no data.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what to do I&amp;#39;ve been wracking my brain, Google, forums and chatgpt for hours. &lt;/p&gt;\n\n&lt;p&gt;For context this is part of a data flow which upserts into a table if 1==1 &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18j7386", "is_robot_indexable": true, "report_reasons": null, "author": "Two_5536", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18j7386/trouble_with_derived_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18j7386/trouble_with_derived_column/", "subreddit_subscribers": 146168, "created_utc": 1702665819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nFor those of you who use Astronomer Cosmos, what is the best community to discuss it in your opinion? The Airflow slack? This subreddit? \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_1jh436du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best place to discuss Astronomer Cosmos / debug / Best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18iy492", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702640050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;For those of you who use Astronomer Cosmos, what is the best community to discuss it in your opinion? The Airflow slack? This subreddit? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18iy492", "is_robot_indexable": true, "report_reasons": null, "author": "Jeannetton", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18iy492/best_place_to_discuss_astronomer_cosmos_debug/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18iy492/best_place_to_discuss_astronomer_cosmos_debug/", "subreddit_subscribers": 146168, "created_utc": 1702640050.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}