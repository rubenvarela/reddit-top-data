{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for the noob question if it sounds ignorant. Here is the situation. \n\nI joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.\n\n```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame\n\ndf.loc[df['value']&gt;20] # Pandas form\n\nquery:str='SELECT * FROM df WHERE value&gt;20;'\nduckdb.sql(query=query).df() # SQL Form\n```\n\nSo it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. \n\nSo the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DuckDB Used For?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f0tju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702204519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the noob question if it sounds ignorant. Here is the situation. &lt;/p&gt;\n\n&lt;p&gt;I joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.&lt;/p&gt;\n\n&lt;p&gt;```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame&lt;/p&gt;\n\n&lt;p&gt;df.loc[df[&amp;#39;value&amp;#39;]&amp;gt;20] # Pandas form&lt;/p&gt;\n\n&lt;p&gt;query:str=&amp;#39;SELECT * FROM df WHERE value&amp;gt;20;&amp;#39;\nduckdb.sql(query=query).df() # SQL Form\n```&lt;/p&gt;\n\n&lt;p&gt;So it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. &lt;/p&gt;\n\n&lt;p&gt;So the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f0tju", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "subreddit_subscribers": 145111, "created_utc": 1702204519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn't made those decisions, that's tended to be a cto-level decision at companies I've worked at, even de managers aren't making those calls. The other similar line was around how what the interviewer called the \"operational layer\" and \"analytics layer\" should be stored. Same story I've never been involved in those decisions but I communicated that obviously we'd want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  \n\n\nI don't think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don't even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I'm a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that's literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!", "author_fullname": "t2_sg5v2ix5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downleveled for Data Governance, How to Study", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evqnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702183547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn&amp;#39;t made those decisions, that&amp;#39;s tended to be a cto-level decision at companies I&amp;#39;ve worked at, even de managers aren&amp;#39;t making those calls. The other similar line was around how what the interviewer called the &amp;quot;operational layer&amp;quot; and &amp;quot;analytics layer&amp;quot; should be stored. Same story I&amp;#39;ve never been involved in those decisions but I communicated that obviously we&amp;#39;d want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don&amp;#39;t even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I&amp;#39;m a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that&amp;#39;s literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18evqnt", "is_robot_indexable": true, "report_reasons": null, "author": "BoysenberryLanky6112", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "subreddit_subscribers": 145111, "created_utc": 1702183547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does Spark SQL functions (bull-in) like collect_list, collect_set etc. get converted into SQL anyway ? If so am I better off using SQL anyway? \n\nNote: I am using databricks", "author_fullname": "t2_b3q52q4ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark SQL vs SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18emgl4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702154152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Spark SQL functions (bull-in) like collect_list, collect_set etc. get converted into SQL anyway ? If so am I better off using SQL anyway? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using databricks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18emgl4", "is_robot_indexable": true, "report_reasons": null, "author": "SriRamaJayam", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18emgl4/spark_sql_vs_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18emgl4/spark_sql_vs_sql/", "subreddit_subscribers": 145111, "created_utc": 1702154152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nRecently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?", "author_fullname": "t2_gci8d6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here that works as data engineer in Canada as an immigrant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4upz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702218794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;Recently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f4upz", "is_robot_indexable": true, "report_reasons": null, "author": "Entropico_88", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "subreddit_subscribers": 145111, "created_utc": 1702218794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.\n\nEncrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage PII?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7m1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702226742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.&lt;/p&gt;\n\n&lt;p&gt;Encrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f7m1t", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "subreddit_subscribers": 145111, "created_utc": 1702226742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will soon start working with Delta lakes, and would like to know a bit about people's experiences with it.\n\nAny main reasons why you're using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) \n\nThanks!", "author_fullname": "t2_m84na74fe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Delta Lake, pros, cons etc...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f3wo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702215831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will soon start working with Delta lakes, and would like to know a bit about people&amp;#39;s experiences with it.&lt;/p&gt;\n\n&lt;p&gt;Any main reasons why you&amp;#39;re using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f3wo9", "is_robot_indexable": true, "report_reasons": null, "author": "Jazzlike-Change8493", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "subreddit_subscribers": 145111, "created_utc": 1702215831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don't like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source CDC options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evjr2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don&amp;#39;t like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evjr2", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "subreddit_subscribers": 145111, "created_utc": 1702182830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do the two compare w.r.t:\n1. Pay\n2. Career growth\n\nI have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.", "author_fullname": "t2_9815cpn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering vs Tech Consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f15jg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702205920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do the two compare w.r.t:\n1. Pay\n2. Career growth&lt;/p&gt;\n\n&lt;p&gt;I have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f15jg", "is_robot_indexable": true, "report_reasons": null, "author": "brokeRichieRich", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "subreddit_subscribers": 145111, "created_utc": 1702205920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?\n\nOne of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. \n\nIf anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.", "author_fullname": "t2_344hea7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evk3q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?&lt;/p&gt;\n\n&lt;p&gt;One of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. &lt;/p&gt;\n\n&lt;p&gt;If anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evk3q", "is_robot_indexable": true, "report_reasons": null, "author": "Ashu6410", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evk3q/database_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evk3q/database_optimization/", "subreddit_subscribers": 145111, "created_utc": 1702182871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI got laid off in Oct from a health care company (i used to believe that health industry is the most stable), i have been applying to jobs since then.\n\nGot a call from Sony Music for a Data Engineer mid level job role. Reaching out to gather insights and advice from those who gave interviews at Sony departments. Any help would be useful! I really dont want to miss this chance as it has been really bad job market recently(I have applied to at least 2000+ jobs \u0ca5\u2060\u203f\u2060\u0ca5)\n\nThank you.", "author_fullname": "t2_8xi5q2nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sony Music Data Engineer Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ennzf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702157634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I got laid off in Oct from a health care company (i used to believe that health industry is the most stable), i have been applying to jobs since then.&lt;/p&gt;\n\n&lt;p&gt;Got a call from Sony Music for a Data Engineer mid level job role. Reaching out to gather insights and advice from those who gave interviews at Sony departments. Any help would be useful! I really dont want to miss this chance as it has been really bad job market recently(I have applied to at least 2000+ jobs \u0ca5\u2060\u203f\u2060\u0ca5)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18ennzf", "is_robot_indexable": true, "report_reasons": null, "author": "weOutBottle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ennzf/sony_music_data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ennzf/sony_music_data_engineer_interview/", "subreddit_subscribers": 145111, "created_utc": 1702157634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_p5gqem6h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please Stop Using Google Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18f38wx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18f38wx", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/MscxPo2ozHM1_dkAkbfqJDEUuZR-KsA4pgLKk8yVF38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702213632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?auto=webp&amp;s=99f9cf88fbf8b204bdfb554c5686774edfb04ac9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c862112d73985f4a7ca6e8d7d7f8e854eb7a2ddf", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8852ce395422244d48cb3c7045dd899e496c484f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6073c63ec3325df2713fd5c40a4a8f12c62edc9", "width": 320, "height": 240}], "variants": {}, "id": "_Oc0viLfsdEgxq9iRXrD-Gc4J8e4EtHURDly-KXakWM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f38wx", "is_robot_indexable": true, "report_reasons": null, "author": "MySpermIs-Unvaxxd-01", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f38wx/please_stop_using_google_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "subreddit_subscribers": 145111, "created_utc": 1702213632.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\nSo I am in a contract job with a big company where I am responsible for taking the external data and upload it to snowflake. \nI want to automate the process and also make sure that I reduce the size of the data. \nHow can I go about this?\nI am told to you dask and convert the files to parquet. \nI am completely new to this. \nAll suggestions are welcome.", "author_fullname": "t2_8446gqlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice from the pros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18eu373", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702177565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,\nSo I am in a contract job with a big company where I am responsible for taking the external data and upload it to snowflake. \nI want to automate the process and also make sure that I reduce the size of the data. \nHow can I go about this?\nI am told to you dask and convert the files to parquet. \nI am completely new to this. \nAll suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18eu373", "is_robot_indexable": true, "report_reasons": null, "author": "Fair-Bed-5771", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18eu373/advice_from_the_pros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18eu373/advice_from_the_pros/", "subreddit_subscribers": 145111, "created_utc": 1702177565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.\n\nI\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.\n\nBut I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.", "author_fullname": "t2_4jozahcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimise my upserts - Azure Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18epq5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702163600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18epq5x", "is_robot_indexable": true, "report_reasons": null, "author": "anxiouscrimp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "subreddit_subscribers": 145111, "created_utc": 1702163600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is using Azure Databricks and I'm an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a \"/FileStore/file\\_path/file\\_name.xlsx\" file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn't really helped me up to this point.\n\n`w = WorkspaceClient(host=server_hostname,token = api_key)`\n\nI can successfully run `w.dbfs.download('/FileStore/file_path/file_name.xlsx')`, but can't figure out how to save the file down to local storage as an Excel file.\n\n`w.dbutils.fs.cp('dbfs:/FileStore/file_path/file_name.xlsx', 'C:/Users/user_name/file_path/file_name.xlsx')` results in \"java.net.URISyntaxException: Relative path in absolute URI\" for the C drive file path. This looks like an absolute path to me. Am I missing something?\n\n`w.files.get_status('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"Get-status API is not enabled.\" How do I enable it? I don't see a toggle for it in Admin Settings.\n\n`w.files.download('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"The ':' character is not supported\", but when I remove it the file path is no longer valid.\n\nI feel like I'm close on one of these but just can't seem to get it across the finish line.", "author_fullname": "t2_yk6x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone moved files between Azure Databricks DBFS and local storage using the Databricks API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7vcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702227447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is using Azure Databricks and I&amp;#39;m an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a &amp;quot;/FileStore/file_path/file_name.xlsx&amp;quot; file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn&amp;#39;t really helped me up to this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w = WorkspaceClient(host=server_hostname,token = api_key)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I can successfully run &lt;code&gt;w.dbfs.download(&amp;#39;/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt;, but can&amp;#39;t figure out how to save the file down to local storage as an Excel file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.dbutils.fs.cp(&amp;#39;dbfs:/FileStore/file_path/file_name.xlsx&amp;#39;, &amp;#39;C:/Users/user_name/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;java.net.URISyntaxException: Relative path in absolute URI&amp;quot; for the C drive file path. This looks like an absolute path to me. Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.get_status(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;Get-status API is not enabled.&amp;quot; How do I enable it? I don&amp;#39;t see a toggle for it in Admin Settings.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.download(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;The &amp;#39;:&amp;#39; character is not supported&amp;quot;, but when I remove it the file path is no longer valid.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m close on one of these but just can&amp;#39;t seem to get it across the finish line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f7vcb", "is_robot_indexable": true, "report_reasons": null, "author": "CurlyW15", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "subreddit_subscribers": 145111, "created_utc": 1702227447.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}