{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a weird meeting where My department head pulled me into a meeting with HR and started it with \"why do you think youre a computer programmer\" \n\n\nAnd spent the next 30 minutes trying to say I don't program and never was paid to program but am instead, a scripter who only scripts. \n\n\n\nNow. My department head isn't a tech guy. He's a finance guy. My best guess is HR informed him of the California Computer Professional law and now he's trying to dismantle any beliefs that he owes me more money. \n\n\n\nSigh. I took a break from my government data engineering job (extended leave while they are building a new office) to try out Healthcare and I love the work. Its rewarding. I feel as though I've improved greatly. But it's very hard when one person above you constantly throws wrenches in the system.\n\n\nAs he puts it \"I don't understand it because it's just a glorified excel\"\n\n\nAm I annoyed? Yeah. More insulted then anything else. But, this is a side gig to help me improve my skill set so im not worried", "author_fullname": "t2_gzuzr62b1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Why do you think you're a programmer\" a rant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e8r3g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702109138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a weird meeting where My department head pulled me into a meeting with HR and started it with &amp;quot;why do you think youre a computer programmer&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;And spent the next 30 minutes trying to say I don&amp;#39;t program and never was paid to program but am instead, a scripter who only scripts. &lt;/p&gt;\n\n&lt;p&gt;Now. My department head isn&amp;#39;t a tech guy. He&amp;#39;s a finance guy. My best guess is HR informed him of the California Computer Professional law and now he&amp;#39;s trying to dismantle any beliefs that he owes me more money. &lt;/p&gt;\n\n&lt;p&gt;Sigh. I took a break from my government data engineering job (extended leave while they are building a new office) to try out Healthcare and I love the work. Its rewarding. I feel as though I&amp;#39;ve improved greatly. But it&amp;#39;s very hard when one person above you constantly throws wrenches in the system.&lt;/p&gt;\n\n&lt;p&gt;As he puts it &amp;quot;I don&amp;#39;t understand it because it&amp;#39;s just a glorified excel&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Am I annoyed? Yeah. More insulted then anything else. But, this is a side gig to help me improve my skill set so im not worried&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18e8r3g", "is_robot_indexable": true, "report_reasons": null, "author": "Not_Another_Cookbook", "discussion_type": null, "num_comments": 90, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e8r3g/why_do_you_think_youre_a_programmer_a_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e8r3g/why_do_you_think_youre_a_programmer_a_rant/", "subreddit_subscribers": 144982, "created_utc": 1702109138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently a data engineer with 7YoE, in line for promotion to Senior DE.\nThis is how promotions work in my company. I should start working like a senior would, make a case with management that I'm already operating on a senior level and need a promotion. Management evaluates the case and gets it done.\n\nI was told I'm ready in December '22, I made the case after critical deliverable in March '23 (Fool to have delayed so much) manager stalled until September to make me look better for upper management until another critical deliverable. All I hear after it is \"It's happening\". Yesterday I heard it might not even happen until April '24.\n\nI feel stupid to be working as a senior without raise for 1.5 Years now. Is it a fair game? Any insights that can help me navigate this are appreciated", "author_fullname": "t2_8b8jp1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delayed promotion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ekmn6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702149016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently a data engineer with 7YoE, in line for promotion to Senior DE.\nThis is how promotions work in my company. I should start working like a senior would, make a case with management that I&amp;#39;m already operating on a senior level and need a promotion. Management evaluates the case and gets it done.&lt;/p&gt;\n\n&lt;p&gt;I was told I&amp;#39;m ready in December &amp;#39;22, I made the case after critical deliverable in March &amp;#39;23 (Fool to have delayed so much) manager stalled until September to make me look better for upper management until another critical deliverable. All I hear after it is &amp;quot;It&amp;#39;s happening&amp;quot;. Yesterday I heard it might not even happen until April &amp;#39;24.&lt;/p&gt;\n\n&lt;p&gt;I feel stupid to be working as a senior without raise for 1.5 Years now. Is it a fair game? Any insights that can help me navigate this are appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ekmn6", "is_robot_indexable": true, "report_reasons": null, "author": "EstablishmentTop3908", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ekmn6/delayed_promotion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ekmn6/delayed_promotion/", "subreddit_subscribers": 144982, "created_utc": 1702149016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn't made those decisions, that's tended to be a cto-level decision at companies I've worked at, even de managers aren't making those calls. The other similar line was around how what the interviewer called the \"operational layer\" and \"analytics layer\" should be stored. Same story I've never been involved in those decisions but I communicated that obviously we'd want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  \n\n\nI don't think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don't even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I'm a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that's literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!", "author_fullname": "t2_sg5v2ix5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downleveled for Data Governance, How to Study", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evqnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702183547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn&amp;#39;t made those decisions, that&amp;#39;s tended to be a cto-level decision at companies I&amp;#39;ve worked at, even de managers aren&amp;#39;t making those calls. The other similar line was around how what the interviewer called the &amp;quot;operational layer&amp;quot; and &amp;quot;analytics layer&amp;quot; should be stored. Same story I&amp;#39;ve never been involved in those decisions but I communicated that obviously we&amp;#39;d want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don&amp;#39;t even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I&amp;#39;m a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that&amp;#39;s literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18evqnt", "is_robot_indexable": true, "report_reasons": null, "author": "BoysenberryLanky6112", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "subreddit_subscribers": 144982, "created_utc": 1702183547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nSo i just got hired into a role as a Data Engineer. No prior work experience as a data engineer, I have done only a couple of self projects.\n\nI had my bachelors in Business Administration and later transitioned my studies into tech some three years ago. Most of my self taught projects were on the basic knowledge of how to use sql, python, Azure Data Factory, Azure Databricks, powerBI.\n\nIn this new role, the organization uses SSiS, SSmS and the above tools i know. I have no knowledge in Microsoft Sql server and i tried watching YouTube vids but still don\u2019t really get.( It probably might take time, but the environment is fast paced and i doubt no one would make time to teach me the abc\u2019s of it) Most of the terminology used by colleagues go over my head because I dont really have a foundation in tech and i am scared to show that i don\u2019t understand too many of their terms because they might think i was a wrong hire. I google most at times to understand but sometimes they just don\u2019t make sense in my head. Although, I am willing and ready to learn cos I\u2019ve been itching and fascinated to get into the data space.Or maybe it is imposter syndrome.\n\nAny advice or encouragement would help me.", "author_fullname": "t2_kb86mmkp9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New role as DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ee32l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702130053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;So i just got hired into a role as a Data Engineer. No prior work experience as a data engineer, I have done only a couple of self projects.&lt;/p&gt;\n\n&lt;p&gt;I had my bachelors in Business Administration and later transitioned my studies into tech some three years ago. Most of my self taught projects were on the basic knowledge of how to use sql, python, Azure Data Factory, Azure Databricks, powerBI.&lt;/p&gt;\n\n&lt;p&gt;In this new role, the organization uses SSiS, SSmS and the above tools i know. I have no knowledge in Microsoft Sql server and i tried watching YouTube vids but still don\u2019t really get.( It probably might take time, but the environment is fast paced and i doubt no one would make time to teach me the abc\u2019s of it) Most of the terminology used by colleagues go over my head because I dont really have a foundation in tech and i am scared to show that i don\u2019t understand too many of their terms because they might think i was a wrong hire. I google most at times to understand but sometimes they just don\u2019t make sense in my head. Although, I am willing and ready to learn cos I\u2019ve been itching and fascinated to get into the data space.Or maybe it is imposter syndrome.&lt;/p&gt;\n\n&lt;p&gt;Any advice or encouragement would help me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ee32l", "is_robot_indexable": true, "report_reasons": null, "author": "StatusSport2335", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ee32l/new_role_as_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ee32l/new_role_as_de/", "subreddit_subscribers": 144982, "created_utc": 1702130053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does Spark SQL functions (bull-in) like collect_list, collect_set etc. get converted into SQL anyway ? If so am I better off using SQL anyway? \n\nNote: I am using databricks", "author_fullname": "t2_b3q52q4ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark SQL vs SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18emgl4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702154152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Spark SQL functions (bull-in) like collect_list, collect_set etc. get converted into SQL anyway ? If so am I better off using SQL anyway? &lt;/p&gt;\n\n&lt;p&gt;Note: I am using databricks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18emgl4", "is_robot_indexable": true, "report_reasons": null, "author": "SriRamaJayam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18emgl4/spark_sql_vs_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18emgl4/spark_sql_vs_sql/", "subreddit_subscribers": 144982, "created_utc": 1702154152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am tasked with designing and ETL solution for my company and I'm trying to decide between azure data factory or databricks... Or maybe find an even better alternative?\n\nThe data is small, ranging from 10gb to hundreds of mb. It is relational, ingested from either API or data warehouse once per day. There are less than 10 data pipelines in total I'd say.\n\nI need to do transformations and quality checks on the data and I would like to use dbt for this part.\n\nData is loaded into a db for backend to use (no OLAP).\n\nThere are some ml initiatives incoming and I'm the only person in the data team at the moment.\n\nI'm considering either databricks which seems a bit overkill since I'd be using mainly notebooks and workflows features and I know it offers a lot more but is pricy OR azure data factory which looks cheaper but less flexible, however it could be enough for my needs\n\nI'm trying to find if there's a way to run dbt core on azure data factory.", "author_fullname": "t2_6d2ltdsv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have experience with azure data factory for \"small\" data ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ejka3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702146037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am tasked with designing and ETL solution for my company and I&amp;#39;m trying to decide between azure data factory or databricks... Or maybe find an even better alternative?&lt;/p&gt;\n\n&lt;p&gt;The data is small, ranging from 10gb to hundreds of mb. It is relational, ingested from either API or data warehouse once per day. There are less than 10 data pipelines in total I&amp;#39;d say.&lt;/p&gt;\n\n&lt;p&gt;I need to do transformations and quality checks on the data and I would like to use dbt for this part.&lt;/p&gt;\n\n&lt;p&gt;Data is loaded into a db for backend to use (no OLAP).&lt;/p&gt;\n\n&lt;p&gt;There are some ml initiatives incoming and I&amp;#39;m the only person in the data team at the moment.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering either databricks which seems a bit overkill since I&amp;#39;d be using mainly notebooks and workflows features and I know it offers a lot more but is pricy OR azure data factory which looks cheaper but less flexible, however it could be enough for my needs&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find if there&amp;#39;s a way to run dbt core on azure data factory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ejka3", "is_robot_indexable": true, "report_reasons": null, "author": "No_Potential6129", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ejka3/anyone_have_experience_with_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ejka3/anyone_have_experience_with_azure_data_factory/", "subreddit_subscribers": 144982, "created_utc": 1702146037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nCurrently I have a task to ingest terabyte big table from onprem SQL Server to ADLS using ADF. After that Databricks will take care of it.\n\nI am wondering if someone had similar situation and had some advice to share when it comes to ADF (and Databricks). What would be dos and donts when it comes to copying this large table. I am mostly concerned how long will it take to ingest this. If this is not optimal, any different approaches to extracting data from this table to ADLS on a schedule? \n\nAny advice is appreciated!", "author_fullname": "t2_4c7udteq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory - Copy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e8i1k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702108056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;Currently I have a task to ingest terabyte big table from onprem SQL Server to ADLS using ADF. After that Databricks will take care of it.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if someone had similar situation and had some advice to share when it comes to ADF (and Databricks). What would be dos and donts when it comes to copying this large table. I am mostly concerned how long will it take to ingest this. If this is not optimal, any different approaches to extracting data from this table to ADLS on a schedule? &lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e8i1k", "is_robot_indexable": true, "report_reasons": null, "author": "MahoYami", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e8i1k/azure_data_factory_copy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e8i1k/azure_data_factory_copy/", "subreddit_subscribers": 144982, "created_utc": 1702108056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI got laid off in Oct from a health care company (i used to believe that health industry is the most stable), i have been applying to jobs since then.\n\nGot a call from Sony Music for a Data Engineer mid level job role. Reaching out to gather insights and advice from those who gave interviews at Sony departments. Any help would be useful! I really dont want to miss this chance as it has been really bad job market recently(I have applied to at least 2000+ jobs \u0ca5\u2060\u203f\u2060\u0ca5)\n\nThank you.", "author_fullname": "t2_8xi5q2nz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sony Music Data Engineer Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ennzf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702157634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I got laid off in Oct from a health care company (i used to believe that health industry is the most stable), i have been applying to jobs since then.&lt;/p&gt;\n\n&lt;p&gt;Got a call from Sony Music for a Data Engineer mid level job role. Reaching out to gather insights and advice from those who gave interviews at Sony departments. Any help would be useful! I really dont want to miss this chance as it has been really bad job market recently(I have applied to at least 2000+ jobs \u0ca5\u2060\u203f\u2060\u0ca5)&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18ennzf", "is_robot_indexable": true, "report_reasons": null, "author": "weOutBottle", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ennzf/sony_music_data_engineer_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ennzf/sony_music_data_engineer_interview/", "subreddit_subscribers": 144982, "created_utc": 1702157634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a junior data engineer at my company and was asked to move some of the transformations/metrics we have running as bigquery tables created with Airflow to EMR/Spark jobs instead. This will speed things up.\nAlthough this is an interesting project I have not much experience migrating stuff like this. Any advice on what should be taken into account/what could go wrong?", "author_fullname": "t2_8dnn00ks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating BigQuery workloads to Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ekh4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702148586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a junior data engineer at my company and was asked to move some of the transformations/metrics we have running as bigquery tables created with Airflow to EMR/Spark jobs instead. This will speed things up.\nAlthough this is an interesting project I have not much experience migrating stuff like this. Any advice on what should be taken into account/what could go wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ekh4l", "is_robot_indexable": true, "report_reasons": null, "author": "Se7enEl11ven", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ekh4l/migrating_bigquery_workloads_to_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ekh4l/migrating_bigquery_workloads_to_spark/", "subreddit_subscribers": 144982, "created_utc": 1702148586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working past 4 months with azure data explorer. Seems to be quite efficient to get simple project like this done. I wonder qhy there is no more traction? Maybe I am missing something?", "author_fullname": "t2_3rp9bhqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Zero to Hero in Azure Data Explorer: Key Learnings and Architecture Advice - Brightly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18ekhpd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5bUsXShqhV-n82Q4B4TnIr-gzVWTvODFzL-XJryPzos.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702148630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "brightly.fi", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working past 4 months with azure data explorer. Seems to be quite efficient to get simple project like this done. I wonder qhy there is no more traction? Maybe I am missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.brightly.fi/post/from-zero-to-hero-in-azure-data-explorer", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?auto=webp&amp;s=7e6500272703fb8b9ec6fc23833514c5595ac278", "width": 2600, "height": 1451}, "resolutions": [{"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1be11848531d95fb56df3eb27159129a8a458256", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dc72d929baa109c03242f0129dc30c80e6fd497", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c5d9bc695075ecdd4804de687c1656a3ddde951", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df7d16e0f0974d82cfa0e494c244d1c37d1be1ad", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11b4d3a8e7b21afbfba9b6779dc298846d6bf05d", "width": 960, "height": 535}, {"url": "https://external-preview.redd.it/hWlcXv-cSrCRsNgn55RinZh_qrJI1T3k8pOM2eQQ64A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c01e51d73f8e593217fa171dd8cd764cce0e1eb2", "width": 1080, "height": 602}], "variants": {}, "id": "8yxiEYGgojstocdTycr8ijHgNQrlNxRkXdeF7f6weaw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ekhpd", "is_robot_indexable": true, "report_reasons": null, "author": "ayananda", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ekhpd/from_zero_to_hero_in_azure_data_explorer_key/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.brightly.fi/post/from-zero-to-hero-in-azure-data-explorer", "subreddit_subscribers": 144982, "created_utc": 1702148630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don't like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source CDC options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evjr2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don&amp;#39;t like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evjr2", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "subreddit_subscribers": 144982, "created_utc": 1702182830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.\n\nI\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.\n\nBut I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.", "author_fullname": "t2_4jozahcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimise my upserts - Azure Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18epq5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702163600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18epq5x", "is_robot_indexable": true, "report_reasons": null, "author": "anxiouscrimp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "subreddit_subscribers": 144982, "created_utc": 1702163600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHow do we calculate, budget, or project the server costs for a data science platform?\n\nWe are currently tasked with budgeting and projecting the server costs for our upcoming data science platform. I was wondering if you could provide some guidance or recommend any tools or best practices that would help us estimate these costs effectively.\n\nWe are currently using/testing AWS, Google Cloud, Hugging Face, Lambda, and OpenAI with a small amount of data, and the cost falls within the range of hundreds of dollars. However, we want to estimate the potential long-term costs we may incur.", "author_fullname": "t2_m0r3pnug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Calculating Server Costs for Data Science Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18e945b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702110658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do we calculate, budget, or project the server costs for a data science platform?&lt;/p&gt;\n\n&lt;p&gt;We are currently tasked with budgeting and projecting the server costs for our upcoming data science platform. I was wondering if you could provide some guidance or recommend any tools or best practices that would help us estimate these costs effectively.&lt;/p&gt;\n\n&lt;p&gt;We are currently using/testing AWS, Google Cloud, Hugging Face, Lambda, and OpenAI with a small amount of data, and the cost falls within the range of hundreds of dollars. However, we want to estimate the potential long-term costs we may incur.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18e945b", "is_robot_indexable": true, "report_reasons": null, "author": "Alertt_53", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18e945b/calculating_server_costs_for_data_science_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18e945b/calculating_server_costs_for_data_science_platform/", "subreddit_subscribers": 144982, "created_utc": 1702110658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?\n\nOne of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. \n\nIf anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.", "author_fullname": "t2_344hea7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evk3q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?&lt;/p&gt;\n\n&lt;p&gt;One of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. &lt;/p&gt;\n\n&lt;p&gt;If anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evk3q", "is_robot_indexable": true, "report_reasons": null, "author": "Ashu6410", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evk3q/database_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evk3q/database_optimization/", "subreddit_subscribers": 144982, "created_utc": 1702182871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\nSo I am in a contract job with a big company where I am responsible for taking the external data and upload it to snowflake. \nI want to automate the process and also make sure that I reduce the size of the data. \nHow can I go about this?\nI am told to you dask and convert the files to parquet. \nI am completely new to this. \nAll suggestions are welcome.", "author_fullname": "t2_8446gqlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice from the pros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18eu373", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702177565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,\nSo I am in a contract job with a big company where I am responsible for taking the external data and upload it to snowflake. \nI want to automate the process and also make sure that I reduce the size of the data. \nHow can I go about this?\nI am told to you dask and convert the files to parquet. \nI am completely new to this. \nAll suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18eu373", "is_robot_indexable": true, "report_reasons": null, "author": "Fair-Bed-5771", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18eu373/advice_from_the_pros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18eu373/advice_from_the_pros/", "subreddit_subscribers": 144982, "created_utc": 1702177565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nIm a junior DE working on implementing the medallion architecture in our current data platform. We have moved from ssis to adf/databricks. Essentially our notebooks contain logic (load, tranform, save in silver) for a specific table (bronze to silver) and we orchestrate these using adf activities. \n\nProblem is its getting to a point where the silver pipeline is just a bunch of unorganized databricks notebook activities. We do have a dynamic notebook activity that reads from a yaml file that contains a definition for all of our silver tables (columns and transformations) and iterates through every table.\n\nShould we just do one notebook per \u201clayer\u201d and have the columns definitions in a yaml file/config? Wouldn\u2019t this make it harder to debug when a table fails? Was thinking of having multiple pipelines per source (api/sql db) and target 1 notebook per \u201clayer\u201d. Is this the right path? Any help would be appreciated!", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating multiple notebooks in ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ek9ec", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702148006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Im a junior DE working on implementing the medallion architecture in our current data platform. We have moved from ssis to adf/databricks. Essentially our notebooks contain logic (load, tranform, save in silver) for a specific table (bronze to silver) and we orchestrate these using adf activities. &lt;/p&gt;\n\n&lt;p&gt;Problem is its getting to a point where the silver pipeline is just a bunch of unorganized databricks notebook activities. We do have a dynamic notebook activity that reads from a yaml file that contains a definition for all of our silver tables (columns and transformations) and iterates through every table.&lt;/p&gt;\n\n&lt;p&gt;Should we just do one notebook per \u201clayer\u201d and have the columns definitions in a yaml file/config? Wouldn\u2019t this make it harder to debug when a table fails? Was thinking of having multiple pipelines per source (api/sql db) and target 1 notebook per \u201clayer\u201d. Is this the right path? Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ek9ec", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18ek9ec/orchestrating_multiple_notebooks_in_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ek9ec/orchestrating_multiple_notebooks_in_adf/", "subreddit_subscribers": 144982, "created_utc": 1702148006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nI work as a data ingestion engineer where my roles most lies in creating connec\ntions to different systems and extracting the data, sources I have worked till now: SFTP, REST APIs, JDBS, ERPs etc.,\n\nPost that cleansing the data and moving the data for other teams to make use of and sometimes I work on building creating summarized datasets as well based on the requirements and I am bored of the is work and asked my PO to help me in transition to a different type of role and asked for data science and he is like you might masters or phd to get the good use/ making meaningful work in data science and later asked me to think of different area of work in data domain which excites me and now I am lost, can anyone guide me finding different aspects of data world which would help me in taking a decision.\n\nThanks in advance.", "author_fullname": "t2_afx451tf0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need career advice!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ek7ap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702147839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I work as a data ingestion engineer where my roles most lies in creating connec\ntions to different systems and extracting the data, sources I have worked till now: SFTP, REST APIs, JDBS, ERPs etc.,&lt;/p&gt;\n\n&lt;p&gt;Post that cleansing the data and moving the data for other teams to make use of and sometimes I work on building creating summarized datasets as well based on the requirements and I am bored of the is work and asked my PO to help me in transition to a different type of role and asked for data science and he is like you might masters or phd to get the good use/ making meaningful work in data science and later asked me to think of different area of work in data domain which excites me and now I am lost, can anyone guide me finding different aspects of data world which would help me in taking a decision.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18ek7ap", "is_robot_indexable": true, "report_reasons": null, "author": "SeaWind5021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ek7ap/need_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ek7ap/need_career_advice/", "subreddit_subscribers": 144982, "created_utc": 1702147839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short: I'm from Spain and would love to work remotely for a company based in the UK, but I don't know if I'm prepared culturally nor technically.\n\nExperience:\n- 1.5 years as data engineer\n- 1.5 years as data scientist\n\nTechnologies:\n- Python (pro level)\n- SQL (I would say mid level)\n- Azure (noob but on my way to get the DP-203 certification)\n- AWS (I use lambdas daily at my current job. But that's all)\n- GCP (I have some cloud functions running for personal projects)\n\nLanguages:\n- Spanish (native)\n- English (C1 level. Great reading and writing skills, but speaking is rusty)\n\nFor the cultural part, I don't know if job hunting is a lot different from Spain and recruiters behave differently if that makes sense \ud83d\ude02.\n\nAny advice/thoughts on this?\n\nThank you all in advance!", "author_fullname": "t2_p4nzh8v1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to land a job in the UK?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18egjt5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702137576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short: I&amp;#39;m from Spain and would love to work remotely for a company based in the UK, but I don&amp;#39;t know if I&amp;#39;m prepared culturally nor technically.&lt;/p&gt;\n\n&lt;p&gt;Experience:\n- 1.5 years as data engineer\n- 1.5 years as data scientist&lt;/p&gt;\n\n&lt;p&gt;Technologies:\n- Python (pro level)\n- SQL (I would say mid level)\n- Azure (noob but on my way to get the DP-203 certification)\n- AWS (I use lambdas daily at my current job. But that&amp;#39;s all)\n- GCP (I have some cloud functions running for personal projects)&lt;/p&gt;\n\n&lt;p&gt;Languages:\n- Spanish (native)\n- English (C1 level. Great reading and writing skills, but speaking is rusty)&lt;/p&gt;\n\n&lt;p&gt;For the cultural part, I don&amp;#39;t know if job hunting is a lot different from Spain and recruiters behave differently if that makes sense \ud83d\ude02.&lt;/p&gt;\n\n&lt;p&gt;Any advice/thoughts on this?&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18egjt5", "is_robot_indexable": true, "report_reasons": null, "author": "data_macrolide", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18egjt5/how_to_land_a_job_in_the_uk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18egjt5/how_to_land_a_job_in_the_uk/", "subreddit_subscribers": 144982, "created_utc": 1702137576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am currently aiming to become a data engineer. (currently I am a 4th year student and interning at a DE position). I'm wondering whether I should buy the 8gb/512 16gb/256 16gb/512 m2 series or should I buy the m3. Am I really wondering? Can you explain it to me?", "author_fullname": "t2_8ybn4z2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I choose the macbook pro version?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18efcs2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702134076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am currently aiming to become a data engineer. (currently I am a 4th year student and interning at a DE position). I&amp;#39;m wondering whether I should buy the 8gb/512 16gb/256 16gb/512 m2 series or should I buy the m3. Am I really wondering? Can you explain it to me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18efcs2", "is_robot_indexable": true, "report_reasons": null, "author": "TrainerLongjumping17", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18efcs2/how_should_i_choose_the_macbook_pro_version/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18efcs2/how_should_i_choose_the_macbook_pro_version/", "subreddit_subscribers": 144982, "created_utc": 1702134076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading data to Cloud SQL directly instead of routing via GCS - What are trade offs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ec31z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702122953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ec31z", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ec31z/loading_data_to_cloud_sql_directly_instead_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ec31z/loading_data_to_cloud_sql_directly_instead_of/", "subreddit_subscribers": 144982, "created_utc": 1702122953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know of one? Particularly anything written on top of Azure Durable Functions.", "author_fullname": "t2_noyg0bcjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dotnet equivalent of prefect/dragster/etc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ebykn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702122457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of one? Particularly anything written on top of Azure Durable Functions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ebykn", "is_robot_indexable": true, "report_reasons": null, "author": "_chipper____", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ebykn/dotnet_equivalent_of_prefectdragsteretc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ebykn/dotnet_equivalent_of_prefectdragsteretc/", "subreddit_subscribers": 144982, "created_utc": 1702122457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nHello All, We want some guidance, if the below design looks okay for our customer use case.\n\nWe currently have financial systems transaction data streams to Oracle exadata(X9) on-premise. This database supports processing of 400million transactions per day. A single transaction for us is a combination of 7-8 inserts into different transaction tables with Indexes , unique constraints etc defined on those. The transactions processed/committed in batches(\\~1000 batch size) in the database. And this system persists data for \\~6 months. We do have all sorts of OLAP(daily/monthly batch reports running) applications run on the same database along with some user facing UI applications showing customer transactions. So it's basically currently serving a hybrid workload and is one stop solution for all use cases.\n\nMany of the applications are moving from on premise to AWS cloud as part of modernization journey and AWS being chosen cloud partner also the product is expected to expand across more regions and this system is expected to serve increase in the transaction volume. And also we have a requirement to persist transaction data for \\~10years to have those available for analytics/data science use cases.\n\nSo the team is thinking of splitting it into two parts 1)OLTP type use case in which we will persist/write the transaction data faster and show it to the UI related apps , in near real time/quickest possible time. and this database will store Max 60-90 days of transaction data. Not sure if we have an option of Oracle exadata equivalent on AWS, so team planning of using/experimenting with Aurora postgres. Please correct me, if there are any other options we should use otherwise?\n\n2)Then move the data beyond \\~90 days into another database or object storage S3 which will keep it there for \\~10 years and will be queryable using the necessary API's. That is supposed to cater to Olap/analytics/data science use cases etc.\n\nIs the above design is okay? and also in regards to the second point above i.e. persisting the historical data (that to be in queryable state), should we go for some database like snowflake or should just keep it on S3 as is and make those queryable through APIs. Please advice?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_cxqpzxgel", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Design related question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ebser", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702121763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hello All, We want some guidance, if the below design looks okay for our customer use case.&lt;/p&gt;\n\n&lt;p&gt;We currently have financial systems transaction data streams to Oracle exadata(X9) on-premise. This database supports processing of 400million transactions per day. A single transaction for us is a combination of 7-8 inserts into different transaction tables with Indexes , unique constraints etc defined on those. The transactions processed/committed in batches(~1000 batch size) in the database. And this system persists data for ~6 months. We do have all sorts of OLAP(daily/monthly batch reports running) applications run on the same database along with some user facing UI applications showing customer transactions. So it&amp;#39;s basically currently serving a hybrid workload and is one stop solution for all use cases.&lt;/p&gt;\n\n&lt;p&gt;Many of the applications are moving from on premise to AWS cloud as part of modernization journey and AWS being chosen cloud partner also the product is expected to expand across more regions and this system is expected to serve increase in the transaction volume. And also we have a requirement to persist transaction data for ~10years to have those available for analytics/data science use cases.&lt;/p&gt;\n\n&lt;p&gt;So the team is thinking of splitting it into two parts 1)OLTP type use case in which we will persist/write the transaction data faster and show it to the UI related apps , in near real time/quickest possible time. and this database will store Max 60-90 days of transaction data. Not sure if we have an option of Oracle exadata equivalent on AWS, so team planning of using/experimenting with Aurora postgres. Please correct me, if there are any other options we should use otherwise?&lt;/p&gt;\n\n&lt;p&gt;2)Then move the data beyond ~90 days into another database or object storage S3 which will keep it there for ~10 years and will be queryable using the necessary API&amp;#39;s. That is supposed to cater to Olap/analytics/data science use cases etc.&lt;/p&gt;\n\n&lt;p&gt;Is the above design is okay? and also in regards to the second point above i.e. persisting the historical data (that to be in queryable state), should we go for some database like snowflake or should just keep it on S3 as is and make those queryable through APIs. Please advice?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ebser", "is_robot_indexable": true, "report_reasons": null, "author": "Upper-Lifeguard-8478", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ebser/design_related_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ebser/design_related_question/", "subreddit_subscribers": 144982, "created_utc": 1702121763.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}