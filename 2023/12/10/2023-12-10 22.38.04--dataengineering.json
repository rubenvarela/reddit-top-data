{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for the noob question if it sounds ignorant. Here is the situation. \n\nI joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.\n\n```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame\n\ndf.loc[df['value']&gt;20] # Pandas form\n\nquery:str='SELECT * FROM df WHERE value&gt;20;'\nduckdb.sql(query=query).df() # SQL Form\n```\n\nSo it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. \n\nSo the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?", "author_fullname": "t2_kkymhi5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DuckDB Used For?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f0tju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702204519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the noob question if it sounds ignorant. Here is the situation. &lt;/p&gt;\n\n&lt;p&gt;I joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.&lt;/p&gt;\n\n&lt;p&gt;```python3\nimport duckdb, pandas as pd\ndf:pd.DataFrame&lt;/p&gt;\n\n&lt;p&gt;df.loc[df[&amp;#39;value&amp;#39;]&amp;gt;20] # Pandas form&lt;/p&gt;\n\n&lt;p&gt;query:str=&amp;#39;SELECT * FROM df WHERE value&amp;gt;20;&amp;#39;\nduckdb.sql(query=query).df() # SQL Form\n```&lt;/p&gt;\n\n&lt;p&gt;So it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. &lt;/p&gt;\n\n&lt;p&gt;So the basic questions are \n* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? \n* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f0tju", "is_robot_indexable": true, "report_reasons": null, "author": "SpiderMangauntlet", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/", "subreddit_subscribers": 145132, "created_utc": 1702204519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn't made those decisions, that's tended to be a cto-level decision at companies I've worked at, even de managers aren't making those calls. The other similar line was around how what the interviewer called the \"operational layer\" and \"analytics layer\" should be stored. Same story I've never been involved in those decisions but I communicated that obviously we'd want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  \n\n\nI don't think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don't even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I'm a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that's literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!", "author_fullname": "t2_sg5v2ix5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downleveled for Data Governance, How to Study", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evqnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702183547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;10 yoe, senior data engineer at a tech startup, I just finished an interview process with a big tech company for a lead data engineer role and got to the final round which included a coding round, case round, and behavioral round. I got the feedback at the end that I crushed the coding and behavioral rounds but performed poorly on the case round due to my lack of data governance knowledge. Questions they asked were around things like cloud storage retention policies and I communicated that I knew the tradeoffs were generally around storage cost vs access cost so stuff that was only available for audit purposes or other logs unlikely to be used should be in lower storage tiers, but was very honest that I hadn&amp;#39;t made those decisions, that&amp;#39;s tended to be a cto-level decision at companies I&amp;#39;ve worked at, even de managers aren&amp;#39;t making those calls. The other similar line was around how what the interviewer called the &amp;quot;operational layer&amp;quot; and &amp;quot;analytics layer&amp;quot; should be stored. Same story I&amp;#39;ve never been involved in those decisions but I communicated that obviously we&amp;#39;d want some sort of layer of more raw data in a more raw storage and then a layer more optimized for analytics, this company uses GCP so it would be bigquery in that example. I also discussed personalized data and the potential need for multiple layers some including no pii and others that only few could access for audit purposes including that pii.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think they were lying to me about the feedback because I did get an offer but it was for a senior role which would have been a pay cut, and I don&amp;#39;t even fully disagree with them on the feedback because I knew this role was a stretch when applying for it in the first place I think I&amp;#39;m a strong senior but a lead role is still a bit of a stretch. But at my current org I have no opportunities to be involved in data governance that&amp;#39;s literally a C-suite decision, and anything I did for a personal project would be just me guessing at stuff and probably getting more wrong than right. Are there any maybe textbooks or articles I could read or courses I could take that could help me learn and improve my data governance knowledge? My current company has a learning budget so it could definitely include paid courses. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18evqnt", "is_robot_indexable": true, "report_reasons": null, "author": "BoysenberryLanky6112", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evqnt/downleveled_for_data_governance_how_to_study/", "subreddit_subscribers": 145132, "created_utc": 1702183547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.\n\nEncrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage PII?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7m1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702226742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Particularly interested in data lake / lakehouse governance cases when replicating a large number of operational source tables with PII columns randomly mixed in throughout.&lt;/p&gt;\n\n&lt;p&gt;Encrypt/hash? What do you do for legitimate queries of PII by approved users? Replacing column data with surrogate key values and routing to separate PII storage? Simply hiding by tagging (looks like the only out of the box solution on AWS S3 with Lake Formation)? Recommended best practices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f7m1t", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7m1t/how_do_you_manage_pii/", "subreddit_subscribers": 145132, "created_utc": 1702226742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will soon start working with Delta lakes, and would like to know a bit about people's experiences with it.\n\nAny main reasons why you're using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) \n\nThanks!", "author_fullname": "t2_m84na74fe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Delta Lake, pros, cons etc...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f3wo9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702215831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will soon start working with Delta lakes, and would like to know a bit about people&amp;#39;s experiences with it.&lt;/p&gt;\n\n&lt;p&gt;Any main reasons why you&amp;#39;re using it? did it fulfill your data needs? was it worth the time investment? Please feel free to share everything :) &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f3wo9", "is_robot_indexable": true, "report_reasons": null, "author": "Jazzlike-Change8493", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f3wo9/thoughts_on_delta_lake_pros_cons_etc/", "subreddit_subscribers": 145132, "created_utc": 1702215831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone\n\nRecently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?", "author_fullname": "t2_gci8d6f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone here that works as data engineer in Canada as an immigrant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f4upz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702218794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone&lt;/p&gt;\n\n&lt;p&gt;Recently I have been thinking a lot to move to another country, specially Canada. So I wonder how is the job market for data engineers in Canada? How hard would be for a foreigner to landing a job? Anyone here that lives and work in Canada as a immigrant that can share some tips to move there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f4upz", "is_robot_indexable": true, "report_reasons": null, "author": "Entropico_88", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f4upz/anyone_here_that_works_as_data_engineer_in_canada/", "subreddit_subscribers": 145132, "created_utc": 1702218794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All,\n\nI was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don't like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source CDC options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evjr2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I was looking for some open source CDC options to implement in my company. I am the only data engineer and the way I am handling CDC is right now is by using watermarks for the delta load and using change tables whenever we need to store the deltas for further use in the same or other pipelines. When it comes to hard deletes at the source I am doing a lookup to check what has been deleted and marking them as deleted using a isDeleted flag. Our data is not that large so this is fine for now but I don&amp;#39;t like the way I am handling deletes, it is very bad and I am looking for some open source CDC options that I can use. I am considering debezium but was wondering if it is a good choice or are there any other alternatives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evjr2", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evjr2/open_source_cdc_options/", "subreddit_subscribers": 145132, "created_utc": 1702182830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ibis ([https://ibis-project.org/](https://ibis-project.org/)) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.\n\nIt looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.\n\nHowever, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?\n\nIbis' documentation is relatively limited and I haven't found much information about what organizations back this project.", "author_fullname": "t2_7v7x1pqr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Ibis reliably be used in production?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18fcbf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702239463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ibis (&lt;a href=\"https://ibis-project.org/\"&gt;https://ibis-project.org/&lt;/a&gt;) is a dataframe API that can interface with a variety of backends, including duckdb, Spark, pandas, etc.&lt;/p&gt;\n\n&lt;p&gt;It looks very convenient, as it provides a shared pandas-like dataframe API that will be familiar to data scientists. For instance, without Ibis, working with duckdb in Python would require using SQLAlchemy, a query constructor, or raw SQL. With Ibis, you get pandas-like data manipulation verbs, and you can more easily change your backend in the future if needed.&lt;/p&gt;\n\n&lt;p&gt;However, is Ibis reliable enough for relatively low-stakes production applications, like offline and time-insensitive data processing pipelines?&lt;/p&gt;\n\n&lt;p&gt;Ibis&amp;#39; documentation is relatively limited and I haven&amp;#39;t found much information about what organizations back this project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fcbf0", "is_robot_indexable": true, "report_reasons": null, "author": "cantthinkofoneuse", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fcbf0/can_ibis_reliably_be_used_in_production/", "subreddit_subscribers": 145132, "created_utc": 1702239463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do the two compare w.r.t:\n1. Pay\n2. Career growth\n\nI have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.", "author_fullname": "t2_9815cpn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering vs Tech Consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f15jg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702205920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do the two compare w.r.t:\n1. Pay\n2. Career growth&lt;/p&gt;\n\n&lt;p&gt;I have 9 years of experience. Currently enrolled in the EPGP (MBA) program at IIM Kozhikode.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18f15jg", "is_robot_indexable": true, "report_reasons": null, "author": "brokeRichieRich", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f15jg/data_engineering_vs_tech_consulting/", "subreddit_subscribers": 145132, "created_utc": 1702205920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?\n\nOne of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. \n\nIf anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.", "author_fullname": "t2_344hea7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18evk3q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702182871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my work place we use django and postgresql  for the backend. We have all the tables, functions and stored procedures messed up. The database design has all the relations covered using foreign keys etc., For API calls we are using functions which return json documents. What i observed was the APIs are taking a lot more time to load on the frontend. The DB function calls are taking a long time even with the pagination. The functions had complex joins on many tables. How do i optimize these kind of complex functions ?&lt;/p&gt;\n\n&lt;p&gt;One of the answers I found online was to use indexing on the joining keys. What is Indexing and how can i use it to optimize the joins in postgresql. &lt;/p&gt;\n\n&lt;p&gt;If anyone has an idea of indexing or any other methods to optimize I would be grateful if anyone help me out thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18evk3q", "is_robot_indexable": true, "report_reasons": null, "author": "Ashu6410", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18evk3q/database_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18evk3q/database_optimization/", "subreddit_subscribers": 145132, "created_utc": 1702182871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It is for building AI (into your) apps easily by integrating AI at the data's source.\n\nNot another database, but rather making your existing favorite database intelligent/super-duper (funny name for serious tech); think:\u00a0`db = superduper(your_database)`\n\nCurrently supported databases: MongoDB, Postgres, MySQL, S3, DuckDB, SQLite, Snowflake, BigQuery, ClickHouse and more.\n\nDefinitely check it out:\u00a0[https://github.com/SuperDuperDB/superduperdb](https://github.com/SuperDuperDB/superduperdb)", "author_fullname": "t2_sj4cy7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trending on GitHub top 10 for the 4th day in a row: Open-source framework for integrating AI with major databases, to elimitate the need to move data into complex pipelines and specialized vector dbs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fdaoz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702242870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1702242089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It is for building AI (into your) apps easily by integrating AI at the data&amp;#39;s source.&lt;/p&gt;\n\n&lt;p&gt;Not another database, but rather making your existing favorite database intelligent/super-duper (funny name for serious tech); think:\u00a0&lt;code&gt;db = superduper(your_database)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently supported databases: MongoDB, Postgres, MySQL, S3, DuckDB, SQLite, Snowflake, BigQuery, ClickHouse and more.&lt;/p&gt;\n\n&lt;p&gt;Definitely check it out:\u00a0&lt;a href=\"https://github.com/SuperDuperDB/superduperdb\"&gt;https://github.com/SuperDuperDB/superduperdb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?auto=webp&amp;s=396409da2da4f40baf19284bab4053dcc28afa4e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe78d32a43569da51807866133ba782442e90225", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fd324f44ce51883fe61257857b78805f8cdee76", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=28fb518349163e869c86486384dffa06ea18c4b1", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9256e4c6ce588d64e4a24b44d0d026205943ff9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=98c95689491edbc6246a40886aa2b9bb47a9de8c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6QlF-fcns_8JEM8aqZb1K110slWxYPXDHSs7GvEpDvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bebf450f0241110b243b9f76ca84cea2174b9da2", "width": 1080, "height": 540}], "variants": {}, "id": "Dm5OzYBAS-qzFRjPkJ4vzFTkoenLemIsEb4OJX6z6FQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18fdaoz", "is_robot_indexable": true, "report_reasons": null, "author": "escalize", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fdaoz/trending_on_github_top_10_for_the_4th_day_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fdaoz/trending_on_github_top_10_for_the_4th_day_in_a/", "subreddit_subscribers": 145132, "created_utc": 1702242089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.\n\nI\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.\n\nBut I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.", "author_fullname": "t2_4jozahcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimise my upserts - Azure Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18epq5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702163600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve built some Azure synapse pipelines which are using the upsert copy function to move data from staging tables into my data warehouse tables. But it seems to be slightly slower than I expect. For example - the stock table has a bit less than 100m rows and it takes around 18 minutes to load another day of data - roughly 300k records. This seems like quite a long time to me.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m toying with deleting any records from the dwh table that exists in staging and then just copying them across. That should be quicker.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019m wondering if there is either a better approach, or a better way to optimise this? I feel like I am missing something.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18epq5x", "is_robot_indexable": true, "report_reasons": null, "author": "anxiouscrimp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18epq5x/optimise_my_upserts_azure_synapse/", "subreddit_subscribers": 145132, "created_utc": 1702163600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nYou know what they say, \u201cculture eats strategy for breakfast.\u201d\n\nI\u2019m in a position where my success can be measured through reform. Our data infrastructure contains nontechnical humans connecting systems together with spreadsheets and several hours of manual labor. Or, we have procedures in place where 6-7 people will \u201cgo get the data\u201d and dump it into a folder where an 8th guy can spend hours/days reading through it all and structuring it into a report. These parts of the infrastructure are critical at the moment.\n\nMy job is basically to introduce good habits and good systems so that the business becomes more efficient. For example, I\u2019ve been working on a system that collects data from various sources and automatically generates one of there datasets in a clean and consistent format\u2014 without waiting several days for it to happen.\n\nI\u2019ve also been working on webapps, redesigning non-scalable schedule management systems, the cloud network to host all of this, generic self-hosted analytics platforms\u2026 the list goes on.\n\nSomething I\u2019ve noticed is that high quality solutions don\u2019t necessarily drive adoption. Great technical solutions can fail for cultural reasons and terrible solutions can rise up in their place. This is weird to me, but it begs the question: what actually drives high adoption rates?\n\nMy theory is that it\u2019s good politics. Nonetheless, Id like to hear from the community. What patterns have you noticed trend alongside high adoption rates? What patterns have you seen kill adoption?\n\nThanks for any feedback everyone! Really curious to see everyone\u2019s insights on this.", "author_fullname": "t2_uqm6fk35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What sort of tips do you have for driving high adoption rates for new internal tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18fdh5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1702245762.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702242572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You know what they say, \u201cculture eats strategy for breakfast.\u201d&lt;/p&gt;\n\n&lt;p&gt;I\u2019m in a position where my success can be measured through reform. Our data infrastructure contains nontechnical humans connecting systems together with spreadsheets and several hours of manual labor. Or, we have procedures in place where 6-7 people will \u201cgo get the data\u201d and dump it into a folder where an 8th guy can spend hours/days reading through it all and structuring it into a report. These parts of the infrastructure are critical at the moment.&lt;/p&gt;\n\n&lt;p&gt;My job is basically to introduce good habits and good systems so that the business becomes more efficient. For example, I\u2019ve been working on a system that collects data from various sources and automatically generates one of there datasets in a clean and consistent format\u2014 without waiting several days for it to happen.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve also been working on webapps, redesigning non-scalable schedule management systems, the cloud network to host all of this, generic self-hosted analytics platforms\u2026 the list goes on.&lt;/p&gt;\n\n&lt;p&gt;Something I\u2019ve noticed is that high quality solutions don\u2019t necessarily drive adoption. Great technical solutions can fail for cultural reasons and terrible solutions can rise up in their place. This is weird to me, but it begs the question: what actually drives high adoption rates?&lt;/p&gt;\n\n&lt;p&gt;My theory is that it\u2019s good politics. Nonetheless, Id like to hear from the community. What patterns have you noticed trend alongside high adoption rates? What patterns have you seen kill adoption?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any feedback everyone! Really curious to see everyone\u2019s insights on this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18fdh5y", "is_robot_indexable": true, "report_reasons": null, "author": "DuckDatum", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18fdh5y/what_sort_of_tips_do_you_have_for_driving_high/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18fdh5y/what_sort_of_tips_do_you_have_for_driving_high/", "subreddit_subscribers": 145132, "created_utc": 1702242572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is using Azure Databricks and I'm an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a \"/FileStore/file\\_path/file\\_name.xlsx\" file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn't really helped me up to this point.\n\n`w = WorkspaceClient(host=server_hostname,token = api_key)`\n\nI can successfully run `w.dbfs.download('/FileStore/file_path/file_name.xlsx')`, but can't figure out how to save the file down to local storage as an Excel file.\n\n`w.dbutils.fs.cp('dbfs:/FileStore/file_path/file_name.xlsx', 'C:/Users/user_name/file_path/file_name.xlsx')` results in \"java.net.URISyntaxException: Relative path in absolute URI\" for the C drive file path. This looks like an absolute path to me. Am I missing something?\n\n`w.files.get_status('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"Get-status API is not enabled.\" How do I enable it? I don't see a toggle for it in Admin Settings.\n\n`w.files.download('dbfs:/Volumes/FileStore/file_path/file_name.xlsx')` results in \"The ':' character is not supported\", but when I remove it the file path is no longer valid.\n\nI feel like I'm close on one of these but just can't seem to get it across the finish line.", "author_fullname": "t2_yk6x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone moved files between Azure Databricks DBFS and local storage using the Databricks API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18f7vcb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1702227447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is using Azure Databricks and I&amp;#39;m an administrator. I am trying to allow our devs to programmatically connect to the API using personal access token for push / pull ability in DBFS up and down from local storage. I have been able to set up a token for myself and successfully explore the dbutils.fs.ls using a &amp;quot;/FileStore/file_path/file_name.xlsx&amp;quot; file path structure on my local machine in Python (outside of Azure Databricks using an IDE). We need to be able to move the full file back and forth because each one is a formatted Excel file and needs to be maintained exactly as is. The Databricks API documentation hasn&amp;#39;t really helped me up to this point.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w = WorkspaceClient(host=server_hostname,token = api_key)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I can successfully run &lt;code&gt;w.dbfs.download(&amp;#39;/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt;, but can&amp;#39;t figure out how to save the file down to local storage as an Excel file.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.dbutils.fs.cp(&amp;#39;dbfs:/FileStore/file_path/file_name.xlsx&amp;#39;, &amp;#39;C:/Users/user_name/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;java.net.URISyntaxException: Relative path in absolute URI&amp;quot; for the C drive file path. This looks like an absolute path to me. Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.get_status(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;Get-status API is not enabled.&amp;quot; How do I enable it? I don&amp;#39;t see a toggle for it in Admin Settings.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;w.files.download(&amp;#39;dbfs:/Volumes/FileStore/file_path/file_name.xlsx&amp;#39;)&lt;/code&gt; results in &amp;quot;The &amp;#39;:&amp;#39; character is not supported&amp;quot;, but when I remove it the file path is no longer valid.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m close on one of these but just can&amp;#39;t seem to get it across the finish line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18f7vcb", "is_robot_indexable": true, "report_reasons": null, "author": "CurlyW15", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18f7vcb/has_anyone_moved_files_between_azure_databricks/", "subreddit_subscribers": 145132, "created_utc": 1702227447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_p5gqem6h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please Stop Using Google Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_18f38wx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.46, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/18f38wx", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/MscxPo2ozHM1_dkAkbfqJDEUuZR-KsA4pgLKk8yVF38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1702213632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?auto=webp&amp;s=99f9cf88fbf8b204bdfb554c5686774edfb04ac9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c862112d73985f4a7ca6e8d7d7f8e854eb7a2ddf", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8852ce395422244d48cb3c7045dd899e496c484f", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Mf_wE1kM6_N-5vKrnfK0jWSoB6DejMv1BcfCkss2f4E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6073c63ec3325df2713fd5c40a4a8f12c62edc9", "width": 320, "height": 240}], "variants": {}, "id": "_Oc0viLfsdEgxq9iRXrD-Gc4J8e4EtHURDly-KXakWM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18f38wx", "is_robot_indexable": true, "report_reasons": null, "author": "MySpermIs-Unvaxxd-01", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18f38wx/please_stop_using_google_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=6xXSsu0YXWo", "subreddit_subscribers": 145132, "created_utc": 1702213632.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Please Stop Using Google Analytics", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/6xXSsu0YXWo?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Please Stop Using Google Analytics\"&gt;&lt;/iframe&gt;", "author_name": "Theo - t3\u2024gg", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/6xXSsu0YXWo/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@t3dotgg"}}, "is_video": false}}], "before": null}}