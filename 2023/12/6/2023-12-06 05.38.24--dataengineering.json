{"kind": "Listing", "data": {"after": "t3_18bgn7k", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the [Metabase Community Data Stack Report](https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house) 31% of responders said they're using in-house ingestion. \n\nWhy do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? ", "author_fullname": "t2_e5fjdth0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do companies still build data ingestion tooling instead of using a third-party tool like Airbyte?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bi2xj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701799473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the &lt;a href=\"https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house\"&gt;Metabase Community Data Stack Report&lt;/a&gt; 31% of responders said they&amp;#39;re using in-house ingestion. &lt;/p&gt;\n\n&lt;p&gt;Why do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?auto=webp&amp;s=a918001adbcbdf0507b477cb314c79c6fa6890b2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=beab2fb16fa06d31be725b3c39e0cebf62af8b6a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c0012b27a40b3722d7239204936cbc7d8a0f0ac", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1430fd4f043f9832a6f3740c48f801c8d56c6fb7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e76e9eb0b2f08f73ef01612cdc9b63d7b480f92", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d75bc2370d5dde94891301fb285d00362f9374fe", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07791994b1d6632f6a5c11d1fed9a8e6481756a5", "width": 1080, "height": 564}], "variants": {}, "id": "ftoBvI6rzzCZb3wJpMIOqE5SrDrO0vg2LvvVJ6z3_GE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bi2xj", "is_robot_indexable": true, "report_reasons": null, "author": "Miserable_Fold4086", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "subreddit_subscribers": 144081, "created_utc": 1701799473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data \"activation.\"\n\nWhat are the new trends/buzzwords that the data industry will lean into this year? ", "author_fullname": "t2_nypbpupk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data trends for 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhf64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data &amp;quot;activation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What are the new trends/buzzwords that the data industry will lean into this year? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bhf64", "is_robot_indexable": true, "report_reasons": null, "author": "Techxpeare", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "subreddit_subscribers": 144081, "created_utc": 1701797755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a scenario where we are receiving information on a Kafka Topic and are decomposing the complex message structure and storing the records to a relational DB. We originally tried using jq to decompose the json messages but we are not seeing good speeds. \n\nIn your opinion is it possible to have a performant pipeline in Python or will Java be be required for such a task?", "author_fullname": "t2_5ts1q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Near Real Time Ingestion to DB using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18be34k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701789322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a scenario where we are receiving information on a Kafka Topic and are decomposing the complex message structure and storing the records to a relational DB. We originally tried using jq to decompose the json messages but we are not seeing good speeds. &lt;/p&gt;\n\n&lt;p&gt;In your opinion is it possible to have a performant pipeline in Python or will Java be be required for such a task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18be34k", "is_robot_indexable": true, "report_reasons": null, "author": "lspdv18", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18be34k/near_real_time_ingestion_to_db_using_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18be34k/near_real_time_ingestion_to_db_using_python/", "subreddit_subscribers": 144081, "created_utc": 1701789322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n0ywlxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-Time ML: A Collection of 300+ Case Studies, each tagged properly and accompanied by engineering blogs for a deep dive. Personally, I am exploring these to understand not just the ML models but also the data infra supporting these systems, highlighting the growing significance of streaming.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18b9q56", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hqOeBTd9REnubNF9BjnEsrwznp8PPKnkQABYRNP8YYg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701774737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "evidentlyai.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.evidentlyai.com/ml-system-design", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?auto=webp&amp;s=2a960a10bcb94146d03fd9340e48df7eeee3578d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=300ba58fa86e509ec4080507dc1a7185c3a2fefb", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=592303c0d3d63b69a1c3b66a7964b3804c7846f3", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a770c418fa19a60c0fde7b56a23aa502937db490", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a60fa146ad2523066d0edf58146b142f1c8ee5c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=71b854a40b256c24dbd38099c7b7b1742373afc9", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/JRa-2yyxrO6lxdMua6O5PfOIzFc1kkgt2V-ZdsQtj7I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96190d3785e9efa4720c498c90523d2c4836e9db", "width": 1080, "height": 564}], "variants": {}, "id": "jqoclQmWoAxFYjwbGE7GQt9lG-yEiYAl5EtPWQXQK84"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18b9q56", "is_robot_indexable": true, "report_reasons": null, "author": "muditjps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b9q56/realtime_ml_a_collection_of_300_case_studies_each/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.evidentlyai.com/ml-system-design", "subreddit_subscribers": 144081, "created_utc": 1701774737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.\n\nto provide a brief overview, this project  Have the following components : \n\n1. Real time that the processing and analysis :\n\nI have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.\n\n2.data warehousing: \n\nthe next closest step involved, setting up a robust data Warehouse  solution .\n \nThank you  you in advance for your time and assistance", "author_fullname": "t2_e1i3duqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18blijw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701808243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.&lt;/p&gt;\n\n&lt;p&gt;to provide a brief overview, this project  Have the following components : &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Real time that the processing and analysis :&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.&lt;/p&gt;\n\n&lt;p&gt;2.data warehousing: &lt;/p&gt;\n\n&lt;p&gt;the next closest step involved, setting up a robust data Warehouse  solution .&lt;/p&gt;\n\n&lt;p&gt;Thank you  you in advance for your time and assistance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18blijw", "is_robot_indexable": true, "report_reasons": null, "author": "ryan7ait", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18blijw/dear_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18blijw/dear_data_engineers/", "subreddit_subscribers": 144081, "created_utc": 1701808243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started my data career as DataOps, 5 months ago. I just learned excel, SQL, and Python barely a year in and Im already being pushed to be the acting data engineer for a big backend dev project. The project was forced on me because the old backend which I was originally tasked to maintain is not flexible to the files provided. Altho it is an opportunity, it is stressful since the backend needs to deliver monthly data submissions. I am pressured to develop the new backend and  submit data every month. Ive been forced to work during the wkends and OT for the past 2-3wks because of this project. Worst of all the data engineers given me for assistance are new hires and I need to guide them in every process of the development (ALL AS A NEW DATA OPS). Please send advice and prayers. Thanks.", "author_fullname": "t2_ie84p724", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Off my chest: Is this normal?? Please send advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b7m3o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701765384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started my data career as DataOps, 5 months ago. I just learned excel, SQL, and Python barely a year in and Im already being pushed to be the acting data engineer for a big backend dev project. The project was forced on me because the old backend which I was originally tasked to maintain is not flexible to the files provided. Altho it is an opportunity, it is stressful since the backend needs to deliver monthly data submissions. I am pressured to develop the new backend and  submit data every month. Ive been forced to work during the wkends and OT for the past 2-3wks because of this project. Worst of all the data engineers given me for assistance are new hires and I need to guide them in every process of the development (ALL AS A NEW DATA OPS). Please send advice and prayers. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18b7m3o", "is_robot_indexable": true, "report_reasons": null, "author": "awKnoodle", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b7m3o/off_my_chest_is_this_normal_please_send_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b7m3o/off_my_chest_is_this_normal_please_send_advice/", "subreddit_subscribers": 144081, "created_utc": 1701765384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What data controls do you implement day to day?\n\nMy project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a \"get it done by any means\" direction due to a strict deadline, which although doesn't excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.\n\nI currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on\n\nThe audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn't changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server\n\nIs this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.", "author_fullname": "t2_79a43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Controls", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmawo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What data controls do you implement day to day?&lt;/p&gt;\n\n&lt;p&gt;My project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a &amp;quot;get it done by any means&amp;quot; direction due to a strict deadline, which although doesn&amp;#39;t excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.&lt;/p&gt;\n\n&lt;p&gt;I currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on&lt;/p&gt;\n\n&lt;p&gt;The audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn&amp;#39;t changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server&lt;/p&gt;\n\n&lt;p&gt;Is this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bmawo", "is_robot_indexable": true, "report_reasons": null, "author": "Stychey", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmawo/data_controls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmawo/data_controls/", "subreddit_subscribers": 144081, "created_utc": 1701810230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this list the other night. It has a ton of good resources for working with dbt\n\n&amp;#x200B;\n\n[https://github.com/Hiflylabs/awesome-dbt](https://github.com/Hiflylabs/awesome-dbt)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good list of dbt tools/resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhcp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this list the other night. It has a ton of good resources for working with dbt&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Hiflylabs/awesome-dbt\"&gt;https://github.com/Hiflylabs/awesome-dbt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?auto=webp&amp;s=176e13ff72d3539d5767f43a8afe1a74ff147c22", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dd3283325149ef5d275d3a81fa389126e63f6dc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a4f30011f8a49d95b307b2a23ddfe55cd2d0ac6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f50ff1fd15562d44a72d1c9e9ef3501d106e438", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c4aa808f0987e4fb5ae998545416dddb38f7f93", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57cb9b22361687461d619af9ed1204a212844246", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f5895bf2b2b84890a02c2022006d133e6cc9723", "width": 1080, "height": 540}], "variants": {}, "id": "-hQXbWYxMy1LKsqRjHBv0nu0FBgk3UtXPqm_FMKcUbE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18bhcp4", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "subreddit_subscribers": 144081, "created_utc": 1701797581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI hope you\u2019re fine. \n\nRight now, I\u2019m working on a Data Quality Proof Of Concept for my company. \nI managed to find some good libraries like \u00ab\u00a0ydata-profiling\u00a0\u00bb. \nNow the hierarchy wants me to add another layer to the solution which consists of cleaning the \u00ab\u00a0bad\u00a0\u00bb data that doesn\u2019t comply to our requirements. \n\nI\u2019ve implemented one using Pandas, as I receive only csv files.\n\nI was wondering if someone know or has already implemented something like this, cleaning Data and subtracting rows that don\u2019t comply to some requirements from the original dataset, using an open-source library in Python ?\n\nGuidance appreciated,\nThanks in advance folks.", "author_fullname": "t2_mrrjzrpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any Python libraries for Data Cleansing ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b8t2x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701770804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I hope you\u2019re fine. &lt;/p&gt;\n\n&lt;p&gt;Right now, I\u2019m working on a Data Quality Proof Of Concept for my company. \nI managed to find some good libraries like \u00ab\u00a0ydata-profiling\u00a0\u00bb. \nNow the hierarchy wants me to add another layer to the solution which consists of cleaning the \u00ab\u00a0bad\u00a0\u00bb data that doesn\u2019t comply to our requirements. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve implemented one using Pandas, as I receive only csv files.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if someone know or has already implemented something like this, cleaning Data and subtracting rows that don\u2019t comply to some requirements from the original dataset, using an open-source library in Python ?&lt;/p&gt;\n\n&lt;p&gt;Guidance appreciated,\nThanks in advance folks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b8t2x", "is_robot_indexable": true, "report_reasons": null, "author": "TheLazyDataEng", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b8t2x/are_there_any_python_libraries_for_data_cleansing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b8t2x/are_there_any_python_libraries_for_data_cleansing/", "subreddit_subscribers": 144081, "created_utc": 1701770804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.\n\nHave a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).\n\nGithub repo:  [https://github.com/Kaizen91/spark-housing-market-canada](https://github.com/Kaizen91/spark-housing-market-canada)\n\n[Dataflow](https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a)\n\nThe main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.", "author_fullname": "t2_es03w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline using Dataproc and Pyspark to Build a BigQuery dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nt0l632b0j4c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f71b3510ccf7e37efd8e896e9cc6bafafdb672df"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a08da1ce9b88c07ac0c18ff6c742311f7ff4abf3"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=559d0cfe801beca97c01548ced39eead77ee7e92"}, {"y": 423, "x": 640, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eda773773344d5f542ddf98debc57227d964032"}, {"y": 635, "x": 960, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16fd81d560dfa3c80c582dac0643d02a902131c9"}, {"y": 714, "x": 1080, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5144c0d9914948a9bafabf8e6026849538311d76"}], "s": {"y": 891, "x": 1346, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a"}, "id": "nt0l632b0j4c1"}}, "name": "t3_18bjrw6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KzeSekC5wRmyiK6QYlsfiFanFErH26SsToOv35Ig4vg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1701803857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.&lt;/p&gt;\n\n&lt;p&gt;Have a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).&lt;/p&gt;\n\n&lt;p&gt;Github repo:  &lt;a href=\"https://github.com/Kaizen91/spark-housing-market-canada\"&gt;https://github.com/Kaizen91/spark-housing-market-canada&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a\"&gt;Dataflow&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?auto=webp&amp;s=6d7a27fb64915a5965dae69c1e99d9244f9e59b7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9860095d4a53941571b8cc81fc35924502222ea4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=001de624dab65e9800d86afa7a4ceb81ddbb5315", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d413903a6dc8a57f70ae18bf31d541e1a32cea54", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22c2910f2dda528aa033c6586204b79aa3b67e2b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd3fd7060d5aacc57186140e857cc85d8074f764", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e7f868569092adbf738725c62b3f4d9e6a61bef", "width": 1080, "height": 540}], "variants": {}, "id": "Ln8mP_mwXDuC3OX-8V42DgslTrqg3_oCYk-rjdF319Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18bjrw6", "is_robot_indexable": true, "report_reasons": null, "author": "skrillavilla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "subreddit_subscribers": 144081, "created_utc": 1701803857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit: putting TLDR at top.\n\nTLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?\n\nContext: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.\n\nProblem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.\n\nOver the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.\n\nSolution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?", "author_fullname": "t2_9oy62qlm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better ETL options than current set up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bied6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701800489.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701800278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: putting TLDR at top.&lt;/p&gt;\n\n&lt;p&gt;TLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?&lt;/p&gt;\n\n&lt;p&gt;Context: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.&lt;/p&gt;\n\n&lt;p&gt;Problem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.&lt;/p&gt;\n\n&lt;p&gt;Over the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.&lt;/p&gt;\n\n&lt;p&gt;Solution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bied6", "is_robot_indexable": true, "report_reasons": null, "author": "Talk-Much", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "subreddit_subscribers": 144081, "created_utc": 1701800278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow data enthusiasts! I've been thinking about leveling up my data engineering skills, and I'm on the hunt for the best online data engineering bootcamp. I've done some digging, here's what  I've come across. Would like to hear your thoughts on these and whether you'd choose them or not, and why?\n\nSpringboard Data Engineering Career Track\n\nI've heard they offer solid mentorship and hands-on projects, plus that job guarantee is a sweet deal, but pricey, maybe? I'm on a budget.\n\nGeneral Assembly's Data Engineering Immersive\n\nI like their reputation, and I'm all about those real-world projects and networking chances. I heard its boring and it can be intensive, anyone tried their course before?\n\nDataCamp's Data Engineering Track\n\nI like the idea of a more flexible, go-at-my-own-pace approach, and it's lighter on the wallet, but not sure if it will be as in depth enough to land me a job.\n\nI also been looking at Data Engineer Academy, I been seeing their ad on Instagram saying they will guarantee a job, but they seem new and not enough reviews online.\n\nOverall I'm leaning towards Springboard, but I'm open to your wisdom and personal experiences. Let me know if you've got any tips or other bootcamps in mind.", "author_fullname": "t2_jtekxc8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the Best Data Engineering bootcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18btyeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701831062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow data enthusiasts! I&amp;#39;ve been thinking about leveling up my data engineering skills, and I&amp;#39;m on the hunt for the best online data engineering bootcamp. I&amp;#39;ve done some digging, here&amp;#39;s what  I&amp;#39;ve come across. Would like to hear your thoughts on these and whether you&amp;#39;d choose them or not, and why?&lt;/p&gt;\n\n&lt;p&gt;Springboard Data Engineering Career Track&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard they offer solid mentorship and hands-on projects, plus that job guarantee is a sweet deal, but pricey, maybe? I&amp;#39;m on a budget.&lt;/p&gt;\n\n&lt;p&gt;General Assembly&amp;#39;s Data Engineering Immersive&lt;/p&gt;\n\n&lt;p&gt;I like their reputation, and I&amp;#39;m all about those real-world projects and networking chances. I heard its boring and it can be intensive, anyone tried their course before?&lt;/p&gt;\n\n&lt;p&gt;DataCamp&amp;#39;s Data Engineering Track&lt;/p&gt;\n\n&lt;p&gt;I like the idea of a more flexible, go-at-my-own-pace approach, and it&amp;#39;s lighter on the wallet, but not sure if it will be as in depth enough to land me a job.&lt;/p&gt;\n\n&lt;p&gt;I also been looking at Data Engineer Academy, I been seeing their ad on Instagram saying they will guarantee a job, but they seem new and not enough reviews online.&lt;/p&gt;\n\n&lt;p&gt;Overall I&amp;#39;m leaning towards Springboard, but I&amp;#39;m open to your wisdom and personal experiences. Let me know if you&amp;#39;ve got any tips or other bootcamps in mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18btyeq", "is_robot_indexable": true, "report_reasons": null, "author": "Zack-s21", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18btyeq/looking_for_the_best_data_engineering_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18btyeq/looking_for_the_best_data_engineering_bootcamp/", "subreddit_subscribers": 144081, "created_utc": 1701831062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi , I\u2019m 23 years old and am working as a data engineer at amazon for 2 years now. I have just been put on \u2018coaching plan\u2019/focus at Amazon by my manager saying that i am underperforming and they will give a project/tasks and monitor me for 8 weeks. Post that, the decision will be made to either let me stay or put me in the pip. I cannot apply internally to other teams as long as i am in focus. Its almost likely that i will loose my job in 2 months. \nI believe in myself and i wanna ace in whatever he is planning to assign to me but i cant help but be concerned about things going wrong. So i also plan to start my interview prep. Also i want to get ready with plan b if things hit rock bottom, that is masters in computer science for 2024 Aug( I have already ady given IELTS score).\nPlease suggest/guide if anyone has been through this kind of situation. The mental stress is getting to me.", "author_fullname": "t2_nusmz3fhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice! On career plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18brlq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701823971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi , I\u2019m 23 years old and am working as a data engineer at amazon for 2 years now. I have just been put on \u2018coaching plan\u2019/focus at Amazon by my manager saying that i am underperforming and they will give a project/tasks and monitor me for 8 weeks. Post that, the decision will be made to either let me stay or put me in the pip. I cannot apply internally to other teams as long as i am in focus. Its almost likely that i will loose my job in 2 months. \nI believe in myself and i wanna ace in whatever he is planning to assign to me but i cant help but be concerned about things going wrong. So i also plan to start my interview prep. Also i want to get ready with plan b if things hit rock bottom, that is masters in computer science for 2024 Aug( I have already ady given IELTS score).\nPlease suggest/guide if anyone has been through this kind of situation. The mental stress is getting to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18brlq7", "is_robot_indexable": true, "report_reasons": null, "author": "mad_peace", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18brlq7/need_advice_on_career_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18brlq7/need_advice_on_career_plan/", "subreddit_subscribers": 144081, "created_utc": 1701823971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI've been trying to self-educate myself for data engineering roles as a way of scaling my current experience as a Data Analyst (yeah, another one of those.)\n\nWell, I've been working through the Heinz 57 of DE tools and it's amazing to me how much cost goes into compute and storage for a lot of these.\n\nI recently had a horror story with Azure when I thought I was simply setting up a serverless SQL pool, and after doing nothing with it for 2 weeks, ended up spending $400 on digital air.\n\nAnyways, now I'm trying to learn Databricks, but there are a ton of charges with that as well it seems.\n\nI know, I could be learning from videos and other resources, but there's real value in being able to use the tool itself and learn by breaking everything you see.\n\nI guess I'm just surprised because I'm not aware of many fields where playing with the tool can cost you a ton of money if you don't understand the charges on a comprehensive level. I'm just learning these concepts for the first time, so maybe a beginner interface that blocks out any buttons that could make things fatal.\n\nWould love to know your horror stories. Maybe tips for less risky hands-on learning of these tools.\n\nCheers!", "author_fullname": "t2_3djr756e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost Barrier to Hands-On Education", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bqesu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701820561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to self-educate myself for data engineering roles as a way of scaling my current experience as a Data Analyst (yeah, another one of those.)&lt;/p&gt;\n\n&lt;p&gt;Well, I&amp;#39;ve been working through the Heinz 57 of DE tools and it&amp;#39;s amazing to me how much cost goes into compute and storage for a lot of these.&lt;/p&gt;\n\n&lt;p&gt;I recently had a horror story with Azure when I thought I was simply setting up a serverless SQL pool, and after doing nothing with it for 2 weeks, ended up spending $400 on digital air.&lt;/p&gt;\n\n&lt;p&gt;Anyways, now I&amp;#39;m trying to learn Databricks, but there are a ton of charges with that as well it seems.&lt;/p&gt;\n\n&lt;p&gt;I know, I could be learning from videos and other resources, but there&amp;#39;s real value in being able to use the tool itself and learn by breaking everything you see.&lt;/p&gt;\n\n&lt;p&gt;I guess I&amp;#39;m just surprised because I&amp;#39;m not aware of many fields where playing with the tool can cost you a ton of money if you don&amp;#39;t understand the charges on a comprehensive level. I&amp;#39;m just learning these concepts for the first time, so maybe a beginner interface that blocks out any buttons that could make things fatal.&lt;/p&gt;\n\n&lt;p&gt;Would love to know your horror stories. Maybe tips for less risky hands-on learning of these tools.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bqesu", "is_robot_indexable": true, "report_reasons": null, "author": "Ablueblaze", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bqesu/cost_barrier_to_handson_education/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bqesu/cost_barrier_to_handson_education/", "subreddit_subscribers": 144081, "created_utc": 1701820561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am quite new to data modeling other than a few projects at school where we learned about kimball etc. I have a bit of a strange task right where I need to make a data model for a decision system (rules engine). I am dealing with a lot of complex medical data, and relationships between entities. The model needs to be reasoned over, and be logically coherent for the use case. I am however at a bit of a loss as to how to go about this. My first thought was to use an ontology to create a knowledge base, and then build rules on top of this. So for example make the ontology using OWL, then use SWRL to create the conditions and rules. I would then potentially use RDFlib or owlready2 to create Python functions to test the usability of the model by creating instances. The use case is, given a diagnosis what is the preferred medication of choice. If anyone has any other ideas, I would love to hear them. ", "author_fullname": "t2_2u9k11la", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling Ontologies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b85x2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701767919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am quite new to data modeling other than a few projects at school where we learned about kimball etc. I have a bit of a strange task right where I need to make a data model for a decision system (rules engine). I am dealing with a lot of complex medical data, and relationships between entities. The model needs to be reasoned over, and be logically coherent for the use case. I am however at a bit of a loss as to how to go about this. My first thought was to use an ontology to create a knowledge base, and then build rules on top of this. So for example make the ontology using OWL, then use SWRL to create the conditions and rules. I would then potentially use RDFlib or owlready2 to create Python functions to test the usability of the model by creating instances. The use case is, given a diagnosis what is the preferred medication of choice. If anyone has any other ideas, I would love to hear them. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b85x2", "is_robot_indexable": true, "report_reasons": null, "author": "koobakak-kid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b85x2/data_modeling_ontologies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b85x2/data_modeling_ontologies/", "subreddit_subscribers": 144081, "created_utc": 1701767919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry for probably niche question, but I'm practicing my data modeling skills for interviews, so I'm trying to apply Kimball modeling to something I'm sort of familiar with (Teamfight Tactics). Basically how the game works is that a match has 8 players, each with their own board with several champions and the API returns information on who is in each game and some statistics/information about the state of their board at the end of the game (remaining gold, units played, units' items, level, traits activated, etc. [Link to API docs for reference](https://developer.riotgames.com/apis#tft-match-v1/GET_getMatch)) \n\nIf I'm trying to take a Kimball approach to modeling, these are the dimensions that I think I would include:\n\n* dim\\_user\n* dim\\_set\n* dim\\_trait\n* dim\\_champion\n* dim\\_item\n\nThen, I would have a couple of fact tables defined as follows:\n\n* fact\\_game\n   * game\\_id\n   * set\\_id (FK to dim\\_set)\n   * start\\_timestamp\n   * end\\_timestamp\n   * duration\n   * user\\_ids (List of ids of users who are in the game, FK to dim\\_user)\n* fact\\_final\\_board\n   * game\\_id\n   * user\\_id\n   * death\\_timestamp\n   * champions (List of ids of champions on the board, FK to dim\\_champion)\n   * active\\_traits (List of ids of active traits, FK to dim\\_trait)\n   * remaining\\_gold\n   * last\\_round\n   * placement\n   * player\\_level\n\nThere are a couple of issues that I've found with how I'm solving this. First, AFAIK I generally don't see tutorials online using a List type for ids connecting Facts to Dimensions and I'm wondering if there would be performance issues querying if I were to design the Fact tables that way. Second, with the way that I have designed the schema right now I'm not including anything that uses dim\\_item and I am unable to query to perform analysis on which units are holding what items. Would the solution to this be for me to create another fact table like so?\n\n* fact\\_final\\_board\\_champion\n   * game\\_id\n   * user\\_id\n   * champion\\_id\n   * items (List of ids of up to 3 items that champion is holding, FK to dim\\_item)\n\nDoing this I think I'm repeating bad design where I'm using a List to hold foreign keys. So from this point I'm kind of confused as to how I would improve my design. Thanks for any help possible", "author_fullname": "t2_co1kho03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would I take a Kimball approach to creating a data model for TFT matches?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b76rr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701763480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for probably niche question, but I&amp;#39;m practicing my data modeling skills for interviews, so I&amp;#39;m trying to apply Kimball modeling to something I&amp;#39;m sort of familiar with (Teamfight Tactics). Basically how the game works is that a match has 8 players, each with their own board with several champions and the API returns information on who is in each game and some statistics/information about the state of their board at the end of the game (remaining gold, units played, units&amp;#39; items, level, traits activated, etc. &lt;a href=\"https://developer.riotgames.com/apis#tft-match-v1/GET_getMatch\"&gt;Link to API docs for reference&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m trying to take a Kimball approach to modeling, these are the dimensions that I think I would include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dim_user&lt;/li&gt;\n&lt;li&gt;dim_set&lt;/li&gt;\n&lt;li&gt;dim_trait&lt;/li&gt;\n&lt;li&gt;dim_champion&lt;/li&gt;\n&lt;li&gt;dim_item&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then, I would have a couple of fact tables defined as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;fact_game\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;set_id (FK to dim_set)&lt;/li&gt;\n&lt;li&gt;start_timestamp&lt;/li&gt;\n&lt;li&gt;end_timestamp&lt;/li&gt;\n&lt;li&gt;duration&lt;/li&gt;\n&lt;li&gt;user_ids (List of ids of users who are in the game, FK to dim_user)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;fact_final_board\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;death_timestamp&lt;/li&gt;\n&lt;li&gt;champions (List of ids of champions on the board, FK to dim_champion)&lt;/li&gt;\n&lt;li&gt;active_traits (List of ids of active traits, FK to dim_trait)&lt;/li&gt;\n&lt;li&gt;remaining_gold&lt;/li&gt;\n&lt;li&gt;last_round&lt;/li&gt;\n&lt;li&gt;placement&lt;/li&gt;\n&lt;li&gt;player_level&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There are a couple of issues that I&amp;#39;ve found with how I&amp;#39;m solving this. First, AFAIK I generally don&amp;#39;t see tutorials online using a List type for ids connecting Facts to Dimensions and I&amp;#39;m wondering if there would be performance issues querying if I were to design the Fact tables that way. Second, with the way that I have designed the schema right now I&amp;#39;m not including anything that uses dim_item and I am unable to query to perform analysis on which units are holding what items. Would the solution to this be for me to create another fact table like so?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;fact_final_board_champion\n\n&lt;ul&gt;\n&lt;li&gt;game_id&lt;/li&gt;\n&lt;li&gt;user_id&lt;/li&gt;\n&lt;li&gt;champion_id&lt;/li&gt;\n&lt;li&gt;items (List of ids of up to 3 items that champion is holding, FK to dim_item)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Doing this I think I&amp;#39;m repeating bad design where I&amp;#39;m using a List to hold foreign keys. So from this point I&amp;#39;m kind of confused as to how I would improve my design. Thanks for any help possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18b76rr", "is_robot_indexable": true, "report_reasons": null, "author": "69hehehe69", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18b76rr/how_would_i_take_a_kimball_approach_to_creating_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b76rr/how_would_i_take_a_kimball_approach_to_creating_a/", "subreddit_subscribers": 144081, "created_utc": 1701763480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a software development leadership background as part of product development teams in tech companies. \n\nI just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. \n\nWhat do you guys think about Data Leadership vs Software Development Leadership?", "author_fullname": "t2_i0rx07jv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Leadership VS. Product Software Development Leadership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmkgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a software development leadership background as part of product development teams in tech companies. &lt;/p&gt;\n\n&lt;p&gt;I just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think about Data Leadership vs Software Development Leadership?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18bmkgf", "is_robot_indexable": true, "report_reasons": null, "author": "Adorable-Diver-1919", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "subreddit_subscribers": 144081, "created_utc": 1701810895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I face data spread over multiple columns.\n\n&amp;#x200B;\n\nSome rows hold duplicate data for a key. \n\nSome semantically similar data\n\nOthers new fresh data (sometimes only for different keys - however sometimes also for the same key)\n\n&amp;#x200B;\n\nWhat could be a great strategy to make this data accessible? I want to have one row for the key.\n\n&amp;#x200B;\n\nA full example can be obtained here: [https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys](https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys)", "author_fullname": "t2_8dvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Harmonizing data spread over multiple rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18b6eyn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701760175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I face data spread over multiple columns.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some rows hold duplicate data for a key. &lt;/p&gt;\n\n&lt;p&gt;Some semantically similar data&lt;/p&gt;\n\n&lt;p&gt;Others new fresh data (sometimes only for different keys - however sometimes also for the same key)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What could be a great strategy to make this data accessible? I want to have one row for the key.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A full example can be obtained here: &lt;a href=\"https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys\"&gt;https://stackoverflow.com/questions/77604377/harmonizing-data-spread-over-multiple-rows-with-duplicate-values-for-some-keys&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "18b6eyn", "is_robot_indexable": true, "report_reasons": null, "author": "geoheil", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/18b6eyn/harmonizing_data_spread_over_multiple_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18b6eyn/harmonizing_data_spread_over_multiple_rows/", "subreddit_subscribers": 144081, "created_utc": 1701760175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner\n\n1. How many orders?\n2. What are the Top 3 ordered dishes (with order count)?\n3. What\u2019s the total sale amount?\n\nThere are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:\n\n1. Last 1 hour\n2. Last 24 hours (= 1 day)\n3. Last 168 hours (= 24h \\* 7 = 1 week)\n\nThe dashboard should get new data every 5 minutes. How do you collect, store and serve data?\n\nMy Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.\n\n&amp;#x200B;\n\nI am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.\n\n&amp;#x200B;", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do Streaming Aggregation Pipelines work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18bwe2f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701838807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How many orders?&lt;/li&gt;\n&lt;li&gt;What are the Top 3 ordered dishes (with order count)?&lt;/li&gt;\n&lt;li&gt;What\u2019s the total sale amount?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Last 1 hour&lt;/li&gt;\n&lt;li&gt;Last 24 hours (= 1 day)&lt;/li&gt;\n&lt;li&gt;Last 168 hours (= 24h * 7 = 1 week)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The dashboard should get new data every 5 minutes. How do you collect, store and serve data?&lt;/p&gt;\n\n&lt;p&gt;My Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bwe2f", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/", "subreddit_subscribers": 144081, "created_utc": 1701838807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone successfully used AD Service account to connect to Microsoft sql server from airflow? \n\nAirflow is installed on Ubuntu server running on EC2 instances. I execute the Python code on that EC2. My python script which uses pyodbc + alchemy to connect to SQL server to read data then it gets loaded to snowflake. \n\nI have always used local service account created on sql server and always seems to work. But my IT dept is pushing me to use Active Directory service account instead. Historically only local sql service account has worked for me. \nDoes AD service account work for anyone on airflow ?", "author_fullname": "t2_7awnoud6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting to SQL server using AD service account on Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bu09k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701834303.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701831213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone successfully used AD Service account to connect to Microsoft sql server from airflow? &lt;/p&gt;\n\n&lt;p&gt;Airflow is installed on Ubuntu server running on EC2 instances. I execute the Python code on that EC2. My python script which uses pyodbc + alchemy to connect to SQL server to read data then it gets loaded to snowflake. &lt;/p&gt;\n\n&lt;p&gt;I have always used local service account created on sql server and always seems to work. But my IT dept is pushing me to use Active Directory service account instead. Historically only local sql service account has worked for me. \nDoes AD service account work for anyone on airflow ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bu09k", "is_robot_indexable": true, "report_reasons": null, "author": "Industry_Jolly", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bu09k/connecting_to_sql_server_using_ad_service_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bu09k/connecting_to_sql_server_using_ad_service_account/", "subreddit_subscribers": 144081, "created_utc": 1701831213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's my first time working with HIPAA data and was wondering in the extraction process is it okay to use python to call the api to a cloud warehouse ? \n\nDo I need to use a service like Azure Data Factory or Fivetran to get a HIPAA BAA/certification?", "author_fullname": "t2_7euoosst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HIPAA Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bp2tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701817152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s my first time working with HIPAA data and was wondering in the extraction process is it okay to use python to call the api to a cloud warehouse ? &lt;/p&gt;\n\n&lt;p&gt;Do I need to use a service like Azure Data Factory or Fivetran to get a HIPAA BAA/certification?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bp2tu", "is_robot_indexable": true, "report_reasons": null, "author": "rainamlien", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bp2tu/hipaa_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bp2tu/hipaa_data/", "subreddit_subscribers": 144081, "created_utc": 1701817152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. \n\nOne is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.\n\nFor the Bloomberg one I've been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. \n\nWhile for the Booz Allen Hamilton role it's only one round. From what I've heard it's conversational and theoretical in nature and I won't be expected to write code during the interview. If anyone has any experience interviewing with them I'd appreciate any insight you could offer", "author_fullname": "t2_p5wlf0g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmnwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701811132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. &lt;/p&gt;\n\n&lt;p&gt;One is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.&lt;/p&gt;\n\n&lt;p&gt;For the Bloomberg one I&amp;#39;ve been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. &lt;/p&gt;\n\n&lt;p&gt;While for the Booz Allen Hamilton role it&amp;#39;s only one round. From what I&amp;#39;ve heard it&amp;#39;s conversational and theoretical in nature and I won&amp;#39;t be expected to write code during the interview. If anyone has any experience interviewing with them I&amp;#39;d appreciate any insight you could offer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18bmnwy", "is_robot_indexable": true, "report_reasons": null, "author": "El_Cato_Crande", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmnwy/interview_prep_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmnwy/interview_prep_help/", "subreddit_subscribers": 144081, "created_utc": 1701811132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt basics refresher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18bl5sg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tlFUmRGlDFrCPnNGzTHo0xfNtMwHC8XkpUSb1bK-ZAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701807346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/dbt-basics-refresher", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?auto=webp&amp;s=3279967ceb601646cb41028ac8d4ca73b7f49f38", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=711b55ead92676cfed46175950e598fb11169eef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3351e12cdb4729acc293d3189901cd6fb23a05b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=726542216ba15170d4a2b857bf8954b8193335f6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=efa3574a1dbd7076fa3461704248c28e5f1a4e71", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64bd860b29935c769a96e58efa422e8278460f66", "width": 960, "height": 540}], "variants": {}, "id": "c10qH1P3epOVeKPFeR8yyKKpf08s_LIw8MS0wY3eKUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18bl5sg", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bl5sg/dbt_basics_refresher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/dbt-basics-refresher", "subreddit_subscribers": 144081, "created_utc": 1701807346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a requirement to collect data from non technical users. They need a userfriendly tool to be able to login and upload their data. Team is recommending to use Excel, but I am open to other ideas as long as the file format is easy to use for non technical users.\n\nOnce the user have uploaded this file, it needs to be validated. This way we can return the file and our non-technical users fix the errors and reupload the file.\n\nUsers are expected to submit tabular data.\n\nAny recommendations / suggestion are welcome on how such a scenario can be handled or if there are any prebuilt tools that can be used. ", "author_fullname": "t2_3m94wwfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Collecting data from non-technical users.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bkj5d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701836542.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701805780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a requirement to collect data from non technical users. They need a userfriendly tool to be able to login and upload their data. Team is recommending to use Excel, but I am open to other ideas as long as the file format is easy to use for non technical users.&lt;/p&gt;\n\n&lt;p&gt;Once the user have uploaded this file, it needs to be validated. This way we can return the file and our non-technical users fix the errors and reupload the file.&lt;/p&gt;\n\n&lt;p&gt;Users are expected to submit tabular data.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations / suggestion are welcome on how such a scenario can be handled or if there are any prebuilt tools that can be used. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bkj5d", "is_robot_indexable": true, "report_reasons": null, "author": "Gujjubhai2019", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bkj5d/collecting_data_from_nontechnical_users/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bkj5d/collecting_data_from_nontechnical_users/", "subreddit_subscribers": 144081, "created_utc": 1701805780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I'm a student looking to get into data engineering and I came to this thread because I was wondering if the 512G SSD option is necessary in your experience. I read some of the previous posts and saw that it's optional other than 16G ram, so let me know what you guys think.\n\nThanks.", "author_fullname": "t2_djmduqi1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "512G SSD laptop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bgn7k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701795718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m a student looking to get into data engineering and I came to this thread because I was wondering if the 512G SSD option is necessary in your experience. I read some of the previous posts and saw that it&amp;#39;s optional other than 16G ram, so let me know what you guys think.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bgn7k", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive-Chef-277", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bgn7k/512g_ssd_laptop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bgn7k/512g_ssd_laptop/", "subreddit_subscribers": 144081, "created_utc": 1701795718.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}