{"kind": "Listing", "data": {"after": "t3_18c7p3u", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the [Metabase Community Data Stack Report](https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house) 31% of responders said they're using in-house ingestion. \n\nWhy do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? ", "author_fullname": "t2_e5fjdth0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do companies still build data ingestion tooling instead of using a third-party tool like Airbyte?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bi2xj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701799473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the &lt;a href=\"https://www.metabase.com/data-stack-report-2023#data-ingestion-in-house\"&gt;Metabase Community Data Stack Report&lt;/a&gt; 31% of responders said they&amp;#39;re using in-house ingestion. &lt;/p&gt;\n\n&lt;p&gt;Why do companies still build data ingestion tooling instead of using third-party tools? Wouldn\u2019t it be more expensive, in terms of cost and time, to engineer and maintain your own ingestion pipeline? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?auto=webp&amp;s=a918001adbcbdf0507b477cb314c79c6fa6890b2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=beab2fb16fa06d31be725b3c39e0cebf62af8b6a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c0012b27a40b3722d7239204936cbc7d8a0f0ac", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1430fd4f043f9832a6f3740c48f801c8d56c6fb7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e76e9eb0b2f08f73ef01612cdc9b63d7b480f92", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d75bc2370d5dde94891301fb285d00362f9374fe", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/wp0ORqC3mQBHGOqu2iziElwnXKs15PjTtOG-v78ZcrU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07791994b1d6632f6a5c11d1fed9a8e6481756a5", "width": 1080, "height": 564}], "variants": {}, "id": "ftoBvI6rzzCZb3wJpMIOqE5SrDrO0vg2LvvVJ6z3_GE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bi2xj", "is_robot_indexable": true, "report_reasons": null, "author": "Miserable_Fold4086", "discussion_type": null, "num_comments": 88, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bi2xj/why_do_companies_still_build_data_ingestion/", "subreddit_subscribers": 144190, "created_utc": 1701799473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data \"activation.\"\n\nWhat are the new trends/buzzwords that the data industry will lean into this year? ", "author_fullname": "t2_nypbpupk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data trends for 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhf64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every year, the data industry finds a new buzzword to latch onto. This year, I feel like everyone was focused on the idea of data &amp;quot;activation.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What are the new trends/buzzwords that the data industry will lean into this year? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bhf64", "is_robot_indexable": true, "report_reasons": null, "author": "Techxpeare", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhf64/data_trends_for_2024/", "subreddit_subscribers": 144190, "created_utc": 1701797755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner\n\n1. How many orders?\n2. What are the Top 3 ordered dishes (with order count)?\n3. What\u2019s the total sale amount?\n\nThere are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:\n\n1. Last 1 hour\n2. Last 24 hours (= 1 day)\n3. Last 168 hours (= 24h \\* 7 = 1 week)\n\nThe dashboard should get new data every 5 minutes. How do you collect, store and serve data?\n\nMy Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.\n\n&amp;#x200B;\n\nI am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.\n\n&amp;#x200B;", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do Streaming Aggregation Pipelines work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bwe2f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701838807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s assume we are building a data pipeline for Uber Eats where we keep getting orders. We want a dashboard which shows each restaurant owner&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How many orders?&lt;/li&gt;\n&lt;li&gt;What are the Top 3 ordered dishes (with order count)?&lt;/li&gt;\n&lt;li&gt;What\u2019s the total sale amount?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are 3 buttons on the UI which allow the users (restaurant owners) to see these numbers for different time periods:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Last 1 hour&lt;/li&gt;\n&lt;li&gt;Last 24 hours (= 1 day)&lt;/li&gt;\n&lt;li&gt;Last 168 hours (= 24h * 7 = 1 week)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The dashboard should get new data every 5 minutes. How do you collect, store and serve data?&lt;/p&gt;\n\n&lt;p&gt;My Solution:We have a Kafka topic which gets all the order events. We have a Flink job with 5 min window which aggregates the data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am not sure how to go ahead from here. Like how do I store the data efficiently so that it can answer the question from all time periods and the kind of db to use, how to partition etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bwe2f", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bwe2f/how_do_streaming_aggregation_pipelines_work/", "subreddit_subscribers": 144190, "created_utc": 1701838807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to DBT. I found how to use hooks on Internet. But could not find any applications.", "author_fullname": "t2_d034xoja", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the actual use cases or implimentation of Hooks/ on-run-start/end in DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18byri2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701847894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to DBT. I found how to use hooks on Internet. But could not find any applications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18byri2", "is_robot_indexable": true, "report_reasons": null, "author": "Luffykent", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18byri2/what_are_the_actual_use_cases_or_implimentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18byri2/what_are_the_actual_use_cases_or_implimentation/", "subreddit_subscribers": 144190, "created_utc": 1701847894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow data enthusiasts! I've been thinking about leveling up my data engineering skills, and I'm on the hunt for the best online data engineering bootcamp. I've done some digging, here's what  I've come across. Would like to hear your thoughts on these and whether you'd choose them or not, and why?\n\nSpringboard Data Engineering Career Track\n\nI've heard they offer solid mentorship and hands-on projects, plus that job guarantee is a sweet deal, but pricey, maybe? I'm on a budget.\n\nGeneral Assembly's Data Engineering Immersive\n\nI like their reputation, and I'm all about those real-world projects and networking chances. I heard its boring and it can be intensive, anyone tried their course before?\n\nDataCamp's Data Engineering Track\n\nI like the idea of a more flexible, go-at-my-own-pace approach, and it's lighter on the wallet, but not sure if it will be as in depth enough to land me a job.\n\nI also been looking at Data Engineer Academy, I been seeing their ad on Instagram saying they will guarantee a job, but they seem new and not enough reviews online.\n\nOverall I'm leaning towards Springboard, but I'm open to your wisdom and personal experiences. Let me know if you've got any tips or other bootcamps in mind.", "author_fullname": "t2_jtekxc8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for the Best Data Engineering bootcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18btyeq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701831062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow data enthusiasts! I&amp;#39;ve been thinking about leveling up my data engineering skills, and I&amp;#39;m on the hunt for the best online data engineering bootcamp. I&amp;#39;ve done some digging, here&amp;#39;s what  I&amp;#39;ve come across. Would like to hear your thoughts on these and whether you&amp;#39;d choose them or not, and why?&lt;/p&gt;\n\n&lt;p&gt;Springboard Data Engineering Career Track&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard they offer solid mentorship and hands-on projects, plus that job guarantee is a sweet deal, but pricey, maybe? I&amp;#39;m on a budget.&lt;/p&gt;\n\n&lt;p&gt;General Assembly&amp;#39;s Data Engineering Immersive&lt;/p&gt;\n\n&lt;p&gt;I like their reputation, and I&amp;#39;m all about those real-world projects and networking chances. I heard its boring and it can be intensive, anyone tried their course before?&lt;/p&gt;\n\n&lt;p&gt;DataCamp&amp;#39;s Data Engineering Track&lt;/p&gt;\n\n&lt;p&gt;I like the idea of a more flexible, go-at-my-own-pace approach, and it&amp;#39;s lighter on the wallet, but not sure if it will be as in depth enough to land me a job.&lt;/p&gt;\n\n&lt;p&gt;I also been looking at Data Engineer Academy, I been seeing their ad on Instagram saying they will guarantee a job, but they seem new and not enough reviews online.&lt;/p&gt;\n\n&lt;p&gt;Overall I&amp;#39;m leaning towards Springboard, but I&amp;#39;m open to your wisdom and personal experiences. Let me know if you&amp;#39;ve got any tips or other bootcamps in mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18btyeq", "is_robot_indexable": true, "report_reasons": null, "author": "Zack-s21", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18btyeq/looking_for_the_best_data_engineering_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18btyeq/looking_for_the_best_data_engineering_bootcamp/", "subreddit_subscribers": 144190, "created_utc": 1701831062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.\n\nto provide a brief overview, this project  Have the following components : \n\n1. Real time that the processing and analysis :\n\nI have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.\n\n2.data warehousing: \n\nthe next closest step involved, setting up a robust data Warehouse  solution .\n \nThank you  you in advance for your time and assistance", "author_fullname": "t2_e1i3duqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dear data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18blijw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701808243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on existing project that involves Data warehousing  in IoT system, specifically dealing with real time data collection from GPS trackers on cars. I have successfully implemented the initial data collection using PHP, but I am now at juncture where I would greatly appreciate your expertise in proposing optional architecture for subsequent stage of data processing analysis and visualization.&lt;/p&gt;\n\n&lt;p&gt;to provide a brief overview, this project  Have the following components : &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Real time that the processing and analysis :&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have  data streaming in from GPS trackers on cars in real time and I am keen on understanding. The best practices and open source tools that  can be employed to process and analyze this data efficiently. The goal is to extract valuable insights in real time.&lt;/p&gt;\n\n&lt;p&gt;2.data warehousing: &lt;/p&gt;\n\n&lt;p&gt;the next closest step involved, setting up a robust data Warehouse  solution .&lt;/p&gt;\n\n&lt;p&gt;Thank you  you in advance for your time and assistance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18blijw", "is_robot_indexable": true, "report_reasons": null, "author": "ryan7ait", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18blijw/dear_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18blijw/dear_data_engineers/", "subreddit_subscribers": 144190, "created_utc": 1701808243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nSituation is this: I need to make replication stream from one postgres (db1) to another (db2). In db2 I will have analytical workload with creation of datamarts.\n\nAlso db1 needs to send replication stream to datalake somewhere.\n\nPossible solutions:\n\n1. Logical replication pg-to-pg. This one is good to transfer data between db1 and db2, but seems like additional load because it seems not suitable for db1 to datalake stream. Also DBA on my project thinks that logical replication is very hard to deal with if some problems occur.\n2. Logical replication pg-to-debezium-to-kafka. After that kafka topic is consumed by another debezium to replicate all data to db2 and by datalake consumer. Seems ok despite \"DBA on my project thinks that logical replication is very hard to deal with if some problems occur.\"\n3. Db1.Outbox -&gt; send to kafka.\n\nThe architect on my project chose option 3.\n\nI've seen some articles about 3rd option, but I don't understand why would I use debezium in this case. If all debezium does is sending outbox to kafka and then kafka event (INSERT/UPDATE/DELETE) to db2, than it seems not that useful - maybe it is not that hard to do it manually. What additional functionality that is hard to implement (or is unknown to a common developer) there is?\n\nAnd if you have also any opinions about option 1 and 2, I'll gladly read them.\n\nThanks. ", "author_fullname": "t2_7nzkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is debezium worth the trouble?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c3yvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701869300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;Situation is this: I need to make replication stream from one postgres (db1) to another (db2). In db2 I will have analytical workload with creation of datamarts.&lt;/p&gt;\n\n&lt;p&gt;Also db1 needs to send replication stream to datalake somewhere.&lt;/p&gt;\n\n&lt;p&gt;Possible solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Logical replication pg-to-pg. This one is good to transfer data between db1 and db2, but seems like additional load because it seems not suitable for db1 to datalake stream. Also DBA on my project thinks that logical replication is very hard to deal with if some problems occur.&lt;/li&gt;\n&lt;li&gt;Logical replication pg-to-debezium-to-kafka. After that kafka topic is consumed by another debezium to replicate all data to db2 and by datalake consumer. Seems ok despite &amp;quot;DBA on my project thinks that logical replication is very hard to deal with if some problems occur.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Db1.Outbox -&amp;gt; send to kafka.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The architect on my project chose option 3.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen some articles about 3rd option, but I don&amp;#39;t understand why would I use debezium in this case. If all debezium does is sending outbox to kafka and then kafka event (INSERT/UPDATE/DELETE) to db2, than it seems not that useful - maybe it is not that hard to do it manually. What additional functionality that is hard to implement (or is unknown to a common developer) there is?&lt;/p&gt;\n\n&lt;p&gt;And if you have also any opinions about option 1 and 2, I&amp;#39;ll gladly read them.&lt;/p&gt;\n\n&lt;p&gt;Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c3yvr", "is_robot_indexable": true, "report_reasons": null, "author": "popfalushi", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c3yvr/is_debezium_worth_the_trouble/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c3yvr/is_debezium_worth_the_trouble/", "subreddit_subscribers": 144190, "created_utc": 1701869300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.\n\nHave a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).\n\nGithub repo:  [https://github.com/Kaizen91/spark-housing-market-canada](https://github.com/Kaizen91/spark-housing-market-canada)\n\n[Dataflow](https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a)\n\nThe main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.", "author_fullname": "t2_es03w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipeline using Dataproc and Pyspark to Build a BigQuery dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nt0l632b0j4c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f71b3510ccf7e37efd8e896e9cc6bafafdb672df"}, {"y": 142, "x": 216, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a08da1ce9b88c07ac0c18ff6c742311f7ff4abf3"}, {"y": 211, "x": 320, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=559d0cfe801beca97c01548ced39eead77ee7e92"}, {"y": 423, "x": 640, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eda773773344d5f542ddf98debc57227d964032"}, {"y": 635, "x": 960, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16fd81d560dfa3c80c582dac0643d02a902131c9"}, {"y": 714, "x": 1080, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5144c0d9914948a9bafabf8e6026849538311d76"}], "s": {"y": 891, "x": 1346, "u": "https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;format=png&amp;auto=webp&amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a"}, "id": "nt0l632b0j4c1"}}, "name": "t3_18bjrw6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KzeSekC5wRmyiK6QYlsfiFanFErH26SsToOv35Ig4vg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1701803857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just finished putting together a project trying to demonstrate some knowledge of Terraform, Dataproc, and Pyspark.&lt;/p&gt;\n\n&lt;p&gt;Have a look here, and let me know your thoughts:  things to improve, things you think I did well here, random insults, whatever :).&lt;/p&gt;\n\n&lt;p&gt;Github repo:  &lt;a href=\"https://github.com/Kaizen91/spark-housing-market-canada\"&gt;https://github.com/Kaizen91/spark-housing-market-canada&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nt0l632b0j4c1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2810e1dcf00b8d2110d2e7e03bde69df52e821a\"&gt;Dataflow&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The main.tf terraform file will create all the infrastructure needed for this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a Dataproc job, and a BigQuery dataset. It will also upload the source csv file and the transform.py script to the Google Cloud Storage bucket, so that they can be accessed by the Dataproc Pyspark Job running on the Dataproc cluster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?auto=webp&amp;s=6d7a27fb64915a5965dae69c1e99d9244f9e59b7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9860095d4a53941571b8cc81fc35924502222ea4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=001de624dab65e9800d86afa7a4ceb81ddbb5315", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d413903a6dc8a57f70ae18bf31d541e1a32cea54", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22c2910f2dda528aa033c6586204b79aa3b67e2b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd3fd7060d5aacc57186140e857cc85d8074f764", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EqZfCEpzhEgf5xMLyl7jRgGHNJz-Oj1TuaDm0OFzfvs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e7f868569092adbf738725c62b3f4d9e6a61bef", "width": 1080, "height": 540}], "variants": {}, "id": "Ln8mP_mwXDuC3OX-8V42DgslTrqg3_oCYk-rjdF319Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "18bjrw6", "is_robot_indexable": true, "report_reasons": null, "author": "skrillavilla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bjrw6/pipeline_using_dataproc_and_pyspark_to_build_a/", "subreddit_subscribers": 144190, "created_utc": 1701803857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Background\n\nI work for a small company, we're mostly focussing on small retail and e-commerce customers. We provides data analysis and automated data connections between their platforms.\n\n# Our Problem\n\nMost of our datasets are things like order data, google ads click data, and are quite small. Most of them are in the range of few megabytes up to a gigabyte of data.\n\nProblem 1 is that we have a LOT of daily pipelines (around 2000 pipeline runs each day), which all need custom configuration. These are not very standard pipelines because of lots of 'custom' data sources specific to our customers' needs. These data sources are pulled by us using in-house developed connectors to, for example, REST api's, external databases, protobuf, GraphQL etc. We do operations like merging data on data that can only be retrieved from an API, for example, order data with a customer ID merge on customer data, by retrieving customers one by one from their retail platform service.\n\nProblem 2 is that because of the limitations of these APIs, most take quite long, some up to an hour to load in the entire dataset. Because of this our server is idle for a long time, waiting for new data to come in.\n\nIn a nutshell, we have a large amount of ETL pipelines that need to be orchestrated to run on a schedule,  daily, weekly, and sometimes hourly. The ETL pipelines have a long runtime and very little memory/cpu usage because most of it is data ingestion.\n\n# Current system\n\nUp until this point, we used our in-house developed DAG application, where the data engineers developed the actual application and the backend. The data scientists use the application to create new pipelines. \n\nWhen we require new data operations, we develop and release them to our data scientists in the form of nodes to use in our graphical click-and-drag interface where nodes in the DAG represent data operations. Our backend then orchestrates the scheduling and executions of these pipelines and runs them in parallel on our servers.\n\nThis system is quite old and predates most ETL standards. We want to move on because we want to start working with bigger datasets and creating the click-and-drag pipelines is a long and tedious process in an interface written in 2010. We also want a more flexible way of writing ETL pipelines, like with Python notebooks and data lakes so we can work and innovate faster in modern data analysis areas like machine learning.\n\n# Current proposed solution / Cost issues\n\nThat is why we decided to move to an advanced data platform, namely Azure Databricks. While it seems like the perfect platform for our future workloads, migrating our current pipelines is resulting in quite a problem. Running one is extremely inefficient on the Databricks architecture. Spinning up a cluster takes a long time, because of all our libraries needed for the API connectors. Then loading in the data takes even longer, 20 minutes up to an hour. Then the memory will spike slightly up to 200 MB of the available 14 GB and the server performs the actual transformations within a few seconds. All of this can take half an hour, where a few seconds are effectively used. With 2000 daily tasks the cost of this will grow to a ridiculous size compared to our old system.\n\nWe know that there are ways to run your jobs in parallel in Databricks on all-purpose clusters, however, it doesn't seem like there is a good system in place for load balancing. The system is not built for loads of minuscule long waiting tasks at once, but rather huge, compute-optimized tasks.\n\nWhat would be the best and simplest way for us to move on? We are looking at things like utilizing Azure functions/Containers to load our data and write it to the datalake, then using Databricks for the actual transformation. This can work but would result in some serious architectural headaches. How would you tackle this problem? Move away from Databricks entirely?\n\n&amp;#x200B;", "author_fullname": "t2_3pywt6jw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best automated data pipeline platform for lots of small Datasets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c1o7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701863905.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701860952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Background&lt;/h1&gt;\n\n&lt;p&gt;I work for a small company, we&amp;#39;re mostly focussing on small retail and e-commerce customers. We provides data analysis and automated data connections between their platforms.&lt;/p&gt;\n\n&lt;h1&gt;Our Problem&lt;/h1&gt;\n\n&lt;p&gt;Most of our datasets are things like order data, google ads click data, and are quite small. Most of them are in the range of few megabytes up to a gigabyte of data.&lt;/p&gt;\n\n&lt;p&gt;Problem 1 is that we have a LOT of daily pipelines (around 2000 pipeline runs each day), which all need custom configuration. These are not very standard pipelines because of lots of &amp;#39;custom&amp;#39; data sources specific to our customers&amp;#39; needs. These data sources are pulled by us using in-house developed connectors to, for example, REST api&amp;#39;s, external databases, protobuf, GraphQL etc. We do operations like merging data on data that can only be retrieved from an API, for example, order data with a customer ID merge on customer data, by retrieving customers one by one from their retail platform service.&lt;/p&gt;\n\n&lt;p&gt;Problem 2 is that because of the limitations of these APIs, most take quite long, some up to an hour to load in the entire dataset. Because of this our server is idle for a long time, waiting for new data to come in.&lt;/p&gt;\n\n&lt;p&gt;In a nutshell, we have a large amount of ETL pipelines that need to be orchestrated to run on a schedule,  daily, weekly, and sometimes hourly. The ETL pipelines have a long runtime and very little memory/cpu usage because most of it is data ingestion.&lt;/p&gt;\n\n&lt;h1&gt;Current system&lt;/h1&gt;\n\n&lt;p&gt;Up until this point, we used our in-house developed DAG application, where the data engineers developed the actual application and the backend. The data scientists use the application to create new pipelines. &lt;/p&gt;\n\n&lt;p&gt;When we require new data operations, we develop and release them to our data scientists in the form of nodes to use in our graphical click-and-drag interface where nodes in the DAG represent data operations. Our backend then orchestrates the scheduling and executions of these pipelines and runs them in parallel on our servers.&lt;/p&gt;\n\n&lt;p&gt;This system is quite old and predates most ETL standards. We want to move on because we want to start working with bigger datasets and creating the click-and-drag pipelines is a long and tedious process in an interface written in 2010. We also want a more flexible way of writing ETL pipelines, like with Python notebooks and data lakes so we can work and innovate faster in modern data analysis areas like machine learning.&lt;/p&gt;\n\n&lt;h1&gt;Current proposed solution / Cost issues&lt;/h1&gt;\n\n&lt;p&gt;That is why we decided to move to an advanced data platform, namely Azure Databricks. While it seems like the perfect platform for our future workloads, migrating our current pipelines is resulting in quite a problem. Running one is extremely inefficient on the Databricks architecture. Spinning up a cluster takes a long time, because of all our libraries needed for the API connectors. Then loading in the data takes even longer, 20 minutes up to an hour. Then the memory will spike slightly up to 200 MB of the available 14 GB and the server performs the actual transformations within a few seconds. All of this can take half an hour, where a few seconds are effectively used. With 2000 daily tasks the cost of this will grow to a ridiculous size compared to our old system.&lt;/p&gt;\n\n&lt;p&gt;We know that there are ways to run your jobs in parallel in Databricks on all-purpose clusters, however, it doesn&amp;#39;t seem like there is a good system in place for load balancing. The system is not built for loads of minuscule long waiting tasks at once, but rather huge, compute-optimized tasks.&lt;/p&gt;\n\n&lt;p&gt;What would be the best and simplest way for us to move on? We are looking at things like utilizing Azure functions/Containers to load our data and write it to the datalake, then using Databricks for the actual transformation. This can work but would result in some serious architectural headaches. How would you tackle this problem? Move away from Databricks entirely?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c1o7y", "is_robot_indexable": true, "report_reasons": null, "author": "polioio", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c1o7y/best_automated_data_pipeline_platform_for_lots_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c1o7y/best_automated_data_pipeline_platform_for_lots_of/", "subreddit_subscribers": 144190, "created_utc": 1701860952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What data controls do you implement day to day?\n\nMy project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a \"get it done by any means\" direction due to a strict deadline, which although doesn't excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.\n\nI currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on\n\nThe audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn't changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server\n\nIs this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.", "author_fullname": "t2_79a43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Controls", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmawo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What data controls do you implement day to day?&lt;/p&gt;\n\n&lt;p&gt;My project has come under scrutiny by CDO audit team and have asked what controls are in place across a number of circumstances. We had a very tactical build that was under a &amp;quot;get it done by any means&amp;quot; direction due to a strict deadline, which although doesn&amp;#39;t excuse the limited controls, gives reason to why I requested a 6 to 8 week clean up period post delivery. As usual, this has now be superseded by next phase changes, along with planning to move it to a more permanent solution.&lt;/p&gt;\n\n&lt;p&gt;I currently have the below being loaded to a Teradata database:\n* Excel - converted to CSV for easier loading loaded through TPT scripts\n* Tables from a different Teradata server using TPT Easy Loader\n* Data pulled from a Hadoop cluster via QueryGrid\n* Data pulled from tables on the same server we are hosting on&lt;/p&gt;\n\n&lt;p&gt;The audit team have requested we apply the following controls:\n* Validate all data loaded from Excel by providing row counts and doing random manual sampling to ensure that data hasn&amp;#39;t changed\n* Apply row count and data format checks when loading entire tables from the other Teradata instance\n* Provide checks on expected row counts and data integrity when querying Hadoop and our own server&lt;/p&gt;\n\n&lt;p&gt;Is this overkill or just a standard? I have raised my concerns over why we would question if data can change in flight with external forces being applied.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bmawo", "is_robot_indexable": true, "report_reasons": null, "author": "Stychey", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmawo/data_controls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmawo/data_controls/", "subreddit_subscribers": 144190, "created_utc": 1701810230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit: putting TLDR at top.\n\nTLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?\n\nContext: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.\n\nProblem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.\n\nOver the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.\n\nSolution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?", "author_fullname": "t2_9oy62qlm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better ETL options than current set up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bied6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701800489.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701800278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: putting TLDR at top.&lt;/p&gt;\n\n&lt;p&gt;TLDR: our database takes too long to update because transformation takes place in SQL and the procedures are highly complex and intertwined. What tools or processes can we use to improve this?&lt;/p&gt;\n\n&lt;p&gt;Context: I\u2019m a Data Analyst working in a large international corporation. I don\u2019t (currently) have any admin privileges in our Azure Portal and the only team that does is the \u201cNational\u201d team. We have an Azure database with SSMS that we use to connect to it. This database is updated and managed by the National team. I have been in this role about a year and a half. We have five divisions of the same job function that use this database that have the same tables (mostly) created/updated for their division with their data. This database was created organically by one of the divisions to suit their needs and each division just mostly copied their process then made them their own on their own servers before they ultimately got migrated to a centralized azure database.&lt;/p&gt;\n\n&lt;p&gt;Problem: Updating our SQL database takes a full day in the week so we have static data for one full week. My understanding of the process of these updates (from the limited amount I have been told by National) are that there are jobs set up to pull reports from SAP into batch files to import into SQL and then we have a number of (probably over 40+) stored procedures that are run to parse and transform the data into even more tables than procedures (we have like 200 tables\u2026). All of the procedures reference multiple source tables and can, sometimes, even be updating multiple tables within them. A large portion of them are dynamic SQL scripts with table, schema, dates, and temp table names (used for the transformation process) all set to variables so the procedures can be copied and built for the other divisions and the variables just have to change (although even those are still not all alike as changes have been made to them per requests from divisions). The procedures can sometimes be highly complex in their transformations and it can be very hard to change or see what it\u2019s doing.&lt;/p&gt;\n\n&lt;p&gt;Over the last half-year I\u2019ve done my fair share of research into data engineering tools and practices and I know there have to be significantly better methods to handle this data, but no one seems to be looking at this as a serious problem, but I want to push for a better data structure and to possibly integrate our data process with the process/tools of the larger company (like, everyone else that\u2019s not us) that uses Azure Data Warehouses and shares data with teams per requests.&lt;/p&gt;\n\n&lt;p&gt;Solution?: What tools/processes should I be looking into to learn so that I can effectively present a case to change our structure for the better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bied6", "is_robot_indexable": true, "report_reasons": null, "author": "Talk-Much", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bied6/better_etl_options_than_current_set_up/", "subreddit_subscribers": 144190, "created_utc": 1701800278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this list the other night. It has a ton of good resources for working with dbt\n\n&amp;#x200B;\n\n[https://github.com/Hiflylabs/awesome-dbt](https://github.com/Hiflylabs/awesome-dbt)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good list of dbt tools/resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bhcp4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701797581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this list the other night. It has a ton of good resources for working with dbt&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Hiflylabs/awesome-dbt\"&gt;https://github.com/Hiflylabs/awesome-dbt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?auto=webp&amp;s=176e13ff72d3539d5767f43a8afe1a74ff147c22", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dd3283325149ef5d275d3a81fa389126e63f6dc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a4f30011f8a49d95b307b2a23ddfe55cd2d0ac6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f50ff1fd15562d44a72d1c9e9ef3501d106e438", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c4aa808f0987e4fb5ae998545416dddb38f7f93", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57cb9b22361687461d619af9ed1204a212844246", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/aiYWJiwz0QRgOI88-k0cP2Cl-hMjT-4xgxGrWtWayVY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f5895bf2b2b84890a02c2022006d133e6cc9723", "width": 1080, "height": 540}], "variants": {}, "id": "-hQXbWYxMy1LKsqRjHBv0nu0FBgk3UtXPqm_FMKcUbE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "18bhcp4", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bhcp4/good_list_of_dbt_toolsresources/", "subreddit_subscribers": 144190, "created_utc": 1701797581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does a Data Product Strategy Impact the Day-to-Days of Your CMO, CDO, or CFO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18c3gdx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/C9GcrRMwBxuDwCU1bdErjJXhJ_DYHOqrwf1MmdsjJM8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701867601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-does-a-data-product-strategy", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?auto=webp&amp;s=7a223862678a9cde54a90f4ed9d6bad68edd59fa", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4cab7a47d69ad9432d41b3f1b8b5ca92c7e96da", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23cbc75ce20c8a8501d97242d11a3da767ab0132", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0b0a52818eea2477c8bbd62453a787c34329517", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdd1a8bbf8324711b443be95e7684ff60515d28b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0c07dc4e279fa12e496b68ca3b4bfbb0312acf27", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/w05U63gjdSdSI-pALXxf2ukYEDRSu93x1Mci44z-s9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4de8b0d0a5bec9c054c57af332df08e223edcaf", "width": 1080, "height": 540}], "variants": {}, "id": "kFmQZXSdF72mgYIKZgd9m7zyEeCYL5K-bgj5n2tyvVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18c3gdx", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c3gdx/how_does_a_data_product_strategy_impact_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-does-a-data-product-strategy", "subreddit_subscribers": 144190, "created_utc": 1701867601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt basics refresher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_18bl5sg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tlFUmRGlDFrCPnNGzTHo0xfNtMwHC8XkpUSb1bK-ZAw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701807346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/dbt-basics-refresher", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?auto=webp&amp;s=3279967ceb601646cb41028ac8d4ca73b7f49f38", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=711b55ead92676cfed46175950e598fb11169eef", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3351e12cdb4729acc293d3189901cd6fb23a05b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=726542216ba15170d4a2b857bf8954b8193335f6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=efa3574a1dbd7076fa3461704248c28e5f1a4e71", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/jIb01Vk6opjceiZqTUCDTR1HZI7gpd0AGIOixaFxUE8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64bd860b29935c769a96e58efa422e8278460f66", "width": 960, "height": 540}], "variants": {}, "id": "c10qH1P3epOVeKPFeR8yyKKpf08s_LIw8MS0wY3eKUs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18bl5sg", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bl5sg/dbt_basics_refresher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/dbt-basics-refresher", "subreddit_subscribers": 144190, "created_utc": 1701807346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the essential concepts that a person needs to have a good understanding of in the field of data engineering? Currently work as a data analyst/engineer but from a mechanical background so I want to know what should i learn to make my fundamentals strong", "author_fullname": "t2_47qmqn8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fundamentals of DE apart from sql, python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18c6uic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701877719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the essential concepts that a person needs to have a good understanding of in the field of data engineering? Currently work as a data analyst/engineer but from a mechanical background so I want to know what should i learn to make my fundamentals strong&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c6uic", "is_robot_indexable": true, "report_reasons": null, "author": "polonium_biscuit", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c6uic/fundamentals_of_de_apart_from_sql_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c6uic/fundamentals_of_de_apart_from_sql_python/", "subreddit_subscribers": 144190, "created_utc": 1701877719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "You can use MD5\\_NUMBER\\_LOWER64 or MD5\\_NUMBER\\_UPPER64 to generate keys, at the theoretical risk of collision. \n\nIs this a real practical risk though, with a number of unique IDs to be generated at say less than 100 million?\n\nHow I got to this question: The requirement is to use integers, but also to make the keys idempotent. \n\n  \nAnyone doing this?", "author_fullname": "t2_i7h1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MD5_NUMBER_LOWER64 to produce Integer Keys", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c5pno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701874582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can use MD5_NUMBER_LOWER64 or MD5_NUMBER_UPPER64 to generate keys, at the theoretical risk of collision. &lt;/p&gt;\n\n&lt;p&gt;Is this a real practical risk though, with a number of unique IDs to be generated at say less than 100 million?&lt;/p&gt;\n\n&lt;p&gt;How I got to this question: The requirement is to use integers, but also to make the keys idempotent. &lt;/p&gt;\n\n&lt;p&gt;Anyone doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c5pno", "is_robot_indexable": true, "report_reasons": null, "author": "TheLordSaves", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c5pno/md5_number_lower64_to_produce_integer_keys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c5pno/md5_number_lower64_to_produce_integer_keys/", "subreddit_subscribers": 144190, "created_utc": 1701874582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi , I\u2019m 23 years old and am working as a data engineer at amazon for 2 years now. I have just been put on \u2018coaching plan\u2019/focus at Amazon by my manager saying that i am underperforming and they will give a project/tasks and monitor me for 8 weeks. Post that, the decision will be made to either let me stay or put me in the pip. I cannot apply internally to other teams as long as i am in focus. Its almost likely that i will loose my job in 2 months. \nI believe in myself and i wanna ace in whatever he is planning to assign to me but i cant help but be concerned about things going wrong. So i also plan to start my interview prep. Also i want to get ready with plan b if things hit rock bottom, that is masters in computer science for 2024 Aug( I have already ady given IELTS score).\nPlease suggest/guide if anyone has been through this kind of situation. The mental stress is getting to me.", "author_fullname": "t2_nusmz3fhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice! On career plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18brlq7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701823971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi , I\u2019m 23 years old and am working as a data engineer at amazon for 2 years now. I have just been put on \u2018coaching plan\u2019/focus at Amazon by my manager saying that i am underperforming and they will give a project/tasks and monitor me for 8 weeks. Post that, the decision will be made to either let me stay or put me in the pip. I cannot apply internally to other teams as long as i am in focus. Its almost likely that i will loose my job in 2 months. \nI believe in myself and i wanna ace in whatever he is planning to assign to me but i cant help but be concerned about things going wrong. So i also plan to start my interview prep. Also i want to get ready with plan b if things hit rock bottom, that is masters in computer science for 2024 Aug( I have already ady given IELTS score).\nPlease suggest/guide if anyone has been through this kind of situation. The mental stress is getting to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18brlq7", "is_robot_indexable": true, "report_reasons": null, "author": "mad_peace", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18brlq7/need_advice_on_career_plan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18brlq7/need_advice_on_career_plan/", "subreddit_subscribers": 144190, "created_utc": 1701823971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI've been trying to self-educate myself for data engineering roles as a way of scaling my current experience as a Data Analyst (yeah, another one of those.)\n\nWell, I've been working through the Heinz 57 of DE tools and it's amazing to me how much cost goes into compute and storage for a lot of these.\n\nI recently had a horror story with Azure when I thought I was simply setting up a serverless SQL pool, and after doing nothing with it for 2 weeks, ended up spending $400 on digital air.\n\nAnyways, now I'm trying to learn Databricks, but there are a ton of charges with that as well it seems.\n\nI know, I could be learning from videos and other resources, but there's real value in being able to use the tool itself and learn by breaking everything you see.\n\nI guess I'm just surprised because I'm not aware of many fields where playing with the tool can cost you a ton of money if you don't understand the charges on a comprehensive level. I'm just learning these concepts for the first time, so maybe a beginner interface that blocks out any buttons that could make things fatal.\n\nWould love to know your horror stories. Maybe tips for less risky hands-on learning of these tools.\n\nCheers!", "author_fullname": "t2_3djr756e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost Barrier to Hands-On Education", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bqesu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701820561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to self-educate myself for data engineering roles as a way of scaling my current experience as a Data Analyst (yeah, another one of those.)&lt;/p&gt;\n\n&lt;p&gt;Well, I&amp;#39;ve been working through the Heinz 57 of DE tools and it&amp;#39;s amazing to me how much cost goes into compute and storage for a lot of these.&lt;/p&gt;\n\n&lt;p&gt;I recently had a horror story with Azure when I thought I was simply setting up a serverless SQL pool, and after doing nothing with it for 2 weeks, ended up spending $400 on digital air.&lt;/p&gt;\n\n&lt;p&gt;Anyways, now I&amp;#39;m trying to learn Databricks, but there are a ton of charges with that as well it seems.&lt;/p&gt;\n\n&lt;p&gt;I know, I could be learning from videos and other resources, but there&amp;#39;s real value in being able to use the tool itself and learn by breaking everything you see.&lt;/p&gt;\n\n&lt;p&gt;I guess I&amp;#39;m just surprised because I&amp;#39;m not aware of many fields where playing with the tool can cost you a ton of money if you don&amp;#39;t understand the charges on a comprehensive level. I&amp;#39;m just learning these concepts for the first time, so maybe a beginner interface that blocks out any buttons that could make things fatal.&lt;/p&gt;\n\n&lt;p&gt;Would love to know your horror stories. Maybe tips for less risky hands-on learning of these tools.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18bqesu", "is_robot_indexable": true, "report_reasons": null, "author": "Ablueblaze", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bqesu/cost_barrier_to_handson_education/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bqesu/cost_barrier_to_handson_education/", "subreddit_subscribers": 144190, "created_utc": 1701820561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a software development leadership background as part of product development teams in tech companies. \n\nI just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. \n\nWhat do you guys think about Data Leadership vs Software Development Leadership?", "author_fullname": "t2_i0rx07jv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Leadership VS. Product Software Development Leadership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmkgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701810895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a software development leadership background as part of product development teams in tech companies. &lt;/p&gt;\n\n&lt;p&gt;I just came across an opportunity where I may get a good jump to become an executive at a company and lead their data team. What I have seen from my experience and talking to other friends in the industry, data leadership jobs are shitty jobs where you always get hammered by all stakeholders (CFO, CIO, Product Executives etc,) and no matter how good you do there are always some issues (data quality, platform incidents, pipeline incidents etc) as data itself is a complex field. Also, you never got to work on the real meat (analytics/AI) as business or product teams who have domain knowledge are doing that. &lt;/p&gt;\n\n&lt;p&gt;What do you guys think about Data Leadership vs Software Development Leadership?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18bmkgf", "is_robot_indexable": true, "report_reasons": null, "author": "Adorable-Diver-1919", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmkgf/data_engineering_leadership_vs_product_software/", "subreddit_subscribers": 144190, "created_utc": 1701810895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am reading Kimball's The Data Warehouse Toolkit (chap. 1 done), and I understand that fact tables in a dimensional model are supposed to represent granular events that occur from various business processes (sales orders, deliveries, purchase orders, quotes, manufacturing sequences, account payables, and so on). Dimensions complement the fact tables to provide textual context to events as an entry point for analysis (Number of order by &lt;some dimensional attribute(s)&gt; Got it, super clear and makes sense. \n\nKimball says that these Fact and Dimension tables are used in the BI Applications layer for analytical needs, such as Power BI for a report. However, I am confused because these fact tables (as-is) aren't really too valuable to people within a department who want to analyze a specific problem or process. I hardly see the case where I am using a fact table as is in a reporting tool, I would need to perform additional transformations, joins, and business logic before the reporting tool. In a true Kimball warehouse, are these just views that belong in the BI Application Layer and the reporting tool just uses these views instead of the fact tables directly?\n\nFor example, a pricing department would want to have a table that joins the sales orders fact to the quotes fact, filters on certain criteria, and calculates several conditional and mathematical columns. This would then be the table (or view?) that they would use in Power BI to report on conversion rates by region, product, customer, etc...).\n\nAnother example would be a logistics department wanting a report for On Time Deliveries. They would want a table (or view?) that joins the sales orders fact to the deliveries fact to obtain the date that an order line was supposed to be delivered by, create a binary column for if on time, and filter out deliveries of internal or sample orders.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball Fact Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c6fnt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701876582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am reading Kimball&amp;#39;s The Data Warehouse Toolkit (chap. 1 done), and I understand that fact tables in a dimensional model are supposed to represent granular events that occur from various business processes (sales orders, deliveries, purchase orders, quotes, manufacturing sequences, account payables, and so on). Dimensions complement the fact tables to provide textual context to events as an entry point for analysis (Number of order by &amp;lt;some dimensional attribute(s)&amp;gt; Got it, super clear and makes sense. &lt;/p&gt;\n\n&lt;p&gt;Kimball says that these Fact and Dimension tables are used in the BI Applications layer for analytical needs, such as Power BI for a report. However, I am confused because these fact tables (as-is) aren&amp;#39;t really too valuable to people within a department who want to analyze a specific problem or process. I hardly see the case where I am using a fact table as is in a reporting tool, I would need to perform additional transformations, joins, and business logic before the reporting tool. In a true Kimball warehouse, are these just views that belong in the BI Application Layer and the reporting tool just uses these views instead of the fact tables directly?&lt;/p&gt;\n\n&lt;p&gt;For example, a pricing department would want to have a table that joins the sales orders fact to the quotes fact, filters on certain criteria, and calculates several conditional and mathematical columns. This would then be the table (or view?) that they would use in Power BI to report on conversion rates by region, product, customer, etc...).&lt;/p&gt;\n\n&lt;p&gt;Another example would be a logistics department wanting a report for On Time Deliveries. They would want a table (or view?) that joins the sales orders fact to the deliveries fact to obtain the date that an order line was supposed to be delivered by, create a binary column for if on time, and filter out deliveries of internal or sample orders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c6fnt", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c6fnt/kimball_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c6fnt/kimball_fact_tables/", "subreddit_subscribers": 144190, "created_utc": 1701876582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nThanks for clicking on this post despite its non-catchy title. :) \n\nI'm a bit at loss on how to solve a snapshotting issue where rows from source disappear if they're deleted.\n\nI have a table that lists my suppliers per product that looks like below. There can only be one preferred supplier per product. \n\n**SuppliersPerProduct at 2023-12-03:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|\n|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|\n|A|V2|Y|2020-01-01|2021-05-08|...|\n|A|V3|N|2021-01-01|2022-04-01|...|\n\n&amp;#x200B;\n\nOn 2023-12-04 vendor V2 is deleted from the system, vendor V3 takes over as the preferred supplier. \n\n**SuppliersPerProduct at 2023-12-04:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|\n|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|\n|A|V3|Y|2021-01-01|2023-12-04|...|\n\n&amp;#x200B;\n\nI use DBT to snapshot the tables on 2023-12-05 with \\`invalidate\\_hard\\_deletes = True\\` and my result is:\n\n**Snapshot\\_SuppliersPerProduct at 2023-12-05:**\n\n|Product|Vendor|PrefSupplier|CreateDate|LastMut|...|dbt\\_valid\\_from|dbt\\_valid\\_to|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|A|V1|N|2020-01-01|2021-05-08|...|2021-05-08 |null|\n|A|V2|Y|2020-01-01|2021-05-08|...|2021-05-08|**2023-12-05**|\n|A|V3|N|2021-01-01|2022-04-01|...|2022-04-01|2023-12-05 |\n|A|V3|Y|2021-01-01|2023-12-04|...|**2023-12-04**|null|\n\nAccording to the snapshotted table there were two preferred suppliers on 2023-12-04, which is not possible according to our business logic. The problem originates from the 'dbt\\_valid\\_to' being set to the date (or rather, datetime) that dbt runs and dbt has no way of knowing two preferred suppliers is not possible.\n\nFor individual records I could of course update this manually, but at scale this would not be possible.\n\nI could write an additional step where I correct dbt\\_valid\\_to values in case of overlapping 'PrefSupplier' values, but I wonder if anyone has come across a similar scenario and solved it in a better way. \n\nDoes anyone have suggestions on how to best tackle this problem?", "author_fullname": "t2_ko2ybcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt snapshots - invalidate hard deletes dbt_valid_to overlaps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c4tgx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701871966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Thanks for clicking on this post despite its non-catchy title. :) &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a bit at loss on how to solve a snapshotting issue where rows from source disappear if they&amp;#39;re deleted.&lt;/p&gt;\n\n&lt;p&gt;I have a table that lists my suppliers per product that looks like below. There can only be one preferred supplier per product. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SuppliersPerProduct at 2023-12-03:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On 2023-12-04 vendor V2 is deleted from the system, vendor V3 takes over as the preferred supplier. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SuppliersPerProduct at 2023-12-04:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-04&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I use DBT to snapshot the tables on 2023-12-05 with `invalidate_hard_deletes = True` and my result is:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Snapshot_SuppliersPerProduct at 2023-12-05:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Vendor&lt;/th&gt;\n&lt;th align=\"left\"&gt;PrefSupplier&lt;/th&gt;\n&lt;th align=\"left\"&gt;CreateDate&lt;/th&gt;\n&lt;th align=\"left\"&gt;LastMut&lt;/th&gt;\n&lt;th align=\"left\"&gt;...&lt;/th&gt;\n&lt;th align=\"left\"&gt;dbt_valid_from&lt;/th&gt;\n&lt;th align=\"left\"&gt;dbt_valid_to&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V1&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2020-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-05-08&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;2023-12-05&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;N&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;2022-04-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;V3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Y&lt;/td&gt;\n&lt;td align=\"left\"&gt;2021-01-01&lt;/td&gt;\n&lt;td align=\"left\"&gt;2023-12-04&lt;/td&gt;\n&lt;td align=\"left\"&gt;...&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;2023-12-04&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;According to the snapshotted table there were two preferred suppliers on 2023-12-04, which is not possible according to our business logic. The problem originates from the &amp;#39;dbt_valid_to&amp;#39; being set to the date (or rather, datetime) that dbt runs and dbt has no way of knowing two preferred suppliers is not possible.&lt;/p&gt;\n\n&lt;p&gt;For individual records I could of course update this manually, but at scale this would not be possible.&lt;/p&gt;\n\n&lt;p&gt;I could write an additional step where I correct dbt_valid_to values in case of overlapping &amp;#39;PrefSupplier&amp;#39; values, but I wonder if anyone has come across a similar scenario and solved it in a better way. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have suggestions on how to best tackle this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18c4tgx", "is_robot_indexable": true, "report_reasons": null, "author": "nl_dhh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c4tgx/dbt_snapshots_invalidate_hard_deletes_dbt_valid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c4tgx/dbt_snapshots_invalidate_hard_deletes_dbt_valid/", "subreddit_subscribers": 144190, "created_utc": 1701871966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, we have decided to go with dbt as our transformation tool, just wanted to understand how dbt is used in the industry. I am struggling to find a way to orchestrate my dbt models. I would prefer a simple orchestration mechanism that doesn\u2019t involve much setup and development. But I would like to have a UI from where I can orchestrate these dbt runs. Fivetran does this but is in beta currently and I would like to stay away from that.", "author_fullname": "t2_6y1og8oq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for dbt on development and production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18c08c0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701854562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, we have decided to go with dbt as our transformation tool, just wanted to understand how dbt is used in the industry. I am struggling to find a way to orchestrate my dbt models. I would prefer a simple orchestration mechanism that doesn\u2019t involve much setup and development. But I would like to have a UI from where I can orchestrate these dbt runs. Fivetran does this but is in beta currently and I would like to stay away from that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c08c0", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Ninja70", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c08c0/best_practices_for_dbt_on_development_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c08c0/best_practices_for_dbt_on_development_and/", "subreddit_subscribers": 144190, "created_utc": 1701854562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It's my first time working with HIPAA data and was wondering in the extraction process is it okay to use python to call the api to a cloud warehouse ? \n\nDo I need to use a service like Azure Data Factory or Fivetran to get a HIPAA BAA/certification?", "author_fullname": "t2_7euoosst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HIPAA Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bp2tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701817152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s my first time working with HIPAA data and was wondering in the extraction process is it okay to use python to call the api to a cloud warehouse ? &lt;/p&gt;\n\n&lt;p&gt;Do I need to use a service like Azure Data Factory or Fivetran to get a HIPAA BAA/certification?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18bp2tu", "is_robot_indexable": true, "report_reasons": null, "author": "rainamlien", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bp2tu/hipaa_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bp2tu/hipaa_data/", "subreddit_subscribers": 144190, "created_utc": 1701817152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. \n\nOne is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.\n\nFor the Bloomberg one I've been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. \n\nWhile for the Booz Allen Hamilton role it's only one round. From what I've heard it's conversational and theoretical in nature and I won't be expected to write code during the interview. If anyone has any experience interviewing with them I'd appreciate any insight you could offer", "author_fullname": "t2_p5wlf0g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18bmnwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701811132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, asking for some help. Have a couple of interviews next week and would like any help that can be offered in preparing for them. &lt;/p&gt;\n\n&lt;p&gt;One is with Booz Allen Hamilton for a Data Engineer role and the other is the technical for a Bloomberg Senior Data Engineer role.&lt;/p&gt;\n\n&lt;p&gt;For the Bloomberg one I&amp;#39;ve been practising the tagged questions on Leetcode. Any other help/hints someone could give is appreciated. &lt;/p&gt;\n\n&lt;p&gt;While for the Booz Allen Hamilton role it&amp;#39;s only one round. From what I&amp;#39;ve heard it&amp;#39;s conversational and theoretical in nature and I won&amp;#39;t be expected to write code during the interview. If anyone has any experience interviewing with them I&amp;#39;d appreciate any insight you could offer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "18bmnwy", "is_robot_indexable": true, "report_reasons": null, "author": "El_Cato_Crande", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18bmnwy/interview_prep_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18bmnwy/interview_prep_help/", "subreddit_subscribers": 144190, "created_utc": 1701811132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have experience with reading directly from s3 (parquet/csv)?\n\nPandas uses s3fs via fsspect, which depends on  aiobotocore (which can cause dependency problems in many AWS services). AWS Data wrangler uses a custom s3fs which doesn't have this dependency.\n\n* [https://github.com/pandas-dev/pandas/issues/53712](https://github.com/pandas-dev/pandas/issues/53712)\n* [https://github.com/fsspec/s3fs/issues/815](https://github.com/fsspec/s3fs/issues/815)\n\nLooks like they recently relaxed the dependency version, so maybe not that big of a deal?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read files from s3 using Pandas/s3fs or AWS Data Wrangler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18c7p3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701879947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience with reading directly from s3 (parquet/csv)?&lt;/p&gt;\n\n&lt;p&gt;Pandas uses s3fs via fsspect, which depends on  aiobotocore (which can cause dependency problems in many AWS services). AWS Data wrangler uses a custom s3fs which doesn&amp;#39;t have this dependency.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/pandas-dev/pandas/issues/53712\"&gt;https://github.com/pandas-dev/pandas/issues/53712&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/fsspec/s3fs/issues/815\"&gt;https://github.com/fsspec/s3fs/issues/815&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Looks like they recently relaxed the dependency version, so maybe not that big of a deal?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?auto=webp&amp;s=27d795298b9e684a7da038bdcc98907cead4bc7d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a36a3909fa4192f6414e2ab6dba03099d7928c2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=43ad7d6ad95c597b4a7d6786098647b67d73cc28", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=787139355c4fc9a44101186ea68d1ce16d0a979f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=111501e0ea0c9270249754c13594bab3c8e7c1e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad84a9517be2f4cd585ccfa8e4971d3cb556e52", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0bbhU8K_jzQZr2UzL7o12IqZ74m4PM6Z1Ss5_TPamas.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a35e34c09a39e6cbc763d088c7ba7e499c5c65a", "width": 1080, "height": 540}], "variants": {}, "id": "QTNQeuXf5QeCcn9jEsbHOley3wu3l3gJcmCyM0NuerI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18c7p3u", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18c7p3u/read_files_from_s3_using_pandass3fs_or_aws_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18c7p3u/read_files_from_s3_using_pandass3fs_or_aws_data/", "subreddit_subscribers": 144190, "created_utc": 1701879947.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}