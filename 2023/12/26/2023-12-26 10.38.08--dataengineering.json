{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Is it practical to use Delta Lake in an on-premises environment? Are there any detailed tutorials for this? I need to write real-time data from Kafka to Delta Lake using Spark Structured Streaming. What do you recommend? ", "author_fullname": "t2_enf7z6r5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake without Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qjniu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703514417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it practical to use Delta Lake in an on-premises environment? Are there any detailed tutorials for this? I need to write real-time data from Kafka to Delta Lake using Spark Structured Streaming. What do you recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qjniu", "is_robot_indexable": true, "report_reasons": null, "author": "ANAKSIMANDR0S", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qjniu/delta_lake_without_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qjniu/delta_lake_without_databricks/", "subreddit_subscribers": 148427, "created_utc": 1703514417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. \n\nI also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.", "author_fullname": "t2_mil58gc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently accepted an offer as a DE for the first time! Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qumit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. &lt;/p&gt;\n\n&lt;p&gt;I also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18qumit", "is_robot_indexable": true, "report_reasons": null, "author": "spunkytale", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "subreddit_subscribers": 148427, "created_utc": 1703549033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nIs it just me or is the leetcode's MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode's SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.\n\nNow you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?", "author_fullname": "t2_9mm5jt001", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LEETCODE'S T-SQL issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qwtej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Is it just me or is the leetcode&amp;#39;s MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode&amp;#39;s SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.&lt;/p&gt;\n\n&lt;p&gt;Now you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwtej", "is_robot_indexable": true, "report_reasons": null, "author": "Master-Influence7539", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "subreddit_subscribers": 148427, "created_utc": 1703555730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I keep hearing anecdotes about it being painful to use reliably in production. Is anyone using it over other solutions like ELT specialized vendors or self-managed Debezium+Kafka? Any issues or pitfalls?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DMS for CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qkjwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703517602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep hearing anecdotes about it being painful to use reliably in production. Is anyone using it over other solutions like ELT specialized vendors or self-managed Debezium+Kafka? Any issues or pitfalls?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18qkjwx", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qkjwx/aws_dms_for_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qkjwx/aws_dms_for_cdc/", "subreddit_subscribers": 148427, "created_utc": 1703517602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nCurrently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company's needs. I would appreciate your assistance in making design choices.\n\nWe have a relatively **small dataset** of around 50,000 **large** S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.\n\nOur goal is to train Deep Learning models using these images and establish the capability **to search and group them based on their metadata**. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.\n\nHow can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for **efficient searching and data extraction by our team**?  \n\n\nDo you have any resources/blog posts with similar problems and solutions to those described above?  \n\n\nThank you!", "author_fullname": "t2_yu7cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance: Designing a Data Platform for Efficient Image Annotation, Deep Learning, and Metadata Search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qut3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Currently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company&amp;#39;s needs. I would appreciate your assistance in making design choices.&lt;/p&gt;\n\n&lt;p&gt;We have a relatively &lt;strong&gt;small dataset&lt;/strong&gt; of around 50,000 &lt;strong&gt;large&lt;/strong&gt; S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to train Deep Learning models using these images and establish the capability &lt;strong&gt;to search and group them based on their metadata&lt;/strong&gt;. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.&lt;/p&gt;\n\n&lt;p&gt;How can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for &lt;strong&gt;efficient searching and data extraction by our team&lt;/strong&gt;?  &lt;/p&gt;\n\n&lt;p&gt;Do you have any resources/blog posts with similar problems and solutions to those described above?  &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18qut3h", "is_robot_indexable": true, "report_reasons": null, "author": "UserPobro", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "subreddit_subscribers": 148427, "created_utc": 1703549598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data pipeline, extracts data from APIs using Cloud Functions and drop them in GCS bucket. Another cloud function that takes those files and pushes them to Bigqquery.   \nThe second Cloud function has a linear backoff to handle 10ops/5sec limit. But data is too much and coming at much faster rate that we still see failed instances of data loaded to bigquery.  \n\n\nI can think of 2 ways to fix it:  \n\\- Reduce the speed to data extraction. This is done via Cloud Tasks and may be I can add throttling to it so that API calls are made at lower rate.   \n\\- I think there is also a possibility to add a a messaging service in between but I have not used pub sub before. So not entirely sure about it. The idea could be to push data to pub sub instead of Google Storage (since we are not really making any use of this data, in case of issues we prefer to just extract it again).  \n\n\nDo you have any recommendations or suggestion what might be a better option? Our whole extraction process is simply API calls to a dozen or more sources (each in its own cloud function and then source table) and processing later in Bigquery.   \n", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data loading to bigquery failing with rate limit error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r1fou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703570735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data pipeline, extracts data from APIs using Cloud Functions and drop them in GCS bucket. Another cloud function that takes those files and pushes them to Bigqquery.&lt;br/&gt;\nThe second Cloud function has a linear backoff to handle 10ops/5sec limit. But data is too much and coming at much faster rate that we still see failed instances of data loaded to bigquery.  &lt;/p&gt;\n\n&lt;p&gt;I can think of 2 ways to fix it:&lt;br/&gt;\n- Reduce the speed to data extraction. This is done via Cloud Tasks and may be I can add throttling to it so that API calls are made at lower rate.&lt;br/&gt;\n- I think there is also a possibility to add a a messaging service in between but I have not used pub sub before. So not entirely sure about it. The idea could be to push data to pub sub instead of Google Storage (since we are not really making any use of this data, in case of issues we prefer to just extract it again).  &lt;/p&gt;\n\n&lt;p&gt;Do you have any recommendations or suggestion what might be a better option? Our whole extraction process is simply API calls to a dozen or more sources (each in its own cloud function and then source table) and processing later in Bigquery.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r1fou", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r1fou/data_loading_to_bigquery_failing_with_rate_limit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r1fou/data_loading_to_bigquery_failing_with_rate_limit/", "subreddit_subscribers": 148427, "created_utc": 1703570735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently introduced DBT in our data pipeline, and I am trying to orchestrate the pipeline with airflow and create a tool so that the analytics team can make their changes on their own.  \n\n\nCan anyone suggest some guidelines for this process?", "author_fullname": "t2_ao3wkczf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core orchestration in-house implementation - OPINION", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r0n54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703567977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently introduced DBT in our data pipeline, and I am trying to orchestrate the pipeline with airflow and create a tool so that the analytics team can make their changes on their own.  &lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest some guidelines for this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r0n54", "is_robot_indexable": true, "report_reasons": null, "author": "Beautiful-Big-75", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r0n54/dbt_core_orchestration_inhouse_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r0n54/dbt_core_orchestration_inhouse_implementation/", "subreddit_subscribers": 148427, "created_utc": 1703567977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Folks,   \n \n\nI come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   \n\n\nfor me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  \n\n\nmore or less need ideas for data science \"tasks\" to help with me playing around data", "author_fullname": "t2_8x6hlh81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion for Tasks to play around with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qwm1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,   &lt;/p&gt;\n\n&lt;p&gt;I come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   &lt;/p&gt;\n\n&lt;p&gt;for me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  &lt;/p&gt;\n\n&lt;p&gt;more or less need ideas for data science &amp;quot;tasks&amp;quot; to help with me playing around data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwm1g", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful-Award5471", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "subreddit_subscribers": 148427, "created_utc": 1703555091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Azure Synapse Analytics: A Step-by-Step Guide.\" Get ready to explore this comprehensive resource! \ud83d\udca1  \n\n\nAre you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We'll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  \n\n\nDon't miss out on this valuable resource! Read the full article here:  \n[https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/](https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/)  \n[\\#AzureSynapseAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#StepByStepGuide](https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataInsights](https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#LinkedInArticle](https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#dataanalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#data](https://www.linkedin.com/feed/hashtag/?keywords=data&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688)", "author_fullname": "t2_blyyz3sy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics: A Step-by-Step Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18quryl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Azure Synapse Analytics: A Step-by-Step Guide.&amp;quot; Get ready to explore this comprehensive resource! \ud83d\udca1  &lt;/p&gt;\n\n&lt;p&gt;Are you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We&amp;#39;ll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t miss out on this valuable resource! Read the full article here:&lt;br/&gt;\n&lt;a href=\"https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/\"&gt;https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#AzureSynapseAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#StepByStepGuide&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataInsights&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#LinkedInArticle&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#dataanalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=data&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#data&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18quryl", "is_robot_indexable": true, "report_reasons": null, "author": "Bubbly_Bed_4478", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "subreddit_subscribers": 148427, "created_utc": 1703549503.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&gt; resources must be publicly accessible and can't be behind a firewall or secured in a [vnet]\n\nThis is from [this](https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake) page in ms docs.\n\nReally!?  Is anyone actually using it and if so, how!?", "author_fullname": "t2_5r4sezi25", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure event hubs and delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qufc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703548452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;resources must be publicly accessible and can&amp;#39;t be behind a firewall or secured in a [vnet]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is from &lt;a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake\"&gt;this&lt;/a&gt; page in ms docs.&lt;/p&gt;\n\n&lt;p&gt;Really!?  Is anyone actually using it and if so, how!?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qufc5", "is_robot_indexable": true, "report_reasons": null, "author": "MachineLooning", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "subreddit_subscribers": 148427, "created_utc": 1703548452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy,\n\nI've been tasked to migrate one of the existing solutions from AWS to GCP. The solution is rather simple, it's a combination of Lambda, Batch Job (fargate running ecr image) and S3. Basically lambda triggers batch job which does data processing and saves the outcome to s3. On top of that we have Athena running queries on that outcome s3 bucket.\n\nWould you be so kind and recommend me a blog post, video tutorial or some other material, that would help person that is fairly proficient with AWS services to build something similar on GCP considering mentioned architecture?\n\nAny recommendation is helpful.\nThanks!", "author_fullname": "t2_fw1zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data transformation solution from AWS to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18r4573", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703581708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked to migrate one of the existing solutions from AWS to GCP. The solution is rather simple, it&amp;#39;s a combination of Lambda, Batch Job (fargate running ecr image) and S3. Basically lambda triggers batch job which does data processing and saves the outcome to s3. On top of that we have Athena running queries on that outcome s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Would you be so kind and recommend me a blog post, video tutorial or some other material, that would help person that is fairly proficient with AWS services to build something similar on GCP considering mentioned architecture?&lt;/p&gt;\n\n&lt;p&gt;Any recommendation is helpful.\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r4573", "is_robot_indexable": true, "report_reasons": null, "author": "saif3r", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r4573/moving_data_transformation_solution_from_aws_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r4573/moving_data_transformation_solution_from_aws_to/", "subreddit_subscribers": 148427, "created_utc": 1703581708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.\n\nIs there a (preferably a visual and free) tool for helping mapping such datasets?\n\nWhen googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.", "author_fullname": "t2_xfhxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging datasets with different structure.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qv3lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703550473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.&lt;/p&gt;\n\n&lt;p&gt;Is there a (preferably a visual and free) tool for helping mapping such datasets?&lt;/p&gt;\n\n&lt;p&gt;When googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qv3lf", "is_robot_indexable": true, "report_reasons": null, "author": "LarsSorensen", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "subreddit_subscribers": 148427, "created_utc": 1703550473.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}