{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Is it practical to use Delta Lake in an on-premises environment? Are there any detailed tutorials for this? I need to write real-time data from Kafka to Delta Lake using Spark Structured Streaming. What do you recommend? ", "author_fullname": "t2_enf7z6r5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake without Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qjniu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703514417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it practical to use Delta Lake in an on-premises environment? Are there any detailed tutorials for this? I need to write real-time data from Kafka to Delta Lake using Spark Structured Streaming. What do you recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qjniu", "is_robot_indexable": true, "report_reasons": null, "author": "ANAKSIMANDR0S", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qjniu/delta_lake_without_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qjniu/delta_lake_without_databricks/", "subreddit_subscribers": 148344, "created_utc": 1703514417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I keep hearing anecdotes about it being painful to use reliably in production. Is anyone using it over other solutions like ELT specialized vendors or self-managed Debezium+Kafka? Any issues or pitfalls?", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS DMS for CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qkjwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703517602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep hearing anecdotes about it being painful to use reliably in production. Is anyone using it over other solutions like ELT specialized vendors or self-managed Debezium+Kafka? Any issues or pitfalls?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18qkjwx", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qkjwx/aws_dms_for_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qkjwx/aws_dms_for_cdc/", "subreddit_subscribers": 148344, "created_utc": 1703517602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. \n\nI also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.", "author_fullname": "t2_mil58gc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently accepted an offer as a DE for the first time! Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qumit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. &lt;/p&gt;\n\n&lt;p&gt;I also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18qumit", "is_robot_indexable": true, "report_reasons": null, "author": "spunkytale", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "subreddit_subscribers": 148344, "created_utc": 1703549033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nIs it just me or is the leetcode's MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode's SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.\n\nNow you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?", "author_fullname": "t2_9mm5jt001", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LEETCODE'S T-SQL issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18qwtej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Is it just me or is the leetcode&amp;#39;s MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode&amp;#39;s SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.&lt;/p&gt;\n\n&lt;p&gt;Now you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwtej", "is_robot_indexable": true, "report_reasons": null, "author": "Master-Influence7539", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "subreddit_subscribers": 148344, "created_utc": 1703555730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was testing the Airbyte normally but after reboot the VM I lost the browser connection to the tool.\n\nBelow is the Docker setup information.\n\nLooks like port fowarding problem from server or Docker. \n\nAnyone can help me to understand what is happening?\n\n    [\n        {\n            \"Id\": \"608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4\",\n            \"Created\": \"2023-12-26T00:32:30.239866257Z\",\n            \"Path\": \"/bin/bash\",\n            \"Args\": [\n                \"-c\",\n                \"airbyte-app/bin/${APPLICATION}\"\n            ],\n            \"State\": {\n                \"Status\": \"running\",\n                \"Running\": true,\n                \"Paused\": false,\n                \"Restarting\": false,\n                \"OOMKilled\": false,\n                \"Dead\": false,\n                \"Pid\": 28861,\n                \"ExitCode\": 0,\n                \"Error\": \"\",\n                \"StartedAt\": \"2023-12-26T01:37:43.053729625Z\",\n                \"FinishedAt\": \"2023-12-26T01:37:40.299906415Z\"\n            },\n            \"Image\": \"sha256:2dd8aec8a791cb58db4469ff22ed38d9facf4f53db459784196e21f46cb23c4d\",\n            \"ResolvConfPath\": \"/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/resolv.conf\",\n            \"HostnamePath\": \"/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/hostname\",\n            \"HostsPath\": \"/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/hosts\",\n            \"LogPath\": \"/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4-json.log\",\n            \"Name\": \"/airbyte-server\",\n            \"RestartCount\": 0,\n            \"Driver\": \"overlay2\",\n            \"Platform\": \"linux\",\n            \"MountLabel\": \"\",\n            \"ProcessLabel\": \"\",\n            \"AppArmorProfile\": \"docker-default\",\n            \"ExecIDs\": null,\n            \"HostConfig\": {\n                \"Binds\": [\n                    \"/tmp/airbyte_local:/tmp/airbyte_local:rw\",\n                    \"/home/marconi_medeiros/airbyte/configs:/app/configs:ro\"\n                ],\n                \"ContainerIDFile\": \"\",\n                \"LogConfig\": {\n                    \"Type\": \"json-file\",\n                    \"Config\": {\n                        \"max-file\": \"5\",\n                        \"max-size\": \"100m\"\n                    }\n                },\n                \"NetworkMode\": \"airbyte_airbyte_internal\",\n                \"PortBindings\": {\n                    \"8001/tcp\": [\n                        {\n                            \"HostIp\": \"\",\n                            \"HostPort\": \"\"\n                        }\n                    ]\n                },\n                \"RestartPolicy\": {\n                    \"Name\": \"unless-stopped\",\n                    \"MaximumRetryCount\": 0\n                },\n                \"AutoRemove\": false,\n                \"VolumeDriver\": \"\",\n                \"VolumesFrom\": null,\n                \"ConsoleSize\": [\n                    0,\n                    0\n                ],\n                \"CapAdd\": null,\n                \"CapDrop\": null,\n                \"CgroupnsMode\": \"host\",\n                \"Dns\": [],\n                \"DnsOptions\": [],\n                \"DnsSearch\": [],\n                \"ExtraHosts\": [],\n                \"GroupAdd\": null,\n                \"IpcMode\": \"private\",\n                \"Cgroup\": \"\",\n                \"Links\": null,\n                \"OomScoreAdj\": 0,\n                \"PidMode\": \"\",\n                \"Privileged\": false,\n                \"PublishAllPorts\": false,\n                \"ReadonlyRootfs\": false,\n                \"SecurityOpt\": null,\n                \"UTSMode\": \"\",\n                \"UsernsMode\": \"\",\n                \"ShmSize\": 67108864,\n                \"Runtime\": \"runc\",\n                \"Isolation\": \"\",\n                \"CpuShares\": 0,\n                \"Memory\": 0,\n                \"NanoCpus\": 0,\n                \"CgroupParent\": \"\",\n                \"BlkioWeight\": 0,\n                \"BlkioWeightDevice\": null,\n                \"BlkioDeviceReadBps\": null,\n                \"BlkioDeviceWriteBps\": null,\n                \"BlkioDeviceReadIOps\": null,\n                \"BlkioDeviceWriteIOps\": null,\n                \"CpuPeriod\": 0,\n                \"CpuQuota\": 0,\n                \"CpuRealtimePeriod\": 0,\n                \"CpuRealtimeRuntime\": 0,\n                \"CpusetCpus\": \"\",\n                \"CpusetMems\": \"\",\n                \"Devices\": null,\n                \"DeviceCgroupRules\": null,\n                \"DeviceRequests\": null,\n                \"MemoryReservation\": 0,\n                \"MemorySwap\": 0,\n                \"MemorySwappiness\": null,\n                \"OomKillDisable\": false,\n                \"PidsLimit\": null,\n                \"Ulimits\": null,\n                \"CpuCount\": 0,\n                \"CpuPercent\": 0,\n                \"IOMaximumIOps\": 0,\n                \"IOMaximumBandwidth\": 0,\n                \"Mounts\": [\n                    {\n                        \"Type\": \"volume\",\n                        \"Source\": \"airbyte_workspace\",\n                        \"Target\": \"/tmp/workspace\",\n                        \"VolumeOptions\": {}\n                    },\n                    {\n                        \"Type\": \"volume\",\n                        \"Source\": \"airbyte_data\",\n                        \"Target\": \"/data\",\n                        \"VolumeOptions\": {}\n                    },\n                    {\n                        \"Type\": \"bind\",\n                        \"Source\": \"/home/marconi_medeiros/airbyte/flags.yml\",\n                        \"Target\": \"/flags\",\n                        \"ReadOnly\": true\n                    }\n                ],\n                \"MaskedPaths\": [\n                    \"/proc/asound\",\n                    \"/proc/acpi\",\n                    \"/proc/kcore\",\n                    \"/proc/keys\",\n                    \"/proc/latency_stats\",\n                    \"/proc/timer_list\",\n                    \"/proc/timer_stats\",\n                    \"/proc/sched_debug\",\n                    \"/proc/scsi\",\n                    \"/sys/firmware\",\n                    \"/sys/devices/virtual/powercap\"\n                ],\n                \"ReadonlyPaths\": [\n                    \"/proc/bus\",\n                    \"/proc/fs\",\n                    \"/proc/irq\",\n                    \"/proc/sys\",\n                    \"/proc/sysrq-trigger\"\n                ]\n            },\n            \"GraphDriver\": {\n                \"Data\": {\n                    \"LowerDir\": \"/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35-init/diff:/var/lib/docker/overlay2/86d4469575cfe4cf9bd75b0ea667686e7524848aa7e95c811a712520c776a3c6/diff:/var/lib/docker/overlay2/1cda7fe85b404f4245becf0a65c3de9bb47ea514b79f7f361d62b97f2944a03d/diff:/var/lib/docker/overlay2/f0e9905faf280df4b1f204dd764605027cf992e4d160c6682ec20af78d9274ec/diff:/var/lib/docker/overlay2/f04c7d25612b9081720eb19f36b88b5a57087df423a094c2c76caa11f64bfe14/diff:/var/lib/docker/overlay2/58d62321c1dfd24e971359ce9e71a81f9a39eb7107d8d1ead3681fb7e6cb1478/diff:/var/lib/docker/overlay2/a7aad538563470acbc8607926e5373b5e30ffc80920fbb5d16b7351bd7ba4e35/diff:/var/lib/docker/overlay2/164145d9b36d64bd5fcd70cffb90f9345d5536edb2ca350df178ddecd31d3a0f/diff:/var/lib/docker/overlay2/18362139df4a8f8784850dcc295033d6680b5bbeb130f5cf2879792af4a7af88/diff\",\n                    \"MergedDir\": \"/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/merged\",\n                    \"UpperDir\": \"/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/diff\",\n                    \"WorkDir\": \"/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/work\"\n                },\n                \"Name\": \"overlay2\"\n            },\n            \"Mounts\": [\n                {\n                    \"Type\": \"bind\",\n                    \"Source\": \"/home/marconi_medeiros/airbyte/configs\",\n                    \"Destination\": \"/app/configs\",\n                    \"Mode\": \"ro\",\n                    \"RW\": false,\n                    \"Propagation\": \"rprivate\"\n                },\n                {\n                    \"Type\": \"volume\",\n                    \"Name\": \"airbyte_data\",\n                    \"Source\": \"/var/lib/docker/volumes/airbyte_data/_data\",\n                    \"Destination\": \"/data\",\n                    \"Driver\": \"local\",\n                    \"Mode\": \"z\",\n                    \"RW\": true,\n                    \"Propagation\": \"\"\n                },\n                {\n                    \"Type\": \"bind\",\n                    \"Source\": \"/home/marconi_medeiros/airbyte/flags.yml\",\n                    \"Destination\": \"/flags\",\n                    \"Mode\": \"\",\n                    \"RW\": false,\n                    \"Propagation\": \"rprivate\"\n                },\n                {\n                    \"Type\": \"bind\",\n                    \"Source\": \"/tmp/airbyte_local\",\n                    \"Destination\": \"/tmp/airbyte_local\",\n                    \"Mode\": \"rw\",\n                    \"RW\": true,\n                    \"Propagation\": \"rprivate\"\n                },\n                {\n                    \"Type\": \"volume\",\n                    \"Name\": \"airbyte_workspace\",\n                    \"Source\": \"/var/lib/docker/volumes/airbyte_workspace/_data\",\n                    \"Destination\": \"/tmp/workspace\",\n                    \"Driver\": \"local\",\n                    \"Mode\": \"z\",\n                    \"RW\": true,\n                    \"Propagation\": \"\"\n                }\n            ],\n            \"Config\": {\n                \"Hostname\": \"608aea695e05\",\n                \"Domainname\": \"\",\n                \"User\": \"\",\n                \"AttachStdin\": false,\n                \"AttachStdout\": true,\n                \"AttachStderr\": true,\n                \"ExposedPorts\": {\n                    \"5005/tcp\": {},\n                    \"8000/tcp\": {},\n                    \"8001/tcp\": {}\n                },\n                \"Tty\": false,\n                \"OpenStdin\": false,\n                \"StdinOnce\": false,\n                \"Env\": [\n                    \"LAUNCHDARKLY_KEY=\",\n                    \"LOG_LEVEL=INFO\",\n                    \"AIRBYTE_VERSION=0.50.39\",\n                    \"FEATURE_FLAG_CLIENT=config\",\n                    \"SEGMENT_WRITE_KEY=7UDdp5K55CyiGgsauOr2pNNujGvmhaeu\",\n                    \"WORKER_ENVIRONMENT=\",\n                    \"JOB_MAIN_CONTAINER_MEMORY_LIMIT=\",\n                    \"DD_DOGSTATSD_PORT=\",\n                    \"JOB_MAIN_CONTAINER_CPU_LIMIT=\",\n                    \"AUTO_DETECT_SCHEMA=true\",\n                    \"DATABASE_PASSWORD=docker\",\n                    \"MAX_NOTIFY_WORKERS=5\",\n                    \"CONFIG_DATABASE_USER=\",\n                    \"CONNECTOR_REGISTRY_BASE_URL=\",\n                    \"AIRBYTE_ROLE=\",\n                    \"PUBLISH_METRICS=false\",\n                    \"TRACKING_STRATEGY=segment\",\n                    \"JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=0.40.26.001\",\n                    \"JOB_MAIN_CONTAINER_CPU_REQUEST=\",\n                    \"CONFIG_DATABASE_PASSWORD=\",\n                    \"CONFIG_DATABASE_URL=\",\n                    \"SECRET_PERSISTENCE=TESTING_CONFIG_DB_TABLE\",\n                    \"GITHUB_STORE_BRANCH=\",\n                    \"SHOULD_RUN_NOTIFY_WORKFLOWS=true\",\n                    \"DD_AGENT_HOST=\",\n                    \"TEMPORAL_HOST=airbyte-temporal:7233\",\n                    \"NEW_SCHEDULER=\",\n                    \"METRIC_CLIENT=\",\n                    \"JOB_ERROR_REPORTING_SENTRY_DSN=\",\n                    \"MICRONAUT_ENVIRONMENTS=control-plane\",\n                    \"WORKSPACE_ROOT=/tmp/workspace\",\n                    \"CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=0.40.23.002\",\n                    \"WEBAPP_URL=http://localhost:8000/\",\n                    \"CONFIG_ROOT=/data\",\n                    \"JOB_MAIN_CONTAINER_MEMORY_REQUEST=\",\n                    \"DATABASE_USER=docker\",\n                    \"DATABASE_URL=jdbc:postgresql://db:5432/airbyte\",\n                    \"JOB_ERROR_REPORTING_STRATEGY=logging\",\n                    \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                    \"LANG=C.UTF-8\",\n                    \"JAVA_HOME=/usr/lib/jvm/java-21-amazon-corretto\",\n                    \"APPLICATION=airbyte-server\",\n                    \"VERSION=0.50.39\"\n                ],\n                \"Cmd\": null,\n                \"Image\": \"airbyte/server:0.50.39\",\n                \"Volumes\": null,\n                \"WorkingDir\": \"/app\",\n                \"Entrypoint\": [\n                    \"/bin/bash\",\n                    \"-c\",\n                    \"airbyte-app/bin/${APPLICATION}\"\n                ],\n                \"OnBuild\": null,\n                \"Labels\": {\n                    \"com.docker.compose.config-hash\": \"441abf94eb3ba81e930ec9ac68a4bdbbce4a27b3a5800e6800498b4e5c32d294\",\n                    \"com.docker.compose.container-number\": \"1\",\n                    \"com.docker.compose.depends_on\": \"bootloader:service_completed_successfully:false\",\n                    \"com.docker.compose.image\": \"sha256:2dd8aec8a791cb58db4469ff22ed38d9facf4f53db459784196e21f46cb23c4d\",\n                    \"com.docker.compose.oneoff\": \"False\",\n                    \"com.docker.compose.project\": \"airbyte\",\n                    \"com.docker.compose.project.config_files\": \"/home/marconi_medeiros/airbyte/docker-compose.yaml\",\n                    \"com.docker.compose.project.working_dir\": \"/home/marconi_medeiros/airbyte\",\n                    \"com.docker.compose.replace\": \"c600b842bba15fdda4f30323e07be77f32f17d98dc96db91317d199053380424\",\n                    \"com.docker.compose.service\": \"server\",\n                    \"com.docker.compose.version\": \"2.21.0\"\n                }\n            },\n            \"NetworkSettings\": {\n                \"Bridge\": \"\",\n                \"SandboxID\": \"7f628a9e8c539a52e38ed082b7e284d769a89aa6f5993bf88804a7baec2e67bb\",\n                \"HairpinMode\": false,\n                \"LinkLocalIPv6Address\": \"\",\n                \"LinkLocalIPv6PrefixLen\": 0,\n                \"Ports\": {\n                    \"5005/tcp\": null,\n                    \"8000/tcp\": null,\n                    \"8001/tcp\": [\n                        {\n                            \"HostIp\": \"0.0.0.0\",\n                            \"HostPort\": \"32783\"\n                        },\n                        {\n                            \"HostIp\": \"::\",\n                            \"HostPort\": \"32782\"\n                        }\n                    ]\n                },\n                \"SandboxKey\": \"/var/run/docker/netns/7f628a9e8c53\",\n                \"SecondaryIPAddresses\": null,\n                \"SecondaryIPv6Addresses\": null,\n                \"EndpointID\": \"\",\n                \"Gateway\": \"\",\n                \"GlobalIPv6Address\": \"\",\n                \"GlobalIPv6PrefixLen\": 0,\n                \"IPAddress\": \"\",\n                \"IPPrefixLen\": 0,\n                \"IPv6Gateway\": \"\",\n                \"MacAddress\": \"\",\n                \"Networks\": {\n                    \"airbyte_airbyte_internal\": {\n                        \"IPAMConfig\": null,\n                        \"Links\": null,\n                        \"Aliases\": [\n                            \"airbyte-server\",\n                            \"server\",\n                            \"608aea695e05\"\n                        ],\n                        \"NetworkID\": \"1b3d2150a3836a5cdfc591062965194ffbbb030d2ec94eba51f5819b239dd8c3\",\n                        \"EndpointID\": \"ba6c2e2ef548299873a1f08fc93fa3fb883f521865ff0c481c9cf2747bd24cf7\",\n                        \"Gateway\": \"172.19.0.1\",\n                        \"IPAddress\": \"172.19.0.4\",\n                        \"IPPrefixLen\": 16,\n                        \"IPv6Gateway\": \"\",\n                        \"GlobalIPv6Address\": \"\",\n                        \"GlobalIPv6PrefixLen\": 0,\n                        \"MacAddress\": \"02:42:ac:13:00:04\",\n                        \"DriverOpts\": null\n                    }\n                }\n            }\n        }\n    ]\n\n&amp;#x200B;", "author_fullname": "t2_ahryr1kv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte + Docker - Can't access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18qwova", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703556090.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing the Airbyte normally but after reboot the VM I lost the browser connection to the tool.&lt;/p&gt;\n\n&lt;p&gt;Below is the Docker setup information.&lt;/p&gt;\n\n&lt;p&gt;Looks like port fowarding problem from server or Docker. &lt;/p&gt;\n\n&lt;p&gt;Anyone can help me to understand what is happening?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[\n    {\n        &amp;quot;Id&amp;quot;: &amp;quot;608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4&amp;quot;,\n        &amp;quot;Created&amp;quot;: &amp;quot;2023-12-26T00:32:30.239866257Z&amp;quot;,\n        &amp;quot;Path&amp;quot;: &amp;quot;/bin/bash&amp;quot;,\n        &amp;quot;Args&amp;quot;: [\n            &amp;quot;-c&amp;quot;,\n            &amp;quot;airbyte-app/bin/${APPLICATION}&amp;quot;\n        ],\n        &amp;quot;State&amp;quot;: {\n            &amp;quot;Status&amp;quot;: &amp;quot;running&amp;quot;,\n            &amp;quot;Running&amp;quot;: true,\n            &amp;quot;Paused&amp;quot;: false,\n            &amp;quot;Restarting&amp;quot;: false,\n            &amp;quot;OOMKilled&amp;quot;: false,\n            &amp;quot;Dead&amp;quot;: false,\n            &amp;quot;Pid&amp;quot;: 28861,\n            &amp;quot;ExitCode&amp;quot;: 0,\n            &amp;quot;Error&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;StartedAt&amp;quot;: &amp;quot;2023-12-26T01:37:43.053729625Z&amp;quot;,\n            &amp;quot;FinishedAt&amp;quot;: &amp;quot;2023-12-26T01:37:40.299906415Z&amp;quot;\n        },\n        &amp;quot;Image&amp;quot;: &amp;quot;sha256:2dd8aec8a791cb58db4469ff22ed38d9facf4f53db459784196e21f46cb23c4d&amp;quot;,\n        &amp;quot;ResolvConfPath&amp;quot;: &amp;quot;/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/resolv.conf&amp;quot;,\n        &amp;quot;HostnamePath&amp;quot;: &amp;quot;/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/hostname&amp;quot;,\n        &amp;quot;HostsPath&amp;quot;: &amp;quot;/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/hosts&amp;quot;,\n        &amp;quot;LogPath&amp;quot;: &amp;quot;/var/lib/docker/containers/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4/608aea695e053711b0afb99123b787cdf01da0cb68ac89ccd48502a5b107b1d4-json.log&amp;quot;,\n        &amp;quot;Name&amp;quot;: &amp;quot;/airbyte-server&amp;quot;,\n        &amp;quot;RestartCount&amp;quot;: 0,\n        &amp;quot;Driver&amp;quot;: &amp;quot;overlay2&amp;quot;,\n        &amp;quot;Platform&amp;quot;: &amp;quot;linux&amp;quot;,\n        &amp;quot;MountLabel&amp;quot;: &amp;quot;&amp;quot;,\n        &amp;quot;ProcessLabel&amp;quot;: &amp;quot;&amp;quot;,\n        &amp;quot;AppArmorProfile&amp;quot;: &amp;quot;docker-default&amp;quot;,\n        &amp;quot;ExecIDs&amp;quot;: null,\n        &amp;quot;HostConfig&amp;quot;: {\n            &amp;quot;Binds&amp;quot;: [\n                &amp;quot;/tmp/airbyte_local:/tmp/airbyte_local:rw&amp;quot;,\n                &amp;quot;/home/marconi_medeiros/airbyte/configs:/app/configs:ro&amp;quot;\n            ],\n            &amp;quot;ContainerIDFile&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;LogConfig&amp;quot;: {\n                &amp;quot;Type&amp;quot;: &amp;quot;json-file&amp;quot;,\n                &amp;quot;Config&amp;quot;: {\n                    &amp;quot;max-file&amp;quot;: &amp;quot;5&amp;quot;,\n                    &amp;quot;max-size&amp;quot;: &amp;quot;100m&amp;quot;\n                }\n            },\n            &amp;quot;NetworkMode&amp;quot;: &amp;quot;airbyte_airbyte_internal&amp;quot;,\n            &amp;quot;PortBindings&amp;quot;: {\n                &amp;quot;8001/tcp&amp;quot;: [\n                    {\n                        &amp;quot;HostIp&amp;quot;: &amp;quot;&amp;quot;,\n                        &amp;quot;HostPort&amp;quot;: &amp;quot;&amp;quot;\n                    }\n                ]\n            },\n            &amp;quot;RestartPolicy&amp;quot;: {\n                &amp;quot;Name&amp;quot;: &amp;quot;unless-stopped&amp;quot;,\n                &amp;quot;MaximumRetryCount&amp;quot;: 0\n            },\n            &amp;quot;AutoRemove&amp;quot;: false,\n            &amp;quot;VolumeDriver&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;VolumesFrom&amp;quot;: null,\n            &amp;quot;ConsoleSize&amp;quot;: [\n                0,\n                0\n            ],\n            &amp;quot;CapAdd&amp;quot;: null,\n            &amp;quot;CapDrop&amp;quot;: null,\n            &amp;quot;CgroupnsMode&amp;quot;: &amp;quot;host&amp;quot;,\n            &amp;quot;Dns&amp;quot;: [],\n            &amp;quot;DnsOptions&amp;quot;: [],\n            &amp;quot;DnsSearch&amp;quot;: [],\n            &amp;quot;ExtraHosts&amp;quot;: [],\n            &amp;quot;GroupAdd&amp;quot;: null,\n            &amp;quot;IpcMode&amp;quot;: &amp;quot;private&amp;quot;,\n            &amp;quot;Cgroup&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;Links&amp;quot;: null,\n            &amp;quot;OomScoreAdj&amp;quot;: 0,\n            &amp;quot;PidMode&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;Privileged&amp;quot;: false,\n            &amp;quot;PublishAllPorts&amp;quot;: false,\n            &amp;quot;ReadonlyRootfs&amp;quot;: false,\n            &amp;quot;SecurityOpt&amp;quot;: null,\n            &amp;quot;UTSMode&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;UsernsMode&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;ShmSize&amp;quot;: 67108864,\n            &amp;quot;Runtime&amp;quot;: &amp;quot;runc&amp;quot;,\n            &amp;quot;Isolation&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;CpuShares&amp;quot;: 0,\n            &amp;quot;Memory&amp;quot;: 0,\n            &amp;quot;NanoCpus&amp;quot;: 0,\n            &amp;quot;CgroupParent&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;BlkioWeight&amp;quot;: 0,\n            &amp;quot;BlkioWeightDevice&amp;quot;: null,\n            &amp;quot;BlkioDeviceReadBps&amp;quot;: null,\n            &amp;quot;BlkioDeviceWriteBps&amp;quot;: null,\n            &amp;quot;BlkioDeviceReadIOps&amp;quot;: null,\n            &amp;quot;BlkioDeviceWriteIOps&amp;quot;: null,\n            &amp;quot;CpuPeriod&amp;quot;: 0,\n            &amp;quot;CpuQuota&amp;quot;: 0,\n            &amp;quot;CpuRealtimePeriod&amp;quot;: 0,\n            &amp;quot;CpuRealtimeRuntime&amp;quot;: 0,\n            &amp;quot;CpusetCpus&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;CpusetMems&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;Devices&amp;quot;: null,\n            &amp;quot;DeviceCgroupRules&amp;quot;: null,\n            &amp;quot;DeviceRequests&amp;quot;: null,\n            &amp;quot;MemoryReservation&amp;quot;: 0,\n            &amp;quot;MemorySwap&amp;quot;: 0,\n            &amp;quot;MemorySwappiness&amp;quot;: null,\n            &amp;quot;OomKillDisable&amp;quot;: false,\n            &amp;quot;PidsLimit&amp;quot;: null,\n            &amp;quot;Ulimits&amp;quot;: null,\n            &amp;quot;CpuCount&amp;quot;: 0,\n            &amp;quot;CpuPercent&amp;quot;: 0,\n            &amp;quot;IOMaximumIOps&amp;quot;: 0,\n            &amp;quot;IOMaximumBandwidth&amp;quot;: 0,\n            &amp;quot;Mounts&amp;quot;: [\n                {\n                    &amp;quot;Type&amp;quot;: &amp;quot;volume&amp;quot;,\n                    &amp;quot;Source&amp;quot;: &amp;quot;airbyte_workspace&amp;quot;,\n                    &amp;quot;Target&amp;quot;: &amp;quot;/tmp/workspace&amp;quot;,\n                    &amp;quot;VolumeOptions&amp;quot;: {}\n                },\n                {\n                    &amp;quot;Type&amp;quot;: &amp;quot;volume&amp;quot;,\n                    &amp;quot;Source&amp;quot;: &amp;quot;airbyte_data&amp;quot;,\n                    &amp;quot;Target&amp;quot;: &amp;quot;/data&amp;quot;,\n                    &amp;quot;VolumeOptions&amp;quot;: {}\n                },\n                {\n                    &amp;quot;Type&amp;quot;: &amp;quot;bind&amp;quot;,\n                    &amp;quot;Source&amp;quot;: &amp;quot;/home/marconi_medeiros/airbyte/flags.yml&amp;quot;,\n                    &amp;quot;Target&amp;quot;: &amp;quot;/flags&amp;quot;,\n                    &amp;quot;ReadOnly&amp;quot;: true\n                }\n            ],\n            &amp;quot;MaskedPaths&amp;quot;: [\n                &amp;quot;/proc/asound&amp;quot;,\n                &amp;quot;/proc/acpi&amp;quot;,\n                &amp;quot;/proc/kcore&amp;quot;,\n                &amp;quot;/proc/keys&amp;quot;,\n                &amp;quot;/proc/latency_stats&amp;quot;,\n                &amp;quot;/proc/timer_list&amp;quot;,\n                &amp;quot;/proc/timer_stats&amp;quot;,\n                &amp;quot;/proc/sched_debug&amp;quot;,\n                &amp;quot;/proc/scsi&amp;quot;,\n                &amp;quot;/sys/firmware&amp;quot;,\n                &amp;quot;/sys/devices/virtual/powercap&amp;quot;\n            ],\n            &amp;quot;ReadonlyPaths&amp;quot;: [\n                &amp;quot;/proc/bus&amp;quot;,\n                &amp;quot;/proc/fs&amp;quot;,\n                &amp;quot;/proc/irq&amp;quot;,\n                &amp;quot;/proc/sys&amp;quot;,\n                &amp;quot;/proc/sysrq-trigger&amp;quot;\n            ]\n        },\n        &amp;quot;GraphDriver&amp;quot;: {\n            &amp;quot;Data&amp;quot;: {\n                &amp;quot;LowerDir&amp;quot;: &amp;quot;/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35-init/diff:/var/lib/docker/overlay2/86d4469575cfe4cf9bd75b0ea667686e7524848aa7e95c811a712520c776a3c6/diff:/var/lib/docker/overlay2/1cda7fe85b404f4245becf0a65c3de9bb47ea514b79f7f361d62b97f2944a03d/diff:/var/lib/docker/overlay2/f0e9905faf280df4b1f204dd764605027cf992e4d160c6682ec20af78d9274ec/diff:/var/lib/docker/overlay2/f04c7d25612b9081720eb19f36b88b5a57087df423a094c2c76caa11f64bfe14/diff:/var/lib/docker/overlay2/58d62321c1dfd24e971359ce9e71a81f9a39eb7107d8d1ead3681fb7e6cb1478/diff:/var/lib/docker/overlay2/a7aad538563470acbc8607926e5373b5e30ffc80920fbb5d16b7351bd7ba4e35/diff:/var/lib/docker/overlay2/164145d9b36d64bd5fcd70cffb90f9345d5536edb2ca350df178ddecd31d3a0f/diff:/var/lib/docker/overlay2/18362139df4a8f8784850dcc295033d6680b5bbeb130f5cf2879792af4a7af88/diff&amp;quot;,\n                &amp;quot;MergedDir&amp;quot;: &amp;quot;/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/merged&amp;quot;,\n                &amp;quot;UpperDir&amp;quot;: &amp;quot;/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/diff&amp;quot;,\n                &amp;quot;WorkDir&amp;quot;: &amp;quot;/var/lib/docker/overlay2/7716ad0c51ecb89bedd32b5ab2a4bfa13bbc3feed8c23060aa71ff1b5f79de35/work&amp;quot;\n            },\n            &amp;quot;Name&amp;quot;: &amp;quot;overlay2&amp;quot;\n        },\n        &amp;quot;Mounts&amp;quot;: [\n            {\n                &amp;quot;Type&amp;quot;: &amp;quot;bind&amp;quot;,\n                &amp;quot;Source&amp;quot;: &amp;quot;/home/marconi_medeiros/airbyte/configs&amp;quot;,\n                &amp;quot;Destination&amp;quot;: &amp;quot;/app/configs&amp;quot;,\n                &amp;quot;Mode&amp;quot;: &amp;quot;ro&amp;quot;,\n                &amp;quot;RW&amp;quot;: false,\n                &amp;quot;Propagation&amp;quot;: &amp;quot;rprivate&amp;quot;\n            },\n            {\n                &amp;quot;Type&amp;quot;: &amp;quot;volume&amp;quot;,\n                &amp;quot;Name&amp;quot;: &amp;quot;airbyte_data&amp;quot;,\n                &amp;quot;Source&amp;quot;: &amp;quot;/var/lib/docker/volumes/airbyte_data/_data&amp;quot;,\n                &amp;quot;Destination&amp;quot;: &amp;quot;/data&amp;quot;,\n                &amp;quot;Driver&amp;quot;: &amp;quot;local&amp;quot;,\n                &amp;quot;Mode&amp;quot;: &amp;quot;z&amp;quot;,\n                &amp;quot;RW&amp;quot;: true,\n                &amp;quot;Propagation&amp;quot;: &amp;quot;&amp;quot;\n            },\n            {\n                &amp;quot;Type&amp;quot;: &amp;quot;bind&amp;quot;,\n                &amp;quot;Source&amp;quot;: &amp;quot;/home/marconi_medeiros/airbyte/flags.yml&amp;quot;,\n                &amp;quot;Destination&amp;quot;: &amp;quot;/flags&amp;quot;,\n                &amp;quot;Mode&amp;quot;: &amp;quot;&amp;quot;,\n                &amp;quot;RW&amp;quot;: false,\n                &amp;quot;Propagation&amp;quot;: &amp;quot;rprivate&amp;quot;\n            },\n            {\n                &amp;quot;Type&amp;quot;: &amp;quot;bind&amp;quot;,\n                &amp;quot;Source&amp;quot;: &amp;quot;/tmp/airbyte_local&amp;quot;,\n                &amp;quot;Destination&amp;quot;: &amp;quot;/tmp/airbyte_local&amp;quot;,\n                &amp;quot;Mode&amp;quot;: &amp;quot;rw&amp;quot;,\n                &amp;quot;RW&amp;quot;: true,\n                &amp;quot;Propagation&amp;quot;: &amp;quot;rprivate&amp;quot;\n            },\n            {\n                &amp;quot;Type&amp;quot;: &amp;quot;volume&amp;quot;,\n                &amp;quot;Name&amp;quot;: &amp;quot;airbyte_workspace&amp;quot;,\n                &amp;quot;Source&amp;quot;: &amp;quot;/var/lib/docker/volumes/airbyte_workspace/_data&amp;quot;,\n                &amp;quot;Destination&amp;quot;: &amp;quot;/tmp/workspace&amp;quot;,\n                &amp;quot;Driver&amp;quot;: &amp;quot;local&amp;quot;,\n                &amp;quot;Mode&amp;quot;: &amp;quot;z&amp;quot;,\n                &amp;quot;RW&amp;quot;: true,\n                &amp;quot;Propagation&amp;quot;: &amp;quot;&amp;quot;\n            }\n        ],\n        &amp;quot;Config&amp;quot;: {\n            &amp;quot;Hostname&amp;quot;: &amp;quot;608aea695e05&amp;quot;,\n            &amp;quot;Domainname&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;User&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;AttachStdin&amp;quot;: false,\n            &amp;quot;AttachStdout&amp;quot;: true,\n            &amp;quot;AttachStderr&amp;quot;: true,\n            &amp;quot;ExposedPorts&amp;quot;: {\n                &amp;quot;5005/tcp&amp;quot;: {},\n                &amp;quot;8000/tcp&amp;quot;: {},\n                &amp;quot;8001/tcp&amp;quot;: {}\n            },\n            &amp;quot;Tty&amp;quot;: false,\n            &amp;quot;OpenStdin&amp;quot;: false,\n            &amp;quot;StdinOnce&amp;quot;: false,\n            &amp;quot;Env&amp;quot;: [\n                &amp;quot;LAUNCHDARKLY_KEY=&amp;quot;,\n                &amp;quot;LOG_LEVEL=INFO&amp;quot;,\n                &amp;quot;AIRBYTE_VERSION=0.50.39&amp;quot;,\n                &amp;quot;FEATURE_FLAG_CLIENT=config&amp;quot;,\n                &amp;quot;SEGMENT_WRITE_KEY=7UDdp5K55CyiGgsauOr2pNNujGvmhaeu&amp;quot;,\n                &amp;quot;WORKER_ENVIRONMENT=&amp;quot;,\n                &amp;quot;JOB_MAIN_CONTAINER_MEMORY_LIMIT=&amp;quot;,\n                &amp;quot;DD_DOGSTATSD_PORT=&amp;quot;,\n                &amp;quot;JOB_MAIN_CONTAINER_CPU_LIMIT=&amp;quot;,\n                &amp;quot;AUTO_DETECT_SCHEMA=true&amp;quot;,\n                &amp;quot;DATABASE_PASSWORD=docker&amp;quot;,\n                &amp;quot;MAX_NOTIFY_WORKERS=5&amp;quot;,\n                &amp;quot;CONFIG_DATABASE_USER=&amp;quot;,\n                &amp;quot;CONNECTOR_REGISTRY_BASE_URL=&amp;quot;,\n                &amp;quot;AIRBYTE_ROLE=&amp;quot;,\n                &amp;quot;PUBLISH_METRICS=false&amp;quot;,\n                &amp;quot;TRACKING_STRATEGY=segment&amp;quot;,\n                &amp;quot;JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=0.40.26.001&amp;quot;,\n                &amp;quot;JOB_MAIN_CONTAINER_CPU_REQUEST=&amp;quot;,\n                &amp;quot;CONFIG_DATABASE_PASSWORD=&amp;quot;,\n                &amp;quot;CONFIG_DATABASE_URL=&amp;quot;,\n                &amp;quot;SECRET_PERSISTENCE=TESTING_CONFIG_DB_TABLE&amp;quot;,\n                &amp;quot;GITHUB_STORE_BRANCH=&amp;quot;,\n                &amp;quot;SHOULD_RUN_NOTIFY_WORKFLOWS=true&amp;quot;,\n                &amp;quot;DD_AGENT_HOST=&amp;quot;,\n                &amp;quot;TEMPORAL_HOST=airbyte-temporal:7233&amp;quot;,\n                &amp;quot;NEW_SCHEDULER=&amp;quot;,\n                &amp;quot;METRIC_CLIENT=&amp;quot;,\n                &amp;quot;JOB_ERROR_REPORTING_SENTRY_DSN=&amp;quot;,\n                &amp;quot;MICRONAUT_ENVIRONMENTS=control-plane&amp;quot;,\n                &amp;quot;WORKSPACE_ROOT=/tmp/workspace&amp;quot;,\n                &amp;quot;CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=0.40.23.002&amp;quot;,\n                &amp;quot;WEBAPP_URL=http://localhost:8000/&amp;quot;,\n                &amp;quot;CONFIG_ROOT=/data&amp;quot;,\n                &amp;quot;JOB_MAIN_CONTAINER_MEMORY_REQUEST=&amp;quot;,\n                &amp;quot;DATABASE_USER=docker&amp;quot;,\n                &amp;quot;DATABASE_URL=jdbc:postgresql://db:5432/airbyte&amp;quot;,\n                &amp;quot;JOB_ERROR_REPORTING_STRATEGY=logging&amp;quot;,\n                &amp;quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&amp;quot;,\n                &amp;quot;LANG=C.UTF-8&amp;quot;,\n                &amp;quot;JAVA_HOME=/usr/lib/jvm/java-21-amazon-corretto&amp;quot;,\n                &amp;quot;APPLICATION=airbyte-server&amp;quot;,\n                &amp;quot;VERSION=0.50.39&amp;quot;\n            ],\n            &amp;quot;Cmd&amp;quot;: null,\n            &amp;quot;Image&amp;quot;: &amp;quot;airbyte/server:0.50.39&amp;quot;,\n            &amp;quot;Volumes&amp;quot;: null,\n            &amp;quot;WorkingDir&amp;quot;: &amp;quot;/app&amp;quot;,\n            &amp;quot;Entrypoint&amp;quot;: [\n                &amp;quot;/bin/bash&amp;quot;,\n                &amp;quot;-c&amp;quot;,\n                &amp;quot;airbyte-app/bin/${APPLICATION}&amp;quot;\n            ],\n            &amp;quot;OnBuild&amp;quot;: null,\n            &amp;quot;Labels&amp;quot;: {\n                &amp;quot;com.docker.compose.config-hash&amp;quot;: &amp;quot;441abf94eb3ba81e930ec9ac68a4bdbbce4a27b3a5800e6800498b4e5c32d294&amp;quot;,\n                &amp;quot;com.docker.compose.container-number&amp;quot;: &amp;quot;1&amp;quot;,\n                &amp;quot;com.docker.compose.depends_on&amp;quot;: &amp;quot;bootloader:service_completed_successfully:false&amp;quot;,\n                &amp;quot;com.docker.compose.image&amp;quot;: &amp;quot;sha256:2dd8aec8a791cb58db4469ff22ed38d9facf4f53db459784196e21f46cb23c4d&amp;quot;,\n                &amp;quot;com.docker.compose.oneoff&amp;quot;: &amp;quot;False&amp;quot;,\n                &amp;quot;com.docker.compose.project&amp;quot;: &amp;quot;airbyte&amp;quot;,\n                &amp;quot;com.docker.compose.project.config_files&amp;quot;: &amp;quot;/home/marconi_medeiros/airbyte/docker-compose.yaml&amp;quot;,\n                &amp;quot;com.docker.compose.project.working_dir&amp;quot;: &amp;quot;/home/marconi_medeiros/airbyte&amp;quot;,\n                &amp;quot;com.docker.compose.replace&amp;quot;: &amp;quot;c600b842bba15fdda4f30323e07be77f32f17d98dc96db91317d199053380424&amp;quot;,\n                &amp;quot;com.docker.compose.service&amp;quot;: &amp;quot;server&amp;quot;,\n                &amp;quot;com.docker.compose.version&amp;quot;: &amp;quot;2.21.0&amp;quot;\n            }\n        },\n        &amp;quot;NetworkSettings&amp;quot;: {\n            &amp;quot;Bridge&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;SandboxID&amp;quot;: &amp;quot;7f628a9e8c539a52e38ed082b7e284d769a89aa6f5993bf88804a7baec2e67bb&amp;quot;,\n            &amp;quot;HairpinMode&amp;quot;: false,\n            &amp;quot;LinkLocalIPv6Address&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;LinkLocalIPv6PrefixLen&amp;quot;: 0,\n            &amp;quot;Ports&amp;quot;: {\n                &amp;quot;5005/tcp&amp;quot;: null,\n                &amp;quot;8000/tcp&amp;quot;: null,\n                &amp;quot;8001/tcp&amp;quot;: [\n                    {\n                        &amp;quot;HostIp&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,\n                        &amp;quot;HostPort&amp;quot;: &amp;quot;32783&amp;quot;\n                    },\n                    {\n                        &amp;quot;HostIp&amp;quot;: &amp;quot;::&amp;quot;,\n                        &amp;quot;HostPort&amp;quot;: &amp;quot;32782&amp;quot;\n                    }\n                ]\n            },\n            &amp;quot;SandboxKey&amp;quot;: &amp;quot;/var/run/docker/netns/7f628a9e8c53&amp;quot;,\n            &amp;quot;SecondaryIPAddresses&amp;quot;: null,\n            &amp;quot;SecondaryIPv6Addresses&amp;quot;: null,\n            &amp;quot;EndpointID&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;Gateway&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;GlobalIPv6Address&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;GlobalIPv6PrefixLen&amp;quot;: 0,\n            &amp;quot;IPAddress&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;IPPrefixLen&amp;quot;: 0,\n            &amp;quot;IPv6Gateway&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;MacAddress&amp;quot;: &amp;quot;&amp;quot;,\n            &amp;quot;Networks&amp;quot;: {\n                &amp;quot;airbyte_airbyte_internal&amp;quot;: {\n                    &amp;quot;IPAMConfig&amp;quot;: null,\n                    &amp;quot;Links&amp;quot;: null,\n                    &amp;quot;Aliases&amp;quot;: [\n                        &amp;quot;airbyte-server&amp;quot;,\n                        &amp;quot;server&amp;quot;,\n                        &amp;quot;608aea695e05&amp;quot;\n                    ],\n                    &amp;quot;NetworkID&amp;quot;: &amp;quot;1b3d2150a3836a5cdfc591062965194ffbbb030d2ec94eba51f5819b239dd8c3&amp;quot;,\n                    &amp;quot;EndpointID&amp;quot;: &amp;quot;ba6c2e2ef548299873a1f08fc93fa3fb883f521865ff0c481c9cf2747bd24cf7&amp;quot;,\n                    &amp;quot;Gateway&amp;quot;: &amp;quot;172.19.0.1&amp;quot;,\n                    &amp;quot;IPAddress&amp;quot;: &amp;quot;172.19.0.4&amp;quot;,\n                    &amp;quot;IPPrefixLen&amp;quot;: 16,\n                    &amp;quot;IPv6Gateway&amp;quot;: &amp;quot;&amp;quot;,\n                    &amp;quot;GlobalIPv6Address&amp;quot;: &amp;quot;&amp;quot;,\n                    &amp;quot;GlobalIPv6PrefixLen&amp;quot;: 0,\n                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:13:00:04&amp;quot;,\n                    &amp;quot;DriverOpts&amp;quot;: null\n                }\n            }\n        }\n    }\n]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwova", "is_robot_indexable": true, "report_reasons": null, "author": "Koninhooz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwova/airbyte_docker_cant_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwova/airbyte_docker_cant_access/", "subreddit_subscribers": 148344, "created_utc": 1703555337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Folks,   \n \n\nI come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   \n\n\nfor me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  \n\n\nmore or less need ideas for data science \"tasks\" to help with me playing around data", "author_fullname": "t2_8x6hlh81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion for Tasks to play around with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18qwm1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,   &lt;/p&gt;\n\n&lt;p&gt;I come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   &lt;/p&gt;\n\n&lt;p&gt;for me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  &lt;/p&gt;\n\n&lt;p&gt;more or less need ideas for data science &amp;quot;tasks&amp;quot; to help with me playing around data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwm1g", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful-Award5471", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "subreddit_subscribers": 148344, "created_utc": 1703555091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.\n\nIs there a (preferably a visual and free) tool for helping mapping such datasets?\n\nWhen googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.", "author_fullname": "t2_xfhxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging datasets with different structure.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qv3lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703550473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.&lt;/p&gt;\n\n&lt;p&gt;Is there a (preferably a visual and free) tool for helping mapping such datasets?&lt;/p&gt;\n\n&lt;p&gt;When googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qv3lf", "is_robot_indexable": true, "report_reasons": null, "author": "LarsSorensen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "subreddit_subscribers": 148344, "created_utc": 1703550473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nCurrently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company's needs. I would appreciate your assistance in making design choices.\n\nWe have a relatively **small dataset** of around 50,000 **large** S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.\n\nOur goal is to train Deep Learning models using these images and establish the capability **to search and group them based on their metadata**. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.\n\nHow can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for **efficient searching and data extraction by our team**?  \n\n\nDo you have any resources/blog posts with similar problems and solutions to those described above?  \n\n\nThank you!", "author_fullname": "t2_yu7cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance: Designing a Data Platform for Efficient Image Annotation, Deep Learning, and Metadata Search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qut3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Currently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company&amp;#39;s needs. I would appreciate your assistance in making design choices.&lt;/p&gt;\n\n&lt;p&gt;We have a relatively &lt;strong&gt;small dataset&lt;/strong&gt; of around 50,000 &lt;strong&gt;large&lt;/strong&gt; S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to train Deep Learning models using these images and establish the capability &lt;strong&gt;to search and group them based on their metadata&lt;/strong&gt;. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.&lt;/p&gt;\n\n&lt;p&gt;How can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for &lt;strong&gt;efficient searching and data extraction by our team&lt;/strong&gt;?  &lt;/p&gt;\n\n&lt;p&gt;Do you have any resources/blog posts with similar problems and solutions to those described above?  &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18qut3h", "is_robot_indexable": true, "report_reasons": null, "author": "UserPobro", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "subreddit_subscribers": 148344, "created_utc": 1703549598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Azure Synapse Analytics: A Step-by-Step Guide.\" Get ready to explore this comprehensive resource! \ud83d\udca1  \n\n\nAre you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We'll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  \n\n\nDon't miss out on this valuable resource! Read the full article here:  \n[https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/](https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/)  \n[\\#AzureSynapseAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#StepByStepGuide](https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataInsights](https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#LinkedInArticle](https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#dataanalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#data](https://www.linkedin.com/feed/hashtag/?keywords=data&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688)", "author_fullname": "t2_blyyz3sy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics: A Step-by-Step Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18quryl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Azure Synapse Analytics: A Step-by-Step Guide.&amp;quot; Get ready to explore this comprehensive resource! \ud83d\udca1  &lt;/p&gt;\n\n&lt;p&gt;Are you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We&amp;#39;ll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t miss out on this valuable resource! Read the full article here:&lt;br/&gt;\n&lt;a href=\"https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/\"&gt;https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#AzureSynapseAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#StepByStepGuide&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataInsights&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#LinkedInArticle&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#dataanalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=data&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#data&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18quryl", "is_robot_indexable": true, "report_reasons": null, "author": "Bubbly_Bed_4478", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "subreddit_subscribers": 148344, "created_utc": 1703549503.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&gt; resources must be publicly accessible and can't be behind a firewall or secured in a [vnet]\n\nThis is from [this](https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake) page in ms docs.\n\nReally!?  Is anyone actually using it and if so, how!?", "author_fullname": "t2_5r4sezi25", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure event hubs and delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qufc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703548452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;resources must be publicly accessible and can&amp;#39;t be behind a firewall or secured in a [vnet]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is from &lt;a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake\"&gt;this&lt;/a&gt; page in ms docs.&lt;/p&gt;\n\n&lt;p&gt;Really!?  Is anyone actually using it and if so, how!?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qufc5", "is_robot_indexable": true, "report_reasons": null, "author": "MachineLooning", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "subreddit_subscribers": 148344, "created_utc": 1703548452.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}