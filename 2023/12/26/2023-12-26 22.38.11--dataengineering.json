{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. \n\nI also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.", "author_fullname": "t2_mil58gc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently accepted an offer as a DE for the first time! Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qumit", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any tips career wise for someone in a DE position for the first time? It seems to be a bit analytics heavy but still has engineering components like cloud, infrastructure, pipelining etc. &lt;/p&gt;\n\n&lt;p&gt;I also was in the camp of being a DA then transitioning to DE. It\u2019s definitely possible and I did a lot of projects and made them public on my GitHub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18qumit", "is_robot_indexable": true, "report_reasons": null, "author": "spunkytale", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qumit/recently_accepted_an_offer_as_a_de_for_the_first/", "subreddit_subscribers": 148545, "created_utc": 1703549033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nIs it just me or is the leetcode's MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode's SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.\n\nNow you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?", "author_fullname": "t2_9mm5jt001", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LEETCODE'S T-SQL issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qwtej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Is it just me or is the leetcode&amp;#39;s MS SQL Server compiler really pathetic. I am currently trying to solve Leetcode&amp;#39;s SQL 50 challenge and for most of the questions MySQL solutions run perfectly. But with MS SQL server, I get stuck at almost all the problems after the 8th question. I am specifically trying to answer in T-SQL as my knowledge in Azure domain and if I do get a job in any data engineering companies it would probably be in that. So I wanted to get my hands dirty but my God, with every solution it is so pathetic. I know I just starting and maybe there are optimizations which might be there for MySQL which accepts the query better across all the portions but seriously it is a time waste to even try in T- SQL. For the same solution I could get run time exception or maybe fail at 1 or 2 testcases, which magically all work on MySQL.&lt;/p&gt;\n\n&lt;p&gt;Now you might be asking why am I bothering a data engineering subreddit instead of maybe leetcode subreddit or the SQL ones? I am doing this because I wish to probably get hired as a data engineer and I know I could pass my interviews using any flavour but I wish to know this, how do you make sure your query runs in any specific model, when you have expertise in the other. Or am I the only buffoon who is facing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwtej", "is_robot_indexable": true, "report_reasons": null, "author": "Master-Influence7539", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwtej/leetcodes_tsql_issues/", "subreddit_subscribers": 148545, "created_utc": 1703555730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am currently evaluating which BI tool will be implemented across the company. In my earlier companies I have used Superset where I wrote SQL queries for each dashboard or insight requirement and created a visualisation directly from them.\n\nI realised that in Power BI I can't write SQL queries directly, I can only connect directly to the database tables and do the transformation inside Power BI. A workaround would be to write sql queries separately and create a view in my db and then connect power BI to the view to create a dashboard.\n\nI want to ask for your opinions on this.\n\n1. Should I move to a tool where I can write queries and make visualisations.\n\n2. Use views with power BI.\n\n3. Simply abandon SQL queries and just do transformations entirely in power BI.", "author_fullname": "t2_158u5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Superset/Metabase vs Power BI/Tableau", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18re6it", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703614155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating which BI tool will be implemented across the company. In my earlier companies I have used Superset where I wrote SQL queries for each dashboard or insight requirement and created a visualisation directly from them.&lt;/p&gt;\n\n&lt;p&gt;I realised that in Power BI I can&amp;#39;t write SQL queries directly, I can only connect directly to the database tables and do the transformation inside Power BI. A workaround would be to write sql queries separately and create a view in my db and then connect power BI to the view to create a dashboard.&lt;/p&gt;\n\n&lt;p&gt;I want to ask for your opinions on this.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Should I move to a tool where I can write queries and make visualisations.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use views with power BI.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Simply abandon SQL queries and just do transformations entirely in power BI.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18re6it", "is_robot_indexable": true, "report_reasons": null, "author": "chutiyaw", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18re6it/supersetmetabase_vs_power_bitableau/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18re6it/supersetmetabase_vs_power_bitableau/", "subreddit_subscribers": 148545, "created_utc": 1703614155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have recently introduced DBT in our data pipeline, and I am trying to orchestrate the pipeline with airflow and create a tool so that the analytics team can make their changes on their own.  \n\n\nCan anyone suggest some guidelines for this process?", "author_fullname": "t2_ao3wkczf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core orchestration in-house implementation - OPINION", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r0n54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703567977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently introduced DBT in our data pipeline, and I am trying to orchestrate the pipeline with airflow and create a tool so that the analytics team can make their changes on their own.  &lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest some guidelines for this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r0n54", "is_robot_indexable": true, "report_reasons": null, "author": "Beautiful-Big-75", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r0n54/dbt_core_orchestration_inhouse_implementation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r0n54/dbt_core_orchestration_inhouse_implementation/", "subreddit_subscribers": 148545, "created_utc": 1703567977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data pipeline, extracts data from APIs using Cloud Functions and drop them in GCS bucket. Another cloud function that takes those files and pushes them to Bigqquery.   \nThe second Cloud function has a linear backoff to handle 10ops/5sec limit. But data is too much and coming at much faster rate that we still see failed instances of data loaded to bigquery.  \n\n\nI can think of 2 ways to fix it:  \n\\- Reduce the speed to data extraction. This is done via Cloud Tasks and may be I can add throttling to it so that API calls are made at lower rate.   \n\\- I think there is also a possibility to add a a messaging service in between but I have not used pub sub before. So not entirely sure about it. The idea could be to push data to pub sub instead of Google Storage (since we are not really making any use of this data, in case of issues we prefer to just extract it again).  \n\n\nDo you have any recommendations or suggestion what might be a better option? Our whole extraction process is simply API calls to a dozen or more sources (each in its own cloud function and then source table) and processing later in Bigquery.   \n", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data loading to bigquery failing with rate limit error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r1fou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703570735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data pipeline, extracts data from APIs using Cloud Functions and drop them in GCS bucket. Another cloud function that takes those files and pushes them to Bigqquery.&lt;br/&gt;\nThe second Cloud function has a linear backoff to handle 10ops/5sec limit. But data is too much and coming at much faster rate that we still see failed instances of data loaded to bigquery.  &lt;/p&gt;\n\n&lt;p&gt;I can think of 2 ways to fix it:&lt;br/&gt;\n- Reduce the speed to data extraction. This is done via Cloud Tasks and may be I can add throttling to it so that API calls are made at lower rate.&lt;br/&gt;\n- I think there is also a possibility to add a a messaging service in between but I have not used pub sub before. So not entirely sure about it. The idea could be to push data to pub sub instead of Google Storage (since we are not really making any use of this data, in case of issues we prefer to just extract it again).  &lt;/p&gt;\n\n&lt;p&gt;Do you have any recommendations or suggestion what might be a better option? Our whole extraction process is simply API calls to a dozen or more sources (each in its own cloud function and then source table) and processing later in Bigquery.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r1fou", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r1fou/data_loading_to_bigquery_failing_with_rate_limit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r1fou/data_loading_to_bigquery_failing_with_rate_limit/", "subreddit_subscribers": 148545, "created_utc": 1703570735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nCurrently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company's needs. I would appreciate your assistance in making design choices.\n\nWe have a relatively **small dataset** of around 50,000 **large** S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.\n\nOur goal is to train Deep Learning models using these images and establish the capability **to search and group them based on their metadata**. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.\n\nHow can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for **efficient searching and data extraction by our team**?  \n\n\nDo you have any resources/blog posts with similar problems and solutions to those described above?  \n\n\nThank you!", "author_fullname": "t2_yu7cj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Guidance: Designing a Data Platform for Efficient Image Annotation, Deep Learning, and Metadata Search", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qut3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Currently, at my company, I am tasked with designing and leading a team to build a data platform to meet the company&amp;#39;s needs. I would appreciate your assistance in making design choices.&lt;/p&gt;\n\n&lt;p&gt;We have a relatively &lt;strong&gt;small dataset&lt;/strong&gt; of around 50,000 &lt;strong&gt;large&lt;/strong&gt; S3 images, with each image having an average of 12 annotations. This results in approximately 600,000 annotations, each serving as both text metadata and images. Additionally, these 50,000 images are expected to grow to 200,000 in a few years.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to train Deep Learning models using these images and establish the capability &lt;strong&gt;to search and group them based on their metadata&lt;/strong&gt;. The plan is to store all images in a data lake (S3) and utilize a database as a metadata layer. We need a database that facilitates the easy addition of new traits/annotations (schema evolution) for images, enabling data scientists and machine learning engineers to seamlessly search and extract data.&lt;/p&gt;\n\n&lt;p&gt;How can we best achieve this goal, considering the growth of our dataset and the need for flexible schema evolution in the database for &lt;strong&gt;efficient searching and data extraction by our team&lt;/strong&gt;?  &lt;/p&gt;\n\n&lt;p&gt;Do you have any resources/blog posts with similar problems and solutions to those described above?  &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18qut3h", "is_robot_indexable": true, "report_reasons": null, "author": "UserPobro", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qut3h/seeking_guidance_designing_a_data_platform_for/", "subreddit_subscribers": 148545, "created_utc": 1703549598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Found this gem on Medium. Probably written by someone on this sub.  I run into a lot of cases where people don't understand where their dbt errors come from. Must read for anyone using dbt.\n\nThanks to u/badketchup for a better link.\n\nhttps://www.arecadata.com/the-definitive-guide-for-debugging-dbt/\n", "author_fullname": "t2_8ov8i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Definitive Guide to Debugging Dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ravdf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703625799.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703605475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Found this gem on Medium. Probably written by someone on this sub.  I run into a lot of cases where people don&amp;#39;t understand where their dbt errors come from. Must read for anyone using dbt.&lt;/p&gt;\n\n&lt;p&gt;Thanks to &lt;a href=\"/u/badketchup\"&gt;u/badketchup&lt;/a&gt; for a better link.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.arecadata.com/the-definitive-guide-for-debugging-dbt/\"&gt;https://www.arecadata.com/the-definitive-guide-for-debugging-dbt/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?auto=webp&amp;s=0f8d5f405e3dafd40ad563bc8e1e54bbb901d19a", "width": 960, "height": 469}, "resolutions": [{"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33949a86e8eb9dcdccaaaa2cf71baf6c16cbe553", "width": 108, "height": 52}, {"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6128a2d28db95339c0ddea963167d1bdfe7ac14", "width": 216, "height": 105}, {"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5748b825ad8e98434c213f635b1fcc3dead983e", "width": 320, "height": 156}, {"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af6f5d2159e5422f8b7b9531dcd128acd364e0e1", "width": 640, "height": 312}, {"url": "https://external-preview.redd.it/0beT3b4GrO61u1cLvjLLcRhklqQWFaoiuMaqABK9fkM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=316b11b304652f6803e1ce477844554304cf039e", "width": 960, "height": 469}], "variants": {}, "id": "pSBIgHxG7KfLR74hJ-LjxaxhReC0MO6sH_ifBI9dx-Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18ravdf", "is_robot_indexable": true, "report_reasons": null, "author": "leogodin217", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ravdf/definitive_guide_to_debugging_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ravdf/definitive_guide_to_debugging_dbt/", "subreddit_subscribers": 148545, "created_utc": 1703605475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI am a seasoned Senior BI Developer with a decade of experience primarily in the Microsoft BI Stack, including SSIS, SSRS, Power BI, SSAS, and T-SQL programming. I also have exposure to Azure cloud tools like Azure Data Factory and Azure Data Warehouse. Recently, I made a career move to a new organization, accepting a junior-level BI specialist position for financial and job security reasons, with the intention of transitioning into data engineering.\n\nHowever, my current role at the new organization is turning out to be quite different from typical BI or data engineering positions. It involves manual processes and lacks exposure to cutting-edge technologies. While the job is stable with low pressure, I am now questioning whether I made the right decision.\n\nI have two options in mind and would appreciate advice on the following:\n\n1. Return to the previous organization. Despite being volatile in terms of job stability, it has expressed interest in retaining me by matching my current salary. However, this would mean returning to the old technology stack.\n\n2. Stick with the current organization for the next 6 months to a year, utilizing the stability and lower job pressure to self-teach and transition into data engineering, eventually moving to a role that aligns better with my career goals.\n\nI am seeking guidance on which path would be more beneficial for my career growth", "author_fullname": "t2_838vdjqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r5efi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703586933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a seasoned Senior BI Developer with a decade of experience primarily in the Microsoft BI Stack, including SSIS, SSRS, Power BI, SSAS, and T-SQL programming. I also have exposure to Azure cloud tools like Azure Data Factory and Azure Data Warehouse. Recently, I made a career move to a new organization, accepting a junior-level BI specialist position for financial and job security reasons, with the intention of transitioning into data engineering.&lt;/p&gt;\n\n&lt;p&gt;However, my current role at the new organization is turning out to be quite different from typical BI or data engineering positions. It involves manual processes and lacks exposure to cutting-edge technologies. While the job is stable with low pressure, I am now questioning whether I made the right decision.&lt;/p&gt;\n\n&lt;p&gt;I have two options in mind and would appreciate advice on the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Return to the previous organization. Despite being volatile in terms of job stability, it has expressed interest in retaining me by matching my current salary. However, this would mean returning to the old technology stack.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Stick with the current organization for the next 6 months to a year, utilizing the stability and lower job pressure to self-teach and transition into data engineering, eventually moving to a role that aligns better with my career goals.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am seeking guidance on which path would be more beneficial for my career growth&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18r5efi", "is_robot_indexable": true, "report_reasons": null, "author": "Adventurous_Ad8087", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r5efi/career_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r5efi/career_dilemma/", "subreddit_subscribers": 148545, "created_utc": 1703586933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&gt; resources must be publicly accessible and can't be behind a firewall or secured in a [vnet]\n\nThis is from [this](https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake) page in ms docs.\n\nReally!?  Is anyone actually using it and if so, how!?", "author_fullname": "t2_5r4sezi25", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure event hubs and delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qufc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703548452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;resources must be publicly accessible and can&amp;#39;t be behind a firewall or secured in a [vnet]&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is from &lt;a href=\"https://learn.microsoft.com/en-us/azure/stream-analytics/capture-event-hub-data-delta-lake\"&gt;this&lt;/a&gt; page in ms docs.&lt;/p&gt;\n\n&lt;p&gt;Really!?  Is anyone actually using it and if so, how!?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?auto=webp&amp;s=41fa146938cd97da5abfeff0d092a2cc151e65fa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3881e36da92b82c6947f6ca4ff3804ca47f2aea", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b5b01e50a969ac9e2353bebb062cd52a99d108", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acadaf004e8aeb6919eabdb0d93065a34f7e89df", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=883009d39175a2f03b76275ed0f7c6011d94a3a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cc62aef83f192d102fa78c83c8f4fcfa85057e3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/CcTKId6ti1J-bMqj-jlWVD1tyE1LbM9FagmfDfaIVmQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ca6913f202be9a9f83b266dd459edc90adbf9dd", "width": 1080, "height": 567}], "variants": {}, "id": "RCFh0Kid3SAqWEkALMGNW1e9Vu6ayZpftekoayP00hY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qufc5", "is_robot_indexable": true, "report_reasons": null, "author": "MachineLooning", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qufc5/azure_event_hubs_and_delta/", "subreddit_subscribers": 148545, "created_utc": 1703548452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yes I know there are already a lot of tools that are out there, both paid and FOSS. I've been through a few of them by now and I always end up becoming this engineer that has the job of stitching together 10 different tools together to finally have a full data engineering \"platform\".\n\nThe only tool I have come across that even gets close to solving this problem is Meltano ([https://docs.meltano.com/](https://docs.meltano.com/)). But it too has it's flaws, just to add some meat to this bone I'm trying to pick: managing bigger projects quickly become tedious, there are some conventions in how other packages implement the Meltano SDK but they are not always fully implemented and I end up just using the native packages (DBT my example here, I'd much rather use the official cli.) Then there's the whole governance (and should I even mention trying out data mesh anymore.)\n\nSo I want to solve my own issues here and (probably) make it open-source. So, dear reddit, I have some ideas of what I want to do but want to get some more broad ideas of what people are struggling with while sharing some thing here as I go.\n\nAnd I'm and going to go 1 step further. It's not going to be done in python, nor java, nor scala. I want to use either go or rust and no, not for hype. These currently have great community support and have great potential for larger scale deployments where we need to run loads of small jobs that need to speak to our sources and stores.\n\nSo then where to next? Source data, store it, transform it and keep going. I'm mainly here looking for issues people are running into and things I can keep in mind as I go. Mine are:\n- self-service (want teams within a single company to manage/secure their own data)\n- observability\n- governance (having a backing auditing system as a really don't want to lose another breath working on this ever again)\n- end to end scheduling (I will not touch airflow, not even with a 10 meter stick)\n- a ui for that is actually useful for more than 1 function (this will probably be done using NextJS as it's what I know but open to suggestions.)\n\nLooking forward to comments, complaints and criticism.\n\n[View Poll](https://www.reddit.com/poll/18ra5tp)", "author_fullname": "t2_qc574", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building another data platform.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ra5tp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703603543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes I know there are already a lot of tools that are out there, both paid and FOSS. I&amp;#39;ve been through a few of them by now and I always end up becoming this engineer that has the job of stitching together 10 different tools together to finally have a full data engineering &amp;quot;platform&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The only tool I have come across that even gets close to solving this problem is Meltano (&lt;a href=\"https://docs.meltano.com/\"&gt;https://docs.meltano.com/&lt;/a&gt;). But it too has it&amp;#39;s flaws, just to add some meat to this bone I&amp;#39;m trying to pick: managing bigger projects quickly become tedious, there are some conventions in how other packages implement the Meltano SDK but they are not always fully implemented and I end up just using the native packages (DBT my example here, I&amp;#39;d much rather use the official cli.) Then there&amp;#39;s the whole governance (and should I even mention trying out data mesh anymore.)&lt;/p&gt;\n\n&lt;p&gt;So I want to solve my own issues here and (probably) make it open-source. So, dear reddit, I have some ideas of what I want to do but want to get some more broad ideas of what people are struggling with while sharing some thing here as I go.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m and going to go 1 step further. It&amp;#39;s not going to be done in python, nor java, nor scala. I want to use either go or rust and no, not for hype. These currently have great community support and have great potential for larger scale deployments where we need to run loads of small jobs that need to speak to our sources and stores.&lt;/p&gt;\n\n&lt;p&gt;So then where to next? Source data, store it, transform it and keep going. I&amp;#39;m mainly here looking for issues people are running into and things I can keep in mind as I go. Mine are:\n- self-service (want teams within a single company to manage/secure their own data)\n- observability\n- governance (having a backing auditing system as a really don&amp;#39;t want to lose another breath working on this ever again)\n- end to end scheduling (I will not touch airflow, not even with a 10 meter stick)\n- a ui for that is actually useful for more than 1 function (this will probably be done using NextJS as it&amp;#39;s what I know but open to suggestions.)&lt;/p&gt;\n\n&lt;p&gt;Looking forward to comments, complaints and criticism.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/18ra5tp\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18ra5tp", "is_robot_indexable": true, "report_reasons": null, "author": "SomeRainbowRays", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1703862743831, "options": [{"text": "Rust", "id": "26511841"}, {"text": "Go", "id": "26511842"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 50, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ra5tp/building_another_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/18ra5tp/building_another_data_platform/", "subreddit_subscribers": 148545, "created_utc": 1703603543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy,\n\nI've been tasked to migrate one of the existing solutions from AWS to GCP. The solution is rather simple, it's a combination of Lambda, Batch Job (fargate running ecr image) and S3. Basically lambda triggers batch job which does data processing and saves the outcome to s3. On top of that we have Athena running queries on that outcome s3 bucket.\n\nWould you be so kind and recommend me a blog post, video tutorial or some other material, that would help person that is fairly proficient with AWS services to build something similar on GCP considering mentioned architecture?\n\nAny recommendation is helpful.\nThanks!", "author_fullname": "t2_fw1zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data transformation solution from AWS to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18r4573", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703581708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked to migrate one of the existing solutions from AWS to GCP. The solution is rather simple, it&amp;#39;s a combination of Lambda, Batch Job (fargate running ecr image) and S3. Basically lambda triggers batch job which does data processing and saves the outcome to s3. On top of that we have Athena running queries on that outcome s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Would you be so kind and recommend me a blog post, video tutorial or some other material, that would help person that is fairly proficient with AWS services to build something similar on GCP considering mentioned architecture?&lt;/p&gt;\n\n&lt;p&gt;Any recommendation is helpful.\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18r4573", "is_robot_indexable": true, "report_reasons": null, "author": "saif3r", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18r4573/moving_data_transformation_solution_from_aws_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18r4573/moving_data_transformation_solution_from_aws_to/", "subreddit_subscribers": 148545, "created_utc": 1703581708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Folks,   \n \n\nI come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   \n\n\nfor me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  \n\n\nmore or less need ideas for data science \"tasks\" to help with me playing around data", "author_fullname": "t2_8x6hlh81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestion for Tasks to play around with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qwm1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703555091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,   &lt;/p&gt;\n\n&lt;p&gt;I come up from a typical SWE back-end backgroud a bit, need to ramp on coding up pipelines. Need some ideas for projects that are bit intermediate on difficulty to learn while doing.   &lt;/p&gt;\n\n&lt;p&gt;for me the most kinda of hard thing to test out is .. what is needed from the Data.. so i can know ok i need to do 1,2,3 during ETL  &lt;/p&gt;\n\n&lt;p&gt;more or less need ideas for data science &amp;quot;tasks&amp;quot; to help with me playing around data&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qwm1g", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful-Award5471", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qwm1g/suggestion_for_tasks_to_play_around_with/", "subreddit_subscribers": 148545, "created_utc": 1703555091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious as to which major would be ideal if I want to go into Data/ML Engineering. I've applied for Statistics and am planning on a CS minor. Or is a direct approach with a Data Science major a good idea?", "author_fullname": "t2_a5l1pcph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Undergraduate major", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18ri1ox", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703624185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious as to which major would be ideal if I want to go into Data/ML Engineering. I&amp;#39;ve applied for Statistics and am planning on a CS minor. Or is a direct approach with a Data Science major a good idea?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18ri1ox", "is_robot_indexable": true, "report_reasons": null, "author": "Complex-Ad-7801", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18ri1ox/undergraduate_major/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18ri1ox/undergraduate_major/", "subreddit_subscribers": 148545, "created_utc": 1703624185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!   \nI am faced with the task of scraping data from a search engine, which takes a string and searches for all occurrences of this string, case-insensitive. Search engine returns only first 15 occurences, no pagination, only way to get some another value -- change query string.  \nNaive implementation of engine like this for example on Python.  \n```python\n@dataclass\nclass DataModel:\n  id: int\n  name: str\n\ndata: List[DataModel] = [ ... ]\n\ndef search(query: str) -&gt; List[DataModel]:\n  return list(value for value in data if query in value.name.lower())[:15]\n```\nMy task is to fetch all the data from this search engine. There is no API, only this option. Please tell me what is the best way to do this? So far, the options have been to sequentially sort through all the characters of the Latin alphabet + numbers, but this is estimated to be a very long option, even if you parallelize this task into several tasks with breaking up the alphabet and processing the resulting \"query batches\".\nThank you!", "author_fullname": "t2_csfi1tdg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to fetch data from substring search engine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rdy2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703613557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;br/&gt;\nI am faced with the task of scraping data from a search engine, which takes a string and searches for all occurrences of this string, case-insensitive. Search engine returns only first 15 occurences, no pagination, only way to get some another value -- change query string.&lt;br/&gt;\nNaive implementation of engine like this for example on Python.&lt;br/&gt;\n```python\n@dataclass\nclass DataModel:\n  id: int\n  name: str&lt;/p&gt;\n\n&lt;p&gt;data: List[DataModel] = [ ... ]&lt;/p&gt;\n\n&lt;p&gt;def search(query: str) -&amp;gt; List[DataModel]:\n  return list(value for value in data if query in value.name.lower())[:15]\n```\nMy task is to fetch all the data from this search engine. There is no API, only this option. Please tell me what is the best way to do this? So far, the options have been to sequentially sort through all the characters of the Latin alphabet + numbers, but this is estimated to be a very long option, even if you parallelize this task into several tasks with breaking up the alphabet and processing the resulting &amp;quot;query batches&amp;quot;.\nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18rdy2r", "is_robot_indexable": true, "report_reasons": null, "author": "After_Thought8437", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18rdy2r/how_to_fetch_data_from_substring_search_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18rdy2r/how_to_fetch_data_from_substring_search_engine/", "subreddit_subscribers": 148545, "created_utc": 1703613557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.\n\nIs there a (preferably a visual and free) tool for helping mapping such datasets?\n\nWhen googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.", "author_fullname": "t2_xfhxx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merging datasets with different structure.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18qv3lf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703550473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple a datasets that I need to combine. However the data structure is noget identical. Some fields are the same, but named differently, some fields are only in one of the sets, but all data should end up in the combined set.&lt;/p&gt;\n\n&lt;p&gt;Is there a (preferably a visual and free) tool for helping mapping such datasets?&lt;/p&gt;\n\n&lt;p&gt;When googling I see tools for continuous integration. However I just need a tool to help me structuring the sets. Then I will make scripts that maps according to whatever structure I end up with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18qv3lf", "is_robot_indexable": true, "report_reasons": null, "author": "LarsSorensen", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18qv3lf/merging_datasets_with_different_structure/", "subreddit_subscribers": 148545, "created_utc": 1703550473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Azure Synapse Analytics: A Step-by-Step Guide.\" Get ready to explore this comprehensive resource! \ud83d\udca1  \n\n\nAre you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We'll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  \n\n\nDon't miss out on this valuable resource! Read the full article here:  \n[https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/](https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/)  \n[\\#AzureSynapseAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#StepByStepGuide](https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#DataInsights](https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#LinkedInArticle](https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#dataanalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688) [\\#data](https://www.linkedin.com/feed/hashtag/?keywords=data&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688)", "author_fullname": "t2_blyyz3sy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics: A Step-by-Step Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18quryl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703549503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Azure Synapse Analytics: A Step-by-Step Guide.&amp;quot; Get ready to explore this comprehensive resource! \ud83d\udca1  &lt;/p&gt;\n\n&lt;p&gt;Are you curious about Azure Synapse Analytics and how it can transform your data analysis efforts? This article has got you covered! We&amp;#39;ll walk you through what Azure Synapse Analytics is and how it works, providing you with a clear understanding of its capabilities and benefits. \ud83d\ude80  &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t miss out on this valuable resource! Read the full article here:&lt;br/&gt;\n&lt;a href=\"https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/\"&gt;https://devblogit.com/azure-synapse-analytics-a-step-by-step-guide-for-data-analytics-beginners/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=azuresynapseanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#AzureSynapseAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=stepbystepguide&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#StepByStepGuide&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datainsights&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#DataInsights&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=linkedinarticle&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#LinkedInArticle&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#dataanalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=data&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7145203826956914688\"&gt;#data&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "18quryl", "is_robot_indexable": true, "report_reasons": null, "author": "Bubbly_Bed_4478", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18quryl/azure_synapse_analytics_a_stepbystep_guide/", "subreddit_subscribers": 148545, "created_utc": 1703549503.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}