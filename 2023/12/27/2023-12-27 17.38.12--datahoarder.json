{"kind": "Listing", "data": {"after": "t3_18rr8uz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Copyright Hinders The Preservation Of Modern, Digital Culture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18rlias", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 156, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 156, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fTybdFZPJcL8xPvqXL1ixSe2-v3FKnp-LJKodGe_Tjo.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703633257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rlias", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rlias/how_copyright_hinders_the_preservation_of_modern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "subreddit_subscribers": 721147, "created_utc": 1703633257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I stumbled upon this community and the lack of faith people have in external hard drives is...eye-opening. I have questions that I can't find elsewhere so I'm just going to ask, sorry if they've been answered (just link me in the right direction!)\n\nThe biggest one: if any source of media storage is so doomed to inevitable and absolute failure at any moment... does this apply to my computer? My phone? \n\nI've been using my macbooks/ipads/iphones for at least 10 years nows and NEVER had a loss of anything whatsoever, and I was under the impression that in the event of something happening, it could usually be recovered somehow anyway (stories of the fbi finding shit about people on wiped laptops etc). I only recently decided I should get an external hard drive (how I ended up here) because I had the unnerving realization that none of my photography/video footage was actually fully on any of my devices. Since I don't have enough space it's all optimized. I've just been subscribing for 2 TB of icloud to hold it, so I got scared that if something happened on the icloud side, my years of work would be stuck in low-res regardless of the fact that it's on multiple devices. I thought I would be safe by just getting a 2tb macbook as my next replacement to hold everything OR just getting a much cheaper external hard drive. So is there a difference between those two? Or are they both as vulnerable?\n\n2nd question: Is people's fear over this really that immanent? or is it more hypothetical? Of course theoretically ANYTHING can fail but how often does it actually happen? People keep bitcoin on ledgers and trezors, important documents on thumb drives backed up by just their computer. Why have I never heard of someone's iphone files becoming corrupted out of nowhere? Still wrapping my head around what is the practical reality here. We can also get hit by a meteor at any second...\n\n3rd question: this is just more of an item of curiosity, but is cloud backup just as vulnerable? I'm sure there's plenty of backups in that scenario but do they regularly have instances where something fails and they have to switch to a backup? Is one cloud server better than another? \n\nThanks for any info, this sort of thing really stresses me out because I'm a content creator but my adhd makes me really bad with organization of \"intangible\" things, and the stress of fucking something up makes it even worse. I know the standard answer seems to be just buy multiple backup drives but I'm so far from understanding how to incorporate that into my workflow and just thinking about it is overwhelming. I'm learning I have subpar data hygiene and I'm trying to just go step by step to get better, but be thorough in the process.", "author_fullname": "t2_c7aj0brw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clarification on some things I've been reading in this community. Is all the paranoia justified?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rh6rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703621893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon this community and the lack of faith people have in external hard drives is...eye-opening. I have questions that I can&amp;#39;t find elsewhere so I&amp;#39;m just going to ask, sorry if they&amp;#39;ve been answered (just link me in the right direction!)&lt;/p&gt;\n\n&lt;p&gt;The biggest one: if any source of media storage is so doomed to inevitable and absolute failure at any moment... does this apply to my computer? My phone? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using my macbooks/ipads/iphones for at least 10 years nows and NEVER had a loss of anything whatsoever, and I was under the impression that in the event of something happening, it could usually be recovered somehow anyway (stories of the fbi finding shit about people on wiped laptops etc). I only recently decided I should get an external hard drive (how I ended up here) because I had the unnerving realization that none of my photography/video footage was actually fully on any of my devices. Since I don&amp;#39;t have enough space it&amp;#39;s all optimized. I&amp;#39;ve just been subscribing for 2 TB of icloud to hold it, so I got scared that if something happened on the icloud side, my years of work would be stuck in low-res regardless of the fact that it&amp;#39;s on multiple devices. I thought I would be safe by just getting a 2tb macbook as my next replacement to hold everything OR just getting a much cheaper external hard drive. So is there a difference between those two? Or are they both as vulnerable?&lt;/p&gt;\n\n&lt;p&gt;2nd question: Is people&amp;#39;s fear over this really that immanent? or is it more hypothetical? Of course theoretically ANYTHING can fail but how often does it actually happen? People keep bitcoin on ledgers and trezors, important documents on thumb drives backed up by just their computer. Why have I never heard of someone&amp;#39;s iphone files becoming corrupted out of nowhere? Still wrapping my head around what is the practical reality here. We can also get hit by a meteor at any second...&lt;/p&gt;\n\n&lt;p&gt;3rd question: this is just more of an item of curiosity, but is cloud backup just as vulnerable? I&amp;#39;m sure there&amp;#39;s plenty of backups in that scenario but do they regularly have instances where something fails and they have to switch to a backup? Is one cloud server better than another? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any info, this sort of thing really stresses me out because I&amp;#39;m a content creator but my adhd makes me really bad with organization of &amp;quot;intangible&amp;quot; things, and the stress of fucking something up makes it even worse. I know the standard answer seems to be just buy multiple backup drives but I&amp;#39;m so far from understanding how to incorporate that into my workflow and just thinking about it is overwhelming. I&amp;#39;m learning I have subpar data hygiene and I&amp;#39;m trying to just go step by step to get better, but be thorough in the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rh6rm", "is_robot_indexable": true, "report_reasons": null, "author": "seeeeeeeeth", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rh6rm/clarification_on_some_things_ive_been_reading_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rh6rm/clarification_on_some_things_ive_been_reading_in/", "subreddit_subscribers": 721147, "created_utc": 1703621893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My mother recently had several hours worth of VHS and 8mm home videos digitized. She now wants to find a way to share the videos with relatives and friends living around the world so that they have the opportunity to download the videos and help her identify all the people and events. I've never dealt with such large video files before and am looking for advice on how to best do this, keeping in mind that most of the people accessing the videos would be 60+ years old and not tech savy at all.\n\nI recommended sharing the files via usb drives but this was shot down due to shipping and other related costs. That and getting the addresses of all the different people who would have an interest in the videos would be difficult. She envisions sharing the videos via email using a link that can be passed on to others she may not have an email address for.  I was thinking of setting up a google drive of the videos and creating a share link but am not sure if it would work or not.  I would need to get a 2tb paid plan and am a bit worried as the paid plan mentions \"share with up to 5 people\" and I would need far more than that to be able to have viewing/downloading access.\n\nIf you have any advice, please let me know.", "author_fullname": "t2_uunpe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to share 250GB of video mp4 files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rf01f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703616277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My mother recently had several hours worth of VHS and 8mm home videos digitized. She now wants to find a way to share the videos with relatives and friends living around the world so that they have the opportunity to download the videos and help her identify all the people and events. I&amp;#39;ve never dealt with such large video files before and am looking for advice on how to best do this, keeping in mind that most of the people accessing the videos would be 60+ years old and not tech savy at all.&lt;/p&gt;\n\n&lt;p&gt;I recommended sharing the files via usb drives but this was shot down due to shipping and other related costs. That and getting the addresses of all the different people who would have an interest in the videos would be difficult. She envisions sharing the videos via email using a link that can be passed on to others she may not have an email address for.  I was thinking of setting up a google drive of the videos and creating a share link but am not sure if it would work or not.  I would need to get a 2tb paid plan and am a bit worried as the paid plan mentions &amp;quot;share with up to 5 people&amp;quot; and I would need far more than that to be able to have viewing/downloading access.&lt;/p&gt;\n\n&lt;p&gt;If you have any advice, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rf01f", "is_robot_indexable": true, "report_reasons": null, "author": "mixxaka", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rf01f/best_way_to_share_250gb_of_video_mp4_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rf01f/best_way_to_share_250gb_of_video_mp4_files/", "subreddit_subscribers": 721147, "created_utc": 1703616277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently moving my entire media file collection to a better file management system ([Hydrus Network](https://hydrusnetwork.github.io/hydrus/index.html) if you're interested) and I realized that I also want to preserve the creation times of the original files.\n\nThis means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.\n\nThis is absurd. I'm substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.\n\nJust wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. \n\nData hoarding is at the end of the day just a kind of hoarding.", "author_fullname": "t2_hokshzbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Addiction to Preserving Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rwdue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703667526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently moving my entire media file collection to a better file management system (&lt;a href=\"https://hydrusnetwork.github.io/hydrus/index.html\"&gt;Hydrus Network&lt;/a&gt; if you&amp;#39;re interested) and I realized that I also want to preserve the creation times of the original files.&lt;/p&gt;\n\n&lt;p&gt;This means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.&lt;/p&gt;\n\n&lt;p&gt;This is absurd. I&amp;#39;m substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. &lt;/p&gt;\n\n&lt;p&gt;Data hoarding is at the end of the day just a kind of hoarding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18rwdue", "is_robot_indexable": true, "report_reasons": null, "author": "Estavenz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "subreddit_subscribers": 721147, "created_utc": 1703667526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good evening all\n\nI have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.\n\nI did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people's entire careers is just this section of how to run a computer.\n\nI am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I've seen, but Start early is my main idea, so it is not severely daunting\n\nTLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage", "author_fullname": "t2_dacwz4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First time here, first time seriously considering big backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rq1ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703646273.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703646051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening all&lt;/p&gt;\n\n&lt;p&gt;I have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.&lt;/p&gt;\n\n&lt;p&gt;I did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people&amp;#39;s entire careers is just this section of how to run a computer.&lt;/p&gt;\n\n&lt;p&gt;I am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I&amp;#39;ve seen, but Start early is my main idea, so it is not severely daunting&lt;/p&gt;\n\n&lt;p&gt;TLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rq1ig", "is_robot_indexable": true, "report_reasons": null, "author": "97cweb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "subreddit_subscribers": 721147, "created_utc": 1703646051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students ", "author_fullname": "t2_ey91h2hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PhD students / academics - how do you go about backing up your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rl0ln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703632796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rl0ln", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Difficulty_678", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "subreddit_subscribers": 721147, "created_utc": 1703631964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone. Wall of text incoming.\n\nI've been amassing a large collection of files over the last couple of years (around 1TB total, which is probably not a lot to most of you lol), and have been looking into finally getting around to creating a setup that adheres to the 3-2-1 backup rule.\n\nI've been browsing and searching for beginner tips on this subreddit for a long time today and am just overwhelmed. Lots of terminology and abbreviations that I don't understand. I wouldn't consider myself computer illiterate, but I have never dived into the hardware side of things before.\n\nMy situation / background: I currently live a semi-nomadic lifestyle, and so want something that is portable. I was planning on purchasing 2 external HDDs (2TB or 4TB) to keep with full backups and update weekly / monthly, and keep one in a storage unit. For my 3rd backup I currently use a Sandisk Extreme SSD 1TB, for files or games that I watch / work / play directly off the drive for and that need fast speeds (I may upgrade to a 2TB or 4TB version). I also use both paid MEGA and Protondrive services, but am unable to upload and store my whole collection in, and only use it for very important files or things that I need access to everywhere.\n\nI guess I was just looking for some advice on a few things:\n\n1: Does this backup plan sound like a good plan? Any modifications you would make?\n\n2: Are there any recommendations for brands / basic external drives? How do you avoid drives with low failure rates, just reading online reviews?\n\n3: Is there any essential software I should have to help manage drives / check on drive health?\n\nIf you have any links to specific resources it would be helpful as well.\n\nThank you in advance!", "author_fullname": "t2_1292eb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner. Overwhelmed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rjcm4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703627694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. Wall of text incoming.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been amassing a large collection of files over the last couple of years (around 1TB total, which is probably not a lot to most of you lol), and have been looking into finally getting around to creating a setup that adheres to the 3-2-1 backup rule.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been browsing and searching for beginner tips on this subreddit for a long time today and am just overwhelmed. Lots of terminology and abbreviations that I don&amp;#39;t understand. I wouldn&amp;#39;t consider myself computer illiterate, but I have never dived into the hardware side of things before.&lt;/p&gt;\n\n&lt;p&gt;My situation / background: I currently live a semi-nomadic lifestyle, and so want something that is portable. I was planning on purchasing 2 external HDDs (2TB or 4TB) to keep with full backups and update weekly / monthly, and keep one in a storage unit. For my 3rd backup I currently use a Sandisk Extreme SSD 1TB, for files or games that I watch / work / play directly off the drive for and that need fast speeds (I may upgrade to a 2TB or 4TB version). I also use both paid MEGA and Protondrive services, but am unable to upload and store my whole collection in, and only use it for very important files or things that I need access to everywhere.&lt;/p&gt;\n\n&lt;p&gt;I guess I was just looking for some advice on a few things:&lt;/p&gt;\n\n&lt;p&gt;1: Does this backup plan sound like a good plan? Any modifications you would make?&lt;/p&gt;\n\n&lt;p&gt;2: Are there any recommendations for brands / basic external drives? How do you avoid drives with low failure rates, just reading online reviews?&lt;/p&gt;\n\n&lt;p&gt;3: Is there any essential software I should have to help manage drives / check on drive health?&lt;/p&gt;\n\n&lt;p&gt;If you have any links to specific resources it would be helpful as well.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rjcm4", "is_robot_indexable": true, "report_reasons": null, "author": "Coolpop19", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rjcm4/beginner_overwhelmed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rjcm4/beginner_overwhelmed/", "subreddit_subscribers": 721147, "created_utc": 1703627694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I would like to upgrade my work computer to have significantly more storage space for high speed NMVE SSDs.  \nI've read that PCI-E SATA Expansion cards aren't reliable and will give me issues.  \nDo you guys have recommendations of a solution that allows me to easily add 4-6 TB (minimum) to my workstation? Nothing too expensive, and easy to buy, if possible.  \nApologies if this is the wrong subreddit, I read the wiki as well and couldn't find a solution that fit me there.", "author_fullname": "t2_aa262", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to expand storage space on my work PC. Heard PCI-E SATA expansion cards are no good. Any recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rgt0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703620914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I would like to upgrade my work computer to have significantly more storage space for high speed NMVE SSDs.&lt;br/&gt;\nI&amp;#39;ve read that PCI-E SATA Expansion cards aren&amp;#39;t reliable and will give me issues.&lt;br/&gt;\nDo you guys have recommendations of a solution that allows me to easily add 4-6 TB (minimum) to my workstation? Nothing too expensive, and easy to buy, if possible.&lt;br/&gt;\nApologies if this is the wrong subreddit, I read the wiki as well and couldn&amp;#39;t find a solution that fit me there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rgt0a", "is_robot_indexable": true, "report_reasons": null, "author": "Asherahi", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rgt0a/need_to_expand_storage_space_on_my_work_pc_heard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rgt0a/need_to_expand_storage_space_on_my_work_pc_heard/", "subreddit_subscribers": 721147, "created_utc": 1703620914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format *2024-01-16 17-45-03.jpg* (or .mov).  \n\n\n**USING PHOTOSYNC**  \nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos **from** your smartphone **to** your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn't make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.\n\n1. Install [PhotoSync](https://www.photosync-app.com/home) on smartphone  \n2. Install PhotoSyncCompanion on Windows  \n3. Set up app to connect to Windows computer on the same WiFi as smartphone  \n4. Click Recents  \n5. Click Update  \n6. Click New  \n7. Click Computer\n\nIt is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it's an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  \n\n\n**MY PRE-2024 METHOD**  \nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn't see to work with the OneDrive app. I'm not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.\n\n*The naming problem*\n\nPhotos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG\\_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won't work for the MOV files because the iPhone stores the video created date in the meta data field *com.apple.quicktime.creationdate*. Better File Rename 5 can't see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It's possible that another renamer application with a GUI can do it, but I haven't tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful [EXIFtool](https://exiftool.org/).\n\nHere is how to rename iOS-created MOV video files to the date on your Windows 10 computer.\n\n1. Put a copy of the MOV files into a folder in, say, C:\\\\RenamingFolder  \n2. Download ExifTool and place the EXE file in that folder.  \n3. Rename the file to exiftool.exe  \n4. Open Notepad, enter the following code, and save to C:\\\\RenamingFolder with the name Rename file to IPhone video creation date.bat\n\n    exiftool \"-FileName&lt;creationdate\" -d \"%%Y-%%m-%%d %%H-%%M-%%S.%%%%e\" K:\\RenamingFolder\ncmd /k\n\n5.  Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. \n\n**CONCLUSION**\n\nPhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I'll have properly named files using the accurate date and time.\n\nFinally, for the record, I should add that while the format *2024-01-16 17-45-03.jpg* helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the *Text - Add text to end* command from the menus, and type in with a leading space *Mike's 8th birthday.* So the final file name with be *2024-01-16 17-45-03 Mike's 8th birthday.jpg.*", "author_fullname": "t2_7jhiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflow: copy iPhone photos and videos to Windows 10 with filenames using the date the photo or video was really created", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ro2lq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703640324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; (or .mov).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;USING PHOTOSYNC&lt;/strong&gt;&lt;br/&gt;\nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos &lt;strong&gt;from&lt;/strong&gt; your smartphone &lt;strong&gt;to&lt;/strong&gt; your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn&amp;#39;t make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Install &lt;a href=\"https://www.photosync-app.com/home\"&gt;PhotoSync&lt;/a&gt; on smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Install PhotoSyncCompanion on Windows&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Set up app to connect to Windows computer on the same WiFi as smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Recents&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Update&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click New&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Computer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it&amp;#39;s an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MY PRE-2024 METHOD&lt;/strong&gt;&lt;br/&gt;\nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn&amp;#39;t see to work with the OneDrive app. I&amp;#39;m not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;The naming problem&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Photos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won&amp;#39;t work for the MOV files because the iPhone stores the video created date in the meta data field &lt;em&gt;com.apple.quicktime.creationdate&lt;/em&gt;. Better File Rename 5 can&amp;#39;t see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It&amp;#39;s possible that another renamer application with a GUI can do it, but I haven&amp;#39;t tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful &lt;a href=\"https://exiftool.org/\"&gt;EXIFtool&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here is how to rename iOS-created MOV video files to the date on your Windows 10 computer.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Put a copy of the MOV files into a folder in, say, C:\\RenamingFolder&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Download ExifTool and place the EXE file in that folder.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Rename the file to exiftool.exe&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Open Notepad, enter the following code, and save to C:\\RenamingFolder with the name Rename file to IPhone video creation date.bat&lt;/p&gt;\n\n&lt;p&gt;exiftool &amp;quot;-FileName&amp;lt;creationdate&amp;quot; -d &amp;quot;%%Y-%%m-%%d %%H-%%M-%%S.%%%%e&amp;quot; K:\\RenamingFolder\ncmd /k&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;CONCLUSION&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;PhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I&amp;#39;ll have properly named files using the accurate date and time.&lt;/p&gt;\n\n&lt;p&gt;Finally, for the record, I should add that while the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the &lt;em&gt;Text - Add text to end&lt;/em&gt; command from the menus, and type in with a leading space &lt;em&gt;Mike&amp;#39;s 8th birthday.&lt;/em&gt; So the final file name with be &lt;em&gt;2024-01-16 17-45-03 Mike&amp;#39;s 8th birthday.jpg.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ro2lq", "is_robot_indexable": true, "report_reasons": null, "author": "ThumperStrauss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "subreddit_subscribers": 721147, "created_utc": 1703640324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dear hoarders, this post is about Newsgears RSS reader/aggregator, [screenshots are located here](https://imgur.com/a/waj1CO6).\n\nI'm a huge fan of RSS; I think it's a fun and interesting way of exploring the Internet.  I began this project with the goal of simply building the best RSS tools I possibly could using free/open-source frameworks like Vue3 and Vuetify on the front-end, and Spring Boot and ROME on the back-end.  \n\nMain areas of focus: free, open-source, accessible, fully responsive/mobile friendly, secure, modern, all while granting the end-user full and accurate access to every aspect of the syndicated feed payload.\n\nHere are some of the basic features of this finished product: \n\n\\* Topical article queues provide a single view of multiple related feeds\n\n\\* Full-text searching using lunrjs\n\n\\* Multiple layouts (tabular, list, and card)\n\n\\* Light and dark themes\n\n\\* OPML support (import and export)\n\n\\* Fully responsive/usable on very small screens\n\n\\* Accessible and fully keyboard navigable\n\n\\* Integrated media player (vue-plyr)\n\n\\* Available in English, Spanish, and French\n\n\\* Native secure image proxy\n\n\\* Scalable architecture can support thousands of concurrent users\n\n\\*\\*To boot-up:\\*\\* \n\nDownload docker-compose.yml:\n\n    curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/docker-compose.single-user.yml.sample &gt; docker-compose.yml\n\nDownload nginx.conf:\n\n    curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/nginx.conf &gt; nginx.conf\n\nStart docker: \n\n    docker-compose up \n\n[Navigate to localhost port 80](http://localhost).\n\nClick the RSS logo icon in the upper-left to start adding feed subscriptions. \n\n**Github main repository for Newsgears: [https://github.com/lostsidewalk/newsgears-app](https://github.com/lostsidewalk/newsgears-app)**\n\nI'd love to hear from anyone who has an interest in RSS.  This project is still in its nascent stages, and I'm highly motivated to make fixes, implement features, etc.  I'm extremely open to suggestions, and collaborating with other devs of any skill level, on this project. Feel free to drop me a line, even if you think this is POS; I'd love to hear from any users.  \n\nNewsgears RSS should be pretty stable on Linux w/Chrome or FF.  I wouldn't be surprised if some chaos resulted from running this on another platform.  I would very much appreciate any form of feedback, especially bug reports.  \n\n\\*\\*Sister project:\\*\\* \n\nComposableRSS is a REST API-driven platform for composing and publishing syndicated web feed content (such as RSS/ATOM feeds, etc.). Basically, you interact with an internal/developer-friendly REST API to create feeds, add content, and generally control all aspects of the web feed lifecycle. ComposableRSS serves those feeds to your users.  ComposableRSS supports formats beyond RSS and ATOM, making it suitable for use as a general-purpose headless content management system.  \n\n**Github main repository for ComposableRSS: [https://github.com/lostsidewalk/composable-rss-app](https://github.com/lostsidewalk/composable-rss-app)**\n\nI have a Discord to discuss matters in real-time if anyone is interested.  Thanks for reading.", "author_fullname": "t2_ojb0752w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Newsgears RSS self-hosted reader/aggregator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18riu9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703629468.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703626333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear hoarders, this post is about Newsgears RSS reader/aggregator, &lt;a href=\"https://imgur.com/a/waj1CO6\"&gt;screenshots are located here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a huge fan of RSS; I think it&amp;#39;s a fun and interesting way of exploring the Internet.  I began this project with the goal of simply building the best RSS tools I possibly could using free/open-source frameworks like Vue3 and Vuetify on the front-end, and Spring Boot and ROME on the back-end.  &lt;/p&gt;\n\n&lt;p&gt;Main areas of focus: free, open-source, accessible, fully responsive/mobile friendly, secure, modern, all while granting the end-user full and accurate access to every aspect of the syndicated feed payload.&lt;/p&gt;\n\n&lt;p&gt;Here are some of the basic features of this finished product: &lt;/p&gt;\n\n&lt;p&gt;* Topical article queues provide a single view of multiple related feeds&lt;/p&gt;\n\n&lt;p&gt;* Full-text searching using lunrjs&lt;/p&gt;\n\n&lt;p&gt;* Multiple layouts (tabular, list, and card)&lt;/p&gt;\n\n&lt;p&gt;* Light and dark themes&lt;/p&gt;\n\n&lt;p&gt;* OPML support (import and export)&lt;/p&gt;\n\n&lt;p&gt;* Fully responsive/usable on very small screens&lt;/p&gt;\n\n&lt;p&gt;* Accessible and fully keyboard navigable&lt;/p&gt;\n\n&lt;p&gt;* Integrated media player (vue-plyr)&lt;/p&gt;\n\n&lt;p&gt;* Available in English, Spanish, and French&lt;/p&gt;\n\n&lt;p&gt;* Native secure image proxy&lt;/p&gt;\n\n&lt;p&gt;* Scalable architecture can support thousands of concurrent users&lt;/p&gt;\n\n&lt;p&gt;**To boot-up:** &lt;/p&gt;\n\n&lt;p&gt;Download docker-compose.yml:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/docker-compose.single-user.yml.sample &amp;gt; docker-compose.yml\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Download nginx.conf:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/nginx.conf &amp;gt; nginx.conf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Start docker: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker-compose up \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"http://localhost\"&gt;Navigate to localhost port 80&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Click the RSS logo icon in the upper-left to start adding feed subscriptions. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Github main repository for Newsgears: &lt;a href=\"https://github.com/lostsidewalk/newsgears-app\"&gt;https://github.com/lostsidewalk/newsgears-app&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from anyone who has an interest in RSS.  This project is still in its nascent stages, and I&amp;#39;m highly motivated to make fixes, implement features, etc.  I&amp;#39;m extremely open to suggestions, and collaborating with other devs of any skill level, on this project. Feel free to drop me a line, even if you think this is POS; I&amp;#39;d love to hear from any users.  &lt;/p&gt;\n\n&lt;p&gt;Newsgears RSS should be pretty stable on Linux w/Chrome or FF.  I wouldn&amp;#39;t be surprised if some chaos resulted from running this on another platform.  I would very much appreciate any form of feedback, especially bug reports.  &lt;/p&gt;\n\n&lt;p&gt;**Sister project:** &lt;/p&gt;\n\n&lt;p&gt;ComposableRSS is a REST API-driven platform for composing and publishing syndicated web feed content (such as RSS/ATOM feeds, etc.). Basically, you interact with an internal/developer-friendly REST API to create feeds, add content, and generally control all aspects of the web feed lifecycle. ComposableRSS serves those feeds to your users.  ComposableRSS supports formats beyond RSS and ATOM, making it suitable for use as a general-purpose headless content management system.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Github main repository for ComposableRSS: &lt;a href=\"https://github.com/lostsidewalk/composable-rss-app\"&gt;https://github.com/lostsidewalk/composable-rss-app&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a Discord to discuss matters in real-time if anyone is interested.  Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?auto=webp&amp;s=65b6c42caa98871c0e7702bf888e21b9fbbbe688", "width": 1774, "height": 1929}, "resolutions": [{"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e528ee1c272443dd3355b63eaa4c9fa56965bfa", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f6d7648284f6d1af7a0e16dd1d089b0bcb5405b", "width": 216, "height": 234}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=760c54796c3fa67b8e1eee91da3af12569828c1d", "width": 320, "height": 347}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90ea9a833dea041d9ffc3290c32cb905e982b209", "width": 640, "height": 695}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b9c2e003ad47ebe5b068c2510b26362db4c42ee", "width": 960, "height": 1043}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d21802bd10f713b6f6a722666ddefae9abeb8da", "width": 1080, "height": 1174}], "variants": {}, "id": "svVu7_zWZstpjaX3XfvSMrfAajp8LwT4qHIbogWRK2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18riu9v", "is_robot_indexable": true, "report_reasons": null, "author": "harrisofpeoria", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18riu9v/introducing_newsgears_rss_selfhosted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18riu9v/introducing_newsgears_rss_selfhosted/", "subreddit_subscribers": 721147, "created_utc": 1703626333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know how to install LTO-6 LTFS Drivers for ubuntu, I cannot seem to find any that work or the links are dead. I know mine currently finds it fine as this is the response.  \n`sudo mt -f /dev/st0 status`\n\n`drive type = 114`\n\n`drive status = 1509949440`\n\n`sense key error = 0`\n\n`residue count = 0`\n\n`file number = 0`\n\n`block number = 0`\n\nI'm using Fiber connection with an LTO-6 HPE tape", "author_fullname": "t2_6cwo4rts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO-6 LTFS Ubuntu", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rird4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703626118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to install LTO-6 LTFS Drivers for ubuntu, I cannot seem to find any that work or the links are dead. I know mine currently finds it fine as this is the response.&lt;br/&gt;\n&lt;code&gt;sudo mt -f /dev/st0 status&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;drive type = 114&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;drive status = 1509949440&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sense key error = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;residue count = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;file number = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;block number = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Fiber connection with an LTO-6 HPE tape&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rird4", "is_robot_indexable": true, "report_reasons": null, "author": "DrBrad__", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rird4/lto6_ltfs_ubuntu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rird4/lto6_ltfs_ubuntu/", "subreddit_subscribers": 721147, "created_utc": 1703626118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn't maintain timestamps for folders.\n\nIs there an alternate software?\n\n&amp;#x200B;", "author_fullname": "t2_12qmo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GUI software that allows me to click which folders/subfolders/files to copy to destination and preserve time stamps and directory/structure/hierarchy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rrbb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703649937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn&amp;#39;t maintain timestamps for folders.&lt;/p&gt;\n\n&lt;p&gt;Is there an alternate software?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rrbb0", "is_robot_indexable": true, "report_reasons": null, "author": "Snowblind45", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "subreddit_subscribers": 721147, "created_utc": 1703649937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I'm new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?\n\nThanks!  \n", "author_fullname": "t2_5cyhhwlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping DVDs/Blu-rays with Multiple Audio Languages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rp8qf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703643685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I&amp;#39;m new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rp8qf", "is_robot_indexable": true, "report_reasons": null, "author": "JoshALogs", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "subreddit_subscribers": 721147, "created_utc": 1703643685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.", "author_fullname": "t2_2kooznfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to capture audio from an 8-track tape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rmt6e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703636740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rmt6e", "is_robot_indexable": true, "report_reasons": null, "author": "dstillloading", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "subreddit_subscribers": 721147, "created_utc": 1703636740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have previously been using cloud hosting such as Dropbox and Google Drive for Plex however now they've canned the unlimited I am building my own server and there are certain requirements that I need when doing this so I would like to know what options I have.\n\nI am going to be having 5 8TB drives to start off, and I will expand these drives over the coming months, and I will be moving files from Google to my server over the next months, but In the meantime I would like to be able to mount them with rclone so Plex can still use these (until I make them local) and I'll use mergerfs to merge the cloud with my local structure which is where new files will go.\n\nThe OS I use will need to support rclone and mergerfs as a minimum while I will be using docker for all my other containers like tautulli, overseers and such.\n\nI have previously tried TrueNAS and was unsuccessful, it also doesn't support the features I need.\n\nSo what options are available for me to achieve this setup? I would ideally like one drive parity incase a drive goes bad.\n\nI already have a Dell PowerEdge r720 with the 5x 8tb HDDS I would ideally like to use this server for everything.\n\nThanks in advanced.", "author_fullname": "t2_9puzn9ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to build a Plex server - what are my options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rkuyy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have previously been using cloud hosting such as Dropbox and Google Drive for Plex however now they&amp;#39;ve canned the unlimited I am building my own server and there are certain requirements that I need when doing this so I would like to know what options I have.&lt;/p&gt;\n\n&lt;p&gt;I am going to be having 5 8TB drives to start off, and I will expand these drives over the coming months, and I will be moving files from Google to my server over the next months, but In the meantime I would like to be able to mount them with rclone so Plex can still use these (until I make them local) and I&amp;#39;ll use mergerfs to merge the cloud with my local structure which is where new files will go.&lt;/p&gt;\n\n&lt;p&gt;The OS I use will need to support rclone and mergerfs as a minimum while I will be using docker for all my other containers like tautulli, overseers and such.&lt;/p&gt;\n\n&lt;p&gt;I have previously tried TrueNAS and was unsuccessful, it also doesn&amp;#39;t support the features I need.&lt;/p&gt;\n\n&lt;p&gt;So what options are available for me to achieve this setup? I would ideally like one drive parity incase a drive goes bad.&lt;/p&gt;\n\n&lt;p&gt;I already have a Dell PowerEdge r720 with the 5x 8tb HDDS I would ideally like to use this server for everything.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advanced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rkuyy", "is_robot_indexable": true, "report_reasons": null, "author": "Abject_Persimmon4020", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rkuyy/i_want_to_build_a_plex_server_what_are_my_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rkuyy/i_want_to_build_a_plex_server_what_are_my_options/", "subreddit_subscribers": 721147, "created_utc": 1703631583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.\n\nNowadays, I have to postprocess my scans in the following way:\n\n1. rotate and crop them so there are not white boundaries\n2. Remove some scratches and dust that was still on the scans\n3. Improve picture quality by tweaking contrast, saturation, etc.\n4. export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)\n\nI do these manually on Affinity photo now, but I'm wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.\n\nAny suggestions?", "author_fullname": "t2_bv035", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automated tool for postprocessing scanned photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rksvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.&lt;/p&gt;\n\n&lt;p&gt;Nowadays, I have to postprocess my scans in the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;rotate and crop them so there are not white boundaries&lt;/li&gt;\n&lt;li&gt;Remove some scratches and dust that was still on the scans&lt;/li&gt;\n&lt;li&gt;Improve picture quality by tweaking contrast, saturation, etc.&lt;/li&gt;\n&lt;li&gt;export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I do these manually on Affinity photo now, but I&amp;#39;m wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "170TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rksvw", "is_robot_indexable": true, "report_reasons": null, "author": "Mathy963", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "subreddit_subscribers": 721147, "created_utc": 1703631429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a simple software with GUI (Windows) that will allow me to scan and catalog all file data for multiple drives, including offline drives. Ideal software would create a thumbnail of each video file so I can visually scan for it as it's probably mislabeled. \n\nI used to catalog all my drives before I moved to NAS units. \n\nMissing a video file and it could be anywhere. \n\nI am NOT LOOKING FOR a way to catalog my physical media collection (DVDs, Blurays, etc). ", "author_fullname": "t2_cvnw1nt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PC Media Cataloging tool? (used to be simple)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rex5c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703616058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a simple software with GUI (Windows) that will allow me to scan and catalog all file data for multiple drives, including offline drives. Ideal software would create a thumbnail of each video file so I can visually scan for it as it&amp;#39;s probably mislabeled. &lt;/p&gt;\n\n&lt;p&gt;I used to catalog all my drives before I moved to NAS units. &lt;/p&gt;\n\n&lt;p&gt;Missing a video file and it could be anywhere. &lt;/p&gt;\n\n&lt;p&gt;I am NOT LOOKING FOR a way to catalog my physical media collection (DVDs, Blurays, etc). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rex5c", "is_robot_indexable": true, "report_reasons": null, "author": "FalsettoChild", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rex5c/pc_media_cataloging_tool_used_to_be_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rex5c/pc_media_cataloging_tool_used_to_be_simple/", "subreddit_subscribers": 721147, "created_utc": 1703616058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nA few questions regarding One Drive\n\n1. Has anyone here successfully used the OneDrive Family 365 accounts together using Rclone's union?\n\n2. Can Rclone Union work across services (Google/ MS)?\n\n3. How big of a drawback is the 400 char limit on OneDrive? Any way I can check which folders/files may break that limit?", "author_fullname": "t2_435uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OneDrive(s) Rclone Union", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18revmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703615951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;A few questions regarding One Drive&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Has anyone here successfully used the OneDrive Family 365 accounts together using Rclone&amp;#39;s union?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can Rclone Union work across services (Google/ MS)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How big of a drawback is the 400 char limit on OneDrive? Any way I can check which folders/files may break that limit?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "0.035PB and climbing", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18revmn", "is_robot_indexable": true, "report_reasons": null, "author": "Xirious", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18revmn/onedrives_rclone_union/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18revmn/onedrives_rclone_union/", "subreddit_subscribers": 721147, "created_utc": 1703615951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apologies if this has been asked, but I've searched and have come up with nothing. I've exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I'd like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?\n\nI figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.", "author_fullname": "t2_479u9c8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tumblr Export - Recreate Blog from the Index HTML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18s4swq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703695061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this has been asked, but I&amp;#39;ve searched and have come up with nothing. I&amp;#39;ve exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I&amp;#39;d like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?&lt;/p&gt;\n\n&lt;p&gt;I figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s4swq", "is_robot_indexable": true, "report_reasons": null, "author": "ggodfrey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "subreddit_subscribers": 721147, "created_utc": 1703695061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\n\nSo recently, me and my girlfriend's chat was read by her mom, and coming from third world countries, our families aren't really supportive about it. \nNow before it causes any more complications, we want to export the entire chat (over 1gb in size) somewhere else and delete the chats on our phone.\nProblem is, we can't use paid services like imyfone or anything without our families knowing.\nPlease help \ud83d\ude2d", "author_fullname": "t2_7s2cty02", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I export an entire whatsapp chat (over 1 GB) for free?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18s4ez3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703694060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So recently, me and my girlfriend&amp;#39;s chat was read by her mom, and coming from third world countries, our families aren&amp;#39;t really supportive about it. \nNow before it causes any more complications, we want to export the entire chat (over 1gb in size) somewhere else and delete the chats on our phone.\nProblem is, we can&amp;#39;t use paid services like imyfone or anything without our families knowing.\nPlease help \ud83d\ude2d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s4ez3", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate_Candle477", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s4ez3/how_do_i_export_an_entire_whatsapp_chat_over_1_gb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s4ez3/how_do_i_export_an_entire_whatsapp_chat_over_1_gb/", "subreddit_subscribers": 721147, "created_utc": 1703694060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My setup is pretty tame:  I have an old tower with about 5 drives in it running OMV.  My main drive for Linux iso's is a 12TB drive, which I currently have setup to rsync to a second 14TB drive in the same tower on a weekly basis.  If drive 1 fails then I potentially lose a week of stuff, but I'm ok with that. \n\nI see most people setup a RAID, but I'm wondering for such a simple setup if theres a benefit to RAID vs rsync that I'm not seeing?\n\nYes, I know RAID is not backup, it's failure protection.  So what would make RAID better for in this use case?", "author_fullname": "t2_im0c2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of RAID vs rsync", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s2kta", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703690189.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703689164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My setup is pretty tame:  I have an old tower with about 5 drives in it running OMV.  My main drive for Linux iso&amp;#39;s is a 12TB drive, which I currently have setup to rsync to a second 14TB drive in the same tower on a weekly basis.  If drive 1 fails then I potentially lose a week of stuff, but I&amp;#39;m ok with that. &lt;/p&gt;\n\n&lt;p&gt;I see most people setup a RAID, but I&amp;#39;m wondering for such a simple setup if theres a benefit to RAID vs rsync that I&amp;#39;m not seeing?&lt;/p&gt;\n\n&lt;p&gt;Yes, I know RAID is not backup, it&amp;#39;s failure protection.  So what would make RAID better for in this use case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s2kta", "is_robot_indexable": true, "report_reasons": null, "author": "zachmorris_cellphone", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s2kta/benefits_of_raid_vs_rsync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s2kta/benefits_of_raid_vs_rsync/", "subreddit_subscribers": 721147, "created_utc": 1703689164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a new WD120EFBX (12tb WD Red Plus)\n\nWhen I got the drive, I did these steps:\n\n1. initialized it in Disk Management (GPT)\n- quick format I believe\n- ran extended SMART test on HDSentinel (perfect)\n- ran write/read surface test on HDSentinel (perfect)\n- bitlockered (full, not quick, encryption)\n- transferred all my data\n\nAt this point, everything looked perfect.\n\n**Rebooted. Now it's not showing up in Windows.**\n\nOpened disk management and it's giving me the \"Initialize Disk - You must initialize a disk before Logical Disk Manager can access it\" popup.\n\nDisk Management can see the \"unknown\" drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there's no problems with the hard drive.\n\nTried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.\n\nThe only possible thing I can see is in the drive properties Events tab, there's a \"Device not migrated event\".\n\nAny ideas? I appreciate any help you can give me.\n\nEdit: Tried a different windows machine and it was still unallocated. I've decided to just re-initialize and do the whole thing over again. If it fails again I'll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.", "author_fullname": "t2_58sktp70", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help - Perfect healthy new hard drive uninitialized/unallocated after reboot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s2b0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703693844.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703688396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a new WD120EFBX (12tb WD Red Plus)&lt;/p&gt;\n\n&lt;p&gt;When I got the drive, I did these steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;initialized it in Disk Management (GPT)&lt;/li&gt;\n&lt;li&gt;quick format I believe&lt;/li&gt;\n&lt;li&gt;ran extended SMART test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;ran write/read surface test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;bitlockered (full, not quick, encryption)&lt;/li&gt;\n&lt;li&gt;transferred all my data&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;At this point, everything looked perfect.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Rebooted. Now it&amp;#39;s not showing up in Windows.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Opened disk management and it&amp;#39;s giving me the &amp;quot;Initialize Disk - You must initialize a disk before Logical Disk Manager can access it&amp;quot; popup.&lt;/p&gt;\n\n&lt;p&gt;Disk Management can see the &amp;quot;unknown&amp;quot; drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there&amp;#39;s no problems with the hard drive.&lt;/p&gt;\n\n&lt;p&gt;Tried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.&lt;/p&gt;\n\n&lt;p&gt;The only possible thing I can see is in the drive properties Events tab, there&amp;#39;s a &amp;quot;Device not migrated event&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? I appreciate any help you can give me.&lt;/p&gt;\n\n&lt;p&gt;Edit: Tried a different windows machine and it was still unallocated. I&amp;#39;ve decided to just re-initialize and do the whole thing over again. If it fails again I&amp;#39;ll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s2b0v", "is_robot_indexable": true, "report_reasons": null, "author": "SpareMenu5", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "subreddit_subscribers": 721147, "created_utc": 1703688396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m planning to get the QNAP TR-004 4 Bay DAS. I currently have an 18TB EasyStore that houses my data. If I set the QNAP with 4 bays and then shuck the EasyStore to swap out with one of the drives in the Raid, will the 18TB get wiped when I swap drives?", "author_fullname": "t2_as8c2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about Raid 5", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s1uto", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703687125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m planning to get the QNAP TR-004 4 Bay DAS. I currently have an 18TB EasyStore that houses my data. If I set the QNAP with 4 bays and then shuck the EasyStore to swap out with one of the drives in the Raid, will the 18TB get wiped when I swap drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s1uto", "is_robot_indexable": true, "report_reasons": null, "author": "browjose", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s1uto/question_about_raid_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s1uto/question_about_raid_5/", "subreddit_subscribers": 721147, "created_utc": 1703687125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**\\[TL;DR at the bottom\\]**\n\nHi, I'm not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.\n\nI'm currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I'm starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. \n\nTaking a look at the market pricing where I live, I came up with two options:\n\n1. 3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&gt; 8TB effective space, \\~57 USD/eff. TB\n2. 3x 18TB drives mirroring each other, thus 2 \"parity\" disks, at least one of which would be offsite -&gt; 18TB effective space, \\~50 USD/eff. TB\n\nMy goal is to build a robust solution, that I can use for many years. While I won't need more than 8TB in the near future, I wouldn't mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.\n\nOther aspects I considered include:\n\n* Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.   \nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?  \nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let's say 16TB) drive that completely replaces the 8TB one as the offsite drive?\n* Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.   \nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.\n* Mirroring: both options require some sort of mirroring. For this, I'm currently using freefilesync. What do you think about this approach?  I'm comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.\n* Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.   \nI'm especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? \n\nThis is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.\n\nTL;DR:   \n\\- RAID with 1 parity + single disk offsite backup with no parity, or  \n\\- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option", "author_fullname": "t2_73ebnj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Choosing a Storage Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rxnid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703672612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;[TL;DR at the bottom]&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I&amp;#39;m starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. &lt;/p&gt;\n\n&lt;p&gt;Taking a look at the market pricing where I live, I came up with two options:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&amp;gt; 8TB effective space, ~57 USD/eff. TB&lt;/li&gt;\n&lt;li&gt;3x 18TB drives mirroring each other, thus 2 &amp;quot;parity&amp;quot; disks, at least one of which would be offsite -&amp;gt; 18TB effective space, ~50 USD/eff. TB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My goal is to build a robust solution, that I can use for many years. While I won&amp;#39;t need more than 8TB in the near future, I wouldn&amp;#39;t mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.&lt;/p&gt;\n\n&lt;p&gt;Other aspects I considered include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.&lt;br/&gt;\nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?&lt;br/&gt;\nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let&amp;#39;s say 16TB) drive that completely replaces the 8TB one as the offsite drive?&lt;/li&gt;\n&lt;li&gt;Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.&lt;br/&gt;\nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.&lt;/li&gt;\n&lt;li&gt;Mirroring: both options require some sort of mirroring. For this, I&amp;#39;m currently using freefilesync. What do you think about this approach?  I&amp;#39;m comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.&lt;/li&gt;\n&lt;li&gt;Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.&lt;br/&gt;\nI&amp;#39;m especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;br/&gt;\n- RAID with 1 parity + single disk offsite backup with no parity, or&lt;br/&gt;\n- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rxnid", "is_robot_indexable": true, "report_reasons": null, "author": "Zorinhou", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "subreddit_subscribers": 721147, "created_utc": 1703672612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Was about to pull the trigger on SSDs from Staples (Samsung T7 seems like the best value / speed ratio I can find right now).\n\nThen I saw some refurbished models on BestBuy \u2014 [bonus that one 1TB model is red](https://www.bestbuy.ca/en-ca/product/refurbished-good-samsung-t7-1tb-usb-3-2-external-solid-state-drive-mu-pc1t0r-am-1tb-red/17649580) on for $90 CAD! And on a huge sale ([2 TB for $140 CAD](https://www.bestbuy.ca/en-ca/product/refurbished-good-samsung-t7-2tb-usb-3-2-external-solid-state-drive-mu-pc2t0t-am-grey-english/17399491)) The seller only offers 90 days warranty vs. the 3 yrs a new product would come with... but Samsung's website says that they'll honour warranty based on the manufacturing date (if there's no original receipt).\n\nSeems like a good deal to me... thoughts?!", "author_fullname": "t2_626ohywn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refurbished SSDs and warranty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rr8uz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703649730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was about to pull the trigger on SSDs from Staples (Samsung T7 seems like the best value / speed ratio I can find right now).&lt;/p&gt;\n\n&lt;p&gt;Then I saw some refurbished models on BestBuy \u2014 &lt;a href=\"https://www.bestbuy.ca/en-ca/product/refurbished-good-samsung-t7-1tb-usb-3-2-external-solid-state-drive-mu-pc1t0r-am-1tb-red/17649580\"&gt;bonus that one 1TB model is red&lt;/a&gt; on for $90 CAD! And on a huge sale (&lt;a href=\"https://www.bestbuy.ca/en-ca/product/refurbished-good-samsung-t7-2tb-usb-3-2-external-solid-state-drive-mu-pc2t0t-am-grey-english/17399491\"&gt;2 TB for $140 CAD&lt;/a&gt;) The seller only offers 90 days warranty vs. the 3 yrs a new product would come with... but Samsung&amp;#39;s website says that they&amp;#39;ll honour warranty based on the manufacturing date (if there&amp;#39;s no original receipt).&lt;/p&gt;\n\n&lt;p&gt;Seems like a good deal to me... thoughts?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rr8uz", "is_robot_indexable": true, "report_reasons": null, "author": "Positive_Guarantee20", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rr8uz/refurbished_ssds_and_warranty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rr8uz/refurbished_ssds_and_warranty/", "subreddit_subscribers": 721147, "created_utc": 1703649730.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}