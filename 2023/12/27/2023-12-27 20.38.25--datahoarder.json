{"kind": "Listing", "data": {"after": "t3_18rxnid", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Copyright Hinders The Preservation Of Modern, Digital Culture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18rlias", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 177, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 177, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fTybdFZPJcL8xPvqXL1ixSe2-v3FKnp-LJKodGe_Tjo.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703633257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rlias", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rlias/how_copyright_hinders_the_preservation_of_modern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "subreddit_subscribers": 721178, "created_utc": 1703633257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I stumbled upon this community and the lack of faith people have in external hard drives is...eye-opening. I have questions that I can't find elsewhere so I'm just going to ask, sorry if they've been answered (just link me in the right direction!)\n\nThe biggest one: if any source of media storage is so doomed to inevitable and absolute failure at any moment... does this apply to my computer? My phone? \n\nI've been using my macbooks/ipads/iphones for at least 10 years nows and NEVER had a loss of anything whatsoever, and I was under the impression that in the event of something happening, it could usually be recovered somehow anyway (stories of the fbi finding shit about people on wiped laptops etc). I only recently decided I should get an external hard drive (how I ended up here) because I had the unnerving realization that none of my photography/video footage was actually fully on any of my devices. Since I don't have enough space it's all optimized. I've just been subscribing for 2 TB of icloud to hold it, so I got scared that if something happened on the icloud side, my years of work would be stuck in low-res regardless of the fact that it's on multiple devices. I thought I would be safe by just getting a 2tb macbook as my next replacement to hold everything OR just getting a much cheaper external hard drive. So is there a difference between those two? Or are they both as vulnerable?\n\n2nd question: Is people's fear over this really that immanent? or is it more hypothetical? Of course theoretically ANYTHING can fail but how often does it actually happen? People keep bitcoin on ledgers and trezors, important documents on thumb drives backed up by just their computer. Why have I never heard of someone's iphone files becoming corrupted out of nowhere? Still wrapping my head around what is the practical reality here. We can also get hit by a meteor at any second...\n\n3rd question: this is just more of an item of curiosity, but is cloud backup just as vulnerable? I'm sure there's plenty of backups in that scenario but do they regularly have instances where something fails and they have to switch to a backup? Is one cloud server better than another? \n\nThanks for any info, this sort of thing really stresses me out because I'm a content creator but my adhd makes me really bad with organization of \"intangible\" things, and the stress of fucking something up makes it even worse. I know the standard answer seems to be just buy multiple backup drives but I'm so far from understanding how to incorporate that into my workflow and just thinking about it is overwhelming. I'm learning I have subpar data hygiene and I'm trying to just go step by step to get better, but be thorough in the process.", "author_fullname": "t2_c7aj0brw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clarification on some things I've been reading in this community. Is all the paranoia justified?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rh6rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703621893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon this community and the lack of faith people have in external hard drives is...eye-opening. I have questions that I can&amp;#39;t find elsewhere so I&amp;#39;m just going to ask, sorry if they&amp;#39;ve been answered (just link me in the right direction!)&lt;/p&gt;\n\n&lt;p&gt;The biggest one: if any source of media storage is so doomed to inevitable and absolute failure at any moment... does this apply to my computer? My phone? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using my macbooks/ipads/iphones for at least 10 years nows and NEVER had a loss of anything whatsoever, and I was under the impression that in the event of something happening, it could usually be recovered somehow anyway (stories of the fbi finding shit about people on wiped laptops etc). I only recently decided I should get an external hard drive (how I ended up here) because I had the unnerving realization that none of my photography/video footage was actually fully on any of my devices. Since I don&amp;#39;t have enough space it&amp;#39;s all optimized. I&amp;#39;ve just been subscribing for 2 TB of icloud to hold it, so I got scared that if something happened on the icloud side, my years of work would be stuck in low-res regardless of the fact that it&amp;#39;s on multiple devices. I thought I would be safe by just getting a 2tb macbook as my next replacement to hold everything OR just getting a much cheaper external hard drive. So is there a difference between those two? Or are they both as vulnerable?&lt;/p&gt;\n\n&lt;p&gt;2nd question: Is people&amp;#39;s fear over this really that immanent? or is it more hypothetical? Of course theoretically ANYTHING can fail but how often does it actually happen? People keep bitcoin on ledgers and trezors, important documents on thumb drives backed up by just their computer. Why have I never heard of someone&amp;#39;s iphone files becoming corrupted out of nowhere? Still wrapping my head around what is the practical reality here. We can also get hit by a meteor at any second...&lt;/p&gt;\n\n&lt;p&gt;3rd question: this is just more of an item of curiosity, but is cloud backup just as vulnerable? I&amp;#39;m sure there&amp;#39;s plenty of backups in that scenario but do they regularly have instances where something fails and they have to switch to a backup? Is one cloud server better than another? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any info, this sort of thing really stresses me out because I&amp;#39;m a content creator but my adhd makes me really bad with organization of &amp;quot;intangible&amp;quot; things, and the stress of fucking something up makes it even worse. I know the standard answer seems to be just buy multiple backup drives but I&amp;#39;m so far from understanding how to incorporate that into my workflow and just thinking about it is overwhelming. I&amp;#39;m learning I have subpar data hygiene and I&amp;#39;m trying to just go step by step to get better, but be thorough in the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rh6rm", "is_robot_indexable": true, "report_reasons": null, "author": "seeeeeeeeth", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rh6rm/clarification_on_some_things_ive_been_reading_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rh6rm/clarification_on_some_things_ive_been_reading_in/", "subreddit_subscribers": 721178, "created_utc": 1703621893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all. I'm new to the DataHoarder community, but not new to having numerous hard drives in my desk. \n\nTLDR: Sent in my 4TB External HDD for recovery because I deleted files to save space. Deleted the corrupted or broken recovered files, organized the remaining good files, de-duplicated them, moved them to my main 8TB HDD, formatted the 8TB transfer drive, then moved all the files back onto the freshly formatted 8TB transfer drive.\n\n&amp;#x200B;\n\nHere's the full story:\n\nBack in October 2020, I decided to delete a bunch of videos off my measly 4TB Seagate External HDD since it was getting full. Later in February 2021, I got an 8TB External HDD that I still use to this day for videography and other purposes that I use daily. I moved all my data from my 4TB HDD to my 8TB HDD, basically leaving the 4TB completely empty. However (and luckily), I did not use it at all since that day.\n\nJust this past November, I saved up enough to hire [$300 Dollar Data Recovery](https://www.300dollardatarecovery.com/) to recover ALL of the data on the 4TB, specifically the videos that I deleted way back in 2020. I filled out their forms, and sent another 8TB Internal HDD I had, and the 4TB from before.\n\nWhen they shipped back both drives, I was surprised to find that there was a little over 5TB worth of data, all coming from a drive that has 3.63TB of storage. Everything from videos to really small files 1KB or less files, totaling about a million files and several tens of thousands of folders.\n\nThe FIRST thing I had to do was painstakingly sort out all the corrupted or \"broken\" files that weren't playable or were distorted beyond recognition. So I had to carefully plan and make the process as efficient and fast as possible.\n\nFor videos and photo, I opened up File Explorer in full screen and put it in TILE view. I used Ctrl + Scroll and zoomed out as far as I could, just before going into a different view, to have as many icons as possible on my screen.\n\nI also sorted the files by size, because most of the corrupt files were either \\~1KB or gigantic (400+ MB for a corrupted photo). Doing this would group all the small and large files together, making it so I could simply drag around the blue box and select a bunch of them at once, and unselect the few good ones from my selection while holding down Ctrl.\n\nI would let File Explorer load all the thumbnails for the photos/videos, and whatever didn't have a thumbnail I simply selected and moved to a folder called \"Broken Files\", since opening them would simply give me a corrupted file error 99% of the time. Then I would simply scroll, wait a couple seconds for File Explorer to load the thumbnails, quickly scan for any files missing a thumbnail, then slowly scroll again.\n\nFor MP3 or other audio files, since all of them combined was only about \\~100GB, I decided it wasn't worth my time to comb through lots of more files to free up less space. For HTML, SVG, WAV, RTF, etc... all of them combined was only about \\~40GB, with over 200k files, so again, it wasn't worth my time trying to free up such little space with so many small files to comb through.\n\nAfter weeding out the corrupted or broken files, I made folders that were organized by year (2017, 2018, 2019, etc etc). I sorted the video/photos by their dates, and moved them into their respective year folders. I did this by again, having File Explorer in fullscreen, then putting it into DETAIL view (so I could see the date), sorted by date, then dragged the mouse all the way down, letting it autoscroll down until I've selected all the files for that year. \n\nFor the other files (HTML, WAV, RTF, etc), I again used File Explorer in full screen, but this time put it in LIST view instead of tile view (although you could use small icon view), and then sorted by file type. I then did the same for above, but instead of dates, it was file type (duh). I then put all the selected files into their respective folders (HTML, WAV, etc).\n\nAt this point in the recovery (or organization, really), I had a bunch of folders ranging from 2015 to 2021, and a few other folders for different file types. I put all of these folders in one big giant folder called \"good files\", then used DupeGuru to remove any duplicates from that folder. I set my 8TB HDD as a reference, and let DupeGuru sort through 300k+ files overnight.\n\nThe next morning I came back and saw a lot of duplicates, all with a 100% match. I made sure to save the results on my computer just in case. I went ahead and sorted by file size, and saw that the video files and all others were all matching up by year, file size, and even filename. The small \\~10kB files I knew luckily weren't that important to me, so I just let it slide, since they were probably some program files or PNG's that aren't family pictures or selfies.\n\nI moved all the duplicate files into another folder called \"good duplicates\". I then moved all of the \"good files\" to my 8TB HDD, totaling 1.5TB being moved via File Explorer. \n\nI then ran DupeGuru once more for good measure, setting the \"good duplicates\" folder against my 8TB HDD and the \"good files\". I saw one photo slide, and \\~200 small files like RTF, HTML, etc, after moving the duplicates to yet another folder called \"good duplicates 2\". I then moved the remaining non-duplicates to my 8TB HDD, and called it a day.\n\nI then went ahead and formatted the 8TB TRANSFER drive (not my 8TB HDD mentioned previously of course!) to get rid of any pesky files or anything that could interfere with storing data. Before I did any deletion or formatting, I made sure that the only files that were on the transfer drive were either broken files, or duplicates, which I confirmed they were.\n\n&amp;#x200B;\n\nThe data recovery itself took a little over a week from shipping to getting it back in the mail. The \n\nI formatted the 8TB transfer drive yesterday, and starting moving all the files I wish to move from my 8TB HDD to the 8TB transfer drive. It's about 5.3TB worth of data being moved over to my transfer drive, which will now be used as extra storage. As I'm typing this story down this morning, my data is still being transferred over, and has been doing so since last night, so I will give it probably a full 24 hours or so to fully transfer 5.3TB worth of data.", "author_fullname": "t2_n10mx4oww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got done recovering ~5TB worth of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s41pk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703693080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I&amp;#39;m new to the DataHoarder community, but not new to having numerous hard drives in my desk. &lt;/p&gt;\n\n&lt;p&gt;TLDR: Sent in my 4TB External HDD for recovery because I deleted files to save space. Deleted the corrupted or broken recovered files, organized the remaining good files, de-duplicated them, moved them to my main 8TB HDD, formatted the 8TB transfer drive, then moved all the files back onto the freshly formatted 8TB transfer drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the full story:&lt;/p&gt;\n\n&lt;p&gt;Back in October 2020, I decided to delete a bunch of videos off my measly 4TB Seagate External HDD since it was getting full. Later in February 2021, I got an 8TB External HDD that I still use to this day for videography and other purposes that I use daily. I moved all my data from my 4TB HDD to my 8TB HDD, basically leaving the 4TB completely empty. However (and luckily), I did not use it at all since that day.&lt;/p&gt;\n\n&lt;p&gt;Just this past November, I saved up enough to hire &lt;a href=\"https://www.300dollardatarecovery.com/\"&gt;$300 Dollar Data Recovery&lt;/a&gt; to recover ALL of the data on the 4TB, specifically the videos that I deleted way back in 2020. I filled out their forms, and sent another 8TB Internal HDD I had, and the 4TB from before.&lt;/p&gt;\n\n&lt;p&gt;When they shipped back both drives, I was surprised to find that there was a little over 5TB worth of data, all coming from a drive that has 3.63TB of storage. Everything from videos to really small files 1KB or less files, totaling about a million files and several tens of thousands of folders.&lt;/p&gt;\n\n&lt;p&gt;The FIRST thing I had to do was painstakingly sort out all the corrupted or &amp;quot;broken&amp;quot; files that weren&amp;#39;t playable or were distorted beyond recognition. So I had to carefully plan and make the process as efficient and fast as possible.&lt;/p&gt;\n\n&lt;p&gt;For videos and photo, I opened up File Explorer in full screen and put it in TILE view. I used Ctrl + Scroll and zoomed out as far as I could, just before going into a different view, to have as many icons as possible on my screen.&lt;/p&gt;\n\n&lt;p&gt;I also sorted the files by size, because most of the corrupt files were either ~1KB or gigantic (400+ MB for a corrupted photo). Doing this would group all the small and large files together, making it so I could simply drag around the blue box and select a bunch of them at once, and unselect the few good ones from my selection while holding down Ctrl.&lt;/p&gt;\n\n&lt;p&gt;I would let File Explorer load all the thumbnails for the photos/videos, and whatever didn&amp;#39;t have a thumbnail I simply selected and moved to a folder called &amp;quot;Broken Files&amp;quot;, since opening them would simply give me a corrupted file error 99% of the time. Then I would simply scroll, wait a couple seconds for File Explorer to load the thumbnails, quickly scan for any files missing a thumbnail, then slowly scroll again.&lt;/p&gt;\n\n&lt;p&gt;For MP3 or other audio files, since all of them combined was only about ~100GB, I decided it wasn&amp;#39;t worth my time to comb through lots of more files to free up less space. For HTML, SVG, WAV, RTF, etc... all of them combined was only about ~40GB, with over 200k files, so again, it wasn&amp;#39;t worth my time trying to free up such little space with so many small files to comb through.&lt;/p&gt;\n\n&lt;p&gt;After weeding out the corrupted or broken files, I made folders that were organized by year (2017, 2018, 2019, etc etc). I sorted the video/photos by their dates, and moved them into their respective year folders. I did this by again, having File Explorer in fullscreen, then putting it into DETAIL view (so I could see the date), sorted by date, then dragged the mouse all the way down, letting it autoscroll down until I&amp;#39;ve selected all the files for that year. &lt;/p&gt;\n\n&lt;p&gt;For the other files (HTML, WAV, RTF, etc), I again used File Explorer in full screen, but this time put it in LIST view instead of tile view (although you could use small icon view), and then sorted by file type. I then did the same for above, but instead of dates, it was file type (duh). I then put all the selected files into their respective folders (HTML, WAV, etc).&lt;/p&gt;\n\n&lt;p&gt;At this point in the recovery (or organization, really), I had a bunch of folders ranging from 2015 to 2021, and a few other folders for different file types. I put all of these folders in one big giant folder called &amp;quot;good files&amp;quot;, then used DupeGuru to remove any duplicates from that folder. I set my 8TB HDD as a reference, and let DupeGuru sort through 300k+ files overnight.&lt;/p&gt;\n\n&lt;p&gt;The next morning I came back and saw a lot of duplicates, all with a 100% match. I made sure to save the results on my computer just in case. I went ahead and sorted by file size, and saw that the video files and all others were all matching up by year, file size, and even filename. The small ~10kB files I knew luckily weren&amp;#39;t that important to me, so I just let it slide, since they were probably some program files or PNG&amp;#39;s that aren&amp;#39;t family pictures or selfies.&lt;/p&gt;\n\n&lt;p&gt;I moved all the duplicate files into another folder called &amp;quot;good duplicates&amp;quot;. I then moved all of the &amp;quot;good files&amp;quot; to my 8TB HDD, totaling 1.5TB being moved via File Explorer. &lt;/p&gt;\n\n&lt;p&gt;I then ran DupeGuru once more for good measure, setting the &amp;quot;good duplicates&amp;quot; folder against my 8TB HDD and the &amp;quot;good files&amp;quot;. I saw one photo slide, and ~200 small files like RTF, HTML, etc, after moving the duplicates to yet another folder called &amp;quot;good duplicates 2&amp;quot;. I then moved the remaining non-duplicates to my 8TB HDD, and called it a day.&lt;/p&gt;\n\n&lt;p&gt;I then went ahead and formatted the 8TB TRANSFER drive (not my 8TB HDD mentioned previously of course!) to get rid of any pesky files or anything that could interfere with storing data. Before I did any deletion or formatting, I made sure that the only files that were on the transfer drive were either broken files, or duplicates, which I confirmed they were.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The data recovery itself took a little over a week from shipping to getting it back in the mail. The &lt;/p&gt;\n\n&lt;p&gt;I formatted the 8TB transfer drive yesterday, and starting moving all the files I wish to move from my 8TB HDD to the 8TB transfer drive. It&amp;#39;s about 5.3TB worth of data being moved over to my transfer drive, which will now be used as extra storage. As I&amp;#39;m typing this story down this morning, my data is still being transferred over, and has been doing so since last night, so I will give it probably a full 24 hours or so to fully transfer 5.3TB worth of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?auto=webp&amp;s=b60e1e5ce678ffb4aedb0243d62264f74de0ad4e", "width": 1000, "height": 474}, "resolutions": [{"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8abdcef72df2093bf758152201afa04bf6e057bf", "width": 108, "height": 51}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6e677071f813a21dcf0f718f62e269a544bceb3", "width": 216, "height": 102}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09c16fd6ebedfd8bc66c13bdd68a4e2636838060", "width": 320, "height": 151}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7eeda53972ba146d14da874ffe1535e7f85ea97", "width": 640, "height": 303}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af269d1b86d58ad8f6938810a1ff26559b9df570", "width": 960, "height": 455}], "variants": {}, "id": "Lj79sn9bvI9itLtGCoV9Mp-NCuBvuY6FKn4cO6LIhnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18s41pk", "is_robot_indexable": true, "report_reasons": null, "author": "EveningDay5261", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s41pk/just_got_done_recovering_5tb_worth_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s41pk/just_got_done_recovering_5tb_worth_of_data/", "subreddit_subscribers": 721178, "created_utc": 1703693080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently moving my entire media file collection to a better file management system ([Hydrus Network](https://hydrusnetwork.github.io/hydrus/index.html) if you're interested) and I realized that I also want to preserve the creation times of the original files.\n\nThis means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.\n\nThis is absurd. I'm substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.\n\nJust wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. \n\nData hoarding is at the end of the day just a kind of hoarding.", "author_fullname": "t2_hokshzbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Addiction to Preserving Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rwdue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703667526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently moving my entire media file collection to a better file management system (&lt;a href=\"https://hydrusnetwork.github.io/hydrus/index.html\"&gt;Hydrus Network&lt;/a&gt; if you&amp;#39;re interested) and I realized that I also want to preserve the creation times of the original files.&lt;/p&gt;\n\n&lt;p&gt;This means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.&lt;/p&gt;\n\n&lt;p&gt;This is absurd. I&amp;#39;m substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. &lt;/p&gt;\n\n&lt;p&gt;Data hoarding is at the end of the day just a kind of hoarding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18rwdue", "is_robot_indexable": true, "report_reasons": null, "author": "Estavenz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "subreddit_subscribers": 721178, "created_utc": 1703667526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good evening all\n\nI have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.\n\nI did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people's entire careers is just this section of how to run a computer.\n\nI am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I've seen, but Start early is my main idea, so it is not severely daunting\n\nTLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage", "author_fullname": "t2_dacwz4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First time here, first time seriously considering big backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rq1ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703646273.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703646051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening all&lt;/p&gt;\n\n&lt;p&gt;I have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.&lt;/p&gt;\n\n&lt;p&gt;I did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people&amp;#39;s entire careers is just this section of how to run a computer.&lt;/p&gt;\n\n&lt;p&gt;I am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I&amp;#39;ve seen, but Start early is my main idea, so it is not severely daunting&lt;/p&gt;\n\n&lt;p&gt;TLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rq1ig", "is_robot_indexable": true, "report_reasons": null, "author": "97cweb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "subreddit_subscribers": 721178, "created_utc": 1703646051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students ", "author_fullname": "t2_ey91h2hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PhD students / academics - how do you go about backing up your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rl0ln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703632796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rl0ln", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Difficulty_678", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "subreddit_subscribers": 721178, "created_utc": 1703631964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone. Wall of text incoming.\n\nI've been amassing a large collection of files over the last couple of years (around 1TB total, which is probably not a lot to most of you lol), and have been looking into finally getting around to creating a setup that adheres to the 3-2-1 backup rule.\n\nI've been browsing and searching for beginner tips on this subreddit for a long time today and am just overwhelmed. Lots of terminology and abbreviations that I don't understand. I wouldn't consider myself computer illiterate, but I have never dived into the hardware side of things before.\n\nMy situation / background: I currently live a semi-nomadic lifestyle, and so want something that is portable. I was planning on purchasing 2 external HDDs (2TB or 4TB) to keep with full backups and update weekly / monthly, and keep one in a storage unit. For my 3rd backup I currently use a Sandisk Extreme SSD 1TB, for files or games that I watch / work / play directly off the drive for and that need fast speeds (I may upgrade to a 2TB or 4TB version). I also use both paid MEGA and Protondrive services, but am unable to upload and store my whole collection in, and only use it for very important files or things that I need access to everywhere.\n\nI guess I was just looking for some advice on a few things:\n\n1: Does this backup plan sound like a good plan? Any modifications you would make?\n\n2: Are there any recommendations for brands / basic external drives? How do you avoid drives with low failure rates, just reading online reviews?\n\n3: Is there any essential software I should have to help manage drives / check on drive health?\n\nIf you have any links to specific resources it would be helpful as well.\n\nThank you in advance!", "author_fullname": "t2_1292eb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner. Overwhelmed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rjcm4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703627694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. Wall of text incoming.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been amassing a large collection of files over the last couple of years (around 1TB total, which is probably not a lot to most of you lol), and have been looking into finally getting around to creating a setup that adheres to the 3-2-1 backup rule.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been browsing and searching for beginner tips on this subreddit for a long time today and am just overwhelmed. Lots of terminology and abbreviations that I don&amp;#39;t understand. I wouldn&amp;#39;t consider myself computer illiterate, but I have never dived into the hardware side of things before.&lt;/p&gt;\n\n&lt;p&gt;My situation / background: I currently live a semi-nomadic lifestyle, and so want something that is portable. I was planning on purchasing 2 external HDDs (2TB or 4TB) to keep with full backups and update weekly / monthly, and keep one in a storage unit. For my 3rd backup I currently use a Sandisk Extreme SSD 1TB, for files or games that I watch / work / play directly off the drive for and that need fast speeds (I may upgrade to a 2TB or 4TB version). I also use both paid MEGA and Protondrive services, but am unable to upload and store my whole collection in, and only use it for very important files or things that I need access to everywhere.&lt;/p&gt;\n\n&lt;p&gt;I guess I was just looking for some advice on a few things:&lt;/p&gt;\n\n&lt;p&gt;1: Does this backup plan sound like a good plan? Any modifications you would make?&lt;/p&gt;\n\n&lt;p&gt;2: Are there any recommendations for brands / basic external drives? How do you avoid drives with low failure rates, just reading online reviews?&lt;/p&gt;\n\n&lt;p&gt;3: Is there any essential software I should have to help manage drives / check on drive health?&lt;/p&gt;\n\n&lt;p&gt;If you have any links to specific resources it would be helpful as well.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rjcm4", "is_robot_indexable": true, "report_reasons": null, "author": "Coolpop19", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rjcm4/beginner_overwhelmed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rjcm4/beginner_overwhelmed/", "subreddit_subscribers": 721178, "created_utc": 1703627694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, \n\n&amp;#x200B;\n\n   I was looking to do some VHS conversion for some old VHS tapes that need to go away.  I have an old TV / VHS combo [https://www.mediapollution.tv/product-page/rca-t19060gy](https://www.mediapollution.tv/product-page/rca-t19060gy) specifically.  \n\n&amp;#x200B;\n\nI was thinking of getting something like this, [https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/](https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/) and converting it myself.  My concern that I have is that the TV/VHS combo seems to only have one audio output.  (ie.  It doesn't have a red, yellow, white but only the white and yellow which I assume is one of the audio channels).  Would this matter? I'd hate to convert and get bad quality audio because of this, when I could have gotten a better quality if I took it to a service or acquired a better VHS player.  \n\n&amp;#x200B;\n\nAny thoughts?\n\n&amp;#x200B;", "author_fullname": "t2_pujhcq07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting VHS Tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s5otb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703697280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was looking to do some VHS conversion for some old VHS tapes that need to go away.  I have an old TV / VHS combo &lt;a href=\"https://www.mediapollution.tv/product-page/rca-t19060gy\"&gt;https://www.mediapollution.tv/product-page/rca-t19060gy&lt;/a&gt; specifically.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was thinking of getting something like this, &lt;a href=\"https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/\"&gt;https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/&lt;/a&gt; and converting it myself.  My concern that I have is that the TV/VHS combo seems to only have one audio output.  (ie.  It doesn&amp;#39;t have a red, yellow, white but only the white and yellow which I assume is one of the audio channels).  Would this matter? I&amp;#39;d hate to convert and get bad quality audio because of this, when I could have gotten a better quality if I took it to a service or acquired a better VHS player.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?auto=webp&amp;s=b600d0192f6abd0abaa216ea632444247206875f", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c56eda873c1d53ea4a01e159d209a85385404fe", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afbdf406e275b69ddf76009ddcef20d2e430aa15", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c2b15856f08a574c232bdeb79aaba3c0bbf7b69", "width": 320, "height": 240}], "variants": {}, "id": "z1CDdnCRodlx8CWMOliStCIDXgB2nzP8ur3vMcLW0jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s5otb", "is_robot_indexable": true, "report_reasons": null, "author": "csgeek3674", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s5otb/converting_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s5otb/converting_vhs_tapes/", "subreddit_subscribers": 721178, "created_utc": 1703697280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I noticed that my drive was frequently coming online and offline. I opened my server to insepct it, and found that the sata connection snapped off.  The drive is fully functional if I carefully insert a cable. I wiped the drive already with the intention of tossing it, since it has already been replaced. It just hurts to throw a 14tb drive out while it still functions. Is there anyway I can salvage this or make a repair? It is a Seagate exos x16.", "author_fullname": "t2_14din1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Any hope or ideas on how to repair this drive, or is it toast?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"jrj2st4lwv8c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68388ab792b994ac8cfa9e100a70f330e5225b6b"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0ed9febfa3019b0fc9cf06dff89d57bf2a9d724"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6076d6ab5b717a3aef7ed4fc4feba4c03ecd0ad1"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=265db3d8fc6185056225dfa107870445c7b77525"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cb9cc45db808d12ebcaefd8b8393596af58c888"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e740caa70be2a48f54431688e7cf6af10ed32fa"}], "s": {"y": 4000, "x": 3000, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=8efa6d4d44c5062703bbc7179d1ee866daa8abdf"}, "id": "jrj2st4lwv8c1"}, "of1cw07lwv8c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89071e1beb9fa0ea201e237b22c73a30c6349559"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55f0a8642433214e6b7b4d4fe0d388f1036456ec"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f228da5f6814bbdc1c763949372272e95475f54"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e97f016dde285e074b4ce214a39c5d866b66169c"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0101573bdc4a5c107db8a1b5d493dee5d97e3e2c"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a84a2e48fecf8f55470d919198228c61bafa596e"}], "s": {"y": 4000, "x": 3000, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=157efd8d562b625d2e1ee4d1fa5a59857726452e"}, "id": "of1cw07lwv8c1"}}, "name": "t3_18s8334", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 9, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "jrj2st4lwv8c1", "id": 379987854}, {"media_id": "of1cw07lwv8c1", "id": 379987855}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TdAjyYAgtdHM7ggd8sISir77F5RdMUBY2tWyEg9rZQI.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703703288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed that my drive was frequently coming online and offline. I opened my server to insepct it, and found that the sata connection snapped off.  The drive is fully functional if I carefully insert a cable. I wiped the drive already with the intention of tossing it, since it has already been replaced. It just hurts to throw a 14tb drive out while it still functions. Is there anyway I can salvage this or make a repair? It is a Seagate exos x16.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18s8334", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "80TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s8334", "is_robot_indexable": true, "report_reasons": null, "author": "My_Name_Is_Not_Mark", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s8334/any_hope_or_ideas_on_how_to_repair_this_drive_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18s8334", "subreddit_subscribers": 721178, "created_utc": 1703703288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn't maintain timestamps for folders.\n\nIs there an alternate software?\n\n&amp;#x200B;", "author_fullname": "t2_12qmo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GUI software that allows me to click which folders/subfolders/files to copy to destination and preserve time stamps and directory/structure/hierarchy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rrbb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703649937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn&amp;#39;t maintain timestamps for folders.&lt;/p&gt;\n\n&lt;p&gt;Is there an alternate software?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rrbb0", "is_robot_indexable": true, "report_reasons": null, "author": "Snowblind45", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "subreddit_subscribers": 721178, "created_utc": 1703649937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I'm new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?\n\nThanks!  \n", "author_fullname": "t2_5cyhhwlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping DVDs/Blu-rays with Multiple Audio Languages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rp8qf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703643685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I&amp;#39;m new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rp8qf", "is_robot_indexable": true, "report_reasons": null, "author": "JoshALogs", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "subreddit_subscribers": 721178, "created_utc": 1703643685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format *2024-01-16 17-45-03.jpg* (or .mov).  \n\n\n**USING PHOTOSYNC**  \nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos **from** your smartphone **to** your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn't make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.\n\n1. Install [PhotoSync](https://www.photosync-app.com/home) on smartphone  \n2. Install PhotoSyncCompanion on Windows  \n3. Set up app to connect to Windows computer on the same WiFi as smartphone  \n4. Click Recents  \n5. Click Update  \n6. Click New  \n7. Click Computer\n\nIt is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it's an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  \n\n\n**MY PRE-2024 METHOD**  \nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn't see to work with the OneDrive app. I'm not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.\n\n*The naming problem*\n\nPhotos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG\\_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won't work for the MOV files because the iPhone stores the video created date in the meta data field *com.apple.quicktime.creationdate*. Better File Rename 5 can't see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It's possible that another renamer application with a GUI can do it, but I haven't tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful [EXIFtool](https://exiftool.org/).\n\nHere is how to rename iOS-created MOV video files to the date on your Windows 10 computer.\n\n1. Put a copy of the MOV files into a folder in, say, C:\\\\RenamingFolder  \n2. Download ExifTool and place the EXE file in that folder.  \n3. Rename the file to exiftool.exe  \n4. Open Notepad, enter the following code, and save to C:\\\\RenamingFolder with the name Rename file to IPhone video creation date.bat\n\n    exiftool \"-FileName&lt;creationdate\" -d \"%%Y-%%m-%%d %%H-%%M-%%S.%%%%e\" K:\\RenamingFolder\ncmd /k\n\n5.  Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. \n\n**CONCLUSION**\n\nPhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I'll have properly named files using the accurate date and time.\n\nFinally, for the record, I should add that while the format *2024-01-16 17-45-03.jpg* helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the *Text - Add text to end* command from the menus, and type in with a leading space *Mike's 8th birthday.* So the final file name with be *2024-01-16 17-45-03 Mike's 8th birthday.jpg.*", "author_fullname": "t2_7jhiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflow: copy iPhone photos and videos to Windows 10 with filenames using the date the photo or video was really created", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ro2lq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703640324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; (or .mov).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;USING PHOTOSYNC&lt;/strong&gt;&lt;br/&gt;\nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos &lt;strong&gt;from&lt;/strong&gt; your smartphone &lt;strong&gt;to&lt;/strong&gt; your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn&amp;#39;t make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Install &lt;a href=\"https://www.photosync-app.com/home\"&gt;PhotoSync&lt;/a&gt; on smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Install PhotoSyncCompanion on Windows&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Set up app to connect to Windows computer on the same WiFi as smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Recents&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Update&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click New&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Computer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it&amp;#39;s an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MY PRE-2024 METHOD&lt;/strong&gt;&lt;br/&gt;\nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn&amp;#39;t see to work with the OneDrive app. I&amp;#39;m not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;The naming problem&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Photos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won&amp;#39;t work for the MOV files because the iPhone stores the video created date in the meta data field &lt;em&gt;com.apple.quicktime.creationdate&lt;/em&gt;. Better File Rename 5 can&amp;#39;t see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It&amp;#39;s possible that another renamer application with a GUI can do it, but I haven&amp;#39;t tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful &lt;a href=\"https://exiftool.org/\"&gt;EXIFtool&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here is how to rename iOS-created MOV video files to the date on your Windows 10 computer.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Put a copy of the MOV files into a folder in, say, C:\\RenamingFolder&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Download ExifTool and place the EXE file in that folder.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Rename the file to exiftool.exe&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Open Notepad, enter the following code, and save to C:\\RenamingFolder with the name Rename file to IPhone video creation date.bat&lt;/p&gt;\n\n&lt;p&gt;exiftool &amp;quot;-FileName&amp;lt;creationdate&amp;quot; -d &amp;quot;%%Y-%%m-%%d %%H-%%M-%%S.%%%%e&amp;quot; K:\\RenamingFolder\ncmd /k&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;CONCLUSION&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;PhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I&amp;#39;ll have properly named files using the accurate date and time.&lt;/p&gt;\n\n&lt;p&gt;Finally, for the record, I should add that while the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the &lt;em&gt;Text - Add text to end&lt;/em&gt; command from the menus, and type in with a leading space &lt;em&gt;Mike&amp;#39;s 8th birthday.&lt;/em&gt; So the final file name with be &lt;em&gt;2024-01-16 17-45-03 Mike&amp;#39;s 8th birthday.jpg.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ro2lq", "is_robot_indexable": true, "report_reasons": null, "author": "ThumperStrauss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "subreddit_subscribers": 721178, "created_utc": 1703640324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dear hoarders, this post is about Newsgears RSS reader/aggregator, [screenshots are located here](https://imgur.com/a/waj1CO6).\n\nI'm a huge fan of RSS; I think it's a fun and interesting way of exploring the Internet.  I began this project with the goal of simply building the best RSS tools I possibly could using free/open-source frameworks like Vue3 and Vuetify on the front-end, and Spring Boot and ROME on the back-end.  \n\nMain areas of focus: free, open-source, accessible, fully responsive/mobile friendly, secure, modern, all while granting the end-user full and accurate access to every aspect of the syndicated feed payload.\n\nHere are some of the basic features of this finished product: \n\n\\* Topical article queues provide a single view of multiple related feeds\n\n\\* Full-text searching using lunrjs\n\n\\* Multiple layouts (tabular, list, and card)\n\n\\* Light and dark themes\n\n\\* OPML support (import and export)\n\n\\* Fully responsive/usable on very small screens\n\n\\* Accessible and fully keyboard navigable\n\n\\* Integrated media player (vue-plyr)\n\n\\* Available in English, Spanish, and French\n\n\\* Native secure image proxy\n\n\\* Scalable architecture can support thousands of concurrent users\n\n\\*\\*To boot-up:\\*\\* \n\nDownload docker-compose.yml:\n\n    curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/docker-compose.single-user.yml.sample &gt; docker-compose.yml\n\nDownload nginx.conf:\n\n    curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/nginx.conf &gt; nginx.conf\n\nStart docker: \n\n    docker-compose up \n\n[Navigate to localhost port 80](http://localhost).\n\nClick the RSS logo icon in the upper-left to start adding feed subscriptions. \n\n**Github main repository for Newsgears: [https://github.com/lostsidewalk/newsgears-app](https://github.com/lostsidewalk/newsgears-app)**\n\nI'd love to hear from anyone who has an interest in RSS.  This project is still in its nascent stages, and I'm highly motivated to make fixes, implement features, etc.  I'm extremely open to suggestions, and collaborating with other devs of any skill level, on this project. Feel free to drop me a line, even if you think this is POS; I'd love to hear from any users.  \n\nNewsgears RSS should be pretty stable on Linux w/Chrome or FF.  I wouldn't be surprised if some chaos resulted from running this on another platform.  I would very much appreciate any form of feedback, especially bug reports.  \n\n\\*\\*Sister project:\\*\\* \n\nComposableRSS is a REST API-driven platform for composing and publishing syndicated web feed content (such as RSS/ATOM feeds, etc.). Basically, you interact with an internal/developer-friendly REST API to create feeds, add content, and generally control all aspects of the web feed lifecycle. ComposableRSS serves those feeds to your users.  ComposableRSS supports formats beyond RSS and ATOM, making it suitable for use as a general-purpose headless content management system.  \n\n**Github main repository for ComposableRSS: [https://github.com/lostsidewalk/composable-rss-app](https://github.com/lostsidewalk/composable-rss-app)**\n\nI have a Discord to discuss matters in real-time if anyone is interested.  Thanks for reading.", "author_fullname": "t2_ojb0752w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Newsgears RSS self-hosted reader/aggregator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18riu9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703629468.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703626333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear hoarders, this post is about Newsgears RSS reader/aggregator, &lt;a href=\"https://imgur.com/a/waj1CO6\"&gt;screenshots are located here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a huge fan of RSS; I think it&amp;#39;s a fun and interesting way of exploring the Internet.  I began this project with the goal of simply building the best RSS tools I possibly could using free/open-source frameworks like Vue3 and Vuetify on the front-end, and Spring Boot and ROME on the back-end.  &lt;/p&gt;\n\n&lt;p&gt;Main areas of focus: free, open-source, accessible, fully responsive/mobile friendly, secure, modern, all while granting the end-user full and accurate access to every aspect of the syndicated feed payload.&lt;/p&gt;\n\n&lt;p&gt;Here are some of the basic features of this finished product: &lt;/p&gt;\n\n&lt;p&gt;* Topical article queues provide a single view of multiple related feeds&lt;/p&gt;\n\n&lt;p&gt;* Full-text searching using lunrjs&lt;/p&gt;\n\n&lt;p&gt;* Multiple layouts (tabular, list, and card)&lt;/p&gt;\n\n&lt;p&gt;* Light and dark themes&lt;/p&gt;\n\n&lt;p&gt;* OPML support (import and export)&lt;/p&gt;\n\n&lt;p&gt;* Fully responsive/usable on very small screens&lt;/p&gt;\n\n&lt;p&gt;* Accessible and fully keyboard navigable&lt;/p&gt;\n\n&lt;p&gt;* Integrated media player (vue-plyr)&lt;/p&gt;\n\n&lt;p&gt;* Available in English, Spanish, and French&lt;/p&gt;\n\n&lt;p&gt;* Native secure image proxy&lt;/p&gt;\n\n&lt;p&gt;* Scalable architecture can support thousands of concurrent users&lt;/p&gt;\n\n&lt;p&gt;**To boot-up:** &lt;/p&gt;\n\n&lt;p&gt;Download docker-compose.yml:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/docker-compose.single-user.yml.sample &amp;gt; docker-compose.yml\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Download nginx.conf:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;curl -X GET https://raw.githubusercontent.com/lostsidewalk/newsgears-app/main/nginx.conf &amp;gt; nginx.conf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Start docker: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker-compose up \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"http://localhost\"&gt;Navigate to localhost port 80&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Click the RSS logo icon in the upper-left to start adding feed subscriptions. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Github main repository for Newsgears: &lt;a href=\"https://github.com/lostsidewalk/newsgears-app\"&gt;https://github.com/lostsidewalk/newsgears-app&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from anyone who has an interest in RSS.  This project is still in its nascent stages, and I&amp;#39;m highly motivated to make fixes, implement features, etc.  I&amp;#39;m extremely open to suggestions, and collaborating with other devs of any skill level, on this project. Feel free to drop me a line, even if you think this is POS; I&amp;#39;d love to hear from any users.  &lt;/p&gt;\n\n&lt;p&gt;Newsgears RSS should be pretty stable on Linux w/Chrome or FF.  I wouldn&amp;#39;t be surprised if some chaos resulted from running this on another platform.  I would very much appreciate any form of feedback, especially bug reports.  &lt;/p&gt;\n\n&lt;p&gt;**Sister project:** &lt;/p&gt;\n\n&lt;p&gt;ComposableRSS is a REST API-driven platform for composing and publishing syndicated web feed content (such as RSS/ATOM feeds, etc.). Basically, you interact with an internal/developer-friendly REST API to create feeds, add content, and generally control all aspects of the web feed lifecycle. ComposableRSS serves those feeds to your users.  ComposableRSS supports formats beyond RSS and ATOM, making it suitable for use as a general-purpose headless content management system.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Github main repository for ComposableRSS: &lt;a href=\"https://github.com/lostsidewalk/composable-rss-app\"&gt;https://github.com/lostsidewalk/composable-rss-app&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have a Discord to discuss matters in real-time if anyone is interested.  Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?auto=webp&amp;s=65b6c42caa98871c0e7702bf888e21b9fbbbe688", "width": 1774, "height": 1929}, "resolutions": [{"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e528ee1c272443dd3355b63eaa4c9fa56965bfa", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f6d7648284f6d1af7a0e16dd1d089b0bcb5405b", "width": 216, "height": 234}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=760c54796c3fa67b8e1eee91da3af12569828c1d", "width": 320, "height": 347}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90ea9a833dea041d9ffc3290c32cb905e982b209", "width": 640, "height": 695}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b9c2e003ad47ebe5b068c2510b26362db4c42ee", "width": 960, "height": 1043}, {"url": "https://external-preview.redd.it/iAX1gytVg8xseGRIrw75vVMBVLushJ4NA5zb8XC3vtM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d21802bd10f713b6f6a722666ddefae9abeb8da", "width": 1080, "height": 1174}], "variants": {}, "id": "svVu7_zWZstpjaX3XfvSMrfAajp8LwT4qHIbogWRK2k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18riu9v", "is_robot_indexable": true, "report_reasons": null, "author": "harrisofpeoria", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18riu9v/introducing_newsgears_rss_selfhosted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18riu9v/introducing_newsgears_rss_selfhosted/", "subreddit_subscribers": 721178, "created_utc": 1703626333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know how to install LTO-6 LTFS Drivers for ubuntu, I cannot seem to find any that work or the links are dead. I know mine currently finds it fine as this is the response.  \n`sudo mt -f /dev/st0 status`\n\n`drive type = 114`\n\n`drive status = 1509949440`\n\n`sense key error = 0`\n\n`residue count = 0`\n\n`file number = 0`\n\n`block number = 0`\n\nI'm using Fiber connection with an LTO-6 HPE tape", "author_fullname": "t2_6cwo4rts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO-6 LTFS Ubuntu", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rird4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703626118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to install LTO-6 LTFS Drivers for ubuntu, I cannot seem to find any that work or the links are dead. I know mine currently finds it fine as this is the response.&lt;br/&gt;\n&lt;code&gt;sudo mt -f /dev/st0 status&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;drive type = 114&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;drive status = 1509949440&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sense key error = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;residue count = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;file number = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;block number = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Fiber connection with an LTO-6 HPE tape&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rird4", "is_robot_indexable": true, "report_reasons": null, "author": "DrBrad__", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rird4/lto6_ltfs_ubuntu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rird4/lto6_ltfs_ubuntu/", "subreddit_subscribers": 721178, "created_utc": 1703626118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "#TL;DR - Powered PC off + unplugged for 3 days. Turned back on, SSD dead. Software doesn't see it except Hard Disk Sentinel. Windows asks to initialize in Disk Manager. Ubuntu doesn't see it via gparted, dmesg shows it failed via softresets. Replaceable data but would rather somehow copy files off if at all possible one last time.\n\nSo, I went on vacation for 3 days, powering off and unplugging my PC before leaving (including turning off the switch on the PSU). Fast forward to when I return home, plug everything back in, and my PC takes an oddly long time to boot.\n\nTurns out my storage SSD, a Samsung 860 Evo, no longer gets detected in Windows. If I open disk management, [it shows this](https://i.imgur.com/CkNWmGI.png) and asks me to initialize it, something I obviously won't do.\n\nNothing else detects it from what I tried, except Hard Disk Sentinel: somehow, making it view it as an offline disk shows [disk activity at 100%](https://i.imgur.com/DjNPPho.png). Yes, this is live, not a snapshot. The average disk activity percentage kept going up until it reached 100%. Yet it remained \"offline\".\n\nI've also tried using gparted and dmesg Ubuntu to no avail. I live booted into Ubuntu via a USB, with the faulty SSD as the only internal drive in a spare desktop I have lying around. [These](https://i.imgur.com/bvixzlq.jpg) are all instances of \"/sd\" in dmesg. These [softreset](https://i.imgur.com/D4gMNNm.jpg) instances are what I believe are the faulty SSD.\n\nOther things I've tried include testdisk, which did not see the SSD in neither Windows nor Ubuntu, R-Studio Technician Edition which did not see the SSD, Macrium Reflect, and Easeus, neither of which saw the SSD either.\n\nI've also tried using a couple USB HDD docks that I have. I found that plugging it in to a dock does not get detected at all, but after a couple minutes of leaving it on, Windows will make a connection sound as though something's getting plugged in, then explorer will start slowing down every now and then. All the while, Windows will not detect a drive, but will claim there's a 0B storage device plugged into H (the drive's original drive letter).\n\nI read about the power cycling method where you unplug the SATA data cable and just leave the power cable connected, and turning on the PC for 30 mins in the BIOS, turning off for 30 secs, turning on for another 30 mins, turning off for 30 secs again, then finally plugging the SATA cable back and hoping for the best. Sadly, this did nothing. \n\nI then repeated this except left my PC on booted in Windows overnight (6 hours). When I woke up, I powered off, plugged SATA back in, and sadly nothing changed. Drive is still undetectable, and still asks to be initialized in Disk Management.\n\nI've done the same trick once more now that I'm at work, and will check back when I get home (should be on for 9 hours this time). But I don't have high hopes.\n\n#Is there anything else I can try? The data isn't important enough to warrant using an expensive data recovery professional. It's all replaceable, it would just be very bothersome and time consuming to replace it all (bunch of portable apps I've been using since 2015 along with music I've been meaning to tag and sync to my library).\n\nI thankfully know exactly what was on it thanks to VoidTools' Search Everything keeping track of what files were last on the drive, so I know what I'm missing. I just really hope there's something I can do to get one last breath of life out of this drive so I can simply copy everything off and save myself some time.\n\nWhat's really annoying is I was planning on making a backup system via Macrium (which I have been using for years) to make backups of all my SSDs, but hadn't gotten around to it yet. If I did, I wouldn't be trying so hard to recover this drive. \n\nP.S. This is my third storage failure this year, all of which are samsung devices (two 512GB microSDs of the same model, and now this 860 Evo SSD). It's bad enough that Samsung has a non-existent warranty in Canada where I can't even get a replacement, as though the higher price of their products wasn't already bad enough.", "author_fullname": "t2_ja9d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 860 Evo 500GB SSD Died - Any recourse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s5x5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703699241.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703697861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;TL;DR - Powered PC off + unplugged for 3 days. Turned back on, SSD dead. Software doesn&amp;#39;t see it except Hard Disk Sentinel. Windows asks to initialize in Disk Manager. Ubuntu doesn&amp;#39;t see it via gparted, dmesg shows it failed via softresets. Replaceable data but would rather somehow copy files off if at all possible one last time.&lt;/h1&gt;\n\n&lt;p&gt;So, I went on vacation for 3 days, powering off and unplugging my PC before leaving (including turning off the switch on the PSU). Fast forward to when I return home, plug everything back in, and my PC takes an oddly long time to boot.&lt;/p&gt;\n\n&lt;p&gt;Turns out my storage SSD, a Samsung 860 Evo, no longer gets detected in Windows. If I open disk management, &lt;a href=\"https://i.imgur.com/CkNWmGI.png\"&gt;it shows this&lt;/a&gt; and asks me to initialize it, something I obviously won&amp;#39;t do.&lt;/p&gt;\n\n&lt;p&gt;Nothing else detects it from what I tried, except Hard Disk Sentinel: somehow, making it view it as an offline disk shows &lt;a href=\"https://i.imgur.com/DjNPPho.png\"&gt;disk activity at 100%&lt;/a&gt;. Yes, this is live, not a snapshot. The average disk activity percentage kept going up until it reached 100%. Yet it remained &amp;quot;offline&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried using gparted and dmesg Ubuntu to no avail. I live booted into Ubuntu via a USB, with the faulty SSD as the only internal drive in a spare desktop I have lying around. &lt;a href=\"https://i.imgur.com/bvixzlq.jpg\"&gt;These&lt;/a&gt; are all instances of &amp;quot;/sd&amp;quot; in dmesg. These &lt;a href=\"https://i.imgur.com/D4gMNNm.jpg\"&gt;softreset&lt;/a&gt; instances are what I believe are the faulty SSD.&lt;/p&gt;\n\n&lt;p&gt;Other things I&amp;#39;ve tried include testdisk, which did not see the SSD in neither Windows nor Ubuntu, R-Studio Technician Edition which did not see the SSD, Macrium Reflect, and Easeus, neither of which saw the SSD either.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried using a couple USB HDD docks that I have. I found that plugging it in to a dock does not get detected at all, but after a couple minutes of leaving it on, Windows will make a connection sound as though something&amp;#39;s getting plugged in, then explorer will start slowing down every now and then. All the while, Windows will not detect a drive, but will claim there&amp;#39;s a 0B storage device plugged into H (the drive&amp;#39;s original drive letter).&lt;/p&gt;\n\n&lt;p&gt;I read about the power cycling method where you unplug the SATA data cable and just leave the power cable connected, and turning on the PC for 30 mins in the BIOS, turning off for 30 secs, turning on for another 30 mins, turning off for 30 secs again, then finally plugging the SATA cable back and hoping for the best. Sadly, this did nothing. &lt;/p&gt;\n\n&lt;p&gt;I then repeated this except left my PC on booted in Windows overnight (6 hours). When I woke up, I powered off, plugged SATA back in, and sadly nothing changed. Drive is still undetectable, and still asks to be initialized in Disk Management.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done the same trick once more now that I&amp;#39;m at work, and will check back when I get home (should be on for 9 hours this time). But I don&amp;#39;t have high hopes.&lt;/p&gt;\n\n&lt;h1&gt;Is there anything else I can try? The data isn&amp;#39;t important enough to warrant using an expensive data recovery professional. It&amp;#39;s all replaceable, it would just be very bothersome and time consuming to replace it all (bunch of portable apps I&amp;#39;ve been using since 2015 along with music I&amp;#39;ve been meaning to tag and sync to my library).&lt;/h1&gt;\n\n&lt;p&gt;I thankfully know exactly what was on it thanks to VoidTools&amp;#39; Search Everything keeping track of what files were last on the drive, so I know what I&amp;#39;m missing. I just really hope there&amp;#39;s something I can do to get one last breath of life out of this drive so I can simply copy everything off and save myself some time.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s really annoying is I was planning on making a backup system via Macrium (which I have been using for years) to make backups of all my SSDs, but hadn&amp;#39;t gotten around to it yet. If I did, I wouldn&amp;#39;t be trying so hard to recover this drive. &lt;/p&gt;\n\n&lt;p&gt;P.S. This is my third storage failure this year, all of which are samsung devices (two 512GB microSDs of the same model, and now this 860 Evo SSD). It&amp;#39;s bad enough that Samsung has a non-existent warranty in Canada where I can&amp;#39;t even get a replacement, as though the higher price of their products wasn&amp;#39;t already bad enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?auto=webp&amp;s=4c16fc4536173d43a5b3d697e42680a256c6c0c5", "width": 754, "height": 597}, "resolutions": [{"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1018555122c35a70dd7b63733295795d0f2e846", "width": 108, "height": 85}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4a545d51f6daa31d7212a758800bcbe632caf5a", "width": 216, "height": 171}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4d56152c5816c210339b47afe7e94e7945b19fd", "width": 320, "height": 253}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b81481999de57c73482541399940a4641125fec1", "width": 640, "height": 506}], "variants": {}, "id": "pMkX3NtqxIdMXFFMCHA53rmqZqUEQjVdDrXKQgk5J4E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "111TB Externals", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s5x5t", "is_robot_indexable": true, "report_reasons": null, "author": "sonicrings4", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s5x5t/samsung_860_evo_500gb_ssd_died_any_recourse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s5x5t/samsung_860_evo_500gb_ssd_died_any_recourse/", "subreddit_subscribers": 721178, "created_utc": 1703697861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.", "author_fullname": "t2_2kooznfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to capture audio from an 8-track tape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rmt6e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703636740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rmt6e", "is_robot_indexable": true, "report_reasons": null, "author": "dstillloading", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "subreddit_subscribers": 721178, "created_utc": 1703636740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.\n\nNowadays, I have to postprocess my scans in the following way:\n\n1. rotate and crop them so there are not white boundaries\n2. Remove some scratches and dust that was still on the scans\n3. Improve picture quality by tweaking contrast, saturation, etc.\n4. export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)\n\nI do these manually on Affinity photo now, but I'm wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.\n\nAny suggestions?", "author_fullname": "t2_bv035", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automated tool for postprocessing scanned photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rksvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.&lt;/p&gt;\n\n&lt;p&gt;Nowadays, I have to postprocess my scans in the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;rotate and crop them so there are not white boundaries&lt;/li&gt;\n&lt;li&gt;Remove some scratches and dust that was still on the scans&lt;/li&gt;\n&lt;li&gt;Improve picture quality by tweaking contrast, saturation, etc.&lt;/li&gt;\n&lt;li&gt;export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I do these manually on Affinity photo now, but I&amp;#39;m wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "170TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rksvw", "is_robot_indexable": true, "report_reasons": null, "author": "Mathy963", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "subreddit_subscribers": 721178, "created_utc": 1703631429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, am I completely off base here?\n\n I'll ask this group because it's data recovery related, but any Linux Admin's out there please chime in as well (and I'll ask other subs for that demographic as well, yes). \n\nI have 6 4TB HDD's in a machine that is failing (mobo/cpu). I have a need to get the data off of these drives and my available resources are a rack mounted server w/ 60 free TB's of space but no room for additional drives, and a second tower machine that does NOT have room for all of the drives or power for more than one or 2 of them. \n\nI tried, putting a 6 port SATA card in tower 2, using tower 1's PSU to power the drives while I boot tower 2 into linux and reassemble and read the array. The issue is that tower 1's CPU issues just make the whole thing reboot after about 20 minutes or so, even causing the PSU to reboot. Disconnecting the mobo in Tower 1 won't let the PSU actually power up for some reason. So that's not going anywhere.\n\nSince I don't have time, or funds right now honestly to order a new mobo/cpu for Tower 1, my current plan (plan F) is to remove one drive at a time, put it into Tower 2, and use something to create an ISO (or something else I might not be aware of) out of each drive into the rack mount NAS... then use the NAS to rebuild the array out of those ISO images and mount it into itself... or something. \n\nAm I smoking dreams and dropped packets here? Or is anything like this even possible? Wife's photography archives are trapped on these 6 disks - which have not had any issues - and we need to get the data off, onto the new NAS, and readable so we can continue working with it. \n\nIs there anyway to hotwire a PSU to just power drives without being hooked to a living MOBO? \n\nIs there anyway to ISO a MDADM drive, and then use mulitple ISO's to re-create the array on another machine? \n\nIs buying hardware and replacing it the only, or far safer (yet less adventurous!), way to do this? The machines are all living at a friends house (my TX 'datacenter') so I only have remote access when I'm in town which is for a few hours, while I'm in town... making ordering hardware, installing, and doing all of this impractical over the next few months.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_nq184", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux MDADM ... from ISO files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18s9867", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question / Sanity-check", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703706149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, am I completely off base here?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll ask this group because it&amp;#39;s data recovery related, but any Linux Admin&amp;#39;s out there please chime in as well (and I&amp;#39;ll ask other subs for that demographic as well, yes). &lt;/p&gt;\n\n&lt;p&gt;I have 6 4TB HDD&amp;#39;s in a machine that is failing (mobo/cpu). I have a need to get the data off of these drives and my available resources are a rack mounted server w/ 60 free TB&amp;#39;s of space but no room for additional drives, and a second tower machine that does NOT have room for all of the drives or power for more than one or 2 of them. &lt;/p&gt;\n\n&lt;p&gt;I tried, putting a 6 port SATA card in tower 2, using tower 1&amp;#39;s PSU to power the drives while I boot tower 2 into linux and reassemble and read the array. The issue is that tower 1&amp;#39;s CPU issues just make the whole thing reboot after about 20 minutes or so, even causing the PSU to reboot. Disconnecting the mobo in Tower 1 won&amp;#39;t let the PSU actually power up for some reason. So that&amp;#39;s not going anywhere.&lt;/p&gt;\n\n&lt;p&gt;Since I don&amp;#39;t have time, or funds right now honestly to order a new mobo/cpu for Tower 1, my current plan (plan F) is to remove one drive at a time, put it into Tower 2, and use something to create an ISO (or something else I might not be aware of) out of each drive into the rack mount NAS... then use the NAS to rebuild the array out of those ISO images and mount it into itself... or something. &lt;/p&gt;\n\n&lt;p&gt;Am I smoking dreams and dropped packets here? Or is anything like this even possible? Wife&amp;#39;s photography archives are trapped on these 6 disks - which have not had any issues - and we need to get the data off, onto the new NAS, and readable so we can continue working with it. &lt;/p&gt;\n\n&lt;p&gt;Is there anyway to hotwire a PSU to just power drives without being hooked to a living MOBO? &lt;/p&gt;\n\n&lt;p&gt;Is there anyway to ISO a MDADM drive, and then use mulitple ISO&amp;#39;s to re-create the array on another machine? &lt;/p&gt;\n\n&lt;p&gt;Is buying hardware and replacing it the only, or far safer (yet less adventurous!), way to do this? The machines are all living at a friends house (my TX &amp;#39;datacenter&amp;#39;) so I only have remote access when I&amp;#39;m in town which is for a few hours, while I&amp;#39;m in town... making ordering hardware, installing, and doing all of this impractical over the next few months.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18s9867", "is_robot_indexable": true, "report_reasons": null, "author": "546875674c6966650d0a", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s9867/linux_mdadm_from_iso_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s9867/linux_mdadm_from_iso_files/", "subreddit_subscribers": 721178, "created_utc": 1703706149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, as I was doing some research on making backups of my photo files, I came across sometime called cold storage. I currently run a Mac Mini system, I have Time Machine turned on, and mostly I'm concerned about keeping my nearly 4TB of photos preserved. With cost also being my factor. Would it be reasonable for me to get a 4TB hard drive, make a copy of all my photos, then store the drive away? I don't plan on keeping it hooked up to the Mac and keep it running. I also read about the format of the backup drive... running a Mac, does it matter what format I pick for the back up drive? APFS? Ex-FAT? And when I make the back up... what's the best way to move a large amount of files safely? Some mentioned copy but not move? And when people suggest I refresh the backup drive every few months, does that mean to re-copy everything over from the working drive again? Thanks in advance.", "author_fullname": "t2_16pgzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just came across the concept of cold storage...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18s8zp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703705542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, as I was doing some research on making backups of my photo files, I came across sometime called cold storage. I currently run a Mac Mini system, I have Time Machine turned on, and mostly I&amp;#39;m concerned about keeping my nearly 4TB of photos preserved. With cost also being my factor. Would it be reasonable for me to get a 4TB hard drive, make a copy of all my photos, then store the drive away? I don&amp;#39;t plan on keeping it hooked up to the Mac and keep it running. I also read about the format of the backup drive... running a Mac, does it matter what format I pick for the back up drive? APFS? Ex-FAT? And when I make the back up... what&amp;#39;s the best way to move a large amount of files safely? Some mentioned copy but not move? And when people suggest I refresh the backup drive every few months, does that mean to re-copy everything over from the working drive again? Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s8zp9", "is_robot_indexable": true, "report_reasons": null, "author": "MonkeyRPN", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s8zp9/just_came_across_the_concept_of_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s8zp9/just_came_across_the_concept_of_cold_storage/", "subreddit_subscribers": 721178, "created_utc": 1703705542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to back-up around 100gb of photos and videos to an HDD. Is there a way to back-up the Samsung gallery directly, so that all my folders are kept?\nAlso, do I need any adapters or can I just plug in an HDD with usb c directly to my phone?\n\nHow long does it usually take to transfer 100gb of data to an average HDD?\n\nI was also contemplating getting an SSD, but I'm not sure I need the faster read speeds, because I would plug the HDD to my phone maybe 2 or 3 times a year to look at/show some photos.\n\nLastly, are there any HDD's that you guys can recommend specifically for my needs?", "author_fullname": "t2_54itytzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photos and videos from Samsung Galaxy S20 to HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18s7qk2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703702415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to back-up around 100gb of photos and videos to an HDD. Is there a way to back-up the Samsung gallery directly, so that all my folders are kept?\nAlso, do I need any adapters or can I just plug in an HDD with usb c directly to my phone?&lt;/p&gt;\n\n&lt;p&gt;How long does it usually take to transfer 100gb of data to an average HDD?&lt;/p&gt;\n\n&lt;p&gt;I was also contemplating getting an SSD, but I&amp;#39;m not sure I need the faster read speeds, because I would plug the HDD to my phone maybe 2 or 3 times a year to look at/show some photos.&lt;/p&gt;\n\n&lt;p&gt;Lastly, are there any HDD&amp;#39;s that you guys can recommend specifically for my needs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s7qk2", "is_robot_indexable": true, "report_reasons": null, "author": "bubble121212", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s7qk2/photos_and_videos_from_samsung_galaxy_s20_to_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s7qk2/photos_and_videos_from_samsung_galaxy_s20_to_hdd/", "subreddit_subscribers": 721178, "created_utc": 1703702415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, I've been browsing for online storage, and iDrive came up with an offer for a year.\n\nI currently run linux mint, and wondered whether I can use rclone or duplicity with it? I asked help chat, the person didn't seem to know and mentioned a script. I'll be happy to stop the service once the year ends", "author_fullname": "t2_6gc7t1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iDrive 5TB - $9.95 (1st year)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s5vif", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703697743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;ve been browsing for online storage, and iDrive came up with an offer for a year.&lt;/p&gt;\n\n&lt;p&gt;I currently run linux mint, and wondered whether I can use rclone or duplicity with it? I asked help chat, the person didn&amp;#39;t seem to know and mentioned a script. I&amp;#39;ll be happy to stop the service once the year ends&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s5vif", "is_robot_indexable": true, "report_reasons": null, "author": "PiratesOfTheArctic", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s5vif/idrive_5tb_995_1st_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s5vif/idrive_5tb_995_1st_year/", "subreddit_subscribers": 721178, "created_utc": 1703697743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apologies if this has been asked, but I've searched and have come up with nothing. I've exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I'd like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?\n\nI figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.", "author_fullname": "t2_479u9c8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tumblr Export - Recreate Blog from the Index HTML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s4swq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703695061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this has been asked, but I&amp;#39;ve searched and have come up with nothing. I&amp;#39;ve exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I&amp;#39;d like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?&lt;/p&gt;\n\n&lt;p&gt;I figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s4swq", "is_robot_indexable": true, "report_reasons": null, "author": "ggodfrey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "subreddit_subscribers": 721178, "created_utc": 1703695061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a new WD120EFBX (12tb WD Red Plus)\n\nWhen I got the drive, I did these steps:\n\n1. initialized it in Disk Management (GPT)\n- quick format I believe\n- ran extended SMART test on HDSentinel (perfect)\n- ran write/read surface test on HDSentinel (perfect)\n- bitlockered (full, not quick, encryption)\n- transferred all my data\n\nAt this point, everything looked perfect.\n\n**Rebooted. Now it's not showing up in Windows.**\n\nOpened disk management and it's giving me the \"Initialize Disk - You must initialize a disk before Logical Disk Manager can access it\" popup.\n\nDisk Management can see the \"unknown\" drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there's no problems with the hard drive.\n\nTried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.\n\nThe only possible thing I can see is in the drive properties Events tab, there's a \"Device not migrated event\".\n\nAny ideas? I appreciate any help you can give me.\n\nEdit: Tried a different windows machine and it was still unallocated. I've decided to just re-initialize and do the whole thing over again. If it fails again I'll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.", "author_fullname": "t2_58sktp70", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help - Perfect healthy new hard drive uninitialized/unallocated after reboot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s2b0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703693844.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703688396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a new WD120EFBX (12tb WD Red Plus)&lt;/p&gt;\n\n&lt;p&gt;When I got the drive, I did these steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;initialized it in Disk Management (GPT)&lt;/li&gt;\n&lt;li&gt;quick format I believe&lt;/li&gt;\n&lt;li&gt;ran extended SMART test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;ran write/read surface test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;bitlockered (full, not quick, encryption)&lt;/li&gt;\n&lt;li&gt;transferred all my data&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;At this point, everything looked perfect.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Rebooted. Now it&amp;#39;s not showing up in Windows.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Opened disk management and it&amp;#39;s giving me the &amp;quot;Initialize Disk - You must initialize a disk before Logical Disk Manager can access it&amp;quot; popup.&lt;/p&gt;\n\n&lt;p&gt;Disk Management can see the &amp;quot;unknown&amp;quot; drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there&amp;#39;s no problems with the hard drive.&lt;/p&gt;\n\n&lt;p&gt;Tried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.&lt;/p&gt;\n\n&lt;p&gt;The only possible thing I can see is in the drive properties Events tab, there&amp;#39;s a &amp;quot;Device not migrated event&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? I appreciate any help you can give me.&lt;/p&gt;\n\n&lt;p&gt;Edit: Tried a different windows machine and it was still unallocated. I&amp;#39;ve decided to just re-initialize and do the whole thing over again. If it fails again I&amp;#39;ll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s2b0v", "is_robot_indexable": true, "report_reasons": null, "author": "SpareMenu5", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "subreddit_subscribers": 721178, "created_utc": 1703688396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, are there any third-party downloaders for Windows to download GDrive files (each file is 80+GB .) to my PC? The default client is awful. I need functions like auto pause / resume and verifying every byte since most are encrypted zips.", "author_fullname": "t2_wdpu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MEGA like downloader for GDrive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rzaln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703678940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, are there any third-party downloaders for Windows to download GDrive files (each file is 80+GB .) to my PC? The default client is awful. I need functions like auto pause / resume and verifying every byte since most are encrypted zips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rzaln", "is_robot_indexable": true, "report_reasons": null, "author": "sdw23", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rzaln/mega_like_downloader_for_gdrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rzaln/mega_like_downloader_for_gdrive/", "subreddit_subscribers": 721178, "created_utc": 1703678940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**\\[TL;DR at the bottom\\]**\n\nHi, I'm not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.\n\nI'm currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I'm starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. \n\nTaking a look at the market pricing where I live, I came up with two options:\n\n1. 3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&gt; 8TB effective space, \\~57 USD/eff. TB\n2. 3x 18TB drives mirroring each other, thus 2 \"parity\" disks, at least one of which would be offsite -&gt; 18TB effective space, \\~50 USD/eff. TB\n\nMy goal is to build a robust solution, that I can use for many years. While I won't need more than 8TB in the near future, I wouldn't mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.\n\nOther aspects I considered include:\n\n* Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.   \nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?  \nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let's say 16TB) drive that completely replaces the 8TB one as the offsite drive?\n* Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.   \nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.\n* Mirroring: both options require some sort of mirroring. For this, I'm currently using freefilesync. What do you think about this approach?  I'm comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.\n* Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.   \nI'm especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? \n\nThis is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.\n\nTL;DR:   \n\\- RAID with 1 parity + single disk offsite backup with no parity, or  \n\\- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option", "author_fullname": "t2_73ebnj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Choosing a Storage Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rxnid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703672612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;[TL;DR at the bottom]&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I&amp;#39;m starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. &lt;/p&gt;\n\n&lt;p&gt;Taking a look at the market pricing where I live, I came up with two options:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&amp;gt; 8TB effective space, ~57 USD/eff. TB&lt;/li&gt;\n&lt;li&gt;3x 18TB drives mirroring each other, thus 2 &amp;quot;parity&amp;quot; disks, at least one of which would be offsite -&amp;gt; 18TB effective space, ~50 USD/eff. TB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My goal is to build a robust solution, that I can use for many years. While I won&amp;#39;t need more than 8TB in the near future, I wouldn&amp;#39;t mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.&lt;/p&gt;\n\n&lt;p&gt;Other aspects I considered include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.&lt;br/&gt;\nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?&lt;br/&gt;\nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let&amp;#39;s say 16TB) drive that completely replaces the 8TB one as the offsite drive?&lt;/li&gt;\n&lt;li&gt;Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.&lt;br/&gt;\nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.&lt;/li&gt;\n&lt;li&gt;Mirroring: both options require some sort of mirroring. For this, I&amp;#39;m currently using freefilesync. What do you think about this approach?  I&amp;#39;m comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.&lt;/li&gt;\n&lt;li&gt;Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.&lt;br/&gt;\nI&amp;#39;m especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;br/&gt;\n- RAID with 1 parity + single disk offsite backup with no parity, or&lt;br/&gt;\n- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rxnid", "is_robot_indexable": true, "report_reasons": null, "author": "Zorinhou", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "subreddit_subscribers": 721178, "created_utc": 1703672612.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}