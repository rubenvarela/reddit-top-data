{"kind": "Listing", "data": {"after": "t3_18rs8qm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ck2fr5mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Copyright Hinders The Preservation Of Modern, Digital Culture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18rlias", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 193, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 193, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fTybdFZPJcL8xPvqXL1ixSe2-v3FKnp-LJKodGe_Tjo.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703633257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techdirt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?auto=webp&amp;s=03a5fde066455710ac1676ca5e55fdf4cff5f177", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd544711a911105c2c84e8778e42492e6627f7ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=581443ef43c416964d464ddd6b5b28eec7b2b77c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=98eca8135ccdbe0daad2aad9f2f21c250d1564ab", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60f612dc25b6693520e6d4a342645939506f3afd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=30ce1c1c245620a8b70a4d0a298b39657711ef0e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/wKm0XDzoASM6BrarQXoG18WnZ5CIUPM3ZBe5zppuyFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b24b6b5a4ae88248a5d6b951b789da794832c41", "width": 1080, "height": 567}], "variants": {}, "id": "86PGtE2qmX3coS9Htmb8TUfXMSg2HaYO4Rk8A0YbGow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rlias", "is_robot_indexable": true, "report_reasons": null, "author": "AbolishDisney", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rlias/how_copyright_hinders_the_preservation_of_modern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.techdirt.com/2023/12/26/how-copyright-hinders-the-preservation-of-modern-digital-culture/", "subreddit_subscribers": 721205, "created_utc": 1703633257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all. I'm new to the DataHoarder community, but not new to having numerous hard drives in my desk. \n\nTLDR: Sent in my 4TB External HDD for recovery because I deleted files to save space. Deleted the corrupted or broken recovered files, organized the remaining good files, de-duplicated them, moved them to my main 8TB HDD, formatted the 8TB transfer drive, then moved all the files back onto the freshly formatted 8TB transfer drive.\n\n&amp;#x200B;\n\nHere's the full story:\n\nBack in October 2020, I decided to delete a bunch of videos off my measly 4TB Seagate External HDD since it was getting full. Later in February 2021, I got an 8TB External HDD that I still use to this day for videography and other purposes that I use daily. I moved all my data from my 4TB HDD to my 8TB HDD, basically leaving the 4TB completely empty. However (and luckily), I did not use it at all since that day.\n\nJust this past November, I saved up enough to hire [$300 Dollar Data Recovery](https://www.300dollardatarecovery.com/) to recover ALL of the data on the 4TB, specifically the videos that I deleted way back in 2020. I filled out their forms, and sent another 8TB Internal HDD I had, and the 4TB from before.\n\nWhen they shipped back both drives, I was surprised to find that there was a little over 5TB worth of data, all coming from a drive that has 3.63TB of storage. Everything from videos to really small files 1KB or less files, totaling about a million files and several tens of thousands of folders.\n\nThe FIRST thing I had to do was painstakingly sort out all the corrupted or \"broken\" files that weren't playable or were distorted beyond recognition. So I had to carefully plan and make the process as efficient and fast as possible.\n\nFor videos and photo, I opened up File Explorer in full screen and put it in TILE view. I used Ctrl + Scroll and zoomed out as far as I could, just before going into a different view, to have as many icons as possible on my screen.\n\nI also sorted the files by size, because most of the corrupt files were either \\~1KB or gigantic (400+ MB for a corrupted photo). Doing this would group all the small and large files together, making it so I could simply drag around the blue box and select a bunch of them at once, and unselect the few good ones from my selection while holding down Ctrl.\n\nI would let File Explorer load all the thumbnails for the photos/videos, and whatever didn't have a thumbnail I simply selected and moved to a folder called \"Broken Files\", since opening them would simply give me a corrupted file error 99% of the time. Then I would simply scroll, wait a couple seconds for File Explorer to load the thumbnails, quickly scan for any files missing a thumbnail, then slowly scroll again.\n\nFor MP3 or other audio files, since all of them combined was only about \\~100GB, I decided it wasn't worth my time to comb through lots of more files to free up less space. For HTML, SVG, WAV, RTF, etc... all of them combined was only about \\~40GB, with over 200k files, so again, it wasn't worth my time trying to free up such little space with so many small files to comb through.\n\nAfter weeding out the corrupted or broken files, I made folders that were organized by year (2017, 2018, 2019, etc etc). I sorted the video/photos by their dates, and moved them into their respective year folders. I did this by again, having File Explorer in fullscreen, then putting it into DETAIL view (so I could see the date), sorted by date, then dragged the mouse all the way down, letting it autoscroll down until I've selected all the files for that year. \n\nFor the other files (HTML, WAV, RTF, etc), I again used File Explorer in full screen, but this time put it in LIST view instead of tile view (although you could use small icon view), and then sorted by file type. I then did the same for above, but instead of dates, it was file type (duh). I then put all the selected files into their respective folders (HTML, WAV, etc).\n\nAt this point in the recovery (or organization, really), I had a bunch of folders ranging from 2015 to 2021, and a few other folders for different file types. I put all of these folders in one big giant folder called \"good files\", then used DupeGuru to remove any duplicates from that folder. I set my 8TB HDD as a reference, and let DupeGuru sort through 300k+ files overnight.\n\nThe next morning I came back and saw a lot of duplicates, all with a 100% match. I made sure to save the results on my computer just in case. I went ahead and sorted by file size, and saw that the video files and all others were all matching up by year, file size, and even filename. The small \\~10kB files I knew luckily weren't that important to me, so I just let it slide, since they were probably some program files or PNG's that aren't family pictures or selfies.\n\nI moved all the duplicate files into another folder called \"good duplicates\". I then moved all of the \"good files\" to my 8TB HDD, totaling 1.5TB being moved via File Explorer. \n\nI then ran DupeGuru once more for good measure, setting the \"good duplicates\" folder against my 8TB HDD and the \"good files\". I saw one photo slide, and \\~200 small files like RTF, HTML, etc, after moving the duplicates to yet another folder called \"good duplicates 2\". I then moved the remaining non-duplicates to my 8TB HDD, and called it a day.\n\nI then went ahead and formatted the 8TB TRANSFER drive (not my 8TB HDD mentioned previously of course!) to get rid of any pesky files or anything that could interfere with storing data. Before I did any deletion or formatting, I made sure that the only files that were on the transfer drive were either broken files, or duplicates, which I confirmed they were.\n\n&amp;#x200B;\n\nThe data recovery itself took a little over a week from shipping to getting it back in the mail. The \n\nI formatted the 8TB transfer drive yesterday, and starting moving all the files I wish to move from my 8TB HDD to the 8TB transfer drive. It's about 5.3TB worth of data being moved over to my transfer drive, which will now be used as extra storage. As I'm typing this story down this morning, my data is still being transferred over, and has been doing so since last night, so I will give it probably a full 24 hours or so to fully transfer 5.3TB worth of data.", "author_fullname": "t2_n10mx4oww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got done recovering ~5TB worth of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s41pk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703693080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all. I&amp;#39;m new to the DataHoarder community, but not new to having numerous hard drives in my desk. &lt;/p&gt;\n\n&lt;p&gt;TLDR: Sent in my 4TB External HDD for recovery because I deleted files to save space. Deleted the corrupted or broken recovered files, organized the remaining good files, de-duplicated them, moved them to my main 8TB HDD, formatted the 8TB transfer drive, then moved all the files back onto the freshly formatted 8TB transfer drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the full story:&lt;/p&gt;\n\n&lt;p&gt;Back in October 2020, I decided to delete a bunch of videos off my measly 4TB Seagate External HDD since it was getting full. Later in February 2021, I got an 8TB External HDD that I still use to this day for videography and other purposes that I use daily. I moved all my data from my 4TB HDD to my 8TB HDD, basically leaving the 4TB completely empty. However (and luckily), I did not use it at all since that day.&lt;/p&gt;\n\n&lt;p&gt;Just this past November, I saved up enough to hire &lt;a href=\"https://www.300dollardatarecovery.com/\"&gt;$300 Dollar Data Recovery&lt;/a&gt; to recover ALL of the data on the 4TB, specifically the videos that I deleted way back in 2020. I filled out their forms, and sent another 8TB Internal HDD I had, and the 4TB from before.&lt;/p&gt;\n\n&lt;p&gt;When they shipped back both drives, I was surprised to find that there was a little over 5TB worth of data, all coming from a drive that has 3.63TB of storage. Everything from videos to really small files 1KB or less files, totaling about a million files and several tens of thousands of folders.&lt;/p&gt;\n\n&lt;p&gt;The FIRST thing I had to do was painstakingly sort out all the corrupted or &amp;quot;broken&amp;quot; files that weren&amp;#39;t playable or were distorted beyond recognition. So I had to carefully plan and make the process as efficient and fast as possible.&lt;/p&gt;\n\n&lt;p&gt;For videos and photo, I opened up File Explorer in full screen and put it in TILE view. I used Ctrl + Scroll and zoomed out as far as I could, just before going into a different view, to have as many icons as possible on my screen.&lt;/p&gt;\n\n&lt;p&gt;I also sorted the files by size, because most of the corrupt files were either ~1KB or gigantic (400+ MB for a corrupted photo). Doing this would group all the small and large files together, making it so I could simply drag around the blue box and select a bunch of them at once, and unselect the few good ones from my selection while holding down Ctrl.&lt;/p&gt;\n\n&lt;p&gt;I would let File Explorer load all the thumbnails for the photos/videos, and whatever didn&amp;#39;t have a thumbnail I simply selected and moved to a folder called &amp;quot;Broken Files&amp;quot;, since opening them would simply give me a corrupted file error 99% of the time. Then I would simply scroll, wait a couple seconds for File Explorer to load the thumbnails, quickly scan for any files missing a thumbnail, then slowly scroll again.&lt;/p&gt;\n\n&lt;p&gt;For MP3 or other audio files, since all of them combined was only about ~100GB, I decided it wasn&amp;#39;t worth my time to comb through lots of more files to free up less space. For HTML, SVG, WAV, RTF, etc... all of them combined was only about ~40GB, with over 200k files, so again, it wasn&amp;#39;t worth my time trying to free up such little space with so many small files to comb through.&lt;/p&gt;\n\n&lt;p&gt;After weeding out the corrupted or broken files, I made folders that were organized by year (2017, 2018, 2019, etc etc). I sorted the video/photos by their dates, and moved them into their respective year folders. I did this by again, having File Explorer in fullscreen, then putting it into DETAIL view (so I could see the date), sorted by date, then dragged the mouse all the way down, letting it autoscroll down until I&amp;#39;ve selected all the files for that year. &lt;/p&gt;\n\n&lt;p&gt;For the other files (HTML, WAV, RTF, etc), I again used File Explorer in full screen, but this time put it in LIST view instead of tile view (although you could use small icon view), and then sorted by file type. I then did the same for above, but instead of dates, it was file type (duh). I then put all the selected files into their respective folders (HTML, WAV, etc).&lt;/p&gt;\n\n&lt;p&gt;At this point in the recovery (or organization, really), I had a bunch of folders ranging from 2015 to 2021, and a few other folders for different file types. I put all of these folders in one big giant folder called &amp;quot;good files&amp;quot;, then used DupeGuru to remove any duplicates from that folder. I set my 8TB HDD as a reference, and let DupeGuru sort through 300k+ files overnight.&lt;/p&gt;\n\n&lt;p&gt;The next morning I came back and saw a lot of duplicates, all with a 100% match. I made sure to save the results on my computer just in case. I went ahead and sorted by file size, and saw that the video files and all others were all matching up by year, file size, and even filename. The small ~10kB files I knew luckily weren&amp;#39;t that important to me, so I just let it slide, since they were probably some program files or PNG&amp;#39;s that aren&amp;#39;t family pictures or selfies.&lt;/p&gt;\n\n&lt;p&gt;I moved all the duplicate files into another folder called &amp;quot;good duplicates&amp;quot;. I then moved all of the &amp;quot;good files&amp;quot; to my 8TB HDD, totaling 1.5TB being moved via File Explorer. &lt;/p&gt;\n\n&lt;p&gt;I then ran DupeGuru once more for good measure, setting the &amp;quot;good duplicates&amp;quot; folder against my 8TB HDD and the &amp;quot;good files&amp;quot;. I saw one photo slide, and ~200 small files like RTF, HTML, etc, after moving the duplicates to yet another folder called &amp;quot;good duplicates 2&amp;quot;. I then moved the remaining non-duplicates to my 8TB HDD, and called it a day.&lt;/p&gt;\n\n&lt;p&gt;I then went ahead and formatted the 8TB TRANSFER drive (not my 8TB HDD mentioned previously of course!) to get rid of any pesky files or anything that could interfere with storing data. Before I did any deletion or formatting, I made sure that the only files that were on the transfer drive were either broken files, or duplicates, which I confirmed they were.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The data recovery itself took a little over a week from shipping to getting it back in the mail. The &lt;/p&gt;\n\n&lt;p&gt;I formatted the 8TB transfer drive yesterday, and starting moving all the files I wish to move from my 8TB HDD to the 8TB transfer drive. It&amp;#39;s about 5.3TB worth of data being moved over to my transfer drive, which will now be used as extra storage. As I&amp;#39;m typing this story down this morning, my data is still being transferred over, and has been doing so since last night, so I will give it probably a full 24 hours or so to fully transfer 5.3TB worth of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?auto=webp&amp;s=b60e1e5ce678ffb4aedb0243d62264f74de0ad4e", "width": 1000, "height": 474}, "resolutions": [{"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8abdcef72df2093bf758152201afa04bf6e057bf", "width": 108, "height": 51}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6e677071f813a21dcf0f718f62e269a544bceb3", "width": 216, "height": 102}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09c16fd6ebedfd8bc66c13bdd68a4e2636838060", "width": 320, "height": 151}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7eeda53972ba146d14da874ffe1535e7f85ea97", "width": 640, "height": 303}, {"url": "https://external-preview.redd.it/SjgP2r8duzoP40999EnERIcTn6JJw0WM2yjankvwgRs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af269d1b86d58ad8f6938810a1ff26559b9df570", "width": 960, "height": 455}], "variants": {}, "id": "Lj79sn9bvI9itLtGCoV9Mp-NCuBvuY6FKn4cO6LIhnA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18s41pk", "is_robot_indexable": true, "report_reasons": null, "author": "EveningDay5261", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s41pk/just_got_done_recovering_5tb_worth_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s41pk/just_got_done_recovering_5tb_worth_of_data/", "subreddit_subscribers": 721205, "created_utc": 1703693080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I noticed that my drive was frequently coming online and offline. I opened my server to insepct it, and found that the sata connection snapped off.  The drive is fully functional if I carefully insert a cable. I wiped the drive already with the intention of tossing it, since it has already been replaced. It just hurts to throw a 14tb drive out while it still functions. Is there anyway I can salvage this or make a repair? It is a Seagate exos x16.", "author_fullname": "t2_14din1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Any hope or ideas on how to repair this drive, or is it toast?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jrj2st4lwv8c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68388ab792b994ac8cfa9e100a70f330e5225b6b"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0ed9febfa3019b0fc9cf06dff89d57bf2a9d724"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6076d6ab5b717a3aef7ed4fc4feba4c03ecd0ad1"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=265db3d8fc6185056225dfa107870445c7b77525"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cb9cc45db808d12ebcaefd8b8393596af58c888"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e740caa70be2a48f54431688e7cf6af10ed32fa"}], "s": {"y": 4000, "x": 3000, "u": "https://preview.redd.it/jrj2st4lwv8c1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=8efa6d4d44c5062703bbc7179d1ee866daa8abdf"}, "id": "jrj2st4lwv8c1"}, "of1cw07lwv8c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89071e1beb9fa0ea201e237b22c73a30c6349559"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55f0a8642433214e6b7b4d4fe0d388f1036456ec"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f228da5f6814bbdc1c763949372272e95475f54"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e97f016dde285e074b4ce214a39c5d866b66169c"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0101573bdc4a5c107db8a1b5d493dee5d97e3e2c"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a84a2e48fecf8f55470d919198228c61bafa596e"}], "s": {"y": 4000, "x": 3000, "u": "https://preview.redd.it/of1cw07lwv8c1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=157efd8d562b625d2e1ee4d1fa5a59857726452e"}, "id": "of1cw07lwv8c1"}}, "name": "t3_18s8334", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 25, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "jrj2st4lwv8c1", "id": 379987854}, {"media_id": "of1cw07lwv8c1", "id": 379987855}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TdAjyYAgtdHM7ggd8sISir77F5RdMUBY2tWyEg9rZQI.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1703703288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I noticed that my drive was frequently coming online and offline. I opened my server to insepct it, and found that the sata connection snapped off.  The drive is fully functional if I carefully insert a cable. I wiped the drive already with the intention of tossing it, since it has already been replaced. It just hurts to throw a 14tb drive out while it still functions. Is there anyway I can salvage this or make a repair? It is a Seagate exos x16.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/18s8334", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "80TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s8334", "is_robot_indexable": true, "report_reasons": null, "author": "My_Name_Is_Not_Mark", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s8334/any_hope_or_ideas_on_how_to_repair_this_drive_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/18s8334", "subreddit_subscribers": 721205, "created_utc": 1703703288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently moving my entire media file collection to a better file management system ([Hydrus Network](https://hydrusnetwork.github.io/hydrus/index.html) if you're interested) and I realized that I also want to preserve the creation times of the original files.\n\nThis means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.\n\nThis is absurd. I'm substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.\n\nJust wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. \n\nData hoarding is at the end of the day just a kind of hoarding.", "author_fullname": "t2_hokshzbw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Addiction to Preserving Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rwdue", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703667526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently moving my entire media file collection to a better file management system (&lt;a href=\"https://hydrusnetwork.github.io/hydrus/index.html\"&gt;Hydrus Network&lt;/a&gt; if you&amp;#39;re interested) and I realized that I also want to preserve the creation times of the original files.&lt;/p&gt;\n\n&lt;p&gt;This means that I have to effectively double the storage space needed for my current collection as I need to store the original files as well to preserve metadata. Eventually I will also spend hours (probably days) developing a custom database solution and script to record the timestamps and all other original metadata for future reference.&lt;/p&gt;\n\n&lt;p&gt;This is absurd. I&amp;#39;m substantially increasing my workload and storage needs simply because I have a stupid desire to preserve all data no matter what. I tell myself that I may need to have the timestamps for some data analysis projects in the future and that just enhances this unnecessary obligation to data preservation.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share my frustration with this impractical addiction to data hoarding and to warn you guys to try and avoid becoming this bad. &lt;/p&gt;\n\n&lt;p&gt;Data hoarding is at the end of the day just a kind of hoarding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18rwdue", "is_robot_indexable": true, "report_reasons": null, "author": "Estavenz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rwdue/addiction_to_preserving_data/", "subreddit_subscribers": 721205, "created_utc": 1703667526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good evening all\n\nI have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.\n\nI did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people's entire careers is just this section of how to run a computer.\n\nI am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I've seen, but Start early is my main idea, so it is not severely daunting\n\nTLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage", "author_fullname": "t2_dacwz4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First time here, first time seriously considering big backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rq1ig", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703646273.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703646051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good evening all&lt;/p&gt;\n\n&lt;p&gt;I have just finished moving data over from my dying laptop to a new PC (windows) and saw the fallibility of single backup. Whilst I do have backblaze for my computer with constant sync, I realized that my main media drive is not constantly plugged in for access, nor is there a more reliable method of storage for it. I am hoping to build something that will allow me to put my DVDs in it (digital copy) and music so I can stream it from other machines. From what I read, this is a good choice for JellyFin to play the media.&lt;/p&gt;\n\n&lt;p&gt;I did some research on material that i need and other software, but I have only gotten more confused reading through this stuff than I learned. I realized this is some people&amp;#39;s entire careers is just this section of how to run a computer.&lt;/p&gt;\n\n&lt;p&gt;I am hoping for an intro to basic storage for 172 DVDs and 160 vhs videos, as well as 40gb of audio. This is probably peanuts compared to some numbers I&amp;#39;ve seen, but Start early is my main idea, so it is not severely daunting&lt;/p&gt;\n\n&lt;p&gt;TLDR: I am looking for what I need to set up a basic linux computer (hardware/software) to have a system of better backup than a dying tiny external drive. I read about JellyFin for playing, but nothing for actual storage&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rq1ig", "is_robot_indexable": true, "report_reasons": null, "author": "97cweb", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rq1ig/first_time_here_first_time_seriously_considering/", "subreddit_subscribers": 721205, "created_utc": 1703646051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students ", "author_fullname": "t2_ey91h2hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PhD students / academics - how do you go about backing up your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rl0ln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703632796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how PhD students go about backing up their data given that they\u2019re often dealing with extremely valuable data but often in informal ways (eg working from personal devices and from home), where there may be no \u201ccompany procedure\u201d or even systems in place for students &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rl0ln", "is_robot_indexable": true, "report_reasons": null, "author": "Royal_Difficulty_678", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rl0ln/phd_students_academics_how_do_you_go_about/", "subreddit_subscribers": 721205, "created_utc": 1703631964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, \n\n&amp;#x200B;\n\n   I was looking to do some VHS conversion for some old VHS tapes that need to go away.  I have an old TV / VHS combo [https://www.mediapollution.tv/product-page/rca-t19060gy](https://www.mediapollution.tv/product-page/rca-t19060gy) specifically.  \n\n&amp;#x200B;\n\nI was thinking of getting something like this, [https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/](https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/) and converting it myself.  My concern that I have is that the TV/VHS combo seems to only have one audio output.  (ie.  It doesn't have a red, yellow, white but only the white and yellow which I assume is one of the audio channels).  Would this matter? I'd hate to convert and get bad quality audio because of this, when I could have gotten a better quality if I took it to a service or acquired a better VHS player.  \n\n&amp;#x200B;\n\nAny thoughts?\n\n&amp;#x200B;", "author_fullname": "t2_pujhcq07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting VHS Tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s5otb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703697280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was looking to do some VHS conversion for some old VHS tapes that need to go away.  I have an old TV / VHS combo &lt;a href=\"https://www.mediapollution.tv/product-page/rca-t19060gy\"&gt;https://www.mediapollution.tv/product-page/rca-t19060gy&lt;/a&gt; specifically.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was thinking of getting something like this, &lt;a href=\"https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/\"&gt;https://www.amazon.com/ClearClick-Digital-Converter-3-0-Generation/dp/B0B8BY5HCG/&lt;/a&gt; and converting it myself.  My concern that I have is that the TV/VHS combo seems to only have one audio output.  (ie.  It doesn&amp;#39;t have a red, yellow, white but only the white and yellow which I assume is one of the audio channels).  Would this matter? I&amp;#39;d hate to convert and get bad quality audio because of this, when I could have gotten a better quality if I took it to a service or acquired a better VHS player.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any thoughts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?auto=webp&amp;s=b600d0192f6abd0abaa216ea632444247206875f", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c56eda873c1d53ea4a01e159d209a85385404fe", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afbdf406e275b69ddf76009ddcef20d2e430aa15", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/4G9BAtiUEPm3uxpZErd7-6gQT9WW_CKJQOxHMa6Sztw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c2b15856f08a574c232bdeb79aaba3c0bbf7b69", "width": 320, "height": 240}], "variants": {}, "id": "z1CDdnCRodlx8CWMOliStCIDXgB2nzP8ur3vMcLW0jE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s5otb", "is_robot_indexable": true, "report_reasons": null, "author": "csgeek3674", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s5otb/converting_vhs_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s5otb/converting_vhs_tapes/", "subreddit_subscribers": 721205, "created_utc": 1703697280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I'm new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?\n\nThanks!  \n", "author_fullname": "t2_5cyhhwlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping DVDs/Blu-rays with Multiple Audio Languages?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rp8qf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703643685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I rip my personal DVD/Blu-ray collection while preserving multiple audio languages? I&amp;#39;m new to ripping and want to digitize my collection. Specifically, I want to keep both the original English and the dubbed Spanish audio tracks. Any advice for a beginner in this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rp8qf", "is_robot_indexable": true, "report_reasons": null, "author": "JoshALogs", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rp8qf/ripping_dvdsblurays_with_multiple_audio_languages/", "subreddit_subscribers": 721205, "created_utc": 1703643685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format *2024-01-16 17-45-03.jpg* (or .mov).  \n\n\n**USING PHOTOSYNC**  \nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos **from** your smartphone **to** your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn't make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.\n\n1. Install [PhotoSync](https://www.photosync-app.com/home) on smartphone  \n2. Install PhotoSyncCompanion on Windows  \n3. Set up app to connect to Windows computer on the same WiFi as smartphone  \n4. Click Recents  \n5. Click Update  \n6. Click New  \n7. Click Computer\n\nIt is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it's an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  \n\n\n**MY PRE-2024 METHOD**  \nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn't see to work with the OneDrive app. I'm not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.\n\n*The naming problem*\n\nPhotos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG\\_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won't work for the MOV files because the iPhone stores the video created date in the meta data field *com.apple.quicktime.creationdate*. Better File Rename 5 can't see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It's possible that another renamer application with a GUI can do it, but I haven't tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful [EXIFtool](https://exiftool.org/).\n\nHere is how to rename iOS-created MOV video files to the date on your Windows 10 computer.\n\n1. Put a copy of the MOV files into a folder in, say, C:\\\\RenamingFolder  \n2. Download ExifTool and place the EXE file in that folder.  \n3. Rename the file to exiftool.exe  \n4. Open Notepad, enter the following code, and save to C:\\\\RenamingFolder with the name Rename file to IPhone video creation date.bat\n\n    exiftool \"-FileName&lt;creationdate\" -d \"%%Y-%%m-%%d %%H-%%M-%%S.%%%%e\" K:\\RenamingFolder\ncmd /k\n\n5.  Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. \n\n**CONCLUSION**\n\nPhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I'll have properly named files using the accurate date and time.\n\nFinally, for the record, I should add that while the format *2024-01-16 17-45-03.jpg* helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the *Text - Add text to end* command from the menus, and type in with a leading space *Mike's 8th birthday.* So the final file name with be *2024-01-16 17-45-03 Mike's 8th birthday.jpg.*", "author_fullname": "t2_7jhiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflow: copy iPhone photos and videos to Windows 10 with filenames using the date the photo or video was really created", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18ro2lq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703640324.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my 2024 workflow for saving photos and videos from my iPhone 14 (iOS 17.1.2) to my Windows 10 computer using the picture taken date or the video created date as the filename in the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; (or .mov).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;USING PHOTOSYNC&lt;/strong&gt;&lt;br/&gt;\nI just found out about this invaluable smartphone software. It provides an alternate way to share your photos and videos &lt;strong&gt;from&lt;/strong&gt; your smartphone &lt;strong&gt;to&lt;/strong&gt; your preferred cloud storage (iCloud, Google Photos, OneDrive, Google Drive, Dropbox) or to your home computer via Wifi while at home. Best of all, it doesn&amp;#39;t make file naming an afterthought (as does the iPhone). Instead, when transferring your photos and video files, the app suggests the same nomenclature that I have been using for years (and allows you to customize it). The app correctly names the files using the correct meta data info.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Install &lt;a href=\"https://www.photosync-app.com/home\"&gt;PhotoSync&lt;/a&gt; on smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Install PhotoSyncCompanion on Windows&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Set up app to connect to Windows computer on the same WiFi as smartphone&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Recents&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Update&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click New&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Click Computer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It is fast, uses correct file name nomenclature, can transfer to Windows computer at home inside your WiFi network, or to Google Photos, OneDrive, etc. from anywhere. If your computer is connected via an ethernet cable to your network (as mine is), it&amp;#39;s an extra step to set up at the start, but it works. You can manually transfer the photos and videos or set it to auto-transfer at a specific time or  when you arrived at a location (eg, home, work, etc.) and are in WiFi.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;MY PRE-2024 METHOD&lt;/strong&gt;&lt;br/&gt;\nPrior to PhotoSync app, I used the following imperfect setup. I set the OneDrive app on my smartphone to automatically backup all the new photos and I videos I recorded. I also had Google Photos do the same. On iPhone, this is not automatic and it can take hours before the sync occurs. I found one can force Google Photos to backup the photos and videos by opening the Google Photos app. But that doesn&amp;#39;t see to work with the OneDrive app. I&amp;#39;m not sure how to force OneDrive to sync the files on demand. For the OneDrive version of the files, I mapped it to Windows Explorer so I can easily access my backed up files.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;The naming problem&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Photos and video files synchronized via Google Photos keep the original name created on the iPhone, for instance, IMG_8273.jpg. So if I download this photo from Google Photos, I will need to rename it to the date the photo was taken. By contrast, OneDrive renames the file to the date it was taken, but uses UTC-0 time. While this is accurate, it is not what I want and requires renaming on my Windows 10 computer. I prefer the interface of the Windows application Better File Rename 5 mostly because I have been using it for many years and could easily rename the JPG files to the date the photo was taken. But this application won&amp;#39;t work for the MOV files because the iPhone stores the video created date in the meta data field &lt;em&gt;com.apple.quicktime.creationdate&lt;/em&gt;. Better File Rename 5 can&amp;#39;t see that particular meta data. Neither can Advanced Renamed 3.88. Neither can Bulk Rename Utility 3.3.2.0. It&amp;#39;s possible that another renamer application with a GUI can do it, but I haven&amp;#39;t tested them all. The one renamer software that will do this job properly is the one that scares many people\u2014the command line based but crazy powerful &lt;a href=\"https://exiftool.org/\"&gt;EXIFtool&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Here is how to rename iOS-created MOV video files to the date on your Windows 10 computer.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Put a copy of the MOV files into a folder in, say, C:\\RenamingFolder&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Download ExifTool and place the EXE file in that folder.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Rename the file to exiftool.exe&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Open Notepad, enter the following code, and save to C:\\RenamingFolder with the name Rename file to IPhone video creation date.bat&lt;/p&gt;\n\n&lt;p&gt;exiftool &amp;quot;-FileName&amp;lt;creationdate&amp;quot; -d &amp;quot;%%Y-%%m-%%d %%H-%%M-%%S.%%%%e&amp;quot; K:\\RenamingFolder\ncmd /k&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Double click the bat file and EXIFtool will rename the MOV files to a filename using the correct meta info. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;CONCLUSION&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;PhotoSync will solve the file naming problem. I love this app. As for the 2023 photos and videos I have not have time to properly rename, the EXIFtool solution will solve that for me. I&amp;#39;ll have properly named files using the accurate date and time.&lt;/p&gt;\n\n&lt;p&gt;Finally, for the record, I should add that while the format &lt;em&gt;2024-01-16 17-45-03.jpg&lt;/em&gt; helps organize the files in order, I like to batch add an ending to the files. For instance, if I have a few dozen photos of a birthday, I select them all in Windows Explorer, right click, select Better File Rename from the context menu, and then apply the &lt;em&gt;Text - Add text to end&lt;/em&gt; command from the menus, and type in with a leading space &lt;em&gt;Mike&amp;#39;s 8th birthday.&lt;/em&gt; So the final file name with be &lt;em&gt;2024-01-16 17-45-03 Mike&amp;#39;s 8th birthday.jpg.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18ro2lq", "is_robot_indexable": true, "report_reasons": null, "author": "ThumperStrauss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18ro2lq/workflow_copy_iphone_photos_and_videos_to_windows/", "subreddit_subscribers": 721205, "created_utc": 1703640324.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "#TL;DR - Powered PC off + unplugged for 3 days. Turned back on, SSD dead. Software doesn't see it except Hard Disk Sentinel. Windows asks to initialize in Disk Manager. Ubuntu doesn't see it via gparted, dmesg shows it failed via softresets. Replaceable data but would rather somehow copy files off if at all possible one last time.\n\nSo, I went on vacation for 3 days, powering off and unplugging my PC before leaving (including turning off the switch on the PSU). Fast forward to when I return home, plug everything back in, and my PC takes an oddly long time to boot.\n\nTurns out my storage SSD, a Samsung 860 Evo, no longer gets detected in Windows. If I open disk management, [it shows this](https://i.imgur.com/CkNWmGI.png) and asks me to initialize it, something I obviously won't do.\n\nNothing else detects it from what I tried, except Hard Disk Sentinel: somehow, making it view it as an offline disk shows [disk activity at 100%](https://i.imgur.com/DjNPPho.png). Yes, this is live, not a snapshot. The average disk activity percentage kept going up until it reached 100%. Yet it remained \"offline\".\n\nI've also tried using gparted and dmesg Ubuntu to no avail. I live booted into Ubuntu via a USB, with the faulty SSD as the only internal drive in a spare desktop I have lying around. [These](https://i.imgur.com/bvixzlq.jpg) are all instances of \"/sd\" in dmesg. These [softreset](https://i.imgur.com/D4gMNNm.jpg) instances are what I believe are the faulty SSD.\n\nOther things I've tried include testdisk, which did not see the SSD in neither Windows nor Ubuntu, R-Studio Technician Edition which did not see the SSD, Macrium Reflect, and Easeus, neither of which saw the SSD either.\n\nI've also tried using a couple USB HDD docks that I have. I found that plugging it in to a dock does not get detected at all, but after a couple minutes of leaving it on, Windows will make a connection sound as though something's getting plugged in, then explorer will start slowing down every now and then. All the while, Windows will not detect a drive, but will claim there's a 0B storage device plugged into H (the drive's original drive letter).\n\nI read about the power cycling method where you unplug the SATA data cable and just leave the power cable connected, and turning on the PC for 30 mins in the BIOS, turning off for 30 secs, turning on for another 30 mins, turning off for 30 secs again, then finally plugging the SATA cable back and hoping for the best. Sadly, this did nothing. \n\nI then repeated this except left my PC on booted in Windows overnight (6 hours). When I woke up, I powered off, plugged SATA back in, and sadly nothing changed. Drive is still undetectable, and still asks to be initialized in Disk Management.\n\nI've done the same trick once more now that I'm at work, and will check back when I get home (should be on for 9 hours this time). But I don't have high hopes.\n\n#Is there anything else I can try? The data isn't important enough to warrant using an expensive data recovery professional. It's all replaceable, it would just be very bothersome and time consuming to replace it all (bunch of portable apps I've been using since 2015 along with music I've been meaning to tag and sync to my library).\n\nI thankfully know exactly what was on it thanks to VoidTools' Search Everything keeping track of what files were last on the drive, so I know what I'm missing. I just really hope there's something I can do to get one last breath of life out of this drive so I can simply copy everything off and save myself some time.\n\nWhat's really annoying is I was planning on making a backup system via Macrium (which I have been using for years) to make backups of all my SSDs, but hadn't gotten around to it yet. If I did, I wouldn't be trying so hard to recover this drive. \n\nP.S. This is my third storage failure this year, all of which are samsung devices (two 512GB microSDs of the same model, and now this 860 Evo SSD). It's bad enough that Samsung has a non-existent warranty in Canada where I can't even get a replacement, as though the higher price of their products wasn't already bad enough.", "author_fullname": "t2_ja9d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 860 Evo 500GB SSD Died - Any recourse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s5x5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703699241.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1703697861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;TL;DR - Powered PC off + unplugged for 3 days. Turned back on, SSD dead. Software doesn&amp;#39;t see it except Hard Disk Sentinel. Windows asks to initialize in Disk Manager. Ubuntu doesn&amp;#39;t see it via gparted, dmesg shows it failed via softresets. Replaceable data but would rather somehow copy files off if at all possible one last time.&lt;/h1&gt;\n\n&lt;p&gt;So, I went on vacation for 3 days, powering off and unplugging my PC before leaving (including turning off the switch on the PSU). Fast forward to when I return home, plug everything back in, and my PC takes an oddly long time to boot.&lt;/p&gt;\n\n&lt;p&gt;Turns out my storage SSD, a Samsung 860 Evo, no longer gets detected in Windows. If I open disk management, &lt;a href=\"https://i.imgur.com/CkNWmGI.png\"&gt;it shows this&lt;/a&gt; and asks me to initialize it, something I obviously won&amp;#39;t do.&lt;/p&gt;\n\n&lt;p&gt;Nothing else detects it from what I tried, except Hard Disk Sentinel: somehow, making it view it as an offline disk shows &lt;a href=\"https://i.imgur.com/DjNPPho.png\"&gt;disk activity at 100%&lt;/a&gt;. Yes, this is live, not a snapshot. The average disk activity percentage kept going up until it reached 100%. Yet it remained &amp;quot;offline&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried using gparted and dmesg Ubuntu to no avail. I live booted into Ubuntu via a USB, with the faulty SSD as the only internal drive in a spare desktop I have lying around. &lt;a href=\"https://i.imgur.com/bvixzlq.jpg\"&gt;These&lt;/a&gt; are all instances of &amp;quot;/sd&amp;quot; in dmesg. These &lt;a href=\"https://i.imgur.com/D4gMNNm.jpg\"&gt;softreset&lt;/a&gt; instances are what I believe are the faulty SSD.&lt;/p&gt;\n\n&lt;p&gt;Other things I&amp;#39;ve tried include testdisk, which did not see the SSD in neither Windows nor Ubuntu, R-Studio Technician Edition which did not see the SSD, Macrium Reflect, and Easeus, neither of which saw the SSD either.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also tried using a couple USB HDD docks that I have. I found that plugging it in to a dock does not get detected at all, but after a couple minutes of leaving it on, Windows will make a connection sound as though something&amp;#39;s getting plugged in, then explorer will start slowing down every now and then. All the while, Windows will not detect a drive, but will claim there&amp;#39;s a 0B storage device plugged into H (the drive&amp;#39;s original drive letter).&lt;/p&gt;\n\n&lt;p&gt;I read about the power cycling method where you unplug the SATA data cable and just leave the power cable connected, and turning on the PC for 30 mins in the BIOS, turning off for 30 secs, turning on for another 30 mins, turning off for 30 secs again, then finally plugging the SATA cable back and hoping for the best. Sadly, this did nothing. &lt;/p&gt;\n\n&lt;p&gt;I then repeated this except left my PC on booted in Windows overnight (6 hours). When I woke up, I powered off, plugged SATA back in, and sadly nothing changed. Drive is still undetectable, and still asks to be initialized in Disk Management.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done the same trick once more now that I&amp;#39;m at work, and will check back when I get home (should be on for 9 hours this time). But I don&amp;#39;t have high hopes.&lt;/p&gt;\n\n&lt;h1&gt;Is there anything else I can try? The data isn&amp;#39;t important enough to warrant using an expensive data recovery professional. It&amp;#39;s all replaceable, it would just be very bothersome and time consuming to replace it all (bunch of portable apps I&amp;#39;ve been using since 2015 along with music I&amp;#39;ve been meaning to tag and sync to my library).&lt;/h1&gt;\n\n&lt;p&gt;I thankfully know exactly what was on it thanks to VoidTools&amp;#39; Search Everything keeping track of what files were last on the drive, so I know what I&amp;#39;m missing. I just really hope there&amp;#39;s something I can do to get one last breath of life out of this drive so I can simply copy everything off and save myself some time.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s really annoying is I was planning on making a backup system via Macrium (which I have been using for years) to make backups of all my SSDs, but hadn&amp;#39;t gotten around to it yet. If I did, I wouldn&amp;#39;t be trying so hard to recover this drive. &lt;/p&gt;\n\n&lt;p&gt;P.S. This is my third storage failure this year, all of which are samsung devices (two 512GB microSDs of the same model, and now this 860 Evo SSD). It&amp;#39;s bad enough that Samsung has a non-existent warranty in Canada where I can&amp;#39;t even get a replacement, as though the higher price of their products wasn&amp;#39;t already bad enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?auto=webp&amp;s=4c16fc4536173d43a5b3d697e42680a256c6c0c5", "width": 754, "height": 597}, "resolutions": [{"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1018555122c35a70dd7b63733295795d0f2e846", "width": 108, "height": 85}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4a545d51f6daa31d7212a758800bcbe632caf5a", "width": 216, "height": 171}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4d56152c5816c210339b47afe7e94e7945b19fd", "width": 320, "height": 253}, {"url": "https://external-preview.redd.it/yht5wobzaXVkPYJGkpm7iaDP42pugRNBTpKM-Ay6kPE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b81481999de57c73482541399940a4641125fec1", "width": 640, "height": 506}], "variants": {}, "id": "pMkX3NtqxIdMXFFMCHA53rmqZqUEQjVdDrXKQgk5J4E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "111TB Externals", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s5x5t", "is_robot_indexable": true, "report_reasons": null, "author": "sonicrings4", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s5x5t/samsung_860_evo_500gb_ssd_died_any_recourse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s5x5t/samsung_860_evo_500gb_ssd_died_any_recourse/", "subreddit_subscribers": 721205, "created_utc": 1703697861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, are there any third-party downloaders for Windows to download GDrive files (each file is 80+GB .) to my PC? The default client is awful. I need functions like auto pause / resume and verifying every byte since most are encrypted zips.", "author_fullname": "t2_wdpu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MEGA like downloader for GDrive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rzaln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703678940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, are there any third-party downloaders for Windows to download GDrive files (each file is 80+GB .) to my PC? The default client is awful. I need functions like auto pause / resume and verifying every byte since most are encrypted zips.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rzaln", "is_robot_indexable": true, "report_reasons": null, "author": "sdw23", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rzaln/mega_like_downloader_for_gdrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rzaln/mega_like_downloader_for_gdrive/", "subreddit_subscribers": 721205, "created_utc": 1703678940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn't maintain timestamps for folders.\n\nIs there an alternate software?\n\n&amp;#x200B;", "author_fullname": "t2_12qmo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GUI software that allows me to click which folders/subfolders/files to copy to destination and preserve time stamps and directory/structure/hierarchy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rrbb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703649937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use TeraCopy and it preserves the time stamps flawlessly, however it copies everything within the directory. I only want to copy some folders/subfolders/files but maintain its folder structure and maintain time stamps. FreeFileSync seems perfect but doesn&amp;#39;t maintain timestamps for folders.&lt;/p&gt;\n\n&lt;p&gt;Is there an alternate software?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rrbb0", "is_robot_indexable": true, "report_reasons": null, "author": "Snowblind45", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rrbb0/gui_software_that_allows_me_to_click_which/", "subreddit_subscribers": 721205, "created_utc": 1703649937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Lots of experiences made with this case out of curiosity...\n\nI got a peculiar case, strangely at the same time as I discovered bit rot on an SSD in rarely ever touched data. It seems now a harddisk has failed at its job, too, because I have a HGST 4 TB in a USB dock that I only use for a differential backup every two days or so. And while it is already ten years old (to my surprise - how time flies!), I never had any issues with the frequent phase of writing new data to the drive. But then eventually I did a full-scale read scan and suddenly discovered lots of severely damaged areas.\n\nIf the harddisk needed refreshes of old data, it should have prioritized it, but didn't. All new write operations until the last day didn't give any indication of platter aging or such, but apparently areas not touched in a long time showed not just soft errors but out of curiosity I have now razed all data on it, triggered several low level formats, SMART data resets etc., did full reads and writes, chkdsk, Victoria (which sadly offered writing only once, mysteriously, me having no idea why it worked once but never else) and AOMEI Partition Assitant (which offers to wipe partitions or empty space, so I can somewhat direct it).\n\nSo now I know various spots that are damaged, of course tried to navigate around them to preserve some lowest-priority storage, but it is so annoying how often the drive freezes up and needs a restart when various software tries to access it. AOMEI is usually more successful at least without partitions on it, and I already flagged it offline to the LVM because that one loves to freeze.\n\nI had phases where chkdsk /r stumbled over certain regions several times, indicating no exclusion happening, but then Victoria blazed over it without any issues. In fact, the beginning sectors are quite fine, and some previously problematic areas have been fixed, some not by remapping which can be heard with the head buzzing when going over it, but some previously extremely problematic areas are now totally fine again.\n\nIt also doesn't help that the internal low level formating process is obscure in when it has finished, and it might even continue its work between power cycles, dunno.\n\nIn-between all this, the reallocated sectors reached a peak point where it seems the drive throttled writes to 7.5 MB/s. But the drive often tickers a little by itself and keeps adding more pending sectors, and then falls asleep after two minutes unless it gets another SMART request, in which case it continues. One time I let this go on for hours and learned that the pending sectors counter maxes at 65535 and then begins from 0 again.\n\nIt is frustrating how badly this all works, because apparently lots of surface can be recovered and the rest excluded, in theory.\n\nBut again, the most upsetting is that the disk didn't give any indication that its old data had already rotted. So much for SMART monitoring. - And now I am wondering whether this hadn't happened if the drive had been running long enough to get bored and do some self-checking of disk surface.\n\nIt is funny. The one time long after suffering through the DeathStar debacle I buy a Deskstar again and it fails on me. (Earlier also a WD Black 4 TB.) I now got myself an Ironwolf 8 TB as main (since I have a whole age range of ancient Seagates that are all still in great condition despite having served as system disks and then secondary/tertiary backup), moving my WD Red 6 TB to primary backup (which has zero motor vibration while the lighter and newer Ironwolf has a little), but I am worried the same might happen to the WD and don't know how long I'd have to keep it online so that it takes good care of its data. The HGST case is really an unexpected surprise, since I took such good care of it.\n\nMy goal was to not have it reallocate to the max but just avoid the bad sectors. Would be great if it was possible to exclude sector ranges manually - much more elegant than the finnicky mess the drive does by itself.\n\nEDIT: It is wacky: Victoria can be tricked into allowing write mode, which works totally fine. I have to trigger an operation that freezes for a bit, like reading a single sector that is bad, and then the options cease being grayed out, and then I have to select write mode and when the stall is over, it is grayed out again but selected. - It is just weird because the first time I had it available, it wasn't even grayed out. - And write mode really works much better than read mode. Read tends to make it hang for a long time, but write actually feels like it is repairing what can be repaired.", "author_fullname": "t2_c79ls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Harddisk only online for backup actually a bad idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18sb54f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703713511.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703711012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lots of experiences made with this case out of curiosity...&lt;/p&gt;\n\n&lt;p&gt;I got a peculiar case, strangely at the same time as I discovered bit rot on an SSD in rarely ever touched data. It seems now a harddisk has failed at its job, too, because I have a HGST 4 TB in a USB dock that I only use for a differential backup every two days or so. And while it is already ten years old (to my surprise - how time flies!), I never had any issues with the frequent phase of writing new data to the drive. But then eventually I did a full-scale read scan and suddenly discovered lots of severely damaged areas.&lt;/p&gt;\n\n&lt;p&gt;If the harddisk needed refreshes of old data, it should have prioritized it, but didn&amp;#39;t. All new write operations until the last day didn&amp;#39;t give any indication of platter aging or such, but apparently areas not touched in a long time showed not just soft errors but out of curiosity I have now razed all data on it, triggered several low level formats, SMART data resets etc., did full reads and writes, chkdsk, Victoria (which sadly offered writing only once, mysteriously, me having no idea why it worked once but never else) and AOMEI Partition Assitant (which offers to wipe partitions or empty space, so I can somewhat direct it).&lt;/p&gt;\n\n&lt;p&gt;So now I know various spots that are damaged, of course tried to navigate around them to preserve some lowest-priority storage, but it is so annoying how often the drive freezes up and needs a restart when various software tries to access it. AOMEI is usually more successful at least without partitions on it, and I already flagged it offline to the LVM because that one loves to freeze.&lt;/p&gt;\n\n&lt;p&gt;I had phases where chkdsk /r stumbled over certain regions several times, indicating no exclusion happening, but then Victoria blazed over it without any issues. In fact, the beginning sectors are quite fine, and some previously problematic areas have been fixed, some not by remapping which can be heard with the head buzzing when going over it, but some previously extremely problematic areas are now totally fine again.&lt;/p&gt;\n\n&lt;p&gt;It also doesn&amp;#39;t help that the internal low level formating process is obscure in when it has finished, and it might even continue its work between power cycles, dunno.&lt;/p&gt;\n\n&lt;p&gt;In-between all this, the reallocated sectors reached a peak point where it seems the drive throttled writes to 7.5 MB/s. But the drive often tickers a little by itself and keeps adding more pending sectors, and then falls asleep after two minutes unless it gets another SMART request, in which case it continues. One time I let this go on for hours and learned that the pending sectors counter maxes at 65535 and then begins from 0 again.&lt;/p&gt;\n\n&lt;p&gt;It is frustrating how badly this all works, because apparently lots of surface can be recovered and the rest excluded, in theory.&lt;/p&gt;\n\n&lt;p&gt;But again, the most upsetting is that the disk didn&amp;#39;t give any indication that its old data had already rotted. So much for SMART monitoring. - And now I am wondering whether this hadn&amp;#39;t happened if the drive had been running long enough to get bored and do some self-checking of disk surface.&lt;/p&gt;\n\n&lt;p&gt;It is funny. The one time long after suffering through the DeathStar debacle I buy a Deskstar again and it fails on me. (Earlier also a WD Black 4 TB.) I now got myself an Ironwolf 8 TB as main (since I have a whole age range of ancient Seagates that are all still in great condition despite having served as system disks and then secondary/tertiary backup), moving my WD Red 6 TB to primary backup (which has zero motor vibration while the lighter and newer Ironwolf has a little), but I am worried the same might happen to the WD and don&amp;#39;t know how long I&amp;#39;d have to keep it online so that it takes good care of its data. The HGST case is really an unexpected surprise, since I took such good care of it.&lt;/p&gt;\n\n&lt;p&gt;My goal was to not have it reallocate to the max but just avoid the bad sectors. Would be great if it was possible to exclude sector ranges manually - much more elegant than the finnicky mess the drive does by itself.&lt;/p&gt;\n\n&lt;p&gt;EDIT: It is wacky: Victoria can be tricked into allowing write mode, which works totally fine. I have to trigger an operation that freezes for a bit, like reading a single sector that is bad, and then the options cease being grayed out, and then I have to select write mode and when the stall is over, it is grayed out again but selected. - It is just weird because the first time I had it available, it wasn&amp;#39;t even grayed out. - And write mode really works much better than read mode. Read tends to make it hang for a long time, but write actually feels like it is repairing what can be repaired.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18sb54f", "is_robot_indexable": true, "report_reasons": null, "author": "Dowlphin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18sb54f/harddisk_only_online_for_backup_actually_a_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18sb54f/harddisk_only_online_for_backup_actually_a_bad/", "subreddit_subscribers": 721205, "created_utc": 1703711012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to back-up around 100gb of photos and videos to an HDD. Is there a way to back-up the Samsung gallery directly, so that all my folders are kept?\nAlso, do I need any adapters or can I just plug in an HDD with usb c directly to my phone?\n\nHow long does it usually take to transfer 100gb of data to an average HDD?\n\nI was also contemplating getting an SSD, but I'm not sure I need the faster read speeds, because I would plug the HDD to my phone maybe 2 or 3 times a year to look at/show some photos.\n\nLastly, are there any HDD's that you guys can recommend specifically for my needs?", "author_fullname": "t2_54itytzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photos and videos from Samsung Galaxy S20 to HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s7qk2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703702415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to back-up around 100gb of photos and videos to an HDD. Is there a way to back-up the Samsung gallery directly, so that all my folders are kept?\nAlso, do I need any adapters or can I just plug in an HDD with usb c directly to my phone?&lt;/p&gt;\n\n&lt;p&gt;How long does it usually take to transfer 100gb of data to an average HDD?&lt;/p&gt;\n\n&lt;p&gt;I was also contemplating getting an SSD, but I&amp;#39;m not sure I need the faster read speeds, because I would plug the HDD to my phone maybe 2 or 3 times a year to look at/show some photos.&lt;/p&gt;\n\n&lt;p&gt;Lastly, are there any HDD&amp;#39;s that you guys can recommend specifically for my needs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s7qk2", "is_robot_indexable": true, "report_reasons": null, "author": "bubble121212", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s7qk2/photos_and_videos_from_samsung_galaxy_s20_to_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s7qk2/photos_and_videos_from_samsung_galaxy_s20_to_hdd/", "subreddit_subscribers": 721205, "created_utc": 1703702415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apologies if this has been asked, but I've searched and have come up with nothing. I've exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I'd like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?\n\nI figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.", "author_fullname": "t2_479u9c8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tumblr Export - Recreate Blog from the Index HTML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s4swq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703695061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this has been asked, but I&amp;#39;ve searched and have come up with nothing. I&amp;#39;ve exported my Tumblr Blog and have all of the post html files and associated images, as well as the index, however, I&amp;#39;d like to be able to scroll through the index and see it all stitched together like on my tumblr page.  Are there any tools or utilities that can do this?&lt;/p&gt;\n\n&lt;p&gt;I figure there must be something that can stich multiple htmls into one long one via an index filed, but am coming up blank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s4swq", "is_robot_indexable": true, "report_reasons": null, "author": "ggodfrey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s4swq/tumblr_export_recreate_blog_from_the_index_html/", "subreddit_subscribers": 721205, "created_utc": 1703695061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.", "author_fullname": "t2_2kooznfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to capture audio from an 8-track tape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rmt6e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703636740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it basically the same approach as capturing audio from vinyl, and just need to do a good job of cleaning the tape and getting a good player? Whenever I search for this there really seems to be no info on it or it gets lost in all of the reel-to-reel 8 tracks, 8 track multi track recorders, and other things that have 8 and tracks in their name.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rmt6e", "is_robot_indexable": true, "report_reasons": null, "author": "dstillloading", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rmt6e/best_way_to_capture_audio_from_an_8track_tape/", "subreddit_subscribers": 721205, "created_utc": 1703636740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.\n\nNowadays, I have to postprocess my scans in the following way:\n\n1. rotate and crop them so there are not white boundaries\n2. Remove some scratches and dust that was still on the scans\n3. Improve picture quality by tweaking contrast, saturation, etc.\n4. export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)\n\nI do these manually on Affinity photo now, but I'm wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.\n\nAny suggestions?", "author_fullname": "t2_bv035", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automated tool for postprocessing scanned photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rksvw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703631429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking for a way to streamline my photo scanning process so I can chomp through my backlog before I die of old age.&lt;/p&gt;\n\n&lt;p&gt;Nowadays, I have to postprocess my scans in the following way:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;rotate and crop them so there are not white boundaries&lt;/li&gt;\n&lt;li&gt;Remove some scratches and dust that was still on the scans&lt;/li&gt;\n&lt;li&gt;Improve picture quality by tweaking contrast, saturation, etc.&lt;/li&gt;\n&lt;li&gt;export them to a more readable format (usually a jpeg of 4000 pixels along its longest axis)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I do these manually on Affinity photo now, but I&amp;#39;m wondering if there is a swiss army knife tool that can automate this process. It would need to be able to handle large TIFFs as input and export high resolution images. Preferably locally installed.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "170TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rksvw", "is_robot_indexable": true, "report_reasons": null, "author": "Mathy963", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rksvw/automated_tool_for_postprocessing_scanned_photos/", "subreddit_subscribers": 721205, "created_utc": 1703631429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I'm transferring old VHS tapes by running my VHS player into my hi8 camera via component jacks and then using a firewire cable to go from my camera to my computer (OBS). But I noticed these moving lines going up and down the screen. I pumped the exposure so it can be easily seen as this is more hidden in the actual footage but I'm trying to figure out what this could be. Old tape? VHS player with a weak signal? Can anyone help identify these lines so I know what's causing them? I can provide more spec info if needed but:\n\nThe Hi8 camera I'm using: sony dcr trv340\n\nVHS player is a Sony VHS/DVD Combo which works well.\n\nThanks!!", "author_fullname": "t2_buyl9wgp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring VHS tapes glitch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18sbs0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703712643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I&amp;#39;m transferring old VHS tapes by running my VHS player into my hi8 camera via component jacks and then using a firewire cable to go from my camera to my computer (OBS). But I noticed these moving lines going up and down the screen. I pumped the exposure so it can be easily seen as this is more hidden in the actual footage but I&amp;#39;m trying to figure out what this could be. Old tape? VHS player with a weak signal? Can anyone help identify these lines so I know what&amp;#39;s causing them? I can provide more spec info if needed but:&lt;/p&gt;\n\n&lt;p&gt;The Hi8 camera I&amp;#39;m using: sony dcr trv340&lt;/p&gt;\n\n&lt;p&gt;VHS player is a Sony VHS/DVD Combo which works well.&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18sbs0i", "is_robot_indexable": true, "report_reasons": null, "author": "desperado491", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18sbs0i/transferring_vhs_tapes_glitch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18sbs0i/transferring_vhs_tapes_glitch/", "subreddit_subscribers": 721205, "created_utc": 1703712643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, am I completely off base here?\n\n I'll ask this group because it's data recovery related, but any Linux Admin's out there please chime in as well (and I'll ask other subs for that demographic as well, yes). \n\nI have 6 4TB HDD's in a machine that is failing (mobo/cpu). I have a need to get the data off of these drives and my available resources are a rack mounted server w/ 60 free TB's of space but no room for additional drives, and a second tower machine that does NOT have room for all of the drives or power for more than one or 2 of them. \n\nI tried, putting a 6 port SATA card in tower 2, using tower 1's PSU to power the drives while I boot tower 2 into linux and reassemble and read the array. The issue is that tower 1's CPU issues just make the whole thing reboot after about 20 minutes or so, even causing the PSU to reboot. Disconnecting the mobo in Tower 1 won't let the PSU actually power up for some reason. So that's not going anywhere.\n\nSince I don't have time, or funds right now honestly to order a new mobo/cpu for Tower 1, my current plan (plan F) is to remove one drive at a time, put it into Tower 2, and use something to create an ISO (or something else I might not be aware of) out of each drive into the rack mount NAS... then use the NAS to rebuild the array out of those ISO images and mount it into itself... or something. \n\nAm I smoking dreams and dropped packets here? Or is anything like this even possible? Wife's photography archives are trapped on these 6 disks - which have not had any issues - and we need to get the data off, onto the new NAS, and readable so we can continue working with it. \n\nIs there anyway to hotwire a PSU to just power drives without being hooked to a living MOBO? \n\nIs there anyway to ISO a MDADM drive, and then use mulitple ISO's to re-create the array on another machine? \n\nIs buying hardware and replacing it the only, or far safer (yet less adventurous!), way to do this? The machines are all living at a friends house (my TX 'datacenter') so I only have remote access when I'm in town which is for a few hours, while I'm in town... making ordering hardware, installing, and doing all of this impractical over the next few months.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_nq184", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux MDADM ... from ISO files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s9867", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question / Sanity-check", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703706149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, am I completely off base here?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll ask this group because it&amp;#39;s data recovery related, but any Linux Admin&amp;#39;s out there please chime in as well (and I&amp;#39;ll ask other subs for that demographic as well, yes). &lt;/p&gt;\n\n&lt;p&gt;I have 6 4TB HDD&amp;#39;s in a machine that is failing (mobo/cpu). I have a need to get the data off of these drives and my available resources are a rack mounted server w/ 60 free TB&amp;#39;s of space but no room for additional drives, and a second tower machine that does NOT have room for all of the drives or power for more than one or 2 of them. &lt;/p&gt;\n\n&lt;p&gt;I tried, putting a 6 port SATA card in tower 2, using tower 1&amp;#39;s PSU to power the drives while I boot tower 2 into linux and reassemble and read the array. The issue is that tower 1&amp;#39;s CPU issues just make the whole thing reboot after about 20 minutes or so, even causing the PSU to reboot. Disconnecting the mobo in Tower 1 won&amp;#39;t let the PSU actually power up for some reason. So that&amp;#39;s not going anywhere.&lt;/p&gt;\n\n&lt;p&gt;Since I don&amp;#39;t have time, or funds right now honestly to order a new mobo/cpu for Tower 1, my current plan (plan F) is to remove one drive at a time, put it into Tower 2, and use something to create an ISO (or something else I might not be aware of) out of each drive into the rack mount NAS... then use the NAS to rebuild the array out of those ISO images and mount it into itself... or something. &lt;/p&gt;\n\n&lt;p&gt;Am I smoking dreams and dropped packets here? Or is anything like this even possible? Wife&amp;#39;s photography archives are trapped on these 6 disks - which have not had any issues - and we need to get the data off, onto the new NAS, and readable so we can continue working with it. &lt;/p&gt;\n\n&lt;p&gt;Is there anyway to hotwire a PSU to just power drives without being hooked to a living MOBO? &lt;/p&gt;\n\n&lt;p&gt;Is there anyway to ISO a MDADM drive, and then use mulitple ISO&amp;#39;s to re-create the array on another machine? &lt;/p&gt;\n\n&lt;p&gt;Is buying hardware and replacing it the only, or far safer (yet less adventurous!), way to do this? The machines are all living at a friends house (my TX &amp;#39;datacenter&amp;#39;) so I only have remote access when I&amp;#39;m in town which is for a few hours, while I&amp;#39;m in town... making ordering hardware, installing, and doing all of this impractical over the next few months.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "18s9867", "is_robot_indexable": true, "report_reasons": null, "author": "546875674c6966650d0a", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/18s9867/linux_mdadm_from_iso_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s9867/linux_mdadm_from_iso_files/", "subreddit_subscribers": 721205, "created_utc": 1703706149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, as I was doing some research on making backups of my photo files, I came across sometime called cold storage. I currently run a Mac Mini system, I have Time Machine turned on, and mostly I'm concerned about keeping my nearly 4TB of photos preserved. With cost also being my factor. Would it be reasonable for me to get a 4TB hard drive, make a copy of all my photos, then store the drive away? I don't plan on keeping it hooked up to the Mac and keep it running. I also read about the format of the backup drive... running a Mac, does it matter what format I pick for the back up drive? APFS? Ex-FAT? And when I make the back up... what's the best way to move a large amount of files safely? Some mentioned copy but not move? And when people suggest I refresh the backup drive every few months, does that mean to re-copy everything over from the working drive again? Thanks in advance.", "author_fullname": "t2_16pgzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just came across the concept of cold storage...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s8zp9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703705542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, as I was doing some research on making backups of my photo files, I came across sometime called cold storage. I currently run a Mac Mini system, I have Time Machine turned on, and mostly I&amp;#39;m concerned about keeping my nearly 4TB of photos preserved. With cost also being my factor. Would it be reasonable for me to get a 4TB hard drive, make a copy of all my photos, then store the drive away? I don&amp;#39;t plan on keeping it hooked up to the Mac and keep it running. I also read about the format of the backup drive... running a Mac, does it matter what format I pick for the back up drive? APFS? Ex-FAT? And when I make the back up... what&amp;#39;s the best way to move a large amount of files safely? Some mentioned copy but not move? And when people suggest I refresh the backup drive every few months, does that mean to re-copy everything over from the working drive again? Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s8zp9", "is_robot_indexable": true, "report_reasons": null, "author": "MonkeyRPN", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s8zp9/just_came_across_the_concept_of_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s8zp9/just_came_across_the_concept_of_cold_storage/", "subreddit_subscribers": 721205, "created_utc": 1703705542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What\u2019s the difference?", "author_fullname": "t2_7fse4amb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD My Cloud vs External HDD plugged into wifi router", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s2vuo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703689982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the difference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s2vuo", "is_robot_indexable": true, "report_reasons": null, "author": "BrickRepulsive", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s2vuo/wd_my_cloud_vs_external_hdd_plugged_into_wifi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s2vuo/wd_my_cloud_vs_external_hdd_plugged_into_wifi/", "subreddit_subscribers": 721205, "created_utc": 1703689982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a new WD120EFBX (12tb WD Red Plus)\n\nWhen I got the drive, I did these steps:\n\n1. initialized it in Disk Management (GPT)\n- quick format I believe\n- ran extended SMART test on HDSentinel (perfect)\n- ran write/read surface test on HDSentinel (perfect)\n- bitlockered (full, not quick, encryption)\n- transferred all my data\n\nAt this point, everything looked perfect.\n\n**Rebooted. Now it's not showing up in Windows.**\n\nOpened disk management and it's giving me the \"Initialize Disk - You must initialize a disk before Logical Disk Manager can access it\" popup.\n\nDisk Management can see the \"unknown\" drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there's no problems with the hard drive.\n\nTried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.\n\nThe only possible thing I can see is in the drive properties Events tab, there's a \"Device not migrated event\".\n\nAny ideas? I appreciate any help you can give me.\n\nEdit: Tried a different windows machine and it was still unallocated. I've decided to just re-initialize and do the whole thing over again. If it fails again I'll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.", "author_fullname": "t2_58sktp70", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help - Perfect healthy new hard drive uninitialized/unallocated after reboot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18s2b0v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1703693844.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703688396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a new WD120EFBX (12tb WD Red Plus)&lt;/p&gt;\n\n&lt;p&gt;When I got the drive, I did these steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;initialized it in Disk Management (GPT)&lt;/li&gt;\n&lt;li&gt;quick format I believe&lt;/li&gt;\n&lt;li&gt;ran extended SMART test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;ran write/read surface test on HDSentinel (perfect)&lt;/li&gt;\n&lt;li&gt;bitlockered (full, not quick, encryption)&lt;/li&gt;\n&lt;li&gt;transferred all my data&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;At this point, everything looked perfect.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Rebooted. Now it&amp;#39;s not showing up in Windows.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Opened disk management and it&amp;#39;s giving me the &amp;quot;Initialize Disk - You must initialize a disk before Logical Disk Manager can access it&amp;quot; popup.&lt;/p&gt;\n\n&lt;p&gt;Disk Management can see the &amp;quot;unknown&amp;quot; drive and its size. SMART is still reading as perfect. Doing a short SMART test yields no problems. As far as I know there&amp;#39;s no problems with the hard drive.&lt;/p&gt;\n\n&lt;p&gt;Tried switching cables, removing other hard drives, using cables that worked for those removed hard drives. Nothing works.&lt;/p&gt;\n\n&lt;p&gt;The only possible thing I can see is in the drive properties Events tab, there&amp;#39;s a &amp;quot;Device not migrated event&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Any ideas? I appreciate any help you can give me.&lt;/p&gt;\n\n&lt;p&gt;Edit: Tried a different windows machine and it was still unallocated. I&amp;#39;ve decided to just re-initialize and do the whole thing over again. If it fails again I&amp;#39;ll return it. Leaving this up in case someone else has experienced this before and has an idea of what the problem might be.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18s2b0v", "is_robot_indexable": true, "report_reasons": null, "author": "SpareMenu5", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18s2b0v/help_perfect_healthy_new_hard_drive/", "subreddit_subscribers": 721205, "created_utc": 1703688396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**\\[TL;DR at the bottom\\]**\n\nHi, I'm not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.\n\nI'm currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I'm starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. \n\nTaking a look at the market pricing where I live, I came up with two options:\n\n1. 3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&gt; 8TB effective space, \\~57 USD/eff. TB\n2. 3x 18TB drives mirroring each other, thus 2 \"parity\" disks, at least one of which would be offsite -&gt; 18TB effective space, \\~50 USD/eff. TB\n\nMy goal is to build a robust solution, that I can use for many years. While I won't need more than 8TB in the near future, I wouldn't mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.\n\nOther aspects I considered include:\n\n* Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.   \nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?  \nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let's say 16TB) drive that completely replaces the 8TB one as the offsite drive?\n* Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.   \nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.\n* Mirroring: both options require some sort of mirroring. For this, I'm currently using freefilesync. What do you think about this approach?  I'm comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.\n* Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.   \nI'm especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? \n\nThis is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.\n\nTL;DR:   \n\\- RAID with 1 parity + single disk offsite backup with no parity, or  \n\\- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option", "author_fullname": "t2_73ebnj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Choosing a Storage Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rxnid", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703672612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;[TL;DR at the bottom]&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, I&amp;#39;m not new to the concept of data hoarding but have no hands-on experience with NASs, RAID arrays or extremely high capacity HDDs, and thus have some questions regarding which architecture I should choose for my upcoming storage system.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently storing my important files (mostly photos and videos) on 2x 3TB drives which are mirrored manually, but I&amp;#39;m starting to fill the drives, and have thus decided to invest in a higher capacity and more redundant solution. &lt;/p&gt;\n\n&lt;p&gt;Taking a look at the market pricing where I live, I came up with two options:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;3x 4TB drives in RAID with 1 parity + 1x 8TB drive as an offsite backup (no parity there, SMR disk) -&amp;gt; 8TB effective space, ~57 USD/eff. TB&lt;/li&gt;\n&lt;li&gt;3x 18TB drives mirroring each other, thus 2 &amp;quot;parity&amp;quot; disks, at least one of which would be offsite -&amp;gt; 18TB effective space, ~50 USD/eff. TB&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My goal is to build a robust solution, that I can use for many years. While I won&amp;#39;t need more than 8TB in the near future, I wouldn&amp;#39;t mind spending the extra for the 18TB option as long as it can be considered just as, if not more reliable. At first, I wanted to build a RAID array with 2 parity disks, but quickly scrapped this idea as many reddit threads advise against that in favor of having an offsite backup, this is why I came up with the possible solutions I listed. All drives are CMR, apart from the 8TB barracuda in option 1.&lt;/p&gt;\n\n&lt;p&gt;Other aspects I considered include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upgradability: I think it would take about 4-7 years for me to outgrow the 8TB of space of option 1, after which, upgrading would be a bigger hassle as I would need to purchase more 4TB drives, extend the RAID array with those and then use my offsite backup to restore data onto the RAID.&lt;br/&gt;\nIs this right, or are there better solutions to this problem, perhaps one that ensures I maintain two copies of the data throughout the upgrade process?&lt;br/&gt;\nAnother consideration: After this, the 8TB offsite drive could not hold all the data of the RAID, so I would have to upgrade that one too. Could I buy another one and extend it or would it be better to buy a single, larger capacity (let&amp;#39;s say 16TB) drive that completely replaces the 8TB one as the offsite drive?&lt;/li&gt;\n&lt;li&gt;Lifespan: The 18TB drives would be enough for me for the next 15-20 years, however, it would not be a wise decision to invest in these drives if they were likely to fail within that time. In that case, I would be much better off investing less at the moment and doing a large-scale upgrade later on.&lt;br/&gt;\nWhat can I expect here? I would only use the drives for storing archive data, occasionally (monthly, at most) powering them on to look at photos. They would otherwise remain powered off and completely disconnected.&lt;/li&gt;\n&lt;li&gt;Mirroring: both options require some sort of mirroring. For this, I&amp;#39;m currently using freefilesync. What do you think about this approach?  I&amp;#39;m comfortable with performing this manually, as I only do it less than once a month, whenever I decide to back up photos from my phone.&lt;/li&gt;\n&lt;li&gt;Skyhawk drives: the 4TB drives that would be used for option 1 are Skyhawk Surveillance drives. I have read many comments about how these sould only be used for CCTV footage as they are more likely to not correct read errors due to the nauture of their firmware, however, I have also seen explanations how this is largely made-up and no evidence exists to support these claims.&lt;br/&gt;\nI&amp;#39;m especially interested in the opinion of those using Skyhawk drives for RAID arrays. Have you encountered any issues attributable to the drives being engineered for CCTV use? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is probably my longest tech-advice post ever, thank you very much if you took the time to read through it.&lt;/p&gt;\n\n&lt;p&gt;TL;DR:&lt;br/&gt;\n- RAID with 1 parity + single disk offsite backup with no parity, or&lt;br/&gt;\n- 3 mirrored drives (1 offsite) with +12% better price/TB but 2x the total price and 2x the usable space compared to the first option&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rxnid", "is_robot_indexable": true, "report_reasons": null, "author": "Zorinhou", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rxnid/need_help_choosing_a_storage_architecture/", "subreddit_subscribers": 721205, "created_utc": 1703672612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In the changelog here (for the beta versions):  \n[https://dl.covecube.com/DrivePoolWindows/beta/download/changes.txt](https://dl.covecube.com/DrivePoolWindows/beta/download/changes.txt)  \nthis is mentioned:  \n\" 1537 \\* \\[D\\] \\[Issue #28834\\] Report no seek penalty for the pool if all of its pool parts are also reporting no seek penalty (see 'dpcmd list-pools'). \"\n\nWhat exactly is this \"seek penalty\" mentioned here? It seems my google-fu is a bit rusty and I can't find anything about this, other than it is also mentioned in Stablebit Scanner, but with no additional information.\n\n&amp;#x200B;", "author_fullname": "t2_3aizh2xr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the \"seek penalty\" mentioned in stablebit drivepool changelog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rxjjz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703672173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the changelog here (for the beta versions):&lt;br/&gt;\n&lt;a href=\"https://dl.covecube.com/DrivePoolWindows/beta/download/changes.txt\"&gt;https://dl.covecube.com/DrivePoolWindows/beta/download/changes.txt&lt;/a&gt;&lt;br/&gt;\nthis is mentioned:&lt;br/&gt;\n&amp;quot; 1537 * [D] [Issue #28834] Report no seek penalty for the pool if all of its pool parts are also reporting no seek penalty (see &amp;#39;dpcmd list-pools&amp;#39;). &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What exactly is this &amp;quot;seek penalty&amp;quot; mentioned here? It seems my google-fu is a bit rusty and I can&amp;#39;t find anything about this, other than it is also mentioned in Stablebit Scanner, but with no additional information.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rxjjz", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Administration663", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rxjjz/what_is_the_seek_penalty_mentioned_in_stablebit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rxjjz/what_is_the_seek_penalty_mentioned_in_stablebit/", "subreddit_subscribers": 721205, "created_utc": 1703672173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a big believer in physical media - I believe in owning the content I purchase. I've held out for a long time, but the tides don't appear to be turning, and, if anything, they're getting worse.  I'm already seeing a future where companies no longer offer physical discs so that they can further exert their control over consumers.  So I want to try and be ahead of it. If I have to suffer through digital, I at least want to remove DRM (after all, DRM-free digital isn't terribly different from physical).\n\n\nFor music, this is trivial, as thankfully, the industry never really tried to clamp down on using DRM files, and MP3s are the de-facto standard (with other higher quality formats like FLAC available).\n\n\nFor books/comics, this is mostly trivial. I'm familiar with exporting books off Google Play and using Calibre and DeDRM to get EPUBs of the novels I purchase.\n\n\nThat leaves me with movies/TV shows (i.e. video content). I've purchased a test movie from Google Play to be used for experiments before I decide to start purchasing more movies through this avenue.\n\n\nNote that I'm not interested in pirating (at least in the definition of obtaining it from somewhere else). This is a personal choice, as I still believe in purchasing the content (and no, I don't want to purchase the content first, and then pirate it from a different website - while self-imposed, these are my morals and my standards; if what I'm asking is not possible at the current time, so be it).\n\n\nI want to be able to (through whichever vector is easiest) take the movie that I purchased from Google Play and somehow either remove the DRM or convert it into an open-standard format. I've looked at different guides (both here and elsewhere), but haven't really found a good solution.  I've also done searches on the sub around this topic, but mostly just see people recommending using PlayOn or some other HDMI/screen recorder.  It appears like there's not a way to decrypt the EXO files that get generated when the video is downloaded for offline viewing onto a mobile device.\n\n\nI'm not in love with those answers, so I'd like to post just to see if there's anything I'm missing or if there have been any updates recently.\n\n\nSo, to recap:\n\n\n1. I have a movie that I have purchased through Google Play that I \"own\" (obviously using the term liberally, but distinguishing from \"not-a-rental\").\n\n\n2. I have access to both a Windows PC and an Android phone (versions can be provided if relevant).\n\n\n3. I can stream the movie from a browser or download it for offline storage on the Android phone (which appear to be EXO files; my understanding is these are not decryptable at present).\n\n\n4. I do not want to use any sort of \"screen recording\" or HDMI recording, if at all possible.\n\n\n5. I do not want to pirate from a different website. I want the rip to come from the file/access that I have purchased.\n\n\nAny hopes for me on this front or is my request simply not feasible at this time?  Thank you.", "author_fullname": "t2_qqbx40aaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asking for help backing up/ripping purchased Google Play Movies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18rs8qm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1703652861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a big believer in physical media - I believe in owning the content I purchase. I&amp;#39;ve held out for a long time, but the tides don&amp;#39;t appear to be turning, and, if anything, they&amp;#39;re getting worse.  I&amp;#39;m already seeing a future where companies no longer offer physical discs so that they can further exert their control over consumers.  So I want to try and be ahead of it. If I have to suffer through digital, I at least want to remove DRM (after all, DRM-free digital isn&amp;#39;t terribly different from physical).&lt;/p&gt;\n\n&lt;p&gt;For music, this is trivial, as thankfully, the industry never really tried to clamp down on using DRM files, and MP3s are the de-facto standard (with other higher quality formats like FLAC available).&lt;/p&gt;\n\n&lt;p&gt;For books/comics, this is mostly trivial. I&amp;#39;m familiar with exporting books off Google Play and using Calibre and DeDRM to get EPUBs of the novels I purchase.&lt;/p&gt;\n\n&lt;p&gt;That leaves me with movies/TV shows (i.e. video content). I&amp;#39;ve purchased a test movie from Google Play to be used for experiments before I decide to start purchasing more movies through this avenue.&lt;/p&gt;\n\n&lt;p&gt;Note that I&amp;#39;m not interested in pirating (at least in the definition of obtaining it from somewhere else). This is a personal choice, as I still believe in purchasing the content (and no, I don&amp;#39;t want to purchase the content first, and then pirate it from a different website - while self-imposed, these are my morals and my standards; if what I&amp;#39;m asking is not possible at the current time, so be it).&lt;/p&gt;\n\n&lt;p&gt;I want to be able to (through whichever vector is easiest) take the movie that I purchased from Google Play and somehow either remove the DRM or convert it into an open-standard format. I&amp;#39;ve looked at different guides (both here and elsewhere), but haven&amp;#39;t really found a good solution.  I&amp;#39;ve also done searches on the sub around this topic, but mostly just see people recommending using PlayOn or some other HDMI/screen recorder.  It appears like there&amp;#39;s not a way to decrypt the EXO files that get generated when the video is downloaded for offline viewing onto a mobile device.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not in love with those answers, so I&amp;#39;d like to post just to see if there&amp;#39;s anything I&amp;#39;m missing or if there have been any updates recently.&lt;/p&gt;\n\n&lt;p&gt;So, to recap:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I have a movie that I have purchased through Google Play that I &amp;quot;own&amp;quot; (obviously using the term liberally, but distinguishing from &amp;quot;not-a-rental&amp;quot;).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have access to both a Windows PC and an Android phone (versions can be provided if relevant).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can stream the movie from a browser or download it for offline storage on the Android phone (which appear to be EXO files; my understanding is these are not decryptable at present).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I do not want to use any sort of &amp;quot;screen recording&amp;quot; or HDMI recording, if at all possible.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I do not want to pirate from a different website. I want the rip to come from the file/access that I have purchased.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any hopes for me on this front or is my request simply not feasible at this time?  Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18rs8qm", "is_robot_indexable": true, "report_reasons": null, "author": "user20231225", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18rs8qm/asking_for_help_backing_upripping_purchased/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18rs8qm/asking_for_help_backing_upripping_purchased/", "subreddit_subscribers": 721205, "created_utc": 1703652861.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}