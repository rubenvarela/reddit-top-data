{"kind": "Listing", "data": {"after": "t3_170ycyq", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fb83g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Fabric: Should Databricks be Worried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_170otm0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KqfXrZoRthY9_0rKj5OFsfqYtxhzW4XWdZseyT5_NcY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696529189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "vantage.sh", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.vantage.sh/blog/databricks-vs-microsoft-fabric-pricing-analysis", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?auto=webp&amp;s=af663ae0121ce0a4793363094c1fec8259eefa84", "width": 1270, "height": 760}, "resolutions": [{"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57e13db27e90db4e7152be7e631452a796c7297a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deb5668b38d81175fea57b520a0c5c713c6d7ce9", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6222d395bbc71108823edfc2628acb961c3bb0cf", "width": 320, "height": 191}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe04e60e1c5b79f9154ec4a8193c93ba8b7c109", "width": 640, "height": 382}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f33a26035314d4789bc40760d8a6abb6837980b", "width": 960, "height": 574}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=394a654d23b5732bcba1b7573a3fc9a6da6897e6", "width": 1080, "height": 646}], "variants": {}, "id": "KJGb3So15RCrofadobr6rSw5w7wrNTNiPNLXDBWpF8o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "170otm0", "is_robot_indexable": true, "report_reasons": null, "author": "include_stdio_h", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170otm0/microsoft_fabric_should_databricks_be_worried/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.vantage.sh/blog/databricks-vs-microsoft-fabric-pricing-analysis", "subreddit_subscribers": 132386, "created_utc": 1696529189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies as my background isn't in DE but come more from BI systems administration. In my career I've seen a lot but the craziest thing I've seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?\n\nIs it literally just because our system doesn't have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it's 1995? Couldn't a REST API send a CSV over HTTPS anyway? Do we not trust it?\n\nIs data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?\n\nI feel like there's something I'm fundamentally misunderstanding but not sure what it is...", "author_fullname": "t2_6c2aryt5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the reason SFTP file transfer in banks, healthcare, etc is so ubiquitous just because they are using older systems without robust REST APIs...?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170rvz3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696536440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies as my background isn&amp;#39;t in DE but come more from BI systems administration. In my career I&amp;#39;ve seen a lot but the craziest thing I&amp;#39;ve seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?&lt;/p&gt;\n\n&lt;p&gt;Is it literally just because our system doesn&amp;#39;t have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it&amp;#39;s 1995? Couldn&amp;#39;t a REST API send a CSV over HTTPS anyway? Do we not trust it?&lt;/p&gt;\n\n&lt;p&gt;Is data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?&lt;/p&gt;\n\n&lt;p&gt;I feel like there&amp;#39;s something I&amp;#39;m fundamentally misunderstanding but not sure what it is...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170rvz3", "is_robot_indexable": true, "report_reasons": null, "author": "TheWikiJedi", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "subreddit_subscribers": 132386, "created_utc": 1696536440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear fellow Data Engineers\n\nYesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.\n\nThen a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.\n\nI stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.\n\nToday, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.\n\nI do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. \n\nTo summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?\n\nI am curious to hear about your experience!", "author_fullname": "t2_ou2h13bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backend Skills for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170vj3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696545043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear fellow Data Engineers&lt;/p&gt;\n\n&lt;p&gt;Yesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.&lt;/p&gt;\n\n&lt;p&gt;Then a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.&lt;/p&gt;\n\n&lt;p&gt;I stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.&lt;/p&gt;\n\n&lt;p&gt;Today, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.&lt;/p&gt;\n\n&lt;p&gt;I do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. &lt;/p&gt;\n\n&lt;p&gt;To summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?&lt;/p&gt;\n\n&lt;p&gt;I am curious to hear about your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "170vj3u", "is_robot_indexable": true, "report_reasons": null, "author": "Present_Salt_1688", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "subreddit_subscribers": 132386, "created_utc": 1696545043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked with Data in some capacity for the last 5-6 years of my career. I am self-taught and have experience with SQL, Python and Bash (as well as specific tools and database engines like MSSQL, MariaDB, Vertica etc).  \n\n\nI code a lot of custom ETL jobs for my work, and have begun to wonder - \\*how\\* can I evaluate performance, especially to identify room for improvement. It's one thing to benchmark script 1 vs script 2, but its more difficult to have a sense of how quickly a given amount of data should load. Are there ways to check network latency, the available computational resources etc to establish a baseline of performance to aim for? What are the best tools for feeling this out, especially ones easily available to me given what I know - ie SQL, Bash, Python? I'm out of my depth here but want to improve and feel answering or otherwise understanding this question is a necessary next step.  \n\n\nFor the sake of this exercise lets assume the data is fairly simple - floats, integers, char/varchar etc rather than more complex or larger formats like CLOB/BLOB.", "author_fullname": "t2_3v8tq4ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I know the extent to which ETLs *could* be running faster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170lxza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696522323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked with Data in some capacity for the last 5-6 years of my career. I am self-taught and have experience with SQL, Python and Bash (as well as specific tools and database engines like MSSQL, MariaDB, Vertica etc).  &lt;/p&gt;\n\n&lt;p&gt;I code a lot of custom ETL jobs for my work, and have begun to wonder - *how* can I evaluate performance, especially to identify room for improvement. It&amp;#39;s one thing to benchmark script 1 vs script 2, but its more difficult to have a sense of how quickly a given amount of data should load. Are there ways to check network latency, the available computational resources etc to establish a baseline of performance to aim for? What are the best tools for feeling this out, especially ones easily available to me given what I know - ie SQL, Bash, Python? I&amp;#39;m out of my depth here but want to improve and feel answering or otherwise understanding this question is a necessary next step.  &lt;/p&gt;\n\n&lt;p&gt;For the sake of this exercise lets assume the data is fairly simple - floats, integers, char/varchar etc rather than more complex or larger formats like CLOB/BLOB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170lxza", "is_robot_indexable": true, "report_reasons": null, "author": "vonkraush1010", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170lxza/how_do_i_know_the_extent_to_which_etls_could_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170lxza/how_do_i_know_the_extent_to_which_etls_could_be/", "subreddit_subscribers": 132386, "created_utc": 1696522323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was a director of data engineering after a datascience background and was offered a job a sr architect \n\nAs a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books\n\nI\u2019m sure I have knowledge gaps. What should I read up on ?", "author_fullname": "t2_l5fycnwh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer to data architect. What do I need to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170us8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696543281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was a director of data engineering after a datascience background and was offered a job a sr architect &lt;/p&gt;\n\n&lt;p&gt;As a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure I have knowledge gaps. What should I read up on ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "170us8m", "is_robot_indexable": true, "report_reasons": null, "author": "updated-quality-485", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "subreddit_subscribers": 132386, "created_utc": 1696543281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have\n\nCode Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks\n\nData lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. \n\nOne of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. \n\nContour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. \n\nOntology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. \n\nSlate - garbage piece of shit UI tech \n\nData Health - kinda useful confusing to use \n\nThey are the principal tools I use everyday in Palantir Foundry \n\nGeneral thoughts:\n\n- it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. \n\n- Data Expectations &amp; testing implementation is really good. \n\n- Their technical support seem pretty good. Their engineers are intimidatingly good. \n\n- really easy to find and search other datasets that could be related.\n\n- Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.\n\nHas anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.\n\nEdit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026", "author_fullname": "t2_nutp89h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How applicable is Palantir foundry to Data engineering in general?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170u6u8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have&lt;/p&gt;\n\n&lt;p&gt;Code Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks&lt;/p&gt;\n\n&lt;p&gt;Data lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. &lt;/p&gt;\n\n&lt;p&gt;One of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. &lt;/p&gt;\n\n&lt;p&gt;Contour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. &lt;/p&gt;\n\n&lt;p&gt;Ontology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. &lt;/p&gt;\n\n&lt;p&gt;Slate - garbage piece of shit UI tech &lt;/p&gt;\n\n&lt;p&gt;Data Health - kinda useful confusing to use &lt;/p&gt;\n\n&lt;p&gt;They are the principal tools I use everyday in Palantir Foundry &lt;/p&gt;\n\n&lt;p&gt;General thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Expectations &amp;amp; testing implementation is really good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Their technical support seem pretty good. Their engineers are intimidatingly good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;really easy to find and search other datasets that could be related.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.&lt;/p&gt;\n\n&lt;p&gt;Edit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170u6u8", "is_robot_indexable": true, "report_reasons": null, "author": "hositir", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "subreddit_subscribers": 132386, "created_utc": 1696541897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:\n\n1. A CRM file is uploaded to S3.\n2. A *sensor* notices the new file and creates a new *dynamic partition*.\n3. The file is then processed and stored to relational database. This would be the first *asset*.\n4. Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two *assets* that depend on the first *asset*.\n5. When both these *assets* are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two *assets*.\n6. Another *sensor* will check whether the last asset has been materialised and subsequently a *job* will be kicked of that is going to send an email, with a download link, to the user.\n\nMainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!", "author_fullname": "t2_trs7vyx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster ETL setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1715bbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696574403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A CRM file is uploaded to S3.&lt;/li&gt;\n&lt;li&gt;A &lt;em&gt;sensor&lt;/em&gt; notices the new file and creates a new &lt;em&gt;dynamic partition&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;The file is then processed and stored to relational database. This would be the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two &lt;em&gt;assets&lt;/em&gt; that depend on the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;When both these &lt;em&gt;assets&lt;/em&gt; are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two &lt;em&gt;assets&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Another &lt;em&gt;sensor&lt;/em&gt; will check whether the last asset has been materialised and subsequently a &lt;em&gt;job&lt;/em&gt; will be kicked of that is going to send an email, with a download link, to the user.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Mainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1715bbb", "is_robot_indexable": true, "report_reasons": null, "author": "WeddingIndependent30", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "subreddit_subscribers": 132386, "created_utc": 1696574403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube integrates its semantic layer with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170n9rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696525451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-dbt-integration-with-cube", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "170n9rf", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170n9rf/cube_integrates_its_semantic_layer_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-dbt-integration-with-cube", "subreddit_subscribers": 132386, "created_utc": 1696525451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Posted this over in /r/Azure , but wanted to get yawls perspective too. \n\n  \nI  am looking to migrate off of dedicated sql pool - it is too expensive. We are currently using  azure synapse studio + dedicated sql pool + adl. here is some workflows  we do:\n\n1) incremental data loads from transnational sql databases. we load this data pretty much straight into the dwh.\n\n2) about  5-6 external sources we pull data via an API or maybe a excel  spreadsheet/csv for adhoc data from my customer facing teams. we load  the data from the external sources into our data lake. I just use azure  batch to execute python code that does this for me. from there we create  external tables to load them into our dedicated sql pool.\n\n3) once  all the data is loaded into the sql pool, we then do various  transformations and aggregations on the data, and store the results in  sql, which are then loaded into our PBI data model. The refreshes happen  1x per day, typically around 4am.\n\nthis  all being said, dedicated sql pools are SUPER expensive. I do like  being able to load the external sources directly into sql, so i was  thinking about migrating off of dedicated sql pool and just using  serverless sql in synapse. We would just export data from our  transactional sql databases onto the lake in parquet format. One of my  concerns with this is everything would essentially be views, as i dont  think you can create/manage the transformations using CTAS statements.  This doesnt seem like it would scale well as we continue to get more  data.\n\nHonestly, were around 266gb  of data total, so i wouldnt be opposed to just using sql, i think  dedicated sql pool is overkill. BUT,  i dont think azure SQL has all of  the virtualisation tech like serverless or dedicated sql pool has, and  my aggregations might take longer, but i think i can fine tune  a lot to  make that work (and azure sql database is MUCHHHH cheaper). If i look  at the pricing of azure sql server managed instance (which from my  understanding does have the virtualisation tech) its around the same as  the dedicated sql pool, so doesnt seem worth to me.\n\nhow  are ya'll doing this in a cost effective way? There are so many  buzzwords and buzz tech out there, im not sure what i should be looking at. The term Synapse is super confusing because it seems interchangable with serverless or dedicated sql pool, when it also can act as an orchestration tool. I seem a lot of people saying databricks + adl + datafactory is the  way to go, but then there is the added expensive of databricks and learning databricks.. I am  also a 1 man team on the reporting and data side, and i am also managing  another separate team on top of this (lol the tech recession is real).  point is, i feel very lost, and i dont want to spend a bunch of time migrating to a solution that wont scale or isnt cost effective. I appreciate any direction and advice you  fellow data engineers can give.\n\nThanks!", "author_fullname": "t2_jrmn04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions to migrate off of dedicated SQL Pool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170ltz3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696522062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Posted this over in &lt;a href=\"/r/Azure\"&gt;/r/Azure&lt;/a&gt; , but wanted to get yawls perspective too. &lt;/p&gt;\n\n&lt;p&gt;I  am looking to migrate off of dedicated sql pool - it is too expensive. We are currently using  azure synapse studio + dedicated sql pool + adl. here is some workflows  we do:&lt;/p&gt;\n\n&lt;p&gt;1) incremental data loads from transnational sql databases. we load this data pretty much straight into the dwh.&lt;/p&gt;\n\n&lt;p&gt;2) about  5-6 external sources we pull data via an API or maybe a excel  spreadsheet/csv for adhoc data from my customer facing teams. we load  the data from the external sources into our data lake. I just use azure  batch to execute python code that does this for me. from there we create  external tables to load them into our dedicated sql pool.&lt;/p&gt;\n\n&lt;p&gt;3) once  all the data is loaded into the sql pool, we then do various  transformations and aggregations on the data, and store the results in  sql, which are then loaded into our PBI data model. The refreshes happen  1x per day, typically around 4am.&lt;/p&gt;\n\n&lt;p&gt;this  all being said, dedicated sql pools are SUPER expensive. I do like  being able to load the external sources directly into sql, so i was  thinking about migrating off of dedicated sql pool and just using  serverless sql in synapse. We would just export data from our  transactional sql databases onto the lake in parquet format. One of my  concerns with this is everything would essentially be views, as i dont  think you can create/manage the transformations using CTAS statements.  This doesnt seem like it would scale well as we continue to get more  data.&lt;/p&gt;\n\n&lt;p&gt;Honestly, were around 266gb  of data total, so i wouldnt be opposed to just using sql, i think  dedicated sql pool is overkill. BUT,  i dont think azure SQL has all of  the virtualisation tech like serverless or dedicated sql pool has, and  my aggregations might take longer, but i think i can fine tune  a lot to  make that work (and azure sql database is MUCHHHH cheaper). If i look  at the pricing of azure sql server managed instance (which from my  understanding does have the virtualisation tech) its around the same as  the dedicated sql pool, so doesnt seem worth to me.&lt;/p&gt;\n\n&lt;p&gt;how  are ya&amp;#39;ll doing this in a cost effective way? There are so many  buzzwords and buzz tech out there, im not sure what i should be looking at. The term Synapse is super confusing because it seems interchangable with serverless or dedicated sql pool, when it also can act as an orchestration tool. I seem a lot of people saying databricks + adl + datafactory is the  way to go, but then there is the added expensive of databricks and learning databricks.. I am  also a 1 man team on the reporting and data side, and i am also managing  another separate team on top of this (lol the tech recession is real).  point is, i feel very lost, and i dont want to spend a bunch of time migrating to a solution that wont scale or isnt cost effective. I appreciate any direction and advice you  fellow data engineers can give.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170ltz3", "is_robot_indexable": true, "report_reasons": null, "author": "soricellia", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170ltz3/solutions_to_migrate_off_of_dedicated_sql_pool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170ltz3/solutions_to_migrate_off_of_dedicated_sql_pool/", "subreddit_subscribers": 132386, "created_utc": 1696522062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Saw this post about how to get similar functionality to dbt Cloud using other OSS tools, wondering if anyone has done this and what your experience has been\n\n  \ntl;dr  \ndbt Cloud or dbt Core + vs code + airflow + cube.dev\n\nhttps://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Cloud extra features worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170l0nw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696520038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this post about how to get similar functionality to dbt Cloud using other OSS tools, wondering if anyone has done this and what your experience has been&lt;/p&gt;\n\n&lt;p&gt;tl;dr&lt;br/&gt;\ndbt Cloud or dbt Core + vs code + airflow + cube.dev&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences\"&gt;https://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?auto=webp&amp;s=d3cc503fb34529b9e99c792a7117e5fc053ac06b", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6aedf9503b00fdb31f500fd840357bf700d9507", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec9a9837c66cb33c17898fd385442393eebb33b7", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=829d1b2e30c6aedbf56d477f7a8419d039d4be73", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a675320e601b25b36e1e6ac4f051e0b27f475414", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6a488a73fbc7d27fce74b0ca11a7b2c828a39519", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=099e281cdfcfe53d01e3290cb938387eb3cee441", "width": 1080, "height": 564}], "variants": {}, "id": "xUIE53y-az5HrMPPgq4ndFAPYxY_HJJQd0C1S2OtT0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170l0nw", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170l0nw/dbt_cloud_extra_features_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170l0nw/dbt_cloud_extra_features_worth_it/", "subreddit_subscribers": 132386, "created_utc": 1696520038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y9qpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source scraping API that returns structured JSON data using GPT.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170m9lu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696523115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/semanser/JsonGenius", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "170m9lu", "is_robot_indexable": true, "report_reasons": null, "author": "semanser", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170m9lu/i_built_an_opensource_scraping_api_that_returns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/semanser/JsonGenius", "subreddit_subscribers": 132386, "created_utc": 1696523115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, there!\n\nI am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.\n\nHere is my current architecture more or less:\n\n* Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response\n* I ingest that response data and pass the json data string into a message queue (azure service bus)\n* write the given event (row if you will) to a Azure data lake storage container called \"ingestion\"\n* move on\n* Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables\n* return the data in a data frame\n\n&amp;#x200B;\n\nI have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:\n\n&amp;#x200B;\n\n* If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record\n* If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row\n* Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.\n\nUltimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.\n\n&amp;#x200B;\n\nAm I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.\n\nAs I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.\n\nI appreciate your wisdom and guidance in advance!\n\n&amp;#x200B;\n\nCheers,\n\nIsaiah", "author_fullname": "t2_92b8a2oo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upsert Parquet Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171168w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696560398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, there!&lt;/p&gt;\n\n&lt;p&gt;I am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.&lt;/p&gt;\n\n&lt;p&gt;Here is my current architecture more or less:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response&lt;/li&gt;\n&lt;li&gt;I ingest that response data and pass the json data string into a message queue (azure service bus)&lt;/li&gt;\n&lt;li&gt;write the given event (row if you will) to a Azure data lake storage container called &amp;quot;ingestion&amp;quot;&lt;/li&gt;\n&lt;li&gt;move on&lt;/li&gt;\n&lt;li&gt;Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables&lt;/li&gt;\n&lt;li&gt;return the data in a data frame&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record&lt;/li&gt;\n&lt;li&gt;If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row&lt;/li&gt;\n&lt;li&gt;Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ultimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Am I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.&lt;/p&gt;\n\n&lt;p&gt;As I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.&lt;/p&gt;\n\n&lt;p&gt;I appreciate your wisdom and guidance in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Isaiah&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171168w", "is_robot_indexable": true, "report_reasons": null, "author": "realeyezayuh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171168w/upsert_parquet_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171168w/upsert_parquet_data/", "subreddit_subscribers": 132386, "created_utc": 1696560398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.\n\nThe strings always have the same format, such as \"MM2525 6562451 Some text\". So from this string I would only want the \"6562451\" bit. I tried to find regular expressions like python but got no luck. Thanks in advance!\n\nP.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting digits from a string column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171a9o6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696593077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.&lt;/p&gt;\n\n&lt;p&gt;The strings always have the same format, such as &amp;quot;MM2525 6562451 Some text&amp;quot;. So from this string I would only want the &amp;quot;6562451&amp;quot; bit. I tried to find regular expressions like python but got no luck. Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;P.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171a9o6", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "subreddit_subscribers": 132386, "created_utc": 1696593077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for an ecommerce company and ever since I started working here about a year ago, I've seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it's customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what's worst is that backend team is so distant to data engineers, that we can't even access their repos or confluence pages. \n\nBecause of this, my new team built (many years ago) a customer identifier, let's call it customer\\_id, which is basically just a hashed value of a bunch of other ids (like user\\_checkout\\_id or anonymous\\_checkout\\_id). This customer\\_id is what's used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer\\_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer\\_id has also left the company years ago and the code is extremely complex.\n\nWe are planning to do a refactor soon but don't really know what's the best practice here. I'm guessing you should ignore some errors based on some threshold and have some uniformity. \n\nhas anyone got any experience on this? any books or blogs? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to deal with complex user_id modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1717jgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696583310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for an ecommerce company and ever since I started working here about a year ago, I&amp;#39;ve seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it&amp;#39;s customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what&amp;#39;s worst is that backend team is so distant to data engineers, that we can&amp;#39;t even access their repos or confluence pages. &lt;/p&gt;\n\n&lt;p&gt;Because of this, my new team built (many years ago) a customer identifier, let&amp;#39;s call it customer_id, which is basically just a hashed value of a bunch of other ids (like user_checkout_id or anonymous_checkout_id). This customer_id is what&amp;#39;s used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer_id has also left the company years ago and the code is extremely complex.&lt;/p&gt;\n\n&lt;p&gt;We are planning to do a refactor soon but don&amp;#39;t really know what&amp;#39;s the best practice here. I&amp;#39;m guessing you should ignore some errors based on some threshold and have some uniformity. &lt;/p&gt;\n\n&lt;p&gt;has anyone got any experience on this? any books or blogs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1717jgi", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "subreddit_subscribers": 132386, "created_utc": 1696583310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don't feel good about this.", "author_fullname": "t2_cj0b4yt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170twld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don&amp;#39;t feel good about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170twld", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Most564", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170twld/data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170twld/data_catalog/", "subreddit_subscribers": 132386, "created_utc": 1696541230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery *and* airflow running.\n\nLong version: \n\nWe have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it's very basic; it reads data from multiple sources, normalizes them and puts em in our database.\n\nBut now that we're in the throes of airflow, we've found it to be a little... wanting. It's pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.\n\nAnother huge con is that we have a lot code that could be shared between our regular ol' django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.\n\nSO now I'm wanting to put our python back on our server, but I'd still like to schedule the tasks with airflow. I guess I'd make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don't want to just have it be a regular http endpoint.\n\nShould I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do initiate long-running jobs in a django server from airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170s9n6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696537347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery &lt;em&gt;and&lt;/em&gt; airflow running.&lt;/p&gt;\n\n&lt;p&gt;Long version: &lt;/p&gt;\n\n&lt;p&gt;We have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it&amp;#39;s very basic; it reads data from multiple sources, normalizes them and puts em in our database.&lt;/p&gt;\n\n&lt;p&gt;But now that we&amp;#39;re in the throes of airflow, we&amp;#39;ve found it to be a little... wanting. It&amp;#39;s pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.&lt;/p&gt;\n\n&lt;p&gt;Another huge con is that we have a lot code that could be shared between our regular ol&amp;#39; django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.&lt;/p&gt;\n\n&lt;p&gt;SO now I&amp;#39;m wanting to put our python back on our server, but I&amp;#39;d still like to schedule the tasks with airflow. I guess I&amp;#39;d make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don&amp;#39;t want to just have it be a regular http endpoint.&lt;/p&gt;\n\n&lt;p&gt;Should I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170s9n6", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "subreddit_subscribers": 132386, "created_utc": 1696537347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to use shipyard to automate a bunch of PostgreSQL procedures. I\u2019m using shipyard as the data orchestrator. Their help desk support response is good, but it\u2019s taken 2 days for them so far not to come up with a solution. The problem \u2026it seems you can do all the pretty stuff like set up a connection to PostgreSQL\u2026and even when you grant execute on the user for PostgreSQL and for the specific procedures to run\u2026shipyard does not run the procedures!\n\nAm I missing something simple (probably!)?\n\nAre there any alternatives to use (airflow and prefect are a tad too involved for me right now\u2026a user interface for the main is what I\u2019m looking to use\u2026just to schedule and keep tabs on the jobs)\n\nCheers", "author_fullname": "t2_hg12i599c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shipyard - PostgreSQL fleet, not working", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170j5u8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696515444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to use shipyard to automate a bunch of PostgreSQL procedures. I\u2019m using shipyard as the data orchestrator. Their help desk support response is good, but it\u2019s taken 2 days for them so far not to come up with a solution. The problem \u2026it seems you can do all the pretty stuff like set up a connection to PostgreSQL\u2026and even when you grant execute on the user for PostgreSQL and for the specific procedures to run\u2026shipyard does not run the procedures!&lt;/p&gt;\n\n&lt;p&gt;Am I missing something simple (probably!)?&lt;/p&gt;\n\n&lt;p&gt;Are there any alternatives to use (airflow and prefect are a tad too involved for me right now\u2026a user interface for the main is what I\u2019m looking to use\u2026just to schedule and keep tabs on the jobs)&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170j5u8", "is_robot_indexable": true, "report_reasons": null, "author": "BumblyWurzle", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170j5u8/shipyard_postgresql_fleet_not_working/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170j5u8/shipyard_postgresql_fleet_not_working/", "subreddit_subscribers": 132386, "created_utc": 1696515444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On the surface it's a basic ETL problem but with some additional complications. I'm curious how other experienced engineers would approach this problem to see if I'm overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.\n\nContext\n\n* 200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they're not NDJSON.\n* The files have to parsed so we can transform them into a unified schema and remove the data we're not interested in.\n* Four of the fields have to be enriched.  \n\n   * We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.\n   * There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.\n   * There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don't already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party\n   * Last we need to call another internal api that can scale to any rps.\n* Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it's accessible but not searchable.\n* We'll have to run this process monthly.", "author_fullname": "t2_42grp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach parsing, enriching, and storing 200M JSON records?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171a00g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696592257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the surface it&amp;#39;s a basic ETL problem but with some additional complications. I&amp;#39;m curious how other experienced engineers would approach this problem to see if I&amp;#39;m overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.&lt;/p&gt;\n\n&lt;p&gt;Context&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they&amp;#39;re not NDJSON.&lt;/li&gt;\n&lt;li&gt;The files have to parsed so we can transform them into a unified schema and remove the data we&amp;#39;re not interested in.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Four of the fields have to be enriched.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.&lt;/li&gt;\n&lt;li&gt;There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.&lt;/li&gt;\n&lt;li&gt;There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don&amp;#39;t already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party&lt;/li&gt;\n&lt;li&gt;Last we need to call another internal api that can scale to any rps.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it&amp;#39;s accessible but not searchable.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ll have to run this process monthly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171a00g", "is_robot_indexable": true, "report_reasons": null, "author": "Detz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "subreddit_subscribers": 132386, "created_utc": 1696592257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: [Enabling static analysis of SQL queries at Meta - (fb.com)](https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/)\n\nAlso would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Static analysers for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1719j9d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696590729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: &lt;a href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\"&gt;Enabling static analysis of SQL queries at Meta - (fb.com)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?auto=webp&amp;s=909387532145f6d6f4319d06cb1d6208fd43c145", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0efd775c5705334b59afbea64c2e78caed9ad31e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe42988ed379e8ee52fae618a3da0840033d3a52", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7c6656435d7d9d9f8c61b4b73fc8c8c24758a35", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0ded8d78913a04e500afd810ac273773bb3f4a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b3a60647f47ba03c6686405d656724bf8664f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13f763cc226b0b456ab56df4143da9da2d4db438", "width": 1080, "height": 607}], "variants": {}, "id": "Ven08R7rW3VUewldA00PYwtDrsf5TziOv0R5SeV6HyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1719j9d", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "subreddit_subscribers": 132386, "created_utc": 1696590729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI worked few years as a data strategist (bridge between business and data scientist to drive use-cases/opportunities end-to-end). Given most issues are at data engineering side, I was considering moving myself to the field (more technical role). Most companies need good data engineering before even considering use-cases...\n\nI was wondering what would be the best ressources/platform to start this transition. It is sometimes a bit overwhelming to know where to start. I have good knowledge on data science in general but need more technical depth.\n\n&amp;#x200B;", "author_fullname": "t2_igwxepcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning to data engineering from data generalist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1717b8z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696582391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I worked few years as a data strategist (bridge between business and data scientist to drive use-cases/opportunities end-to-end). Given most issues are at data engineering side, I was considering moving myself to the field (more technical role). Most companies need good data engineering before even considering use-cases...&lt;/p&gt;\n\n&lt;p&gt;I was wondering what would be the best ressources/platform to start this transition. It is sometimes a bit overwhelming to know where to start. I have good knowledge on data science in general but need more technical depth.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1717b8z", "is_robot_indexable": true, "report_reasons": null, "author": "ImpMas6918", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1717b8z/transitioning_to_data_engineering_from_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1717b8z/transitioning_to_data_engineering_from_data/", "subreddit_subscribers": 132386, "created_utc": 1696582391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nAt my current job as BI Analyst we use MS Access for storing all kinds of data, and we use macros and VBA scripts to do some transformations to create outputs used in Tableau dashboards and Excel reports.\n\nI got a new job where I'll be using SQL Server Management Studio, so I'm looking for a hands-on tutorial that will get me comfortable with the UI and with using stored procedures, temporary tables, views, and any other important things for ETL and analytics pipelines. As you probably know, Access doesn't have those things, since it relies on executing various \"saved queries\" one after the other via a macro.\n\nI'm fairly confident in my ability to write good SQL queries, so I'm not very interested in practicing a bunch of select queries, and would prefer to get into stuff like database design, user-defined functions, triggers, temporary tables, views and stored procedures, specifically with the SSMS UI.\n\nThis course looks like it might fit the bill, but interested to hear recommendations: [https://www.udemy.com/course/advanced-sql-server-masterclass-for-data-analysis/](https://www.udemy.com/course/advanced-sql-server-masterclass-for-data-analysis/)", "author_fullname": "t2_ttuw2ar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tutorial to transition from Access to SSMS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1715ba5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696574400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;At my current job as BI Analyst we use MS Access for storing all kinds of data, and we use macros and VBA scripts to do some transformations to create outputs used in Tableau dashboards and Excel reports.&lt;/p&gt;\n\n&lt;p&gt;I got a new job where I&amp;#39;ll be using SQL Server Management Studio, so I&amp;#39;m looking for a hands-on tutorial that will get me comfortable with the UI and with using stored procedures, temporary tables, views, and any other important things for ETL and analytics pipelines. As you probably know, Access doesn&amp;#39;t have those things, since it relies on executing various &amp;quot;saved queries&amp;quot; one after the other via a macro.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly confident in my ability to write good SQL queries, so I&amp;#39;m not very interested in practicing a bunch of select queries, and would prefer to get into stuff like database design, user-defined functions, triggers, temporary tables, views and stored procedures, specifically with the SSMS UI.&lt;/p&gt;\n\n&lt;p&gt;This course looks like it might fit the bill, but interested to hear recommendations: &lt;a href=\"https://www.udemy.com/course/advanced-sql-server-masterclass-for-data-analysis/\"&gt;https://www.udemy.com/course/advanced-sql-server-masterclass-for-data-analysis/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1715ba5", "is_robot_indexable": true, "report_reasons": null, "author": "Prudent-Draft-9193", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1715ba5/tutorial_to_transition_from_access_to_ssms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1715ba5/tutorial_to_transition_from_access_to_ssms/", "subreddit_subscribers": 132386, "created_utc": 1696574400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I've been working as a data analyst for 7 months now at a public university. I've built what I would consider fairly substantial Python / SQL ETL programs to automate data processes, built a ton of Tableau visualizations using calculated fields, table calculations, joining external data, etc. I've become super interested specifically in ETL pipelines and data engineering in general, so I just recently studied for and passed the AZ-900 cert, and am currently studying for DP-900.\n\nI'm having absolutely no luck applying for all sorts of entry-level ETL / junior data engineering jobs over the past few months (both remote and on-site), so I've considered getting a masters in comp sci, moving to a higher pop city (somewhere like Chicago or Kansas City), delving deep into more advanced certs, etc. Curious if anyone has any guidance they can offer?", "author_fullname": "t2_6f5mo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Guidance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17121ar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696562924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been working as a data analyst for 7 months now at a public university. I&amp;#39;ve built what I would consider fairly substantial Python / SQL ETL programs to automate data processes, built a ton of Tableau visualizations using calculated fields, table calculations, joining external data, etc. I&amp;#39;ve become super interested specifically in ETL pipelines and data engineering in general, so I just recently studied for and passed the AZ-900 cert, and am currently studying for DP-900.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having absolutely no luck applying for all sorts of entry-level ETL / junior data engineering jobs over the past few months (both remote and on-site), so I&amp;#39;ve considered getting a masters in comp sci, moving to a higher pop city (somewhere like Chicago or Kansas City), delving deep into more advanced certs, etc. Curious if anyone has any guidance they can offer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17121ar", "is_robot_indexable": true, "report_reasons": null, "author": "Snyda", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17121ar/career_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17121ar/career_guidance/", "subreddit_subscribers": 132386, "created_utc": 1696562924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone, we are working on a POC where we want to stop physicalization of data as much as possible, Since we use databricks it would mean using delta tables for as few objects as possible and views for everything else. We follow a meddalion architecture so bronze data will be backed by external tables but we are trying for our silver layer to have views where possible. \n\nWhat are the parameters we should consider when choosing when something should be created as a delta table vs as a view and come to an informed decision in terms of compute, storage, etc.", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quantifying when to use views vs tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1711pnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696561954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone, we are working on a POC where we want to stop physicalization of data as much as possible, Since we use databricks it would mean using delta tables for as few objects as possible and views for everything else. We follow a meddalion architecture so bronze data will be backed by external tables but we are trying for our silver layer to have views where possible. &lt;/p&gt;\n\n&lt;p&gt;What are the parameters we should consider when choosing when something should be created as a delta table vs as a view and come to an informed decision in terms of compute, storage, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1711pnv", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1711pnv/quantifying_when_to_use_views_vs_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1711pnv/quantifying_when_to_use_views_vs_tables/", "subreddit_subscribers": 132386, "created_utc": 1696561954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello everyone,\n\nI've been tasked with a project that involves:\n\n1. **Data Source:** Extracting metrics from a Google Managed Prometheus, which is monitoring over N GKE clusters.\n2. **Data Storage:** Loading these metrics into BigQuery.\n3. **Visualization:** Displaying the data using Grafana.\n\n**Key Information:**\n\n* **Programming Language:** My manager suggests using Golang for this task.\n* **Current Approach:**\n   * Write a custom Golang client to scrape metrics from Google Managed Prometheus via its HTTP endpoints.\n   * Load the scraped data into BigQuery. Planning to cluster the table based on specific requested metrics.\n   * Finally, link this BigQuery table to a Grafana dashboard.\n\nI'm fairly new to this (a junior data engineer) and working solo. So, I'd deeply appreciate insights on:\n\n* What potential challenges or considerations I might encounter?\n* Is there a more efficient or recommended way to tackle this project?\n\nThank you in advance for any guidance or suggestions!", "author_fullname": "t2_386krewr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Guidance on Extracting Metrics from Google Managed Prometheus to BigQuery for Grafana Display", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170zkf3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696555821.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked with a project that involves:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Data Source:&lt;/strong&gt; Extracting metrics from a Google Managed Prometheus, which is monitoring over N GKE clusters.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Storage:&lt;/strong&gt; Loading these metrics into BigQuery.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Visualization:&lt;/strong&gt; Displaying the data using Grafana.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Information:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Programming Language:&lt;/strong&gt; My manager suggests using Golang for this task.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Current Approach:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Write a custom Golang client to scrape metrics from Google Managed Prometheus via its HTTP endpoints.&lt;/li&gt;\n&lt;li&gt;Load the scraped data into BigQuery. Planning to cluster the table based on specific requested metrics.&lt;/li&gt;\n&lt;li&gt;Finally, link this BigQuery table to a Grafana dashboard.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m fairly new to this (a junior data engineer) and working solo. So, I&amp;#39;d deeply appreciate insights on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What potential challenges or considerations I might encounter?&lt;/li&gt;\n&lt;li&gt;Is there a more efficient or recommended way to tackle this project?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you in advance for any guidance or suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170zkf3", "is_robot_indexable": true, "report_reasons": null, "author": "Lord_Gonz0", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/170zkf3/need_guidance_on_extracting_metrics_from_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170zkf3/need_guidance_on_extracting_metrics_from_google/", "subreddit_subscribers": 132386, "created_utc": 1696555821.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to integrate a data workload tool with my web app's API, which I know is a fairly non-standard thing to do... but the workloads can be expensive (in terms of $$, e.g. calling OpenAI), so I want to bring good data eng practices along.\n\nI've been playing around with Prefect and Benthos, and surprised to find that when you trigger a workflow that is set to cache results on a given input, there is no locking mechanism to ensure that concurrent requests for the same given input do not result in more than one computation. i.e. while the first request is being handled, subsequent requests should not trigger a new compute but rather wait for the result of the first one.\n\nThis is a problem primarily due to the tight integration with the web app API where users can trigger workloads that are set up to lazy-evaluate / lazy-compute certain data outputs\n\nDoes anyone know of any tools that do solve this cache stampede issue? And is a data eng tool what I should be using, or does it sound more like I am reaching for the wrong set of tooling?", "author_fullname": "t2_vep5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Caching workloads + concurrent requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170ycyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696552391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to integrate a data workload tool with my web app&amp;#39;s API, which I know is a fairly non-standard thing to do... but the workloads can be expensive (in terms of $$, e.g. calling OpenAI), so I want to bring good data eng practices along.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with Prefect and Benthos, and surprised to find that when you trigger a workflow that is set to cache results on a given input, there is no locking mechanism to ensure that concurrent requests for the same given input do not result in more than one computation. i.e. while the first request is being handled, subsequent requests should not trigger a new compute but rather wait for the result of the first one.&lt;/p&gt;\n\n&lt;p&gt;This is a problem primarily due to the tight integration with the web app API where users can trigger workloads that are set up to lazy-evaluate / lazy-compute certain data outputs&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any tools that do solve this cache stampede issue? And is a data eng tool what I should be using, or does it sound more like I am reaching for the wrong set of tooling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170ycyq", "is_robot_indexable": true, "report_reasons": null, "author": "matty_fu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170ycyq/caching_workloads_concurrent_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170ycyq/caching_workloads_concurrent_requests/", "subreddit_subscribers": 132386, "created_utc": 1696552391.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}