{"kind": "Listing", "data": {"after": "t3_170wvxc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies as my background isn't in DE but come more from BI systems administration. In my career I've seen a lot but the craziest thing I've seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?\n\nIs it literally just because our system doesn't have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it's 1995? Couldn't a REST API send a CSV over HTTPS anyway? Do we not trust it?\n\nIs data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?\n\nI feel like there's something I'm fundamentally misunderstanding but not sure what it is...", "author_fullname": "t2_6c2aryt5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the reason SFTP file transfer in banks, healthcare, etc is so ubiquitous just because they are using older systems without robust REST APIs...?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170rvz3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696536440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies as my background isn&amp;#39;t in DE but come more from BI systems administration. In my career I&amp;#39;ve seen a lot but the craziest thing I&amp;#39;ve seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?&lt;/p&gt;\n\n&lt;p&gt;Is it literally just because our system doesn&amp;#39;t have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it&amp;#39;s 1995? Couldn&amp;#39;t a REST API send a CSV over HTTPS anyway? Do we not trust it?&lt;/p&gt;\n\n&lt;p&gt;Is data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?&lt;/p&gt;\n\n&lt;p&gt;I feel like there&amp;#39;s something I&amp;#39;m fundamentally misunderstanding but not sure what it is...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170rvz3", "is_robot_indexable": true, "report_reasons": null, "author": "TheWikiJedi", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "subreddit_subscribers": 132430, "created_utc": 1696536440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear fellow Data Engineers\n\nYesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.\n\nThen a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.\n\nI stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.\n\nToday, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.\n\nI do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. \n\nTo summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?\n\nI am curious to hear about your experience!", "author_fullname": "t2_ou2h13bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backend Skills for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170vj3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696545043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear fellow Data Engineers&lt;/p&gt;\n\n&lt;p&gt;Yesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.&lt;/p&gt;\n\n&lt;p&gt;Then a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.&lt;/p&gt;\n\n&lt;p&gt;I stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.&lt;/p&gt;\n\n&lt;p&gt;Today, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.&lt;/p&gt;\n\n&lt;p&gt;I do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. &lt;/p&gt;\n\n&lt;p&gt;To summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?&lt;/p&gt;\n\n&lt;p&gt;I am curious to hear about your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "170vj3u", "is_robot_indexable": true, "report_reasons": null, "author": "Present_Salt_1688", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "subreddit_subscribers": 132430, "created_utc": 1696545043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was a director of data engineering after a datascience background and was offered a job a sr architect \n\nAs a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books\n\nI\u2019m sure I have knowledge gaps. What should I read up on ?", "author_fullname": "t2_l5fycnwh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer to data architect. What do I need to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170us8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696543281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was a director of data engineering after a datascience background and was offered a job a sr architect &lt;/p&gt;\n\n&lt;p&gt;As a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure I have knowledge gaps. What should I read up on ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "170us8m", "is_robot_indexable": true, "report_reasons": null, "author": "updated-quality-485", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "subreddit_subscribers": 132430, "created_utc": 1696543281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:\n\n1. A CRM file is uploaded to S3.\n2. A *sensor* notices the new file and creates a new *dynamic partition*.\n3. The file is then processed and stored to relational database. This would be the first *asset*.\n4. Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two *assets* that depend on the first *asset*.\n5. When both these *assets* are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two *assets*.\n6. Another *sensor* will check whether the last asset has been materialised and subsequently a *job* will be kicked of that is going to send an email, with a download link, to the user.\n\nMainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!", "author_fullname": "t2_trs7vyx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster ETL setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1715bbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696574403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A CRM file is uploaded to S3.&lt;/li&gt;\n&lt;li&gt;A &lt;em&gt;sensor&lt;/em&gt; notices the new file and creates a new &lt;em&gt;dynamic partition&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;The file is then processed and stored to relational database. This would be the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two &lt;em&gt;assets&lt;/em&gt; that depend on the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;When both these &lt;em&gt;assets&lt;/em&gt; are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two &lt;em&gt;assets&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Another &lt;em&gt;sensor&lt;/em&gt; will check whether the last asset has been materialised and subsequently a &lt;em&gt;job&lt;/em&gt; will be kicked of that is going to send an email, with a download link, to the user.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Mainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1715bbb", "is_robot_indexable": true, "report_reasons": null, "author": "WeddingIndependent30", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "subreddit_subscribers": 132430, "created_utc": 1696574403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have\n\nCode Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks\n\nData lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. \n\nOne of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. \n\nContour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. \n\nOntology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. \n\nSlate - garbage piece of shit UI tech \n\nData Health - kinda useful confusing to use \n\nThey are the principal tools I use everyday in Palantir Foundry \n\nGeneral thoughts:\n\n- it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. \n\n- Data Expectations &amp; testing implementation is really good. \n\n- Their technical support seem pretty good. Their engineers are intimidatingly good. \n\n- really easy to find and search other datasets that could be related.\n\n- Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.\n\nHas anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.\n\nEdit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026", "author_fullname": "t2_nutp89h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How applicable is Palantir foundry to Data engineering in general?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170u6u8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have&lt;/p&gt;\n\n&lt;p&gt;Code Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks&lt;/p&gt;\n\n&lt;p&gt;Data lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. &lt;/p&gt;\n\n&lt;p&gt;One of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. &lt;/p&gt;\n\n&lt;p&gt;Contour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. &lt;/p&gt;\n\n&lt;p&gt;Ontology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. &lt;/p&gt;\n\n&lt;p&gt;Slate - garbage piece of shit UI tech &lt;/p&gt;\n\n&lt;p&gt;Data Health - kinda useful confusing to use &lt;/p&gt;\n\n&lt;p&gt;They are the principal tools I use everyday in Palantir Foundry &lt;/p&gt;\n\n&lt;p&gt;General thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Expectations &amp;amp; testing implementation is really good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Their technical support seem pretty good. Their engineers are intimidatingly good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;really easy to find and search other datasets that could be related.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.&lt;/p&gt;\n\n&lt;p&gt;Edit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170u6u8", "is_robot_indexable": true, "report_reasons": null, "author": "hositir", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "subreddit_subscribers": 132430, "created_utc": 1696541897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m thinking of taking part of a Data Engineering boot camp. I work as a BI developer and have a pretty solid foundation in relational databases, specifically SQL Server.\n\nI\u2019m hoping practicing Data Engineer\u2019s will provide thoughts/ a review of the boot camp based on the syllabus image I\u2019ve attached.\n\nImmediate questions that come to mind:\n\n* Is the tech stack covered in high demand?\n* Are you currently working with any of these tools or would you advise that it\u2019s for a niche role? Maybe the tech stack is too general.\n\nAny thoughts, feedback, or advice is appreciated.", "author_fullname": "t2_vjz1hul6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking input on a DE boot camp from practicing DE\u2019s.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_171eoye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tP4XyebbNWuwtZDXXfk3QtpoiJIeq4E1V6JUbV12jdU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696604708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m thinking of taking part of a Data Engineering boot camp. I work as a BI developer and have a pretty solid foundation in relational databases, specifically SQL Server.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m hoping practicing Data Engineer\u2019s will provide thoughts/ a review of the boot camp based on the syllabus image I\u2019ve attached.&lt;/p&gt;\n\n&lt;p&gt;Immediate questions that come to mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is the tech stack covered in high demand?&lt;/li&gt;\n&lt;li&gt;Are you currently working with any of these tools or would you advise that it\u2019s for a niche role? Maybe the tech stack is too general.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any thoughts, feedback, or advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fb1b310vklsb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?auto=webp&amp;s=504df79d4330bd3a494efc5354b254ff9cad5b4d", "width": 795, "height": 1442}, "resolutions": [{"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad0c25dc490e8f029d618fdcc4bd974092e8577e", "width": 108, "height": 195}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bde5d0f22aae297620fe32a65071ca11f093370", "width": 216, "height": 391}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de747a7722aff5905fe4de274296a552c4369a98", "width": 320, "height": 580}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cd56a2014272e1d42f57d7a10c73f58899ed8c1", "width": 640, "height": 1160}], "variants": {}, "id": "YHgSUNNGdKFvbZ8I4CujIeJaw2aO9a12Ztld64uBplc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171eoye", "is_robot_indexable": true, "report_reasons": null, "author": "FisticuffMetal", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171eoye/seeking_input_on_a_de_boot_camp_from_practicing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fb1b310vklsb1.jpg", "subreddit_subscribers": 132430, "created_utc": 1696604708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On the surface it's a basic ETL problem but with some additional complications. I'm curious how other experienced engineers would approach this problem to see if I'm overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.\n\nContext\n\n* 200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they're not NDJSON.\n* The files have to parsed so we can transform them into a unified schema and remove the data we're not interested in.\n* Four of the fields have to be enriched.  \n\n   * We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.\n   * There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.\n   * There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don't already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party\n   * Last we need to call another internal api that can scale to any rps.\n* Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it's accessible but not searchable.\n* We'll have to run this process monthly.", "author_fullname": "t2_42grp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach parsing, enriching, and storing 200M JSON records?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a00g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696592257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the surface it&amp;#39;s a basic ETL problem but with some additional complications. I&amp;#39;m curious how other experienced engineers would approach this problem to see if I&amp;#39;m overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.&lt;/p&gt;\n\n&lt;p&gt;Context&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they&amp;#39;re not NDJSON.&lt;/li&gt;\n&lt;li&gt;The files have to parsed so we can transform them into a unified schema and remove the data we&amp;#39;re not interested in.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Four of the fields have to be enriched.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.&lt;/li&gt;\n&lt;li&gt;There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.&lt;/li&gt;\n&lt;li&gt;There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don&amp;#39;t already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party&lt;/li&gt;\n&lt;li&gt;Last we need to call another internal api that can scale to any rps.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it&amp;#39;s accessible but not searchable.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ll have to run this process monthly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171a00g", "is_robot_indexable": true, "report_reasons": null, "author": "Detz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "subreddit_subscribers": 132430, "created_utc": 1696592257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:\n\n\\- Airbyte for extracting data and insert into my data warehouse( Snowflake)\n\n\\- DBT Cloud for transformation \n\n\\- A PostgresDB for OLTP, which will be accessed by the user facing API and UI\n\nData freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say `A` and `B` transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don't have to worry about write.\n\nI can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?\n\n&amp;#x200B;", "author_fullname": "t2_ap7kqtwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools to use for extracting data from OLAP back to OLTP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171dzqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696603059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:&lt;/p&gt;\n\n&lt;p&gt;- Airbyte for extracting data and insert into my data warehouse( Snowflake)&lt;/p&gt;\n\n&lt;p&gt;- DBT Cloud for transformation &lt;/p&gt;\n\n&lt;p&gt;- A PostgresDB for OLTP, which will be accessed by the user facing API and UI&lt;/p&gt;\n\n&lt;p&gt;Data freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don&amp;#39;t have to worry about write.&lt;/p&gt;\n\n&lt;p&gt;I can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171dzqk", "is_robot_indexable": true, "report_reasons": null, "author": "hksparrowboy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "subreddit_subscribers": 132430, "created_utc": 1696603059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for an ecommerce company and ever since I started working here about a year ago, I've seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it's customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what's worst is that backend team is so distant to data engineers, that we can't even access their repos or confluence pages. \n\nBecause of this, my new team built (many years ago) a customer identifier, let's call it customer\\_id, which is basically just a hashed value of a bunch of other ids (like user\\_checkout\\_id or anonymous\\_checkout\\_id). This customer\\_id is what's used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer\\_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer\\_id has also left the company years ago and the code is extremely complex.\n\nWe are planning to do a refactor soon but don't really know what's the best practice here. I'm guessing you should ignore some errors based on some threshold and have some uniformity. \n\nhas anyone got any experience on this? any books or blogs? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to deal with complex user_id modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1717jgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696583310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for an ecommerce company and ever since I started working here about a year ago, I&amp;#39;ve seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it&amp;#39;s customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what&amp;#39;s worst is that backend team is so distant to data engineers, that we can&amp;#39;t even access their repos or confluence pages. &lt;/p&gt;\n\n&lt;p&gt;Because of this, my new team built (many years ago) a customer identifier, let&amp;#39;s call it customer_id, which is basically just a hashed value of a bunch of other ids (like user_checkout_id or anonymous_checkout_id). This customer_id is what&amp;#39;s used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer_id has also left the company years ago and the code is extremely complex.&lt;/p&gt;\n\n&lt;p&gt;We are planning to do a refactor soon but don&amp;#39;t really know what&amp;#39;s the best practice here. I&amp;#39;m guessing you should ignore some errors based on some threshold and have some uniformity. &lt;/p&gt;\n\n&lt;p&gt;has anyone got any experience on this? any books or blogs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1717jgi", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "subreddit_subscribers": 132430, "created_utc": 1696583310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: [Enabling static analysis of SQL queries at Meta - (fb.com)](https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/)\n\nAlso would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Static analysers for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1719j9d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696590729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: &lt;a href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\"&gt;Enabling static analysis of SQL queries at Meta - (fb.com)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?auto=webp&amp;s=909387532145f6d6f4319d06cb1d6208fd43c145", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0efd775c5705334b59afbea64c2e78caed9ad31e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe42988ed379e8ee52fae618a3da0840033d3a52", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7c6656435d7d9d9f8c61b4b73fc8c8c24758a35", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0ded8d78913a04e500afd810ac273773bb3f4a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b3a60647f47ba03c6686405d656724bf8664f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13f763cc226b0b456ab56df4143da9da2d4db438", "width": 1080, "height": 607}], "variants": {}, "id": "Ven08R7rW3VUewldA00PYwtDrsf5TziOv0R5SeV6HyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1719j9d", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "subreddit_subscribers": 132430, "created_utc": 1696590729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.\n\nThe strings always have the same format, such as \"MM2525 6562451 Some text\". So from this string I would only want the \"6562451\" bit. I tried to find regular expressions like python but got no luck. Thanks in advance!\n\nP.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting digits from a string column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a9o6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696593077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.&lt;/p&gt;\n\n&lt;p&gt;The strings always have the same format, such as &amp;quot;MM2525 6562451 Some text&amp;quot;. So from this string I would only want the &amp;quot;6562451&amp;quot; bit. I tried to find regular expressions like python but got no luck. Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;P.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171a9o6", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "subreddit_subscribers": 132430, "created_utc": 1696593077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, there!\n\nI am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.\n\nHere is my current architecture more or less:\n\n* Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response\n* I ingest that response data and pass the json data string into a message queue (azure service bus)\n* write the given event (row if you will) to a Azure data lake storage container called \"ingestion\"\n* move on\n* Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables\n* return the data in a data frame\n\n&amp;#x200B;\n\nI have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:\n\n&amp;#x200B;\n\n* If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record\n* If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row\n* Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.\n\nUltimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.\n\n&amp;#x200B;\n\nAm I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.\n\nAs I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.\n\nI appreciate your wisdom and guidance in advance!\n\n&amp;#x200B;\n\nCheers,\n\nIsaiah", "author_fullname": "t2_92b8a2oo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upsert Parquet Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171168w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696560398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, there!&lt;/p&gt;\n\n&lt;p&gt;I am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.&lt;/p&gt;\n\n&lt;p&gt;Here is my current architecture more or less:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response&lt;/li&gt;\n&lt;li&gt;I ingest that response data and pass the json data string into a message queue (azure service bus)&lt;/li&gt;\n&lt;li&gt;write the given event (row if you will) to a Azure data lake storage container called &amp;quot;ingestion&amp;quot;&lt;/li&gt;\n&lt;li&gt;move on&lt;/li&gt;\n&lt;li&gt;Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables&lt;/li&gt;\n&lt;li&gt;return the data in a data frame&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record&lt;/li&gt;\n&lt;li&gt;If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row&lt;/li&gt;\n&lt;li&gt;Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ultimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Am I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.&lt;/p&gt;\n\n&lt;p&gt;As I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.&lt;/p&gt;\n\n&lt;p&gt;I appreciate your wisdom and guidance in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Isaiah&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171168w", "is_robot_indexable": true, "report_reasons": null, "author": "realeyezayuh", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171168w/upsert_parquet_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171168w/upsert_parquet_data/", "subreddit_subscribers": 132430, "created_utc": 1696560398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don't feel good about this.", "author_fullname": "t2_cj0b4yt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170twld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don&amp;#39;t feel good about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170twld", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Most564", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170twld/data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170twld/data_catalog/", "subreddit_subscribers": 132430, "created_utc": 1696541230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a senior DE at a major media streaming company, and am being put up for promotion to become a staff DE.\n\nStarted my career as an accountant and in accounting, the progression is staff accountant -&gt; senior accountant -&gt; accounting manager. There are several other professions that follow that path as well. \n\nWhenever I tell my family \u201cI might get promoted to staff engineer!\u201d They act confused and say it sounds like a demotion \ud83d\ude02\n\nDoes anyone have any idea how/why staff is higher than senior for engineering roles??", "author_fullname": "t2_25bhk0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is staff higher than senior?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171h49k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696610377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a senior DE at a major media streaming company, and am being put up for promotion to become a staff DE.&lt;/p&gt;\n\n&lt;p&gt;Started my career as an accountant and in accounting, the progression is staff accountant -&amp;gt; senior accountant -&amp;gt; accounting manager. There are several other professions that follow that path as well. &lt;/p&gt;\n\n&lt;p&gt;Whenever I tell my family \u201cI might get promoted to staff engineer!\u201d They act confused and say it sounds like a demotion \ud83d\ude02&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any idea how/why staff is higher than senior for engineering roles??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171h49k", "is_robot_indexable": true, "report_reasons": null, "author": "scranice3", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171h49k/why_is_staff_higher_than_senior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171h49k/why_is_staff_higher_than_senior/", "subreddit_subscribers": 132430, "created_utc": 1696610377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?", "author_fullname": "t2_5n2f6pea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing ETL config", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171c29z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696598198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171c29z", "is_robot_indexable": true, "report_reasons": null, "author": "sosa_12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171c29z/managing_etl_config/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171c29z/managing_etl_config/", "subreddit_subscribers": 132430, "created_utc": 1696598198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? ", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using python azure function for data transformation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171b9qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696595996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171b9qn", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "subreddit_subscribers": 132430, "created_utc": 1696595996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery *and* airflow running.\n\nLong version: \n\nWe have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it's very basic; it reads data from multiple sources, normalizes them and puts em in our database.\n\nBut now that we're in the throes of airflow, we've found it to be a little... wanting. It's pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.\n\nAnother huge con is that we have a lot code that could be shared between our regular ol' django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.\n\nSO now I'm wanting to put our python back on our server, but I'd still like to schedule the tasks with airflow. I guess I'd make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don't want to just have it be a regular http endpoint.\n\nShould I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do initiate long-running jobs in a django server from airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170s9n6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696537347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery &lt;em&gt;and&lt;/em&gt; airflow running.&lt;/p&gt;\n\n&lt;p&gt;Long version: &lt;/p&gt;\n\n&lt;p&gt;We have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it&amp;#39;s very basic; it reads data from multiple sources, normalizes them and puts em in our database.&lt;/p&gt;\n\n&lt;p&gt;But now that we&amp;#39;re in the throes of airflow, we&amp;#39;ve found it to be a little... wanting. It&amp;#39;s pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.&lt;/p&gt;\n\n&lt;p&gt;Another huge con is that we have a lot code that could be shared between our regular ol&amp;#39; django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.&lt;/p&gt;\n\n&lt;p&gt;SO now I&amp;#39;m wanting to put our python back on our server, but I&amp;#39;d still like to schedule the tasks with airflow. I guess I&amp;#39;d make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don&amp;#39;t want to just have it be a regular http endpoint.&lt;/p&gt;\n\n&lt;p&gt;Should I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170s9n6", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "subreddit_subscribers": 132430, "created_utc": 1696537347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've read reference architectures about data lakehouse (have to make a proposal), so I would like to hear some practical experiences of what kind of data validation, transformation, remodeling happens in those layers and between them. So I can present them to my DEs\n\nWhat I understand is that:\n\n* Bronze: for raw or lightly validated data, with some basic schema changes/validations (maybe a more appropriate data type, but that's it)\n   * and the question - how much validation , schema changes is done here?\n* Silver: for remodelled data - so I merged/appended same type of data from various sources, so that I have a holistic overview. So there I build my master tables\n   * and the question - is here like one builds an enterprise data model? so my customer master, my product master, my global transactions?\n   * and do you name your columns as you please, or do you follow some sort of approved glossary?\n* Gold: this is where I filtered, aggregated, calculated, answered\n   * question - how much of that analytical processing is done and saved? basically what BI tools take over?\n\nBut maybe I have missed a point.\n\nthank you!", "author_fullname": "t2_49dbxejy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "multi-hop ELT / data storage pattern - what really happens where with data schema?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171idkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696613299.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read reference architectures about data lakehouse (have to make a proposal), so I would like to hear some practical experiences of what kind of data validation, transformation, remodeling happens in those layers and between them. So I can present them to my DEs&lt;/p&gt;\n\n&lt;p&gt;What I understand is that:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Bronze: for raw or lightly validated data, with some basic schema changes/validations (maybe a more appropriate data type, but that&amp;#39;s it)\n\n&lt;ul&gt;\n&lt;li&gt;and the question - how much validation , schema changes is done here?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Silver: for remodelled data - so I merged/appended same type of data from various sources, so that I have a holistic overview. So there I build my master tables\n\n&lt;ul&gt;\n&lt;li&gt;and the question - is here like one builds an enterprise data model? so my customer master, my product master, my global transactions?&lt;/li&gt;\n&lt;li&gt;and do you name your columns as you please, or do you follow some sort of approved glossary?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Gold: this is where I filtered, aggregated, calculated, answered\n\n&lt;ul&gt;\n&lt;li&gt;question - how much of that analytical processing is done and saved? basically what BI tools take over?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But maybe I have missed a point.&lt;/p&gt;\n\n&lt;p&gt;thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171idkd", "is_robot_indexable": true, "report_reasons": null, "author": "HereJustForAnswers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171idkd/multihop_elt_data_storage_pattern_what_really/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171idkd/multihop_elt_data_storage_pattern_what_really/", "subreddit_subscribers": 132430, "created_utc": 1696613299.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working with data for around 4 years now. I started in business intelligence using AWS. At that time, I was the only data analyst, so I had to learn and do everything by myself. This included tasks such as acquiring the data, gaining some knowledge of S3 and Glue for processing and storage, and building dashboards.\n\nIn the last year, I moved to a new company as an \"analytics engineer.\" It started similarly, working end-to-end but with a different tech stack (Power BI, dbt, PostgreSQL, etc.) and with colleagues on my side (rather than being alone).\n\nRecently, our department's structure changed to specialized roles. Now, there are \"data analysts\" for each stream/area building the BIs , and I have taken on the role of the company's \"data engineer.\" I'm not entirely sure what this means.\n\nAnalysts will still be querying in dbt and creating their dashboards, but I don't have many tasks to handle on a regular basis, aside from occasional activities like API integrations or preparing staging models. My boss tells me that data engineering is a \"proactive role\". The thing is that I feel I lack the knowledge and expertise to know new things to implement.\n\nSo, that's was a long way of asking: \"How can I be a good data engineer? For a new starter\"", "author_fullname": "t2_126mha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I be a good data engineer? And what does it means?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171hpr8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696611743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working with data for around 4 years now. I started in business intelligence using AWS. At that time, I was the only data analyst, so I had to learn and do everything by myself. This included tasks such as acquiring the data, gaining some knowledge of S3 and Glue for processing and storage, and building dashboards.&lt;/p&gt;\n\n&lt;p&gt;In the last year, I moved to a new company as an &amp;quot;analytics engineer.&amp;quot; It started similarly, working end-to-end but with a different tech stack (Power BI, dbt, PostgreSQL, etc.) and with colleagues on my side (rather than being alone).&lt;/p&gt;\n\n&lt;p&gt;Recently, our department&amp;#39;s structure changed to specialized roles. Now, there are &amp;quot;data analysts&amp;quot; for each stream/area building the BIs , and I have taken on the role of the company&amp;#39;s &amp;quot;data engineer.&amp;quot; I&amp;#39;m not entirely sure what this means.&lt;/p&gt;\n\n&lt;p&gt;Analysts will still be querying in dbt and creating their dashboards, but I don&amp;#39;t have many tasks to handle on a regular basis, aside from occasional activities like API integrations or preparing staging models. My boss tells me that data engineering is a &amp;quot;proactive role&amp;quot;. The thing is that I feel I lack the knowledge and expertise to know new things to implement.&lt;/p&gt;\n\n&lt;p&gt;So, that&amp;#39;s was a long way of asking: &amp;quot;How can I be a good data engineer? For a new starter&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171hpr8", "is_robot_indexable": true, "report_reasons": null, "author": "Peivol", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171hpr8/how_can_i_be_a_good_data_engineer_and_what_does/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171hpr8/how_can_i_be_a_good_data_engineer_and_what_does/", "subreddit_subscribers": 132430, "created_utc": 1696611743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Checkout this 10-minute video that will introduce you to pandas in a very simple way:\n\n[https://www.youtube.com/watch?v=HIEMQ53WrwE](https://www.youtube.com/watch?v=HIEMQ53WrwE)", "author_fullname": "t2_2lt1q4pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Pandas: filtering, grouping and aggregation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171goj5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696609316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Checkout this 10-minute video that will introduce you to pandas in a very simple way:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=HIEMQ53WrwE\"&gt;https://www.youtube.com/watch?v=HIEMQ53WrwE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TIug9AnYLhwCFTlakBFl9j2ZHjhUxGwcdm4RjhVXrhE.jpg?auto=webp&amp;s=03c2cb1e23029132bddd4c2cbb3cfd9485584cff", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/TIug9AnYLhwCFTlakBFl9j2ZHjhUxGwcdm4RjhVXrhE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2b941323d1c910a10540040ab7cd25c8af285c6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/TIug9AnYLhwCFTlakBFl9j2ZHjhUxGwcdm4RjhVXrhE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ebeec4b98250f734263db3d3ca718d4814b8939", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/TIug9AnYLhwCFTlakBFl9j2ZHjhUxGwcdm4RjhVXrhE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=538d0018c124c38a86fac3b391c91f67eceb25c1", "width": 320, "height": 240}], "variants": {}, "id": "xEnq6oUvSzhmgMfGCQXqxKGEAj7eGF-ijKBFL7s7hjc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "171goj5", "is_robot_indexable": true, "report_reasons": null, "author": "Kairo1004", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171goj5/simple_pandas_filtering_grouping_and_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171goj5/simple_pandas_filtering_grouping_and_aggregation/", "subreddit_subscribers": 132430, "created_utc": 1696609316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.\n\n1. I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)\n\n2. We use Data integration (Informatica) to load the files to Snowflake. \n\nAs you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to collect data via API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171dfvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696601730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use Data integration (Informatica) to load the files to Snowflake. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171dfvx", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "subreddit_subscribers": 132430, "created_utc": 1696601730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.", "author_fullname": "t2_jvtay3bqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meaning of title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171d26s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696600808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "171d26s", "is_robot_indexable": true, "report_reasons": null, "author": "Southern_Version2681", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171d26s/meaning_of_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171d26s/meaning_of_title/", "subreddit_subscribers": 132430, "created_utc": 1696600808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone know of a good way to measure the scheduler CPU utilization for an Airflow instance running locally via docker?  CLI (Docker stats) and desktop provide some live (and limited time based data) but I'm looking to measure CPU of just the scheduler over time.", "author_fullname": "t2_puuzgu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Measuring CPU utilization of airflow scheduler", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171bi57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696596670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know of a good way to measure the scheduler CPU utilization for an Airflow instance running locally via docker?  CLI (Docker stats) and desktop provide some live (and limited time based data) but I&amp;#39;m looking to measure CPU of just the scheduler over time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171bi57", "is_robot_indexable": true, "report_reasons": null, "author": "getafterit123", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171bi57/measuring_cpu_utilization_of_airflow_scheduler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171bi57/measuring_cpu_utilization_of_airflow_scheduler/", "subreddit_subscribers": 132430, "created_utc": 1696596670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to integrate a data workload tool with my web app's API, which I know is a fairly non-standard thing to do... but the workloads can be expensive (in terms of $$, e.g. calling OpenAI), so I want to bring good data eng practices along.\n\nI've been playing around with Prefect and Benthos, and surprised to find that when you trigger a workflow that is set to cache results on a given input, there is no locking mechanism to ensure that concurrent requests for the same given input do not result in more than one computation. i.e. while the first request is being handled, subsequent requests should not trigger a new compute but rather wait for the result of the first one.\n\nThis is a problem primarily due to the tight integration with the web app API where users can trigger workloads that are set up to lazy-evaluate / lazy-compute certain data outputs\n\nDoes anyone know of any tools that do solve this cache stampede issue? And is a data eng tool what I should be using, or does it sound more like I am reaching for the wrong set of tooling?", "author_fullname": "t2_vep5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Caching workloads + concurrent requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170ycyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696552391.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to integrate a data workload tool with my web app&amp;#39;s API, which I know is a fairly non-standard thing to do... but the workloads can be expensive (in terms of $$, e.g. calling OpenAI), so I want to bring good data eng practices along.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with Prefect and Benthos, and surprised to find that when you trigger a workflow that is set to cache results on a given input, there is no locking mechanism to ensure that concurrent requests for the same given input do not result in more than one computation. i.e. while the first request is being handled, subsequent requests should not trigger a new compute but rather wait for the result of the first one.&lt;/p&gt;\n\n&lt;p&gt;This is a problem primarily due to the tight integration with the web app API where users can trigger workloads that are set up to lazy-evaluate / lazy-compute certain data outputs&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any tools that do solve this cache stampede issue? And is a data eng tool what I should be using, or does it sound more like I am reaching for the wrong set of tooling?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170ycyq", "is_robot_indexable": true, "report_reasons": null, "author": "matty_fu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170ycyq/caching_workloads_concurrent_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170ycyq/caching_workloads_concurrent_requests/", "subreddit_subscribers": 132430, "created_utc": 1696552391.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have pipelines that run weekly and they're dependent on what clients send us. Some clients send us the same shit some can't seem to figure it out and their pipe fails. To what degree do you modify a pipeline? Is it just to get things moving or is there a way to gold-plate it and hope they don't find another way to fuck it up.\n\nThings that clients have fucked up in the last week,\nDifferent file format\nDifferent header\nDifferent data types", "author_fullname": "t2_vtm8z2o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do you decide to modify a pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170wvxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696548419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have pipelines that run weekly and they&amp;#39;re dependent on what clients send us. Some clients send us the same shit some can&amp;#39;t seem to figure it out and their pipe fails. To what degree do you modify a pipeline? Is it just to get things moving or is there a way to gold-plate it and hope they don&amp;#39;t find another way to fuck it up.&lt;/p&gt;\n\n&lt;p&gt;Things that clients have fucked up in the last week,\nDifferent file format\nDifferent header\nDifferent data types&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170wvxc", "is_robot_indexable": true, "report_reasons": null, "author": "Action_Maxim", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170wvxc/at_what_point_do_you_decide_to_modify_a_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170wvxc/at_what_point_do_you_decide_to_modify_a_pipeline/", "subreddit_subscribers": 132430, "created_utc": 1696548419.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}