{"kind": "Listing", "data": {"after": "t3_171c29z", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fb83g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Fabric: Should Databricks be Worried?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_170otm0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KqfXrZoRthY9_0rKj5OFsfqYtxhzW4XWdZseyT5_NcY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696529189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "vantage.sh", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.vantage.sh/blog/databricks-vs-microsoft-fabric-pricing-analysis", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?auto=webp&amp;s=af663ae0121ce0a4793363094c1fec8259eefa84", "width": 1270, "height": 760}, "resolutions": [{"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57e13db27e90db4e7152be7e631452a796c7297a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deb5668b38d81175fea57b520a0c5c713c6d7ce9", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6222d395bbc71108823edfc2628acb961c3bb0cf", "width": 320, "height": 191}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe04e60e1c5b79f9154ec4a8193c93ba8b7c109", "width": 640, "height": 382}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f33a26035314d4789bc40760d8a6abb6837980b", "width": 960, "height": 574}, {"url": "https://external-preview.redd.it/fF1rTjn5aYUAQ1MqXyCQYpcmOelAg0OvRV-1i6o4ntg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=394a654d23b5732bcba1b7573a3fc9a6da6897e6", "width": 1080, "height": 646}], "variants": {}, "id": "KJGb3So15RCrofadobr6rSw5w7wrNTNiPNLXDBWpF8o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "170otm0", "is_robot_indexable": true, "report_reasons": null, "author": "include_stdio_h", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170otm0/microsoft_fabric_should_databricks_be_worried/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.vantage.sh/blog/databricks-vs-microsoft-fabric-pricing-analysis", "subreddit_subscribers": 132406, "created_utc": 1696529189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies as my background isn't in DE but come more from BI systems administration. In my career I've seen a lot but the craziest thing I've seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?\n\nIs it literally just because our system doesn't have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it's 1995? Couldn't a REST API send a CSV over HTTPS anyway? Do we not trust it?\n\nIs data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?\n\nI feel like there's something I'm fundamentally misunderstanding but not sure what it is...", "author_fullname": "t2_6c2aryt5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the reason SFTP file transfer in banks, healthcare, etc is so ubiquitous just because they are using older systems without robust REST APIs...?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170rvz3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696536440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies as my background isn&amp;#39;t in DE but come more from BI systems administration. In my career I&amp;#39;ve seen a lot but the craziest thing I&amp;#39;ve seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?&lt;/p&gt;\n\n&lt;p&gt;Is it literally just because our system doesn&amp;#39;t have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it&amp;#39;s 1995? Couldn&amp;#39;t a REST API send a CSV over HTTPS anyway? Do we not trust it?&lt;/p&gt;\n\n&lt;p&gt;Is data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?&lt;/p&gt;\n\n&lt;p&gt;I feel like there&amp;#39;s something I&amp;#39;m fundamentally misunderstanding but not sure what it is...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170rvz3", "is_robot_indexable": true, "report_reasons": null, "author": "TheWikiJedi", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/", "subreddit_subscribers": 132406, "created_utc": 1696536440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear fellow Data Engineers\n\nYesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.\n\nThen a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.\n\nI stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.\n\nToday, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.\n\nI do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. \n\nTo summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?\n\nI am curious to hear about your experience!", "author_fullname": "t2_ou2h13bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backend Skills for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170vj3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696545043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear fellow Data Engineers&lt;/p&gt;\n\n&lt;p&gt;Yesterday, I had a Job Interview for a Senior Data Engieer Position at a local Healthcare Provider in Switzerland. I mastered almost all technical questions about Data Engineering in general (3NF, SCD2, Lakehouse vs DWH, Relational vs Star Schema, CDC, Batch processing etc.) as well as a technical case study how I would design a Warehouse + AI Solution regarding text analysis.&lt;/p&gt;\n\n&lt;p&gt;Then a guy from another Department joined and asked question that were more backend related. E.g. What is REST, and how to design an api accordingly? What is OOP and its benefits? What are pros and cons of using Docker? etc.&lt;/p&gt;\n\n&lt;p&gt;I stumbled across these questions and did not know how to answer them properly. I did not prepare for such questions as the job posting was not asking for backend related skills.&lt;/p&gt;\n\n&lt;p&gt;Today, I got an email explaining that I would be a personal as well as a technical fit from a data engineering perspective. However, they are looking for a person that has more of an IT-background that can be used more flexible within their departments. Thus they declined.&lt;/p&gt;\n\n&lt;p&gt;I do agree that I am not a perfect fit, if they are looking for such a person. But I am questioning if, in general, these backend related skills can be expected from someone that applies for a Data Engineering position. &lt;/p&gt;\n\n&lt;p&gt;To summarize: Should I study backend software engineering in order to increase my chances of finding a Job? Or, are backend related skills usually not asked for and I should not worry about it too much?&lt;/p&gt;\n\n&lt;p&gt;I am curious to hear about your experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "170vj3u", "is_robot_indexable": true, "report_reasons": null, "author": "Present_Salt_1688", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170vj3u/backend_skills_for_data_engineers/", "subreddit_subscribers": 132406, "created_utc": 1696545043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have worked with Data in some capacity for the last 5-6 years of my career. I am self-taught and have experience with SQL, Python and Bash (as well as specific tools and database engines like MSSQL, MariaDB, Vertica etc).  \n\n\nI code a lot of custom ETL jobs for my work, and have begun to wonder - \\*how\\* can I evaluate performance, especially to identify room for improvement. It's one thing to benchmark script 1 vs script 2, but its more difficult to have a sense of how quickly a given amount of data should load. Are there ways to check network latency, the available computational resources etc to establish a baseline of performance to aim for? What are the best tools for feeling this out, especially ones easily available to me given what I know - ie SQL, Bash, Python? I'm out of my depth here but want to improve and feel answering or otherwise understanding this question is a necessary next step.  \n\n\nFor the sake of this exercise lets assume the data is fairly simple - floats, integers, char/varchar etc rather than more complex or larger formats like CLOB/BLOB.", "author_fullname": "t2_3v8tq4ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I know the extent to which ETLs *could* be running faster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170lxza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696522323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have worked with Data in some capacity for the last 5-6 years of my career. I am self-taught and have experience with SQL, Python and Bash (as well as specific tools and database engines like MSSQL, MariaDB, Vertica etc).  &lt;/p&gt;\n\n&lt;p&gt;I code a lot of custom ETL jobs for my work, and have begun to wonder - *how* can I evaluate performance, especially to identify room for improvement. It&amp;#39;s one thing to benchmark script 1 vs script 2, but its more difficult to have a sense of how quickly a given amount of data should load. Are there ways to check network latency, the available computational resources etc to establish a baseline of performance to aim for? What are the best tools for feeling this out, especially ones easily available to me given what I know - ie SQL, Bash, Python? I&amp;#39;m out of my depth here but want to improve and feel answering or otherwise understanding this question is a necessary next step.  &lt;/p&gt;\n\n&lt;p&gt;For the sake of this exercise lets assume the data is fairly simple - floats, integers, char/varchar etc rather than more complex or larger formats like CLOB/BLOB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170lxza", "is_robot_indexable": true, "report_reasons": null, "author": "vonkraush1010", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170lxza/how_do_i_know_the_extent_to_which_etls_could_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170lxza/how_do_i_know_the_extent_to_which_etls_could_be/", "subreddit_subscribers": 132406, "created_utc": 1696522323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was a director of data engineering after a datascience background and was offered a job a sr architect \n\nAs a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books\n\nI\u2019m sure I have knowledge gaps. What should I read up on ?", "author_fullname": "t2_l5fycnwh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer to data architect. What do I need to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170us8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696543281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was a director of data engineering after a datascience background and was offered a job a sr architect &lt;/p&gt;\n\n&lt;p&gt;As a director of data engineering I just pulled data from aws into databricks and built delta pipelines that fed tableau work books&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sure I have knowledge gaps. What should I read up on ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "170us8m", "is_robot_indexable": true, "report_reasons": null, "author": "updated-quality-485", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170us8m/data_engineer_to_data_architect_what_do_i_need_to/", "subreddit_subscribers": 132406, "created_utc": 1696543281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have\n\nCode Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks\n\nData lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. \n\nOne of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. \n\nContour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. \n\nOntology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. \n\nSlate - garbage piece of shit UI tech \n\nData Health - kinda useful confusing to use \n\nThey are the principal tools I use everyday in Palantir Foundry \n\nGeneral thoughts:\n\n- it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. \n\n- Data Expectations &amp; testing implementation is really good. \n\n- Their technical support seem pretty good. Their engineers are intimidatingly good. \n\n- really easy to find and search other datasets that could be related.\n\n- Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.\n\nHas anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.\n\nEdit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026", "author_fullname": "t2_nutp89h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How applicable is Palantir foundry to Data engineering in general?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170u6u8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working in Palantir foundry daily. Sometimes when I check jobs I get worried because I\u2019m not sure if what Palantir use is industry standard.\nTo the best of what I can see this is what they have&lt;/p&gt;\n\n&lt;p&gt;Code Repos / Codeworkbooks - Pyspark (widely applicable) however behind the scenes Palantir create the tables and have a concept of Input, Output which doesn\u2019t exist in real pyspark.\nPandas can be written in Codeworkbooks&lt;/p&gt;\n\n&lt;p&gt;Data lineage - low code visual tool for seeing data lineages and running schedule builds. It uses some type of Airflow technology behind the scenes. It\u2019s incredibly powerful once you get the gist of it and can super quickly run schedule builds. &lt;/p&gt;\n\n&lt;p&gt;One of my biggest \u201cfixes\u201d was finding out some idiot scheduled a build once minute for the past 2 years. My fix was to reduce it to once a day. &lt;/p&gt;\n\n&lt;p&gt;Contour - Qliksense, Apache Superset (maybe) you can do filtering, create charts, pivot tables and use a weird form of what I think is SQL (Expression language). Has no real world equivalent and is very different. Great tool. It helps me break down datasets and see what my deltas are. &lt;/p&gt;\n\n&lt;p&gt;Ontology - dataset storage uses some type of DeltaLake storage but the actual Ontology tool I\u2019ve idea if it has a real world equivalent. Maybe Business Objets in SAP. &lt;/p&gt;\n\n&lt;p&gt;Slate - garbage piece of shit UI tech &lt;/p&gt;\n\n&lt;p&gt;Data Health - kinda useful confusing to use &lt;/p&gt;\n\n&lt;p&gt;They are the principal tools I use everyday in Palantir Foundry &lt;/p&gt;\n\n&lt;p&gt;General thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;it\u2019s great if you\u2019re a beginner because it\u2019s pretty easy to get ok at it. It\u2019s frustrating if you want more control because they abstract a lot of stuff away. They have weird implementations of certain technologies.  Sometimes you want to really get into the Spark API or use the newest release and you\u2019re stuck with what Palantir have. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Expectations &amp;amp; testing implementation is really good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Their technical support seem pretty good. Their engineers are intimidatingly good. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;really easy to find and search other datasets that could be related.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Can be slow unstable when pushing code. My code commits regular revert. Git is all point and click which is bad because for most devs Git is second nature.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone worked with Palantir Foundry? How standard are their tech? I get worried I\u2019m pigeonholed and how to explain to other companies this weird proprietary technology.&lt;/p&gt;\n\n&lt;p&gt;Edit: Just as a last aside it really shows how much some of these big companies have stolen everything from OpenSource and repackaged it as mind blowing technology to the C-suite. All those people who ever corrected something or helped make an open source library should really get a cheque from Amazon or Palantir anyway back to reality\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170u6u8", "is_robot_indexable": true, "report_reasons": null, "author": "hositir", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170u6u8/how_applicable_is_palantir_foundry_to_data/", "subreddit_subscribers": 132406, "created_utc": 1696541897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:\n\n1. A CRM file is uploaded to S3.\n2. A *sensor* notices the new file and creates a new *dynamic partition*.\n3. The file is then processed and stored to relational database. This would be the first *asset*.\n4. Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two *assets* that depend on the first *asset*.\n5. When both these *assets* are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two *assets*.\n6. Another *sensor* will check whether the last asset has been materialised and subsequently a *job* will be kicked of that is going to send an email, with a download link, to the user.\n\nMainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!", "author_fullname": "t2_trs7vyx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster ETL setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1715bbb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696574403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, we\u2019re working on a data cleaning project. Our goal is to allow users to easily clean their CRM contact files. Here\u2019s a quick overview of the process, and we\u2019d love to get your feedback regarding the setup/process:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A CRM file is uploaded to S3.&lt;/li&gt;\n&lt;li&gt;A &lt;em&gt;sensor&lt;/em&gt; notices the new file and creates a new &lt;em&gt;dynamic partition&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;The file is then processed and stored to relational database. This would be the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Next, for each table row we should perform two tasks (e.g. sanitise name components and email components). These can be split into two &lt;em&gt;assets&lt;/em&gt; that depend on the first &lt;em&gt;asset&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;When both these &lt;em&gt;assets&lt;/em&gt; are completed we are going to create a file and store it to S3. Again this is an asset that depends on the previous two &lt;em&gt;assets&lt;/em&gt;.&lt;/li&gt;\n&lt;li&gt;Another &lt;em&gt;sensor&lt;/em&gt; will check whether the last asset has been materialised and subsequently a &lt;em&gt;job&lt;/em&gt; will be kicked of that is going to send an email, with a download link, to the user.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Mainly interested in whether this is a good use-case for Dagster and whether I am using the right concepts. Also wondering how we can optimise the record processing, e.g. when we receive an unexpected error for a record we do not want to lose any progress, mainly because we are using external services which would increase the cost. Any suggestions are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1715bbb", "is_robot_indexable": true, "report_reasons": null, "author": "WeddingIndependent30", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1715bbb/dagster_etl_setup/", "subreddit_subscribers": 132406, "created_utc": 1696574403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Saw this post about how to get similar functionality to dbt Cloud using other OSS tools, wondering if anyone has done this and what your experience has been\n\n  \ntl;dr  \ndbt Cloud or dbt Core + vs code + airflow + cube.dev\n\nhttps://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Cloud extra features worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170l0nw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696520038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this post about how to get similar functionality to dbt Cloud using other OSS tools, wondering if anyone has done this and what your experience has been&lt;/p&gt;\n\n&lt;p&gt;tl;dr&lt;br/&gt;\ndbt Cloud or dbt Core + vs code + airflow + cube.dev&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences\"&gt;https://datacoves.com/post/dbt-core-vs-dbt-cloud-key-differences&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?auto=webp&amp;s=d3cc503fb34529b9e99c792a7117e5fc053ac06b", "width": 2400, "height": 1254}, "resolutions": [{"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6aedf9503b00fdb31f500fd840357bf700d9507", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec9a9837c66cb33c17898fd385442393eebb33b7", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=829d1b2e30c6aedbf56d477f7a8419d039d4be73", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a675320e601b25b36e1e6ac4f051e0b27f475414", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6a488a73fbc7d27fce74b0ca11a7b2c828a39519", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/AkrfqvlANIwcec3NIquBhmDv45PLZVpQy0VFN79knRM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=099e281cdfcfe53d01e3290cb938387eb3cee441", "width": 1080, "height": 564}], "variants": {}, "id": "xUIE53y-az5HrMPPgq4ndFAPYxY_HJJQd0C1S2OtT0E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170l0nw", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170l0nw/dbt_cloud_extra_features_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170l0nw/dbt_cloud_extra_features_worth_it/", "subreddit_subscribers": 132406, "created_utc": 1696520038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for an ecommerce company and ever since I started working here about a year ago, I've seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it's customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what's worst is that backend team is so distant to data engineers, that we can't even access their repos or confluence pages. \n\nBecause of this, my new team built (many years ago) a customer identifier, let's call it customer\\_id, which is basically just a hashed value of a bunch of other ids (like user\\_checkout\\_id or anonymous\\_checkout\\_id). This customer\\_id is what's used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer\\_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer\\_id has also left the company years ago and the code is extremely complex.\n\nWe are planning to do a refactor soon but don't really know what's the best practice here. I'm guessing you should ignore some errors based on some threshold and have some uniformity. \n\nhas anyone got any experience on this? any books or blogs? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to deal with complex user_id modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1717jgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696583310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for an ecommerce company and ever since I started working here about a year ago, I&amp;#39;ve seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it&amp;#39;s customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what&amp;#39;s worst is that backend team is so distant to data engineers, that we can&amp;#39;t even access their repos or confluence pages. &lt;/p&gt;\n\n&lt;p&gt;Because of this, my new team built (many years ago) a customer identifier, let&amp;#39;s call it customer_id, which is basically just a hashed value of a bunch of other ids (like user_checkout_id or anonymous_checkout_id). This customer_id is what&amp;#39;s used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer_id has also left the company years ago and the code is extremely complex.&lt;/p&gt;\n\n&lt;p&gt;We are planning to do a refactor soon but don&amp;#39;t really know what&amp;#39;s the best practice here. I&amp;#39;m guessing you should ignore some errors based on some threshold and have some uniformity. &lt;/p&gt;\n\n&lt;p&gt;has anyone got any experience on this? any books or blogs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1717jgi", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "subreddit_subscribers": 132406, "created_utc": 1696583310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube integrates its semantic layer with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170n9rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696525451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-dbt-integration-with-cube", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "170n9rf", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170n9rf/cube_integrates_its_semantic_layer_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-dbt-integration-with-cube", "subreddit_subscribers": 132406, "created_utc": 1696525451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Posted this over in /r/Azure , but wanted to get yawls perspective too. \n\n  \nI  am looking to migrate off of dedicated sql pool - it is too expensive. We are currently using  azure synapse studio + dedicated sql pool + adl. here is some workflows  we do:\n\n1) incremental data loads from transnational sql databases. we load this data pretty much straight into the dwh.\n\n2) about  5-6 external sources we pull data via an API or maybe a excel  spreadsheet/csv for adhoc data from my customer facing teams. we load  the data from the external sources into our data lake. I just use azure  batch to execute python code that does this for me. from there we create  external tables to load them into our dedicated sql pool.\n\n3) once  all the data is loaded into the sql pool, we then do various  transformations and aggregations on the data, and store the results in  sql, which are then loaded into our PBI data model. The refreshes happen  1x per day, typically around 4am.\n\nthis  all being said, dedicated sql pools are SUPER expensive. I do like  being able to load the external sources directly into sql, so i was  thinking about migrating off of dedicated sql pool and just using  serverless sql in synapse. We would just export data from our  transactional sql databases onto the lake in parquet format. One of my  concerns with this is everything would essentially be views, as i dont  think you can create/manage the transformations using CTAS statements.  This doesnt seem like it would scale well as we continue to get more  data.\n\nHonestly, were around 266gb  of data total, so i wouldnt be opposed to just using sql, i think  dedicated sql pool is overkill. BUT,  i dont think azure SQL has all of  the virtualisation tech like serverless or dedicated sql pool has, and  my aggregations might take longer, but i think i can fine tune  a lot to  make that work (and azure sql database is MUCHHHH cheaper). If i look  at the pricing of azure sql server managed instance (which from my  understanding does have the virtualisation tech) its around the same as  the dedicated sql pool, so doesnt seem worth to me.\n\nhow  are ya'll doing this in a cost effective way? There are so many  buzzwords and buzz tech out there, im not sure what i should be looking at. The term Synapse is super confusing because it seems interchangable with serverless or dedicated sql pool, when it also can act as an orchestration tool. I seem a lot of people saying databricks + adl + datafactory is the  way to go, but then there is the added expensive of databricks and learning databricks.. I am  also a 1 man team on the reporting and data side, and i am also managing  another separate team on top of this (lol the tech recession is real).  point is, i feel very lost, and i dont want to spend a bunch of time migrating to a solution that wont scale or isnt cost effective. I appreciate any direction and advice you  fellow data engineers can give.\n\nThanks!", "author_fullname": "t2_jrmn04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions to migrate off of dedicated SQL Pool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170ltz3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696522062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Posted this over in &lt;a href=\"/r/Azure\"&gt;/r/Azure&lt;/a&gt; , but wanted to get yawls perspective too. &lt;/p&gt;\n\n&lt;p&gt;I  am looking to migrate off of dedicated sql pool - it is too expensive. We are currently using  azure synapse studio + dedicated sql pool + adl. here is some workflows  we do:&lt;/p&gt;\n\n&lt;p&gt;1) incremental data loads from transnational sql databases. we load this data pretty much straight into the dwh.&lt;/p&gt;\n\n&lt;p&gt;2) about  5-6 external sources we pull data via an API or maybe a excel  spreadsheet/csv for adhoc data from my customer facing teams. we load  the data from the external sources into our data lake. I just use azure  batch to execute python code that does this for me. from there we create  external tables to load them into our dedicated sql pool.&lt;/p&gt;\n\n&lt;p&gt;3) once  all the data is loaded into the sql pool, we then do various  transformations and aggregations on the data, and store the results in  sql, which are then loaded into our PBI data model. The refreshes happen  1x per day, typically around 4am.&lt;/p&gt;\n\n&lt;p&gt;this  all being said, dedicated sql pools are SUPER expensive. I do like  being able to load the external sources directly into sql, so i was  thinking about migrating off of dedicated sql pool and just using  serverless sql in synapse. We would just export data from our  transactional sql databases onto the lake in parquet format. One of my  concerns with this is everything would essentially be views, as i dont  think you can create/manage the transformations using CTAS statements.  This doesnt seem like it would scale well as we continue to get more  data.&lt;/p&gt;\n\n&lt;p&gt;Honestly, were around 266gb  of data total, so i wouldnt be opposed to just using sql, i think  dedicated sql pool is overkill. BUT,  i dont think azure SQL has all of  the virtualisation tech like serverless or dedicated sql pool has, and  my aggregations might take longer, but i think i can fine tune  a lot to  make that work (and azure sql database is MUCHHHH cheaper). If i look  at the pricing of azure sql server managed instance (which from my  understanding does have the virtualisation tech) its around the same as  the dedicated sql pool, so doesnt seem worth to me.&lt;/p&gt;\n\n&lt;p&gt;how  are ya&amp;#39;ll doing this in a cost effective way? There are so many  buzzwords and buzz tech out there, im not sure what i should be looking at. The term Synapse is super confusing because it seems interchangable with serverless or dedicated sql pool, when it also can act as an orchestration tool. I seem a lot of people saying databricks + adl + datafactory is the  way to go, but then there is the added expensive of databricks and learning databricks.. I am  also a 1 man team on the reporting and data side, and i am also managing  another separate team on top of this (lol the tech recession is real).  point is, i feel very lost, and i dont want to spend a bunch of time migrating to a solution that wont scale or isnt cost effective. I appreciate any direction and advice you  fellow data engineers can give.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170ltz3", "is_robot_indexable": true, "report_reasons": null, "author": "soricellia", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170ltz3/solutions_to_migrate_off_of_dedicated_sql_pool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170ltz3/solutions_to_migrate_off_of_dedicated_sql_pool/", "subreddit_subscribers": 132406, "created_utc": 1696522062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, there!\n\nI am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.\n\nHere is my current architecture more or less:\n\n* Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response\n* I ingest that response data and pass the json data string into a message queue (azure service bus)\n* write the given event (row if you will) to a Azure data lake storage container called \"ingestion\"\n* move on\n* Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables\n* return the data in a data frame\n\n&amp;#x200B;\n\nI have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:\n\n&amp;#x200B;\n\n* If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record\n* If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row\n* Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.\n\nUltimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.\n\n&amp;#x200B;\n\nAm I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.\n\nAs I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.\n\nI appreciate your wisdom and guidance in advance!\n\n&amp;#x200B;\n\nCheers,\n\nIsaiah", "author_fullname": "t2_92b8a2oo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upsert Parquet Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171168w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696560398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, there!&lt;/p&gt;\n\n&lt;p&gt;I am new to cloud data engineering (Azure) and I am exploring trying to move data from an event driven webhook to data lake storage to a master parquet file via upsert.&lt;/p&gt;\n\n&lt;p&gt;Here is my current architecture more or less:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Our source dataset (point of sale software) will bring in repair order data row as a webhook event as a json response&lt;/li&gt;\n&lt;li&gt;I ingest that response data and pass the json data string into a message queue (azure service bus)&lt;/li&gt;\n&lt;li&gt;write the given event (row if you will) to a Azure data lake storage container called &amp;quot;ingestion&amp;quot;&lt;/li&gt;\n&lt;li&gt;move on&lt;/li&gt;\n&lt;li&gt;Azure data factory (ADF) every minute (interval) will fire to call our receivemessage endpoint that will multiprocess collect all the json data strings and make one efficient query to obtain all the webhook data from our database for point of sale software to get the full data picture (not just basic event driven data) from the raw tables&lt;/li&gt;\n&lt;li&gt;return the data in a data frame&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have preloaded our repair order data (historical if you will to current date as when I last ran it - about 4 Million records). The goal is to have ADF somehow trigger an upsert to either digest the data frame as a parquet to the master preloaded repair order. I would like the following to happen:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If a new record comes in from this webhook that we went and queried data for and it does not exist in the master preloaded repair order parquet file to insert the record&lt;/li&gt;\n&lt;li&gt;If the record comes in and it exists in the pre loaded repair order master file but it has changed (status, sale amount, etc) to update the row&lt;/li&gt;\n&lt;li&gt;Lastly, delete the row if it was deleted on the database side, exists on the master pre loaded file, and needs to be deleted.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ultimately, the goal is for the data lake parquet file for repair orders to match the database so its 1:1. Obviously not real time, but, on event as events occur be as close to a mirror of the database as possible.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Am I on the right path here architecturally? What should I be doing if its not the right path? I was looking into using Delta Lake sink in Mapping Data Flow activity to do this but I have no idea how this would even work to do what I outlined above.&lt;/p&gt;\n\n&lt;p&gt;As I would imagine, I am sure you might have clarifying questions, so please, feel free to ask away and poke holes (with grace :) ) as you see fit.&lt;/p&gt;\n\n&lt;p&gt;I appreciate your wisdom and guidance in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;Isaiah&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171168w", "is_robot_indexable": true, "report_reasons": null, "author": "realeyezayuh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171168w/upsert_parquet_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171168w/upsert_parquet_data/", "subreddit_subscribers": 132406, "created_utc": 1696560398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y9qpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built an open-source scraping API that returns structured JSON data using GPT.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170m9lu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696523115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/semanser/JsonGenius", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "170m9lu", "is_robot_indexable": true, "report_reasons": null, "author": "semanser", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170m9lu/i_built_an_opensource_scraping_api_that_returns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/semanser/JsonGenius", "subreddit_subscribers": 132406, "created_utc": 1696523115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:\n\n\\- Airbyte for extracting data and insert into my data warehouse( Snowflake)\n\n\\- DBT Cloud for transformation \n\n\\- A PostgresDB for OLTP, which will be accessed by the user facing API and UI\n\nData freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say `A` and `B` transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don't have to worry about write.\n\nI can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?\n\n&amp;#x200B;", "author_fullname": "t2_ap7kqtwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools to use for extracting data from OLAP back to OLTP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171dzqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696603059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:&lt;/p&gt;\n\n&lt;p&gt;- Airbyte for extracting data and insert into my data warehouse( Snowflake)&lt;/p&gt;\n\n&lt;p&gt;- DBT Cloud for transformation &lt;/p&gt;\n\n&lt;p&gt;- A PostgresDB for OLTP, which will be accessed by the user facing API and UI&lt;/p&gt;\n\n&lt;p&gt;Data freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don&amp;#39;t have to worry about write.&lt;/p&gt;\n\n&lt;p&gt;I can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171dzqk", "is_robot_indexable": true, "report_reasons": null, "author": "hksparrowboy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "subreddit_subscribers": 132406, "created_utc": 1696603059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.\n\n1. I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)\n\n2. We use Data integration (Informatica) to load the files to Snowflake. \n\nAs you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to collect data via API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171dfvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696601730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use Data integration (Informatica) to load the files to Snowflake. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171dfvx", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "subreddit_subscribers": 132406, "created_utc": 1696601730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.\n\nThe strings always have the same format, such as \"MM2525 6562451 Some text\". So from this string I would only want the \"6562451\" bit. I tried to find regular expressions like python but got no luck. Thanks in advance!\n\nP.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting digits from a string column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a9o6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696593077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.&lt;/p&gt;\n\n&lt;p&gt;The strings always have the same format, such as &amp;quot;MM2525 6562451 Some text&amp;quot;. So from this string I would only want the &amp;quot;6562451&amp;quot; bit. I tried to find regular expressions like python but got no luck. Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;P.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171a9o6", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "subreddit_subscribers": 132406, "created_utc": 1696593077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On the surface it's a basic ETL problem but with some additional complications. I'm curious how other experienced engineers would approach this problem to see if I'm overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.\n\nContext\n\n* 200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they're not NDJSON.\n* The files have to parsed so we can transform them into a unified schema and remove the data we're not interested in.\n* Four of the fields have to be enriched.  \n\n   * We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.\n   * There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.\n   * There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don't already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party\n   * Last we need to call another internal api that can scale to any rps.\n* Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it's accessible but not searchable.\n* We'll have to run this process monthly.", "author_fullname": "t2_42grp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach parsing, enriching, and storing 200M JSON records?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a00g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696592257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the surface it&amp;#39;s a basic ETL problem but with some additional complications. I&amp;#39;m curious how other experienced engineers would approach this problem to see if I&amp;#39;m overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.&lt;/p&gt;\n\n&lt;p&gt;Context&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they&amp;#39;re not NDJSON.&lt;/li&gt;\n&lt;li&gt;The files have to parsed so we can transform them into a unified schema and remove the data we&amp;#39;re not interested in.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Four of the fields have to be enriched.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.&lt;/li&gt;\n&lt;li&gt;There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.&lt;/li&gt;\n&lt;li&gt;There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don&amp;#39;t already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party&lt;/li&gt;\n&lt;li&gt;Last we need to call another internal api that can scale to any rps.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it&amp;#39;s accessible but not searchable.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ll have to run this process monthly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171a00g", "is_robot_indexable": true, "report_reasons": null, "author": "Detz", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "subreddit_subscribers": 132406, "created_utc": 1696592257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: [Enabling static analysis of SQL queries at Meta - (fb.com)](https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/)\n\nAlso would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Static analysers for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1719j9d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696590729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: &lt;a href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\"&gt;Enabling static analysis of SQL queries at Meta - (fb.com)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?auto=webp&amp;s=909387532145f6d6f4319d06cb1d6208fd43c145", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0efd775c5705334b59afbea64c2e78caed9ad31e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe42988ed379e8ee52fae618a3da0840033d3a52", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7c6656435d7d9d9f8c61b4b73fc8c8c24758a35", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0ded8d78913a04e500afd810ac273773bb3f4a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b3a60647f47ba03c6686405d656724bf8664f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13f763cc226b0b456ab56df4143da9da2d4db438", "width": 1080, "height": 607}], "variants": {}, "id": "Ven08R7rW3VUewldA00PYwtDrsf5TziOv0R5SeV6HyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1719j9d", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "subreddit_subscribers": 132406, "created_utc": 1696590729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? ", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using python azure function for data transformation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171b9qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696595996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171b9qn", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "subreddit_subscribers": 132406, "created_utc": 1696595996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have pipelines that run weekly and they're dependent on what clients send us. Some clients send us the same shit some can't seem to figure it out and their pipe fails. To what degree do you modify a pipeline? Is it just to get things moving or is there a way to gold-plate it and hope they don't find another way to fuck it up.\n\nThings that clients have fucked up in the last week,\nDifferent file format\nDifferent header\nDifferent data types", "author_fullname": "t2_vtm8z2o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do you decide to modify a pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170wvxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696548419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have pipelines that run weekly and they&amp;#39;re dependent on what clients send us. Some clients send us the same shit some can&amp;#39;t seem to figure it out and their pipe fails. To what degree do you modify a pipeline? Is it just to get things moving or is there a way to gold-plate it and hope they don&amp;#39;t find another way to fuck it up.&lt;/p&gt;\n\n&lt;p&gt;Things that clients have fucked up in the last week,\nDifferent file format\nDifferent header\nDifferent data types&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170wvxc", "is_robot_indexable": true, "report_reasons": null, "author": "Action_Maxim", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170wvxc/at_what_point_do_you_decide_to_modify_a_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170wvxc/at_what_point_do_you_decide_to_modify_a_pipeline/", "subreddit_subscribers": 132406, "created_utc": 1696548419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don't feel good about this.", "author_fullname": "t2_cj0b4yt6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170twld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696541230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,\nWhat are your views on Microsoft Purview as a Data Cataloguing solution for enterprise?\nMy org is pivoting from Collibra as an Enterprise Data  Cataloguing solution to MS Purview and I don&amp;#39;t feel good about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "170twld", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Most564", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170twld/data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170twld/data_catalog/", "subreddit_subscribers": 132406, "created_utc": 1696541230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery *and* airflow running.\n\nLong version: \n\nWe have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it's very basic; it reads data from multiple sources, normalizes them and puts em in our database.\n\nBut now that we're in the throes of airflow, we've found it to be a little... wanting. It's pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.\n\nAnother huge con is that we have a lot code that could be shared between our regular ol' django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.\n\nSO now I'm wanting to put our python back on our server, but I'd still like to schedule the tasks with airflow. I guess I'd make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don't want to just have it be a regular http endpoint.\n\nShould I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do initiate long-running jobs in a django server from airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_170s9n6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696537347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: I have a django server that I want to handle tasks that take 15+ minutes at a time. I would like airflow to initiate these tasks. I was looking into celery, but it seems weird and complicated to have both celery &lt;em&gt;and&lt;/em&gt; airflow running.&lt;/p&gt;\n\n&lt;p&gt;Long version: &lt;/p&gt;\n\n&lt;p&gt;We have an airflow instance running via cloud composer which handles most of our pipeline. As far as ETL stuff goes, it&amp;#39;s very basic; it reads data from multiple sources, normalizes them and puts em in our database.&lt;/p&gt;\n\n&lt;p&gt;But now that we&amp;#39;re in the throes of airflow, we&amp;#39;ve found it to be a little... wanting. It&amp;#39;s pretty cumbersome to get a decent local development and testing environment set up, so we just upload python files straight into production using the airflow UI. This obviously makes it hard to test.&lt;/p&gt;\n\n&lt;p&gt;Another huge con is that we have a lot code that could be shared between our regular ol&amp;#39; django server and our airflow DAGs. A whole lot of classes and methods are duplicated, copy-pasted back and forth all the time.&lt;/p&gt;\n\n&lt;p&gt;SO now I&amp;#39;m wanting to put our python back on our server, but I&amp;#39;d still like to schedule the tasks with airflow. I guess I&amp;#39;d make an authenticated endpoint for airflow to trigger the job on the server, but the tasks can take so long that I don&amp;#39;t want to just have it be a regular http endpoint.&lt;/p&gt;\n\n&lt;p&gt;Should I get a celery + rabbitMQ setup going on the server? Is that an unreasonably complicated system just for running a few dozen jobs every day?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "170s9n6", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/170s9n6/how_do_initiate_longrunning_jobs_in_a_django/", "subreddit_subscribers": 132406, "created_utc": 1696537347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am having issues setting hive parameters for my hive job in which I am insert overwriting data into my Orc master table from landing table based on updates/deletes.\n\nTable comprises of 116Cr records per partition, underlying orc files and total size per partition is 50 &amp; 15.4GBs respectively. How should i go about setting hive parameters to keep the job healthiest and all green on Dr. Elephant", "author_fullname": "t2_641fncom", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hive Parameters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171e3sq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696603332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am having issues setting hive parameters for my hive job in which I am insert overwriting data into my Orc master table from landing table based on updates/deletes.&lt;/p&gt;\n\n&lt;p&gt;Table comprises of 116Cr records per partition, underlying orc files and total size per partition is 50 &amp;amp; 15.4GBs respectively. How should i go about setting hive parameters to keep the job healthiest and all green on Dr. Elephant&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171e3sq", "is_robot_indexable": true, "report_reasons": null, "author": "Stooooopiied", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171e3sq/hive_parameters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171e3sq/hive_parameters/", "subreddit_subscribers": 132406, "created_utc": 1696603332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.", "author_fullname": "t2_jvtay3bqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meaning of title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_171d26s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696600808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "171d26s", "is_robot_indexable": true, "report_reasons": null, "author": "Southern_Version2681", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171d26s/meaning_of_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171d26s/meaning_of_title/", "subreddit_subscribers": 132406, "created_utc": 1696600808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?", "author_fullname": "t2_5n2f6pea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing ETL config", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171c29z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696598198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171c29z", "is_robot_indexable": true, "report_reasons": null, "author": "sosa_12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171c29z/managing_etl_config/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171c29z/managing_etl_config/", "subreddit_subscribers": 132406, "created_utc": 1696598198.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}