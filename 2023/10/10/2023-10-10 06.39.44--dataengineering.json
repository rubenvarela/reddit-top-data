{"kind": "Listing", "data": {"after": "t3_173tosp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have recently tried snowflake in my new company after using spark for several years.\n\nPersonally, I really like that it delivers what it promises, but I don't like lack of customization and the fact their support/sales have a know-it-all attitude and they ask you to trust everything works automagically.\n\nI am also concerned that in the long term this very strong lock-in in pair with a revamp of billing policies (think about dbt cloud) will cause many troubles to data teams. I already don't like the pricing of warehouses where you can only double the amount of spent credits to improve query performance...\n\nIs this a legitimate concern? With databricks you can always opt out and migrate your spark/delta codebase somewhere else. With snowflake I don't see this option.", "author_fullname": "t2_jni6yu7mo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nkx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696843711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have recently tried snowflake in my new company after using spark for several years.&lt;/p&gt;\n\n&lt;p&gt;Personally, I really like that it delivers what it promises, but I don&amp;#39;t like lack of customization and the fact their support/sales have a know-it-all attitude and they ask you to trust everything works automagically.&lt;/p&gt;\n\n&lt;p&gt;I am also concerned that in the long term this very strong lock-in in pair with a revamp of billing policies (think about dbt cloud) will cause many troubles to data teams. I already don&amp;#39;t like the pricing of warehouses where you can only double the amount of spent credits to improve query performance...&lt;/p&gt;\n\n&lt;p&gt;Is this a legitimate concern? With databricks you can always opt out and migrate your spark/delta codebase somewhere else. With snowflake I don&amp;#39;t see this option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173nkx2", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Data-810", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nkx2/what_are_your_thoughts_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nkx2/what_are_your_thoughts_on_snowflake/", "subreddit_subscribers": 132982, "created_utc": 1696843711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\ud83d\ude80 Exciting News! Just released my latest YouTube video - \"PySpark Tutorial for Beginners: 1-Hour Full Course\" \ud83d\udc0d\ud83d\udca1\n\nAre you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I'll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.\n\n\ud83d\udd25 What You'll Learn:\n\u2705 Spark Introduction\n\u2705 Spark Installation\n\u2705 Setting Up Your PySpark Environment\n\u2705 Spark RDD\n\u2705 DataFrame Operations\n\u2705 Spark SQL\n\u2705 And much more!\n\nWhether you're a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!\n\nWatch it now \ud83d\udc49 https://youtu.be/EB8lfdxpirM\nGitHub Repo \ud83d\udc49 https://github.com/coder2j/pyspark-tutorial\n\nDon't forget to like, subscribe, and share with your network. Let's spread the knowledge together! \ud83d\udcda\ud83d\udcaa\n#PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial", "author_fullname": "t2_h4j43yry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Tutorial for Beginners: 1-Hour Full Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_173lca6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 53, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "PySpark Tutorial for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/EB8lfdxpirM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/173lca6", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nyeAsJ_Iyn208srWSK-sRJw_-S14SsgknHSuV_YPKCM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696834356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\ude80 Exciting News! Just released my latest YouTube video - &amp;quot;PySpark Tutorial for Beginners: 1-Hour Full Course&amp;quot; \ud83d\udc0d\ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;Are you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I&amp;#39;ll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd25 What You&amp;#39;ll Learn:\n\u2705 Spark Introduction\n\u2705 Spark Installation\n\u2705 Setting Up Your PySpark Environment\n\u2705 Spark RDD\n\u2705 DataFrame Operations\n\u2705 Spark SQL\n\u2705 And much more!&lt;/p&gt;\n\n&lt;p&gt;Whether you&amp;#39;re a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!&lt;/p&gt;\n\n&lt;p&gt;Watch it now \ud83d\udc49 &lt;a href=\"https://youtu.be/EB8lfdxpirM\"&gt;https://youtu.be/EB8lfdxpirM&lt;/a&gt;\nGitHub Repo \ud83d\udc49 &lt;a href=\"https://github.com/coder2j/pyspark-tutorial\"&gt;https://github.com/coder2j/pyspark-tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t forget to like, subscribe, and share with your network. Let&amp;#39;s spread the knowledge together! \ud83d\udcda\ud83d\udcaa&lt;/p&gt;\n\n&lt;h1&gt;PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/EB8lfdxpirM", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?auto=webp&amp;s=721ceed222baa9b21597759f40ad168578d0ee74", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9318f73383bcd738f86d14479864d192ae47b270", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6616b558982a89e258beb94901e9d212ce6bf76a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b465b2e80a183e03df5ef8d4f758d78f53dd3454", "width": 320, "height": 240}], "variants": {}, "id": "wp5OUtzYt-eYRgOkWVPGa-zaQJ8PGPB2r-t4qza7MPs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173lca6", "is_robot_indexable": true, "report_reasons": null, "author": "Coder2j", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173lca6/pyspark_tutorial_for_beginners_1hour_full_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/EB8lfdxpirM", "subreddit_subscribers": 132982, "created_utc": 1696834356.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "PySpark Tutorial for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/EB8lfdxpirM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Asset Checks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_173vk4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhiyblumCHOKOoK-lBvHzVvyOgmHNAYhp2z8n_Wp51M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696867500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-asset-checks", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?auto=webp&amp;s=e030a0c6744899cf318d466d091adc29035e5fb8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=143a51bfaca13836974a1eb8e6901d1948b99659", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c025ff90dc104c5449805749243b8dfb2569d2d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5d0aaf3289eda4da7f88663372121f48413ccf8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2bda3089cdbf53d0668586c32c3b345a7baaef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fcd3e79d09c99f8433c137611706c368364eedd", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f23cc1d33afde6adb9d68a9b31c290336a2dce2", "width": 1080, "height": 567}], "variants": {}, "id": "1uxtatdbdRpuzzFcT7KLXP88fRefAJkIBDqgJ_1uvVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "173vk4f", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173vk4f/introducing_asset_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-asset-checks", "subreddit_subscribers": 132982, "created_utc": 1696867500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.", "author_fullname": "t2_uqyn3qdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an equivalent of \u201cThe Data Warehouse Toolkit\u201d but for Lakehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173sl3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696860130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173sl3s", "is_robot_indexable": true, "report_reasons": null, "author": "variance-explained", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "subreddit_subscribers": 132982, "created_utc": 1696860130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.\n\nI then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.\n\nThis has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).\n\nI rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it's 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?\n\nP.S. We are a complete Microsoft shop with Azure databricks.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I losing My Technical Edge By Just Fixing Mistakes By Contractors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1743emg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696886503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.&lt;/p&gt;\n\n&lt;p&gt;I then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.&lt;/p&gt;\n\n&lt;p&gt;This has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).&lt;/p&gt;\n\n&lt;p&gt;I rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it&amp;#39;s 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?&lt;/p&gt;\n\n&lt;p&gt;P.S. We are a complete Microsoft shop with Azure databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1743emg", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "subreddit_subscribers": 132982, "created_utc": 1696886503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked in a medium sized company where there's no data or BI platform to store sql related things. And there's only 2 data+BI people so we work pretty freestyle. \n\nI'm in charge of a mix of data &amp; BI role. the challenge I'm facing are primarily related to BI I guess.\n\nI processed about 10-20 queries each day and ended up copy-pasting everything from my sql client to notion ( to store some context about the task and sometimes the screenshot of the visualization)\n\nBut once the modifications of the same requirement comes back I needed to copy paste things back from the notepad into the sql client. Not to mention I need to make visualizations and share them using Google sheet.\n\nIt still kinda bugs me today. I wonder how do you guys manage those workflows when you don't have a good infra?", "author_fullname": "t2_cr04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you store your sql queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nrq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696848209.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696844494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked in a medium sized company where there&amp;#39;s no data or BI platform to store sql related things. And there&amp;#39;s only 2 data+BI people so we work pretty freestyle. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in charge of a mix of data &amp;amp; BI role. the challenge I&amp;#39;m facing are primarily related to BI I guess.&lt;/p&gt;\n\n&lt;p&gt;I processed about 10-20 queries each day and ended up copy-pasting everything from my sql client to notion ( to store some context about the task and sometimes the screenshot of the visualization)&lt;/p&gt;\n\n&lt;p&gt;But once the modifications of the same requirement comes back I needed to copy paste things back from the notepad into the sql client. Not to mention I need to make visualizations and share them using Google sheet.&lt;/p&gt;\n\n&lt;p&gt;It still kinda bugs me today. I wonder how do you guys manage those workflows when you don&amp;#39;t have a good infra?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173nrq2", "is_robot_indexable": true, "report_reasons": null, "author": "jchnxu", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nrq2/where_do_you_store_your_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nrq2/where_do_you_store_your_sql_queries/", "subreddit_subscribers": 132982, "created_utc": 1696844494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am defining data validation standards for our data pipelines  on ingestion. I am planning that checks should be done in the pipeline itself. So far I have this list:\n\n1. Check for schema changes\n2. Check if received data matches the agreed schema\n3. Nulls, Unique, where applicable\n4. Check for integrity by comparing to other datasets (like that FK and master tables\n5. Maybe check counts vs some periodic averages\n\n&amp;#x200B;\n\nAny other recommendations? Even if we aim to ingest raw data, we will want to check its validity before storing. These are for structured data from structured sources that will be stored in a columnar database.\n\nAlso, maybe those would be an overkill? I am not a DE, but I am responsible for Data Quality. So looking for some practical suggestions that would not seem too crazy for the team.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_49dbxejy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for data validation on data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173kp9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696831831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am defining data validation standards for our data pipelines  on ingestion. I am planning that checks should be done in the pipeline itself. So far I have this list:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Check for schema changes&lt;/li&gt;\n&lt;li&gt;Check if received data matches the agreed schema&lt;/li&gt;\n&lt;li&gt;Nulls, Unique, where applicable&lt;/li&gt;\n&lt;li&gt;Check for integrity by comparing to other datasets (like that FK and master tables&lt;/li&gt;\n&lt;li&gt;Maybe check counts vs some periodic averages&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations? Even if we aim to ingest raw data, we will want to check its validity before storing. These are for structured data from structured sources that will be stored in a columnar database.&lt;/p&gt;\n\n&lt;p&gt;Also, maybe those would be an overkill? I am not a DE, but I am responsible for Data Quality. So looking for some practical suggestions that would not seem too crazy for the team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173kp9o", "is_robot_indexable": true, "report_reasons": null, "author": "HereJustForAnswers", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173kp9o/ideas_for_data_validation_on_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173kp9o/ideas_for_data_validation_on_data_ingestion/", "subreddit_subscribers": 132982, "created_utc": 1696831831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi there \n\nI\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  \n \n\nIs DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?", "author_fullname": "t2_iweexjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow + DBT - question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17481kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696898583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  &lt;/p&gt;\n\n&lt;p&gt;Is DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17481kb", "is_robot_indexable": true, "report_reasons": null, "author": "yeager_doug", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17481kb/airflow_dbt_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17481kb/airflow_dbt_question/", "subreddit_subscribers": 132982, "created_utc": 1696898583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For background I'm a BI analyst, but helping set up a friend's startup infrastructure. They are early on and want to pull data from CRM's using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don't want to spend much or any money on this initially.\n\nFirst step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?\n\nI was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?\n\nAt first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.", "author_fullname": "t2_4qxl8qxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering pipeline for free or low cost in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173x67u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696871430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For background I&amp;#39;m a BI analyst, but helping set up a friend&amp;#39;s startup infrastructure. They are early on and want to pull data from CRM&amp;#39;s using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don&amp;#39;t want to spend much or any money on this initially.&lt;/p&gt;\n\n&lt;p&gt;First step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?&lt;/p&gt;\n\n&lt;p&gt;I was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?&lt;/p&gt;\n\n&lt;p&gt;At first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173x67u", "is_robot_indexable": true, "report_reasons": null, "author": "whittesc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "subreddit_subscribers": 132982, "created_utc": 1696871430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.\n\n&amp;#x200B;\n\nI was having a hard time finding ANY guides about how to set up all the components locally; [so I made a guide](https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;utm_campaign=post&amp;utm_medium=web). It goes over setting up and configuring:\n\n&amp;#x200B;\n\n* Iceberg Rest Catalog\n* Trino\n* Minio (s3 compatible for storage of Iceberg tables)\n* Kafka (3 Node KRaft Mode)\n* Kafka Connect\n* Schema Registry\n* Writing avro encoded data to Kafka in go\n\n&amp;#x200B;\n\nOnce done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  \n\n\nHope this helps :)  \n\n\n&amp;#x200B;", "author_fullname": "t2_31vknkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Data From Kafka into Apache Iceberg + Querying with Trino - Done in Docker Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173w6rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696869032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was having a hard time finding ANY guides about how to set up all the components locally; &lt;a href=\"https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;amp;utm_campaign=post&amp;amp;utm_medium=web\"&gt;so I made a guide&lt;/a&gt;. It goes over setting up and configuring:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Iceberg Rest Catalog&lt;/li&gt;\n&lt;li&gt;Trino&lt;/li&gt;\n&lt;li&gt;Minio (s3 compatible for storage of Iceberg tables)&lt;/li&gt;\n&lt;li&gt;Kafka (3 Node KRaft Mode)&lt;/li&gt;\n&lt;li&gt;Kafka Connect&lt;/li&gt;\n&lt;li&gt;Schema Registry&lt;/li&gt;\n&lt;li&gt;Writing avro encoded data to Kafka in go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Once done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  &lt;/p&gt;\n\n&lt;p&gt;Hope this helps :)  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?auto=webp&amp;s=7de03447d3a223deb509f88e091a03da6c93400c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=572c8360ccf24454b4b7b08d84dea4009543f95c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=337134c15aac8e57593f82b672a5fb12a2ef98e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c89632662beabc291281fd8ae813b346a65979", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63293a10ef238fbeead34150c54341ea690a80e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1985f6e22592db1ecaafb1908b69602ee237fc61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0451f1bef6592955e4a6d41575544c83616059c7", "width": 1080, "height": 540}], "variants": {}, "id": "My3S49gmIim5Z0I62eSqxwTyEEfinz1ddzP28sGEN6k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173w6rf", "is_robot_indexable": true, "report_reasons": null, "author": "thehendoxc", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "subreddit_subscribers": 132982, "created_utc": 1696869032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.", "author_fullname": "t2_bfrtvbx9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer from ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174aeb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696905373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174aeb1", "is_robot_indexable": true, "report_reasons": null, "author": "Charming_Function_35", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "subreddit_subscribers": 132982, "created_utc": 1696905373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to document the data flows of a BI product using a data lineage tool. Basically the to-be-documented flow is as follows: copy data from ERP source --&gt; transform data using SQL --&gt; store in data warehouse --&gt; load in Power BI --&gt; do some small calculations in Power BI.\n\nI have looked into tools which can do this, however many consist of large data catalog/data management systems such as Atlan. These large packages however I do not need. I am only looking for a tool in which I can manually or automatically extract and document the lineage of my data flow, that's all. \n\nDoes anyone have any tips or experience with using these specific types of tools? All information is appreciated!", "author_fullname": "t2_3tjzfxzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lineage tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173o66v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696846089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to document the data flows of a BI product using a data lineage tool. Basically the to-be-documented flow is as follows: copy data from ERP source --&amp;gt; transform data using SQL --&amp;gt; store in data warehouse --&amp;gt; load in Power BI --&amp;gt; do some small calculations in Power BI.&lt;/p&gt;\n\n&lt;p&gt;I have looked into tools which can do this, however many consist of large data catalog/data management systems such as Atlan. These large packages however I do not need. I am only looking for a tool in which I can manually or automatically extract and document the lineage of my data flow, that&amp;#39;s all. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any tips or experience with using these specific types of tools? All information is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173o66v", "is_robot_indexable": true, "report_reasons": null, "author": "basr98", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173o66v/data_lineage_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173o66v/data_lineage_tools/", "subreddit_subscribers": 132982, "created_utc": 1696846089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI've put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you're using dbt for your data transformations, this is for you!\n\n\ud83c\udf1f **What's Inside?**\n\n* Specialized dbt functions not in standard Jinja.\n* Unique macros for dbt workflows.\n* Filters, variables, and context methods to elevate your dbt projects.\n\n\ud83d\udd17 [https://datacoves.com/post/dbt-jinja-functions-cheat-sheet](https://datacoves.com/post/dbt-jinja-functions-cheat-sheet)\n\nWould love to hear your feedback and any suggestions for improvement. \ud83d\udca1\n\n\u2764\ufe0f Data-Queen", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hope this helps!: \ud83d\ude80 Ultimate dbt Jinja Functions Cheat Sheet!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173yg5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696874520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you&amp;#39;re using dbt for your data transformations, this is for you!&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf1f &lt;strong&gt;What&amp;#39;s Inside?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Specialized dbt functions not in standard Jinja.&lt;/li&gt;\n&lt;li&gt;Unique macros for dbt workflows.&lt;/li&gt;\n&lt;li&gt;Filters, variables, and context methods to elevate your dbt projects.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;\ud83d\udd17 &lt;a href=\"https://datacoves.com/post/dbt-jinja-functions-cheat-sheet\"&gt;https://datacoves.com/post/dbt-jinja-functions-cheat-sheet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your feedback and any suggestions for improvement. \ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;\u2764\ufe0f Data-Queen&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?auto=webp&amp;s=761c2339c1c3a5a98098f2be0b77af737b8c59d2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=659627a31f99370f0cbb33fd4514988c6c50bc57", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a2a4756bd7205c1f3c0047c8f0b8ec554362b97", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=432641e4b76d230646cffa01cbbc8405158fe8db", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=662713884f50b946f205be681d5e74fe94225d30", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f76c268382b23cb18587bc9c0dfd00422ee1f65", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceb922cd1c128faa388775cdb58bcd5948f731d3", "width": 1080, "height": 564}], "variants": {}, "id": "ghkewI0Ac4MJEXxpnppvl085nXvleyOWWENREIl1wOE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173yg5n", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "subreddit_subscribers": 132982, "created_utc": 1696874520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  \n\nI work for a very old company so we have sales/financial/build records going all the way back to the 1970's....but I have never worked on any projects where anyone looked at anything older than 5 years.    \n\nEvery time I build anything, the business always wants \"everything\" and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn't exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.", "author_fullname": "t2_l5hazyvs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prevent data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173rxlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696858427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  &lt;/p&gt;\n\n&lt;p&gt;I work for a very old company so we have sales/financial/build records going all the way back to the 1970&amp;#39;s....but I have never worked on any projects where anyone looked at anything older than 5 years.    &lt;/p&gt;\n\n&lt;p&gt;Every time I build anything, the business always wants &amp;quot;everything&amp;quot; and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn&amp;#39;t exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173rxlz", "is_robot_indexable": true, "report_reasons": null, "author": "One_Ball8812", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "subreddit_subscribers": 132982, "created_utc": 1696858427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.\n\nHowever, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don't use it much). The only downside is that the bank doesn't work with cloud solutions, and I'm not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.\n\nOn the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don't want to give up that part of my job), and are willing to provide the necessary training.\n\nThe salaries for both positions are the same, as the company will also give me a raise.\n\nHere's my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that's not true and starting with an on prem solution will be better. What do you think about this?\n\nThanks for all the advice!", "author_fullname": "t2_7jc8db23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173r3am", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696856073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.&lt;/p&gt;\n\n&lt;p&gt;However, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don&amp;#39;t use it much). The only downside is that the bank doesn&amp;#39;t work with cloud solutions, and I&amp;#39;m not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don&amp;#39;t want to give up that part of my job), and are willing to provide the necessary training.&lt;/p&gt;\n\n&lt;p&gt;The salaries for both positions are the same, as the company will also give me a raise.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that&amp;#39;s not true and starting with an on prem solution will be better. What do you think about this?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all the advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173r3am", "is_robot_indexable": true, "report_reasons": null, "author": "eqwlknam", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173r3am/job_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173r3am/job_advice/", "subreddit_subscribers": 132982, "created_utc": 1696856073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.\n\nMy first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.\n\nThe only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.\n\nRepartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.", "author_fullname": "t2_u4zm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark - How to break up extremely compressed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174c1zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696910635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.&lt;/p&gt;\n\n&lt;p&gt;My first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.&lt;/p&gt;\n\n&lt;p&gt;The only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.&lt;/p&gt;\n\n&lt;p&gt;Repartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174c1zs", "is_robot_indexable": true, "report_reasons": null, "author": "JohnStud85", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "subreddit_subscribers": 132982, "created_utc": 1696910635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I have a case of eCommerce data where the transactional database has `orders` and `products`. I'm confused as to how to model this in a star schema.\n\nMy thoughts are that the `orders` should be a fact table (`fct_orders`) containing an aggregate of `products` total value, ie `fct_orders.total_order_value`. However, if an order has multiple products, what would the foreign key look like in `fct_orders`?\n\nI've tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my `products` table the fact table. However, how can I model the aggregate `fct_orders.total_order_value`?\n\nThanks in advance!\n\n&amp;#x200B;\n\nEdit: just learned about bridge tables ([https://www.leapfrogbi.com/bridge-tables/](https://www.leapfrogbi.com/bridge-tables/)), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...", "author_fullname": "t2_9trbw25s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model one fact table that joins to multiple rows in dimension table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174avck", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696908569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696906843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I have a case of eCommerce data where the transactional database has &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;products&lt;/code&gt;. I&amp;#39;m confused as to how to model this in a star schema.&lt;/p&gt;\n\n&lt;p&gt;My thoughts are that the &lt;code&gt;orders&lt;/code&gt; should be a fact table (&lt;code&gt;fct_orders&lt;/code&gt;) containing an aggregate of &lt;code&gt;products&lt;/code&gt; total value, ie &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;. However, if an order has multiple products, what would the foreign key look like in &lt;code&gt;fct_orders&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my &lt;code&gt;products&lt;/code&gt; table the fact table. However, how can I model the aggregate &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: just learned about bridge tables (&lt;a href=\"https://www.leapfrogbi.com/bridge-tables/\"&gt;https://www.leapfrogbi.com/bridge-tables/&lt;/a&gt;), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?auto=webp&amp;s=736d9650919a08025a3b529a262d040d5eccc7c4", "width": 508, "height": 553}, "resolutions": [{"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=709171c289f304a1417184d7d2ebada30a823909", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c77e4d74b3cfefe997d1d3a271f7c82a6eaa5c5d", "width": 216, "height": 235}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6b309475f6fb91e4e9b925f4e3d75edb053c17", "width": 320, "height": 348}], "variants": {}, "id": "bIfqiV67OXElDsn2F_5SezvPv1apC5LPOp1agLPkfo4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174avck", "is_robot_indexable": true, "report_reasons": null, "author": "ternary-thought", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "subreddit_subscribers": 132982, "created_utc": 1696906843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm struggling to efficiently join data when I have multiple failsafe join points.\n\nSpecifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we'll just consider the three utm campaign parameters:\n\n* utm\\_term\n* utm\\_content\n* utm\\_campaign\n\nI want to join my data based on utm\\_term *if that's possible.* But if it's not, I'll fall back on utm\\_content or utm\\_campaign instead.\n\n**The problem** is that any SQL join I'm aware of that uses multiple join points will use every join point possible. So, currently, I'm dealing with this with a two-step process.\n\nFirst, I find the best join point available for each row of data...\n\n    UPDATE session_data a\n    SET a.Join_Type = b.Join_Type\n    FROM (\n        SELECT\n            session_id,\n            CASE\n                WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &gt; 0 THEN 'utm_term'\n                WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &gt; 0 THEN 'utm_content'\n                WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &gt; 0 THEN 'utm_campaign'\n               ELSE 'Channel'\n            END AS Join_Type\n            FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n            LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n            ON ga.channel = ad.channel AND (\n                ga.utm_term = ad.utm_term OR \n                ga.utm_content = ad.utm_content OR \n                ga.utm_campaign = ad.utm_campaign OR \n            GROUP BY session_id\n        )\n    ) b\n    WHERE a.session_id = b.session_id;\n\n... and then I use that label to join by the best join point available only:\n\n    SELECT * \n    FROM `session_data` ga\n    LEFT JOIN `ad_data` ad\n    WHERE \n    CASE\n        WHEN ga.Join_Type = 'utm_term' THEN ga.utm_term = ad.utm_term\n        WHEN ga.Join_Type = 'utm_content' THEN ga.utm_content = ad.utm_content\n        WHEN ga.Join_Type = 'utm_campaign' THEN ga.utm_campaign = ad.utm_campaign\n        WHEN ga.Join_Type = 'Channel' THEN ga.channel = ad.channel\n    END\n\nWhich works! \n\n*(I mean, I'm leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)*\n\nThat first query, in particular, is *super* problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I'm working in), just results in an error message.\n\nThere has got to be a better way to tackle this join. Anyone know of one?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a more efficient way to do a join by multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1742rzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696884953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m struggling to efficiently join data when I have multiple failsafe join points.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we&amp;#39;ll just consider the three utm campaign parameters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;utm_term&lt;/li&gt;\n&lt;li&gt;utm_content&lt;/li&gt;\n&lt;li&gt;utm_campaign&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to join my data based on utm_term &lt;em&gt;if that&amp;#39;s possible.&lt;/em&gt; But if it&amp;#39;s not, I&amp;#39;ll fall back on utm_content or utm_campaign instead.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt; is that any SQL join I&amp;#39;m aware of that uses multiple join points will use every join point possible. So, currently, I&amp;#39;m dealing with this with a two-step process.&lt;/p&gt;\n\n&lt;p&gt;First, I find the best join point available for each row of data...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;UPDATE session_data a\nSET a.Join_Type = b.Join_Type\nFROM (\n    SELECT\n        session_id,\n        CASE\n            WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_term&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_content&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_campaign&amp;#39;\n           ELSE &amp;#39;Channel&amp;#39;\n        END AS Join_Type\n        FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n        LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n        ON ga.channel = ad.channel AND (\n            ga.utm_term = ad.utm_term OR \n            ga.utm_content = ad.utm_content OR \n            ga.utm_campaign = ad.utm_campaign OR \n        GROUP BY session_id\n    )\n) b\nWHERE a.session_id = b.session_id;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;... and then I use that label to join by the best join point available only:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * \nFROM `session_data` ga\nLEFT JOIN `ad_data` ad\nWHERE \nCASE\n    WHEN ga.Join_Type = &amp;#39;utm_term&amp;#39; THEN ga.utm_term = ad.utm_term\n    WHEN ga.Join_Type = &amp;#39;utm_content&amp;#39; THEN ga.utm_content = ad.utm_content\n    WHEN ga.Join_Type = &amp;#39;utm_campaign&amp;#39; THEN ga.utm_campaign = ad.utm_campaign\n    WHEN ga.Join_Type = &amp;#39;Channel&amp;#39; THEN ga.channel = ad.channel\nEND\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Which works! &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(I mean, I&amp;#39;m leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;That first query, in particular, is &lt;em&gt;super&lt;/em&gt; problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I&amp;#39;m working in), just results in an error message.&lt;/p&gt;\n\n&lt;p&gt;There has got to be a better way to tackle this join. Anyone know of one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1742rzg", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "subreddit_subscribers": 132982, "created_utc": 1696884953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an SWE (**not a data scientist**) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.\n\nI started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don't think they solve the issue of validating *changes in data* (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.\n\nWhat I'm looking for is a tool that validates changes in data by comparing the previous value with the new value.\n\nIn some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there's obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).\n\nThe purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.\n\nThis is just an example, but it would be helpful if we can call an API to do this sort of validation for us.\n\nAnd instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I'm just brainstorming here.\n\nWould highly appreciate some recommendations/tips for tackling this problem. Thank you!", "author_fullname": "t2_mxg44sgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to validate data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174236b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696883926.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696883258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an SWE (&lt;strong&gt;not a data scientist&lt;/strong&gt;) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.&lt;/p&gt;\n\n&lt;p&gt;I started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don&amp;#39;t think they solve the issue of validating &lt;em&gt;changes in data&lt;/em&gt; (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a tool that validates changes in data by comparing the previous value with the new value.&lt;/p&gt;\n\n&lt;p&gt;In some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there&amp;#39;s obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).&lt;/p&gt;\n\n&lt;p&gt;The purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.&lt;/p&gt;\n\n&lt;p&gt;This is just an example, but it would be helpful if we can call an API to do this sort of validation for us.&lt;/p&gt;\n\n&lt;p&gt;And instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I&amp;#39;m just brainstorming here.&lt;/p&gt;\n\n&lt;p&gt;Would highly appreciate some recommendations/tips for tackling this problem. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174236b", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Main-6700", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174236b/how_to_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174236b/how_to_validate_data/", "subreddit_subscribers": 132982, "created_utc": 1696883258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as title", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good place where I can get all 2024 summer data engineering internships ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17403jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696878408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17403jr", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17403jr/any_good_place_where_i_can_get_all_2024_summer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17403jr/any_good_place_where_i_can_get_all_2024_summer/", "subreddit_subscribers": 132982, "created_utc": 1696878408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for working with dbt and Snowflake - A practitioner\u2019s guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17401u2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UzBOjGfWVmMW0PI3xw6ehMGg5y-n8aHgUpGbmnCVs8I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696878298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?auto=webp&amp;s=e7612f0c09356ae4bc3d254edd02e167a83a2733", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cafd6d1b68fdc7b4bb1b5c215278d61a2577a01", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56902c994507c9bd1a79cb15dea9b39629b5c9d4", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22cc3d82dc59750bc0cc8283f655d26727bd3da0", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aeea40b0ea537d47169953badb79986134a4da7a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20973f9c6ce81bee6ca454d8a853e4101a883306", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6020d7897990fb7542d5ce7123b8536afa9b7fee", "width": 1080, "height": 720}], "variants": {}, "id": "ucEfKV0PvxKYvUs4LpGpFLHRp3wQfXsbHwo066xOa3c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17401u2", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17401u2/best_practices_for_working_with_dbt_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "subreddit_subscribers": 132982, "created_utc": 1696878298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious how you guys all handle dimensions that have a one to many relationship? I.e. lets say you have a customer dimension, and with the customer dimension (assume non SCD, 1 customer per row), a customer may have multiple contacts (1 to many)  \nThe aggregate would end up double/triple counting if reporting by contacts.   \nSo the Contacts is more of a lookup table.. would you still call it a Dimension or would you have some other standard / naming convention for these?", "author_fullname": "t2_8t7dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process for one to many dimensions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1746kza", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696894621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious how you guys all handle dimensions that have a one to many relationship? I.e. lets say you have a customer dimension, and with the customer dimension (assume non SCD, 1 customer per row), a customer may have multiple contacts (1 to many)&lt;br/&gt;\nThe aggregate would end up double/triple counting if reporting by contacts.&lt;br/&gt;\nSo the Contacts is more of a lookup table.. would you still call it a Dimension or would you have some other standard / naming convention for these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1746kza", "is_robot_indexable": true, "report_reasons": null, "author": "yummypoutine", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1746kza/process_for_one_to_many_dimensions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1746kza/process_for_one_to_many_dimensions/", "subreddit_subscribers": 132982, "created_utc": 1696894621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).\n\nHow do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?\n\nHow do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?\n\nSorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  \n\n\n**NOTE**: when I'm talking about ETL development lifecycle, I actually mean data pipelines in general.", "author_fullname": "t2_21xquoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E2E ETL lifecycle in multi-environment setup (bonus for integration with E2E analytics)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1740atd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696880710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696878890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).&lt;/p&gt;\n\n&lt;p&gt;How do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?&lt;/p&gt;\n\n&lt;p&gt;How do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?&lt;/p&gt;\n\n&lt;p&gt;Sorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: when I&amp;#39;m talking about ETL development lifecycle, I actually mean data pipelines in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1740atd", "is_robot_indexable": true, "report_reasons": null, "author": "kaributas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "subreddit_subscribers": 132982, "created_utc": 1696878890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "`singer-sdk 0.32.0 requires python-dotenv&lt;0.22,&gt;=0.20, but you have python-dotenv 1.0.0 which is incompatible.`\n\nOh yes, let's intall a compatible version of python-dotenv then.\n\n  \n`meltano 3.1.0 requires python-dotenv&lt;2.0.0,&gt;=1.0.0, but you have python-dotenv 0.21.1 which is incompatible.`\n\nWelcome to the loop.\n\nAnd singer-sdk comes from... Meltano.", "author_fullname": "t2_gyvub1d7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Love these impossible dependencies between meltano and singer-sdk", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173y0e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696873459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;code&gt;singer-sdk 0.32.0 requires python-dotenv&amp;lt;0.22,&amp;gt;=0.20, but you have python-dotenv 1.0.0 which is incompatible.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Oh yes, let&amp;#39;s intall a compatible version of python-dotenv then.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;meltano 3.1.0 requires python-dotenv&amp;lt;2.0.0,&amp;gt;=1.0.0, but you have python-dotenv 0.21.1 which is incompatible.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Welcome to the loop.&lt;/p&gt;\n\n&lt;p&gt;And singer-sdk comes from... Meltano.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173y0e4", "is_robot_indexable": true, "report_reasons": null, "author": "Kintsugeek", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173y0e4/love_these_impossible_dependencies_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173y0e4/love_these_impossible_dependencies_between/", "subreddit_subscribers": 132982, "created_utc": 1696873459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nLet's say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.\n\n&amp;#x200B;\n\nHow do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?\n\n&amp;#x200B;\n\nThank you.", "author_fullname": "t2_a7y0xzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you model customer behavior tracking data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173tosp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696862916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173tosp", "is_robot_indexable": true, "report_reasons": null, "author": "masamibb", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "subreddit_subscribers": 132982, "created_utc": 1696862916.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}