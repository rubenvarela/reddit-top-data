{"kind": "Listing", "data": {"after": "t3_174g2q6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Asset Checks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_173vk4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhiyblumCHOKOoK-lBvHzVvyOgmHNAYhp2z8n_Wp51M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696867500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-asset-checks", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?auto=webp&amp;s=e030a0c6744899cf318d466d091adc29035e5fb8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=143a51bfaca13836974a1eb8e6901d1948b99659", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c025ff90dc104c5449805749243b8dfb2569d2d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5d0aaf3289eda4da7f88663372121f48413ccf8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2bda3089cdbf53d0668586c32c3b345a7baaef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fcd3e79d09c99f8433c137611706c368364eedd", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f23cc1d33afde6adb9d68a9b31c290336a2dce2", "width": 1080, "height": 567}], "variants": {}, "id": "1uxtatdbdRpuzzFcT7KLXP88fRefAJkIBDqgJ_1uvVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "173vk4f", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173vk4f/introducing_asset_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-asset-checks", "subreddit_subscribers": 133049, "created_utc": 1696867500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.\n\nI then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.\n\nThis has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).\n\nI rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it's 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?\n\nP.S. We are a complete Microsoft shop with Azure databricks.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I losing My Technical Edge By Just Fixing Mistakes By Contractors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1743emg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696886503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.&lt;/p&gt;\n\n&lt;p&gt;I then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.&lt;/p&gt;\n\n&lt;p&gt;This has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).&lt;/p&gt;\n\n&lt;p&gt;I rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it&amp;#39;s 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?&lt;/p&gt;\n\n&lt;p&gt;P.S. We are a complete Microsoft shop with Azure databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1743emg", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "subreddit_subscribers": 133049, "created_utc": 1696886503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.", "author_fullname": "t2_bfrtvbx9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer from ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174aeb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696905373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174aeb1", "is_robot_indexable": true, "report_reasons": null, "author": "Charming_Function_35", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "subreddit_subscribers": 133049, "created_utc": 1696905373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi there \n\nI\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  \n \n\nIs DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?", "author_fullname": "t2_iweexjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow + DBT - question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17481kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696898583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  &lt;/p&gt;\n\n&lt;p&gt;Is DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17481kb", "is_robot_indexable": true, "report_reasons": null, "author": "yeager_doug", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17481kb/airflow_dbt_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17481kb/airflow_dbt_question/", "subreddit_subscribers": 133049, "created_utc": 1696898583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.\n\n&amp;#x200B;\n\nI was having a hard time finding ANY guides about how to set up all the components locally; [so I made a guide](https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;utm_campaign=post&amp;utm_medium=web). It goes over setting up and configuring:\n\n&amp;#x200B;\n\n* Iceberg Rest Catalog\n* Trino\n* Minio (s3 compatible for storage of Iceberg tables)\n* Kafka (3 Node KRaft Mode)\n* Kafka Connect\n* Schema Registry\n* Writing avro encoded data to Kafka in go\n\n&amp;#x200B;\n\nOnce done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  \n\n\nHope this helps :)  \n\n\n&amp;#x200B;", "author_fullname": "t2_31vknkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Data From Kafka into Apache Iceberg + Querying with Trino - Done in Docker Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173w6rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696869032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was having a hard time finding ANY guides about how to set up all the components locally; &lt;a href=\"https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;amp;utm_campaign=post&amp;amp;utm_medium=web\"&gt;so I made a guide&lt;/a&gt;. It goes over setting up and configuring:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Iceberg Rest Catalog&lt;/li&gt;\n&lt;li&gt;Trino&lt;/li&gt;\n&lt;li&gt;Minio (s3 compatible for storage of Iceberg tables)&lt;/li&gt;\n&lt;li&gt;Kafka (3 Node KRaft Mode)&lt;/li&gt;\n&lt;li&gt;Kafka Connect&lt;/li&gt;\n&lt;li&gt;Schema Registry&lt;/li&gt;\n&lt;li&gt;Writing avro encoded data to Kafka in go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Once done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  &lt;/p&gt;\n\n&lt;p&gt;Hope this helps :)  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?auto=webp&amp;s=7de03447d3a223deb509f88e091a03da6c93400c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=572c8360ccf24454b4b7b08d84dea4009543f95c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=337134c15aac8e57593f82b672a5fb12a2ef98e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c89632662beabc291281fd8ae813b346a65979", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63293a10ef238fbeead34150c54341ea690a80e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1985f6e22592db1ecaafb1908b69602ee237fc61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0451f1bef6592955e4a6d41575544c83616059c7", "width": 1080, "height": 540}], "variants": {}, "id": "My3S49gmIim5Z0I62eSqxwTyEEfinz1ddzP28sGEN6k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173w6rf", "is_robot_indexable": true, "report_reasons": null, "author": "thehendoxc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "subreddit_subscribers": 133049, "created_utc": 1696869032.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For background I'm a BI analyst, but helping set up a friend's startup infrastructure. They are early on and want to pull data from CRM's using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don't want to spend much or any money on this initially.\n\nFirst step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?\n\nI was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?\n\nAt first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.", "author_fullname": "t2_4qxl8qxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering pipeline for free or low cost in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173x67u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696871430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For background I&amp;#39;m a BI analyst, but helping set up a friend&amp;#39;s startup infrastructure. They are early on and want to pull data from CRM&amp;#39;s using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don&amp;#39;t want to spend much or any money on this initially.&lt;/p&gt;\n\n&lt;p&gt;First step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?&lt;/p&gt;\n\n&lt;p&gt;I was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?&lt;/p&gt;\n\n&lt;p&gt;At first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173x67u", "is_robot_indexable": true, "report_reasons": null, "author": "whittesc", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "subreddit_subscribers": 133049, "created_utc": 1696871430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI've put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you're using dbt for your data transformations, this is for you!\n\n\ud83c\udf1f **What's Inside?**\n\n* Specialized dbt functions not in standard Jinja.\n* Unique macros for dbt workflows.\n* Filters, variables, and context methods to elevate your dbt projects.\n\n\ud83d\udd17 [https://datacoves.com/post/dbt-jinja-functions-cheat-sheet](https://datacoves.com/post/dbt-jinja-functions-cheat-sheet)\n\nWould love to hear your feedback and any suggestions for improvement. \ud83d\udca1\n\n\u2764\ufe0f Data-Queen", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hope this helps!: \ud83d\ude80 Ultimate dbt Jinja Functions Cheat Sheet!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173yg5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696874520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you&amp;#39;re using dbt for your data transformations, this is for you!&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf1f &lt;strong&gt;What&amp;#39;s Inside?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Specialized dbt functions not in standard Jinja.&lt;/li&gt;\n&lt;li&gt;Unique macros for dbt workflows.&lt;/li&gt;\n&lt;li&gt;Filters, variables, and context methods to elevate your dbt projects.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;\ud83d\udd17 &lt;a href=\"https://datacoves.com/post/dbt-jinja-functions-cheat-sheet\"&gt;https://datacoves.com/post/dbt-jinja-functions-cheat-sheet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your feedback and any suggestions for improvement. \ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;\u2764\ufe0f Data-Queen&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?auto=webp&amp;s=761c2339c1c3a5a98098f2be0b77af737b8c59d2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=659627a31f99370f0cbb33fd4514988c6c50bc57", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a2a4756bd7205c1f3c0047c8f0b8ec554362b97", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=432641e4b76d230646cffa01cbbc8405158fe8db", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=662713884f50b946f205be681d5e74fe94225d30", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f76c268382b23cb18587bc9c0dfd00422ee1f65", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceb922cd1c128faa388775cdb58bcd5948f731d3", "width": 1080, "height": 564}], "variants": {}, "id": "ghkewI0Ac4MJEXxpnppvl085nXvleyOWWENREIl1wOE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173yg5n", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "subreddit_subscribers": 133049, "created_utc": 1696874520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you'd start a career in data engineering  in 2023 ,\n\n&amp; you've exp in python sql  , \n\nwould you pursue learning ssis or choose cloud technology ? \n\nThank you ", "author_fullname": "t2_t3nz93za", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On Premises Vs Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174jezy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696939266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;d start a career in data engineering  in 2023 ,&lt;/p&gt;\n\n&lt;p&gt;&amp;amp; you&amp;#39;ve exp in python sql  , &lt;/p&gt;\n\n&lt;p&gt;would you pursue learning ssis or choose cloud technology ? &lt;/p&gt;\n\n&lt;p&gt;Thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174jezy", "is_robot_indexable": true, "report_reasons": null, "author": "Repulsive-Ad7769", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174jezy/on_premises_vs_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174jezy/on_premises_vs_cloud/", "subreddit_subscribers": 133049, "created_utc": 1696939266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the difference between Databricks\u2019s Overwatch and System Tables tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_174j9fg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7H7guNkrLzYUixs3AR3Psyd23HlFvAUyaDvJr6r1KQs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696938761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.det.life", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.det.life/whats-the-difference-between-databricks-s-overwatch-and-system-tables-tools-f9d0cd75a2f2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?auto=webp&amp;s=cdf604b54fe85e5e161f564bad80873c3b0b2d73", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=495021a9846e755e37ba06d7fb7b00bdd919d5af", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74fc4c3c060466a5374a28a66f96c5e3a1efaa3e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2f1a08b5cf74574326b34e55fcaa8a7192d42f2", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ae1eaf20860d2e12e3860d79789872c68c554f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8619f09a42879ea79dc90050268fbd11970bf2a0", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfe55cdcb6fd4feaba16b6a4a3591f9e94b9c621", "width": 1080, "height": 720}], "variants": {}, "id": "XTYuj6I-Q2v_xqeIXOzOgfp785bB9G0Zkdojd7TtV_w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "174j9fg", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174j9fg/whats_the_difference_between_databrickss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.det.life/whats-the-difference-between-databricks-s-overwatch-and-system-tables-tools-f9d0cd75a2f2", "subreddit_subscribers": 133049, "created_utc": 1696938761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.\n\nMy first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.\n\nThe only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.\n\nRepartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.", "author_fullname": "t2_u4zm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark - How to break up extremely compressed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174c1zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696910635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.&lt;/p&gt;\n\n&lt;p&gt;My first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.&lt;/p&gt;\n\n&lt;p&gt;The only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.&lt;/p&gt;\n\n&lt;p&gt;Repartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174c1zs", "is_robot_indexable": true, "report_reasons": null, "author": "JohnStud85", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "subreddit_subscribers": 133049, "created_utc": 1696910635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6vz2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Help] Tried highlighting what Databricks does \"in-house\" for a project. Is this accurate?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_174j9ov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/q3Q6PeLx4nf2CkBUckenFrp8x3M87-n6tWdw4vsV20M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696938786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dqmlofum5dtb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dqmlofum5dtb1.png?auto=webp&amp;s=934d980ca188a1b4bf1a6c5a211557f9b454fc6e", "width": 2264, "height": 2323}, "resolutions": [{"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3435cde6ec91dfa9ca9f535eaf0a228bc0b461b", "width": 108, "height": 110}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cd06c19a55477daca085fa59cd60b8ce2189463", "width": 216, "height": 221}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7cf468576834af1e6abfe22f983baf6c0683803", "width": 320, "height": 328}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f780866b1d8438d2d36ea255b01eef82974c2e6", "width": 640, "height": 656}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=816f4f3aee433d66db633a010df2efe0a00a3ba1", "width": 960, "height": 985}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16fa3fcde7e1fddbff3687503e7403c027f6a376", "width": 1080, "height": 1108}], "variants": {}, "id": "1XiupRIo73tDBkiQdN0J0Hw0qRuljcK-TfvELTnl8Bo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174j9ov", "is_robot_indexable": true, "report_reasons": null, "author": "boulking", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174j9ov/help_tried_highlighting_what_databricks_does/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dqmlofum5dtb1.png", "subreddit_subscribers": 133049, "created_utc": 1696938786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the databases, the data reports and do machine learning projects when we have time. The methodology for the last three years, (yes the organization just started to think about data three years ago), has been a low hanging fruit methodology, meaning we start a project, create some value from it, publish it and move on. \n\nNeedless to say that has left a lot of quality control neglected. Name giving is inconsistent, data owners and users are often unknown and some quick fix sh\\*t solutions are still being used like windows scheduler to run some codes. There is hardly any documentation about our data infrastructure to add. \n\nNow I don't think any one is to blame for this as this is a government organization on a budget and the two of us are head over heels in projects but the time has come to tighten loose ends. My question is, has anyone experienced a similar scenario and solved it? How did you solve it? Is there any good literature on the subject or other resources? \n\nFYI we are using Microsoft solutions like Azure and power platform for 90% of what we do. We are also a REIT and construction management type of organization if that is relevant.\n\nThanks a lot in advance for all responses.", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizational documentation for data infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174hlpp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696932690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the databases, the data reports and do machine learning projects when we have time. The methodology for the last three years, (yes the organization just started to think about data three years ago), has been a low hanging fruit methodology, meaning we start a project, create some value from it, publish it and move on. &lt;/p&gt;\n\n&lt;p&gt;Needless to say that has left a lot of quality control neglected. Name giving is inconsistent, data owners and users are often unknown and some quick fix sh*t solutions are still being used like windows scheduler to run some codes. There is hardly any documentation about our data infrastructure to add. &lt;/p&gt;\n\n&lt;p&gt;Now I don&amp;#39;t think any one is to blame for this as this is a government organization on a budget and the two of us are head over heels in projects but the time has come to tighten loose ends. My question is, has anyone experienced a similar scenario and solved it? How did you solve it? Is there any good literature on the subject or other resources? &lt;/p&gt;\n\n&lt;p&gt;FYI we are using Microsoft solutions like Azure and power platform for 90% of what we do. We are also a REIT and construction management type of organization if that is relevant.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance for all responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174hlpp", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/", "subreddit_subscribers": 133049, "created_utc": 1696932690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are interested in massive data processing, [this case](https://doris.apache.org/zh-CN/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second) might help.\n\nhttps://preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=bfddfc33093973663168acaa2faec12eacf0f460", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Log Analysis: How to Digest 15 Billion Logs Per Day and Keep Big Queries Within 1 Second", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cbo62wez7dtb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 51, "x": 108, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a5caa4edbd79f9685695161cdef806fde66dcf1"}, {"y": 102, "x": 216, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=57bd4f7571a2e26560df0d73593faadb8719ce72"}, {"y": 152, "x": 320, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39755c160045c001c5e73a094aa359423c6b977b"}, {"y": 304, "x": 640, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=afe1f4773ac6e4a30eef23670a47ba7aa01ccf1a"}, {"y": 456, "x": 960, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0bb433e300b8c0ca107dfdc6c5950f37b3b1d24b"}, {"y": 513, "x": 1080, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c5d59d668997a10e02f0d0e92ce0fb0abbbb75de"}], "s": {"y": 609, "x": 1280, "u": "https://preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=bfddfc33093973663168acaa2faec12eacf0f460"}, "id": "cbo62wez7dtb1"}}, "name": "t3_174jidr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Kie1SniWpE6PDH5ujmco-oEtxQ7mTnSn37C3nGuPrIk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696939550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are interested in massive data processing, &lt;a href=\"https://doris.apache.org/zh-CN/blog/Log-Analysis-How-to-Digest-15-Billion-Logs-Per-Day-and-Keep-Big-Queries-Within-1-Second\"&gt;this case&lt;/a&gt; might help.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfddfc33093973663168acaa2faec12eacf0f460\"&gt;https://preview.redd.it/cbo62wez7dtb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfddfc33093973663168acaa2faec12eacf0f460&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "174jidr", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174jidr/log_analysis_how_to_digest_15_billion_logs_per/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174jidr/log_analysis_how_to_digest_15_billion_logs_per/", "subreddit_subscribers": 133049, "created_utc": 1696939550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a consultancy but I've been thinking recently that a lot of SME requirements can really be handled by a solo developer and don't need a whole team or business behind them. \n\nThe biggest obstacles I see are client acquisition and also that a client may not be comfortable hiring a one man team. \n\nDoes anyone have any experience striking out as a solo? I'd love to hear any stories. Or if anyone knows of anything online where someone has documented their own journey.\n\nEven if you haven't done it yourself it'd be interesting to hear your general thoughts on the prospect.", "author_fullname": "t2_6o5du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody here started a solo consultancy (or can share a good resource for it)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174irny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696937086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a consultancy but I&amp;#39;ve been thinking recently that a lot of SME requirements can really be handled by a solo developer and don&amp;#39;t need a whole team or business behind them. &lt;/p&gt;\n\n&lt;p&gt;The biggest obstacles I see are client acquisition and also that a client may not be comfortable hiring a one man team. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience striking out as a solo? I&amp;#39;d love to hear any stories. Or if anyone knows of anything online where someone has documented their own journey.&lt;/p&gt;\n\n&lt;p&gt;Even if you haven&amp;#39;t done it yourself it&amp;#39;d be interesting to hear your general thoughts on the prospect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "174irny", "is_robot_indexable": true, "report_reasons": null, "author": "Cypher211", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174irny/anybody_here_started_a_solo_consultancy_or_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174irny/anybody_here_started_a_solo_consultancy_or_can/", "subreddit_subscribers": 133049, "created_utc": 1696937086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I have a case of eCommerce data where the transactional database has `orders` and `products`. I'm confused as to how to model this in a star schema.\n\nMy thoughts are that the `orders` should be a fact table (`fct_orders`) containing an aggregate of `products` total value, ie `fct_orders.total_order_value`. However, if an order has multiple products, what would the foreign key look like in `fct_orders`?\n\nI've tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my `products` table the fact table. However, how can I model the aggregate `fct_orders.total_order_value`?\n\nThanks in advance!\n\n&amp;#x200B;\n\nEdit: just learned about bridge tables ([https://www.leapfrogbi.com/bridge-tables/](https://www.leapfrogbi.com/bridge-tables/)), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...", "author_fullname": "t2_9trbw25s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model one fact table that joins to multiple rows in dimension table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174avck", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696908569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696906843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I have a case of eCommerce data where the transactional database has &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;products&lt;/code&gt;. I&amp;#39;m confused as to how to model this in a star schema.&lt;/p&gt;\n\n&lt;p&gt;My thoughts are that the &lt;code&gt;orders&lt;/code&gt; should be a fact table (&lt;code&gt;fct_orders&lt;/code&gt;) containing an aggregate of &lt;code&gt;products&lt;/code&gt; total value, ie &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;. However, if an order has multiple products, what would the foreign key look like in &lt;code&gt;fct_orders&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my &lt;code&gt;products&lt;/code&gt; table the fact table. However, how can I model the aggregate &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: just learned about bridge tables (&lt;a href=\"https://www.leapfrogbi.com/bridge-tables/\"&gt;https://www.leapfrogbi.com/bridge-tables/&lt;/a&gt;), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?auto=webp&amp;s=736d9650919a08025a3b529a262d040d5eccc7c4", "width": 508, "height": 553}, "resolutions": [{"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=709171c289f304a1417184d7d2ebada30a823909", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c77e4d74b3cfefe997d1d3a271f7c82a6eaa5c5d", "width": 216, "height": 235}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6b309475f6fb91e4e9b925f4e3d75edb053c17", "width": 320, "height": 348}], "variants": {}, "id": "bIfqiV67OXElDsn2F_5SezvPv1apC5LPOp1agLPkfo4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174avck", "is_robot_indexable": true, "report_reasons": null, "author": "ternary-thought", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "subreddit_subscribers": 133049, "created_utc": 1696906843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an SWE (**not a data scientist**) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.\n\nI started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don't think they solve the issue of validating *changes in data* (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.\n\nWhat I'm looking for is a tool that validates changes in data by comparing the previous value with the new value.\n\nIn some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there's obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).\n\nThe purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.\n\nThis is just an example, but it would be helpful if we can call an API to do this sort of validation for us.\n\nAnd instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I'm just brainstorming here.\n\nWould highly appreciate some recommendations/tips for tackling this problem. Thank you!", "author_fullname": "t2_mxg44sgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to validate data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174236b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696883926.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696883258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an SWE (&lt;strong&gt;not a data scientist&lt;/strong&gt;) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.&lt;/p&gt;\n\n&lt;p&gt;I started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don&amp;#39;t think they solve the issue of validating &lt;em&gt;changes in data&lt;/em&gt; (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a tool that validates changes in data by comparing the previous value with the new value.&lt;/p&gt;\n\n&lt;p&gt;In some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there&amp;#39;s obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).&lt;/p&gt;\n\n&lt;p&gt;The purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.&lt;/p&gt;\n\n&lt;p&gt;This is just an example, but it would be helpful if we can call an API to do this sort of validation for us.&lt;/p&gt;\n\n&lt;p&gt;And instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I&amp;#39;m just brainstorming here.&lt;/p&gt;\n\n&lt;p&gt;Would highly appreciate some recommendations/tips for tackling this problem. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174236b", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Main-6700", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174236b/how_to_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174236b/how_to_validate_data/", "subreddit_subscribers": 133049, "created_utc": 1696883258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, my company has about 30GB total on a sharepoint site. I need to get all these files into an Azure storage account.\n\nI was planning on doing this simply with ADF but the catch is that I can't establish the connection due to permissions (no sharepoint linked service).\n\nDoes anyone have a suggestion on how to do this efficiently?\n\nCurrently I'm biting through the pain and downloading everything as zip -&gt; upload zip to storage and unpack using ADF.", "author_fullname": "t2_16t847", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I move 30GB from sharepoint to Azure storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174mh1z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696947696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, my company has about 30GB total on a sharepoint site. I need to get all these files into an Azure storage account.&lt;/p&gt;\n\n&lt;p&gt;I was planning on doing this simply with ADF but the catch is that I can&amp;#39;t establish the connection due to permissions (no sharepoint linked service).&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion on how to do this efficiently?&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m biting through the pain and downloading everything as zip -&amp;gt; upload zip to storage and unpack using ADF.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174mh1z", "is_robot_indexable": true, "report_reasons": null, "author": "drollerfoot7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174mh1z/how_do_i_move_30gb_from_sharepoint_to_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174mh1z/how_do_i_move_30gb_from_sharepoint_to_azure/", "subreddit_subscribers": 133049, "created_utc": 1696947696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I'm a fresher in DA and DE. I was assigned a task to build a user dimension.\n\n**Purpose**: map user between 3 sources (firebase, MAX, appsflyer) in order to create a complete user journey\n\nIdentifiers that can be used in 3 tables: user\\_id (from firebase), idfa, idfv, appsflyer\\_id\n\nI started to analyze how each identifier would change:\n\n* for user\\_id, it will change after re-installs because my app doesn't require any login so the user\\_id is not consistent \n* idfa will change if a user reset OS or turn on limit ad tracking\n* idfv will change if a user switches to new device\n\nWhat I'm struggling is how i can identify a user and develop general logic, given one of real-life scenarios:\n\n* a user can have multiple devices. how can i identify these devices as 1 particular person?\n* if a user re-installs the app (user\\_id changes), how can i identify him/her as old user coming back?\n* what if a user change platform? (from ios to android) how can i identify this user as old one without treating him/her as new one?\n* not all records in 3 tables can be joined with user\\_id, idfa or idfv (separately); sometimes, user\\_id AND idfa; sometimes, idfa only; sometimes, user\\_id and idfv as i tried to figure out how to layer these ids (trying to put things in a nested field)\n* answering and solving above cases (separately) is not too hard but when those 3 happen at the same time. things starts to become too complex and tangled for me to process (it's demoralizing af :((( )\n\nWrong or missed identification will result in wrong analytics, e.g: wrong CLV as $40K revenue for 10K users is different from $40K revenue for 20K users\n\nIs there any aspects or approaches that i should consider to solve this? Thanks in advanced \\^\\^", "author_fullname": "t2_8wrp4ovv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach user identification when building user dimension?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174eeqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696919412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m a fresher in DA and DE. I was assigned a task to build a user dimension.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: map user between 3 sources (firebase, MAX, appsflyer) in order to create a complete user journey&lt;/p&gt;\n\n&lt;p&gt;Identifiers that can be used in 3 tables: user_id (from firebase), idfa, idfv, appsflyer_id&lt;/p&gt;\n\n&lt;p&gt;I started to analyze how each identifier would change:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for user_id, it will change after re-installs because my app doesn&amp;#39;t require any login so the user_id is not consistent &lt;/li&gt;\n&lt;li&gt;idfa will change if a user reset OS or turn on limit ad tracking&lt;/li&gt;\n&lt;li&gt;idfv will change if a user switches to new device&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What I&amp;#39;m struggling is how i can identify a user and develop general logic, given one of real-life scenarios:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a user can have multiple devices. how can i identify these devices as 1 particular person?&lt;/li&gt;\n&lt;li&gt;if a user re-installs the app (user_id changes), how can i identify him/her as old user coming back?&lt;/li&gt;\n&lt;li&gt;what if a user change platform? (from ios to android) how can i identify this user as old one without treating him/her as new one?&lt;/li&gt;\n&lt;li&gt;not all records in 3 tables can be joined with user_id, idfa or idfv (separately); sometimes, user_id AND idfa; sometimes, idfa only; sometimes, user_id and idfv as i tried to figure out how to layer these ids (trying to put things in a nested field)&lt;/li&gt;\n&lt;li&gt;answering and solving above cases (separately) is not too hard but when those 3 happen at the same time. things starts to become too complex and tangled for me to process (it&amp;#39;s demoralizing af :((( )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Wrong or missed identification will result in wrong analytics, e.g: wrong CLV as $40K revenue for 10K users is different from $40K revenue for 20K users&lt;/p&gt;\n\n&lt;p&gt;Is there any aspects or approaches that i should consider to solve this? Thanks in advanced ^^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174eeqe", "is_robot_indexable": true, "report_reasons": null, "author": "girlsyesboysno", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/174eeqe/how_to_approach_user_identification_when_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174eeqe/how_to_approach_user_identification_when_building/", "subreddit_subscribers": 133049, "created_utc": 1696919412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm struggling to efficiently join data when I have multiple failsafe join points.\n\nSpecifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we'll just consider the three utm campaign parameters:\n\n* utm\\_term\n* utm\\_content\n* utm\\_campaign\n\nI want to join my data based on utm\\_term *if that's possible.* But if it's not, I'll fall back on utm\\_content or utm\\_campaign instead.\n\n**The problem** is that any SQL join I'm aware of that uses multiple join points will use every join point possible. So, currently, I'm dealing with this with a two-step process.\n\nFirst, I find the best join point available for each row of data...\n\n    UPDATE session_data a\n    SET a.Join_Type = b.Join_Type\n    FROM (\n        SELECT\n            session_id,\n            CASE\n                WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &gt; 0 THEN 'utm_term'\n                WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &gt; 0 THEN 'utm_content'\n                WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &gt; 0 THEN 'utm_campaign'\n               ELSE 'Channel'\n            END AS Join_Type\n            FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n            LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n            ON ga.channel = ad.channel AND (\n                ga.utm_term = ad.utm_term OR \n                ga.utm_content = ad.utm_content OR \n                ga.utm_campaign = ad.utm_campaign OR \n            GROUP BY session_id\n        )\n    ) b\n    WHERE a.session_id = b.session_id;\n\n... and then I use that label to join by the best join point available only:\n\n    SELECT * \n    FROM `session_data` ga\n    LEFT JOIN `ad_data` ad\n    WHERE \n    CASE\n        WHEN ga.Join_Type = 'utm_term' THEN ga.utm_term = ad.utm_term\n        WHEN ga.Join_Type = 'utm_content' THEN ga.utm_content = ad.utm_content\n        WHEN ga.Join_Type = 'utm_campaign' THEN ga.utm_campaign = ad.utm_campaign\n        WHEN ga.Join_Type = 'Channel' THEN ga.channel = ad.channel\n    END\n\nWhich works! \n\n*(I mean, I'm leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)*\n\nThat first query, in particular, is *super* problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I'm working in), just results in an error message.\n\nThere has got to be a better way to tackle this join. Anyone know of one?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a more efficient way to do a join by multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1742rzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696884953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m struggling to efficiently join data when I have multiple failsafe join points.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we&amp;#39;ll just consider the three utm campaign parameters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;utm_term&lt;/li&gt;\n&lt;li&gt;utm_content&lt;/li&gt;\n&lt;li&gt;utm_campaign&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to join my data based on utm_term &lt;em&gt;if that&amp;#39;s possible.&lt;/em&gt; But if it&amp;#39;s not, I&amp;#39;ll fall back on utm_content or utm_campaign instead.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt; is that any SQL join I&amp;#39;m aware of that uses multiple join points will use every join point possible. So, currently, I&amp;#39;m dealing with this with a two-step process.&lt;/p&gt;\n\n&lt;p&gt;First, I find the best join point available for each row of data...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;UPDATE session_data a\nSET a.Join_Type = b.Join_Type\nFROM (\n    SELECT\n        session_id,\n        CASE\n            WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_term&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_content&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_campaign&amp;#39;\n           ELSE &amp;#39;Channel&amp;#39;\n        END AS Join_Type\n        FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n        LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n        ON ga.channel = ad.channel AND (\n            ga.utm_term = ad.utm_term OR \n            ga.utm_content = ad.utm_content OR \n            ga.utm_campaign = ad.utm_campaign OR \n        GROUP BY session_id\n    )\n) b\nWHERE a.session_id = b.session_id;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;... and then I use that label to join by the best join point available only:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * \nFROM `session_data` ga\nLEFT JOIN `ad_data` ad\nWHERE \nCASE\n    WHEN ga.Join_Type = &amp;#39;utm_term&amp;#39; THEN ga.utm_term = ad.utm_term\n    WHEN ga.Join_Type = &amp;#39;utm_content&amp;#39; THEN ga.utm_content = ad.utm_content\n    WHEN ga.Join_Type = &amp;#39;utm_campaign&amp;#39; THEN ga.utm_campaign = ad.utm_campaign\n    WHEN ga.Join_Type = &amp;#39;Channel&amp;#39; THEN ga.channel = ad.channel\nEND\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Which works! &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(I mean, I&amp;#39;m leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;That first query, in particular, is &lt;em&gt;super&lt;/em&gt; problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I&amp;#39;m working in), just results in an error message.&lt;/p&gt;\n\n&lt;p&gt;There has got to be a better way to tackle this join. Anyone know of one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1742rzg", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "subreddit_subscribers": 133049, "created_utc": 1696884953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).\n\nHow do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?\n\nHow do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?\n\nSorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  \n\n\n**NOTE**: when I'm talking about ETL development lifecycle, I actually mean data pipelines in general.", "author_fullname": "t2_21xquoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E2E ETL lifecycle in multi-environment setup (bonus for integration with E2E analytics)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1740atd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696880710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696878890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).&lt;/p&gt;\n\n&lt;p&gt;How do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?&lt;/p&gt;\n\n&lt;p&gt;How do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?&lt;/p&gt;\n\n&lt;p&gt;Sorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: when I&amp;#39;m talking about ETL development lifecycle, I actually mean data pipelines in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1740atd", "is_robot_indexable": true, "report_reasons": null, "author": "kaributas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "subreddit_subscribers": 133049, "created_utc": 1696878890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for working with dbt and Snowflake - A practitioner\u2019s guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17401u2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UzBOjGfWVmMW0PI3xw6ehMGg5y-n8aHgUpGbmnCVs8I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696878298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?auto=webp&amp;s=e7612f0c09356ae4bc3d254edd02e167a83a2733", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cafd6d1b68fdc7b4bb1b5c215278d61a2577a01", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56902c994507c9bd1a79cb15dea9b39629b5c9d4", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22cc3d82dc59750bc0cc8283f655d26727bd3da0", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aeea40b0ea537d47169953badb79986134a4da7a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20973f9c6ce81bee6ca454d8a853e4101a883306", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6020d7897990fb7542d5ce7123b8536afa9b7fee", "width": 1080, "height": 720}], "variants": {}, "id": "ucEfKV0PvxKYvUs4LpGpFLHRp3wQfXsbHwo066xOa3c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17401u2", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17401u2/best_practices_for_working_with_dbt_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "subreddit_subscribers": 133049, "created_utc": 1696878298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically I did some basic sql projects where I have generated insights through sql queries but now I want to create a schema with er diagrams in it any help/tips will be appreaciated", "author_fullname": "t2_6ybces6n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help needed regarding SQL Project ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174opdw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696953483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I did some basic sql projects where I have generated insights through sql queries but now I want to create a schema with er diagrams in it any help/tips will be appreaciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174opdw", "is_robot_indexable": true, "report_reasons": null, "author": "Narrow-Tea-9187", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174opdw/help_needed_regarding_sql_project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174opdw/help_needed_regarding_sql_project_ideas/", "subreddit_subscribers": 133049, "created_utc": 1696953483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nLooking to do a DE bootcamp which is govt funded in the UK,  after contemplating between this and devops, I think as a beginner this would be the appropriate way to go and something I would actually enjoy\n\nI'm checking out various options, could you guys please share your thoughts on the syllabus and the projects mentioned in the course if it's any good and would it be able to get me somewhat ready for the industry or should I avoid ?\n\nhttps://www.theaicore.com/course\n\nMy only concern is I read one negative review on them here on reddit, which said most of their teachers are former students and their is not much help in getting actual interviews or into the industry itself , however there is also very little to go on tbf, I do understand bootcamp rep also matters because not all are the same either, but I want to give myself the best chance possible to start a new career, please advise, thanks.", "author_fullname": "t2_4jsiips8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI Core Bootcamp ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174myur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696949001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;Looking to do a DE bootcamp which is govt funded in the UK,  after contemplating between this and devops, I think as a beginner this would be the appropriate way to go and something I would actually enjoy&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m checking out various options, could you guys please share your thoughts on the syllabus and the projects mentioned in the course if it&amp;#39;s any good and would it be able to get me somewhat ready for the industry or should I avoid ?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.theaicore.com/course\"&gt;https://www.theaicore.com/course&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My only concern is I read one negative review on them here on reddit, which said most of their teachers are former students and their is not much help in getting actual interviews or into the industry itself , however there is also very little to go on tbf, I do understand bootcamp rep also matters because not all are the same either, but I want to give myself the best chance possible to start a new career, please advise, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?auto=webp&amp;s=6cbaa5437c0d2b8d84feba3d743b207825134f3c", "width": 1240, "height": 610}, "resolutions": [{"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81b002951daa012ecc56322c603051216e32ec93", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b111a20bf237ab20f25c6af398e5f5e7befee8e1", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db669cee24bb1e832d29ee9427863bd43a5eaab7", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=170994247e2eba0c1c5c4bb25e729df5f57688ab", "width": 640, "height": 314}, {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0d42bfe08a51e8ea00c2cc58434c5bf8a1771ad", "width": 960, "height": 472}, {"url": "https://external-preview.redd.it/RCRG1-58lhjlZv1LaNLfcknuuzYvLbI_3sIEWzkxdmM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=be4550e87be5d00b9dfa0576e97c3f06df0941e5", "width": 1080, "height": 531}], "variants": {}, "id": "qBElg6l4Bmzpo_AONeuGwPp4lJLqCIF5bvY1xq4O4h8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "174myur", "is_robot_indexable": true, "report_reasons": null, "author": "saqi786x", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174myur/ai_core_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174myur/ai_core_bootcamp/", "subreddit_subscribers": 133049, "created_utc": 1696949001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_y9qpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a tool that navigates the Internet and scrapes data using GPT-4", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174jlsf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696939855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "singleapi.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://singleapi.co/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "174jlsf", "is_robot_indexable": true, "report_reasons": null, "author": "semanser", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174jlsf/i_created_a_tool_that_navigates_the_internet_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://singleapi.co/", "subreddit_subscribers": 133049, "created_utc": 1696939855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I\u2019m a data analyst based in UK with 4 years experience. I saw an ad for a \u201cData Analyst / Data Engineering\u201d position for a huge company (with 1 day a week university training), and I decided to apply.\nTurned out after the interview and briefing, the position was \u201cAnalytics Engineering\u201d\u2026 I had to Google that\u2026\n\nThe thing is I\u2019ve passed the second round interview and assignment using ChatGPT and previous knowledge to connect dots, however I\u2019m way more skilled as a Data Analyst than an Engineer. I\u2019d honestly struggle to describe what I built for the assignment without notes.\n\nSo naturally, I\u2019m still not 100% sure if Analytics Engineering is right for me although the company and programme looks great. I get the feeling every Analytics Engineering job mightn\u2019t be an equal split.\n\nIs this a role that\u2019s more suited to someone with solely Data Engineer or Data Analytics experience?\n\nTLDR: ended up interviewing as an Analytics engineering. Is my data analyst experience good enough?", "author_fullname": "t2_shaidtoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is an Analytics Engineering position possible to pivot from Data Analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174g2q6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696926815.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696926514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I\u2019m a data analyst based in UK with 4 years experience. I saw an ad for a \u201cData Analyst / Data Engineering\u201d position for a huge company (with 1 day a week university training), and I decided to apply.\nTurned out after the interview and briefing, the position was \u201cAnalytics Engineering\u201d\u2026 I had to Google that\u2026&lt;/p&gt;\n\n&lt;p&gt;The thing is I\u2019ve passed the second round interview and assignment using ChatGPT and previous knowledge to connect dots, however I\u2019m way more skilled as a Data Analyst than an Engineer. I\u2019d honestly struggle to describe what I built for the assignment without notes.&lt;/p&gt;\n\n&lt;p&gt;So naturally, I\u2019m still not 100% sure if Analytics Engineering is right for me although the company and programme looks great. I get the feeling every Analytics Engineering job mightn\u2019t be an equal split.&lt;/p&gt;\n\n&lt;p&gt;Is this a role that\u2019s more suited to someone with solely Data Engineer or Data Analytics experience?&lt;/p&gt;\n\n&lt;p&gt;TLDR: ended up interviewing as an Analytics engineering. Is my data analyst experience good enough?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "174g2q6", "is_robot_indexable": true, "report_reasons": null, "author": "Noot-Noot-456", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174g2q6/is_an_analytics_engineering_position_possible_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174g2q6/is_an_analytics_engineering_position_possible_to/", "subreddit_subscribers": 133049, "created_utc": 1696926514.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}