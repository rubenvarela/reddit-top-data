{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I see a lot of data scientists in this subreddit describing their work as using different types of methods to, in the end, improve company performance and/or profits.\n\nI was wondering, if you have examples for how data science is used for social benefit instead of the bottom line of profits?", "author_fullname": "t2_tf1f2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can data science be used to \"make the world a better place\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174gxvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 85, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 85, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696936462.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696930199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of data scientists in this subreddit describing their work as using different types of methods to, in the end, improve company performance and/or profits.&lt;/p&gt;\n\n&lt;p&gt;I was wondering, if you have examples for how data science is used for social benefit instead of the bottom line of profits?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174gxvo", "is_robot_indexable": true, "report_reasons": null, "author": "Fluxan", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174gxvo/how_can_data_science_be_used_to_make_the_world_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174gxvo/how_can_data_science_be_used_to_make_the_world_a/", "subreddit_subscribers": 1079570, "created_utc": 1696930199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nHey everyone,\n\nI recently joined a company as a data scientist and found that their data warehouse is in dire shape. It seems they haven't invested enough time in validating their data, resulting in most tables being unreliable for modeling or reporting. The analysts are reporting incorrect data and the upper management knows it. To add to the challenge, there's only one overburdened data engineer here, so I'm pretty much on my own in navigating this.\n\nI've been identifying and communicating these data issues to upper management, but I also need to produce some models. The warehouse is poorly built, many tables with no data, a lot of columns in one table meaning they didn't bother creating more dimension tables. And worst of all, the data in tables is simply wrong. My current thought is to pivot temporarily:\n\n1. Use existing, validated CSVs and Excel files to begin my analyses and model building.\n2. Parallelly, work on gradually rectifying the data warehouse issues.\n3. Eventually, transition the models to source data directly from the fixed warehouse.\n\nHas anyone faced a similar situation? How did you handle it? Any advice or alternative approaches would be greatly appreciated!", "author_fullname": "t2_7bhaubdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tough spot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1748bph", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696899397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently joined a company as a data scientist and found that their data warehouse is in dire shape. It seems they haven&amp;#39;t invested enough time in validating their data, resulting in most tables being unreliable for modeling or reporting. The analysts are reporting incorrect data and the upper management knows it. To add to the challenge, there&amp;#39;s only one overburdened data engineer here, so I&amp;#39;m pretty much on my own in navigating this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been identifying and communicating these data issues to upper management, but I also need to produce some models. The warehouse is poorly built, many tables with no data, a lot of columns in one table meaning they didn&amp;#39;t bother creating more dimension tables. And worst of all, the data in tables is simply wrong. My current thought is to pivot temporarily:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use existing, validated CSVs and Excel files to begin my analyses and model building.&lt;/li&gt;\n&lt;li&gt;Parallelly, work on gradually rectifying the data warehouse issues.&lt;/li&gt;\n&lt;li&gt;Eventually, transition the models to source data directly from the fixed warehouse.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Has anyone faced a similar situation? How did you handle it? Any advice or alternative approaches would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1748bph", "is_robot_indexable": true, "report_reasons": null, "author": "AbramoNauseus", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1748bph/tough_spot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1748bph/tough_spot/", "subreddit_subscribers": 1079570, "created_utc": 1696899397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How soon would you expect a new Senior Data Scientist to start churning out models, analysis, reports, experiments, etc? What would you think dictates this expectation?", "author_fullname": "t2_bhwy8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How quickly should you be expected to start producing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174n6w2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696949594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How soon would you expect a new Senior Data Scientist to start churning out models, analysis, reports, experiments, etc? What would you think dictates this expectation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174n6w2", "is_robot_indexable": true, "report_reasons": null, "author": "timusw", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174n6w2/how_quickly_should_you_be_expected_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174n6w2/how_quickly_should_you_be_expected_to_start/", "subreddit_subscribers": 1079570, "created_utc": 1696949594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "One area of data science I think that people struggle to wrap their head around is thinking of problems and frameworks, tools, technologies in simple real world terms. Very hard to understand something You're just writing lines of code and programming into a machine, without really understanding in the real world that you live in what they could possibly be related to...\n\n\nAnd recently I've been learning tensorflow, and apparently tensor is an actual word, like this is a real thing. But what I don't understand is what a real world example of a tensor could possibly be, like an analogy, or metaphor for how to explain them to someone else. For example, tree, box, power plant, etc. I'm not saying those are related to tensor, but those are often things that people use to explain complex concepts in the real world. \n\n\nSo how would you explain what a tensor is in real world terms?", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone provide an easy to understand real world example of tensors, and how they are used?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174pvw9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696956432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One area of data science I think that people struggle to wrap their head around is thinking of problems and frameworks, tools, technologies in simple real world terms. Very hard to understand something You&amp;#39;re just writing lines of code and programming into a machine, without really understanding in the real world that you live in what they could possibly be related to...&lt;/p&gt;\n\n&lt;p&gt;And recently I&amp;#39;ve been learning tensorflow, and apparently tensor is an actual word, like this is a real thing. But what I don&amp;#39;t understand is what a real world example of a tensor could possibly be, like an analogy, or metaphor for how to explain them to someone else. For example, tree, box, power plant, etc. I&amp;#39;m not saying those are related to tensor, but those are often things that people use to explain complex concepts in the real world. &lt;/p&gt;\n\n&lt;p&gt;So how would you explain what a tensor is in real world terms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174pvw9", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174pvw9/can_anyone_provide_an_easy_to_understand_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174pvw9/can_anyone_provide_an_easy_to_understand_real/", "subreddit_subscribers": 1079570, "created_utc": 1696956432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I deployed my first big project on Friday, did some post hoc testing today and called it a day at noon, I think my manager didn\u2019t assign anything to me on purpose, hopefully because he\u2019s cool and not because I suck :)", "author_fullname": "t2_46n9bq51", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You ever get a bye day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174txha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696966461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deployed my first big project on Friday, did some post hoc testing today and called it a day at noon, I think my manager didn\u2019t assign anything to me on purpose, hopefully because he\u2019s cool and not because I suck :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174txha", "is_robot_indexable": true, "report_reasons": null, "author": "econ1mods1are1cucks", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174txha/you_ever_get_a_bye_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174txha/you_ever_get_a_bye_day/", "subreddit_subscribers": 1079570, "created_utc": 1696966461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just curious how many people out there favor explainable boosting machines over bread and butter methods like lgbm or xgbm. Should I learn this or is it a fad?", "author_fullname": "t2_fzrh3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explainable boosting machines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174dzeb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696917692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious how many people out there favor explainable boosting machines over bread and butter methods like lgbm or xgbm. Should I learn this or is it a fad?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174dzeb", "is_robot_indexable": true, "report_reasons": null, "author": "mingzhouren", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174dzeb/explainable_boosting_machines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174dzeb/explainable_boosting_machines/", "subreddit_subscribers": 1079570, "created_utc": 1696917692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!\n\nIs there a simple and robust method for extracting highly tabular data from a PDF without resorting to rule based regex parsing?  I'm currently using PDFminer, PDFplumber and regex to build templates to extract PDFs based on the type of PDF but it's very time-consuming and tedious.  Is there a better way?\n\nI've used Langchain and OpenAI to build \"Chat with your document\" apps which works great for uploading a PDF of a whitepaper and asking it to summarize the paper, but when it comes to extracting table data - I don't think this solution will work.\n\n&amp;#x200B;\n\nThank you for your input,\n\nData Scallion", "author_fullname": "t2_dffy2296", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advancements in extracting tabular data from PDFs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174pkt1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696955619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Is there a simple and robust method for extracting highly tabular data from a PDF without resorting to rule based regex parsing?  I&amp;#39;m currently using PDFminer, PDFplumber and regex to build templates to extract PDFs based on the type of PDF but it&amp;#39;s very time-consuming and tedious.  Is there a better way?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used Langchain and OpenAI to build &amp;quot;Chat with your document&amp;quot; apps which works great for uploading a PDF of a whitepaper and asking it to summarize the paper, but when it comes to extracting table data - I don&amp;#39;t think this solution will work.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your input,&lt;/p&gt;\n\n&lt;p&gt;Data Scallion&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174pkt1", "is_robot_indexable": true, "report_reasons": null, "author": "data_scallion", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174pkt1/advancements_in_extracting_tabular_data_from_pdfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174pkt1/advancements_in_extracting_tabular_data_from_pdfs/", "subreddit_subscribers": 1079570, "created_utc": 1696955619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Asides from skill issue\n\nIs there any benefit to using Tableu/BI over streamlit given that coding isn't the issue? ", "author_fullname": "t2_uhv4bci5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would I use Tableu/BI over Streamlit? Is there any advantage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174f1cc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696922002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Asides from skill issue&lt;/p&gt;\n\n&lt;p&gt;Is there any benefit to using Tableu/BI over streamlit given that coding isn&amp;#39;t the issue? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174f1cc", "is_robot_indexable": true, "report_reasons": null, "author": "Salt-Page1396", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174f1cc/why_would_i_use_tableubi_over_streamlit_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174f1cc/why_would_i_use_tableubi_over_streamlit_is_there/", "subreddit_subscribers": 1079570, "created_utc": 1696922002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Got into my first job about 10 months ago. I study a master\u2019s on data science and I\u2019m about to finish school in 2-3 months. I\u2019m doing okay, my lowest score is B+ and I\u2019m working on a churn project. \n\nI got my job through a friend, the company knew I was recently starting my master\u2019s and that I had no experience in this field. However, they were really interested in what I was (supposedly) going to learn, and were excited that I\u2019d bring a new perspective to the team. \n\nThings started ok and I\u2019m doing pretty good on every day tasks, but whenever I\u2019m handed an analysis task/data science project, it always ends up taking more time than allowed, and the more experienced people in my team usually end up coming in and having to re-do everything, sometimes even work overtime to meet deadlines. \n\nIt\u2019s not that I\u2019m not working on it, like for example I have about 8 hours on this one project I had to do, and all I have is a few tables and metrics. Yet, the customer meeting is tomorrow and I have nothing to show for the time I have put in.\n\nI\u2019m starting to feel like I\u2019m wasting company\u2019s time and resources and more importantly, I feel bad about not having learned anything and not being able to apply anything.", "author_fullname": "t2_m826ekr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sucking at my job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174wmnk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696973097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got into my first job about 10 months ago. I study a master\u2019s on data science and I\u2019m about to finish school in 2-3 months. I\u2019m doing okay, my lowest score is B+ and I\u2019m working on a churn project. &lt;/p&gt;\n\n&lt;p&gt;I got my job through a friend, the company knew I was recently starting my master\u2019s and that I had no experience in this field. However, they were really interested in what I was (supposedly) going to learn, and were excited that I\u2019d bring a new perspective to the team. &lt;/p&gt;\n\n&lt;p&gt;Things started ok and I\u2019m doing pretty good on every day tasks, but whenever I\u2019m handed an analysis task/data science project, it always ends up taking more time than allowed, and the more experienced people in my team usually end up coming in and having to re-do everything, sometimes even work overtime to meet deadlines. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s not that I\u2019m not working on it, like for example I have about 8 hours on this one project I had to do, and all I have is a few tables and metrics. Yet, the customer meeting is tomorrow and I have nothing to show for the time I have put in.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting to feel like I\u2019m wasting company\u2019s time and resources and more importantly, I feel bad about not having learned anything and not being able to apply anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174wmnk", "is_robot_indexable": true, "report_reasons": null, "author": "Utterizi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174wmnk/sucking_at_my_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174wmnk/sucking_at_my_job/", "subreddit_subscribers": 1079570, "created_utc": 1696973097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm working on a project that is looking at the interaction between gps tracked pelicans and oil rigs in the gulf of mexico. The big thing with tracking data analyses such as hidden markov model and step selection functions is to have consistently recorded locations to analyze, looking at the data I realized that there is a gap from 9:30pm to 5am everyday, this is because it was assumed they were sleeping at these times and the rows removed. Should I put back those missing rows if possible or just have the coordinates record at 9:30pm repeated every 90 minutes until the 5am ping for each pelican?", "author_fullname": "t2_mzobbzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question animal tracking data and filling in periods of sleep?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174wama", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696972280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project that is looking at the interaction between gps tracked pelicans and oil rigs in the gulf of mexico. The big thing with tracking data analyses such as hidden markov model and step selection functions is to have consistently recorded locations to analyze, looking at the data I realized that there is a gap from 9:30pm to 5am everyday, this is because it was assumed they were sleeping at these times and the rows removed. Should I put back those missing rows if possible or just have the coordinates record at 9:30pm repeated every 90 minutes until the 5am ping for each pelican?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174wama", "is_robot_indexable": true, "report_reasons": null, "author": "HyenaJack94", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174wama/question_animal_tracking_data_and_filling_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174wama/question_animal_tracking_data_and_filling_in/", "subreddit_subscribers": 1079570, "created_utc": 1696972280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Everyone - Just a quick note to let you know that we just released v.1.4.0 of the [Highcharts for Python Toolkit](https://core-docs.highchartspython.com/) (Highcharts Core for Python, Highcharts Stock for Python, Highcharts Maps for Python, and Highcharts Gantt for Python).\n\nWhile technically this is a minor release since everything remains backwards compatible and new functionality is purely additive, it still brings a ton of significant improvements across all libraries in the toolkit:\n\n**Performance Improvements**\n\n* 50 - 90% faster when rendering a chart in Jupyter (or when serializing it from Python to JS object literal notation)\n* 30 - 90% faster when serializing a chart configuration from Python to JSON\n\nBoth major performance improvements depend somewhat on the chart configuration, but in any case it should be quite significant.\n\n**Usability / Quality of Life Improvements**\n\n* **Support for NumPy**\n\n  Now we can create charts and data series directly from NumPy arrays.\n\n* **Simpler API / Reduced Verbosity**\n\n  While the toolkit still supports the full power of Highcharts (JS), the Python toolkit now supports \"naive\" usage and smart defaults. The toolkit will attempt to assemble charts and data series for you as best it can based on your data, even without an explicit configuration. Great for quick-and-dirty experimentation!\n\n* **Python to JavaScript Conversion**\n\n  Now we can write our Highcharts formatter or callback functions in Python, rather than JavaScript. With one method call, we can convert a Python callable/function into its JavaScript equivalent. This relies on integration with either OpenAI's GPT models or Anthropic's Claude model, so you will need to have an account with one (or both) of them to use the functionality. Because AI is generating the JavaScript code, best practice is to review the generated JS code before including it in any production application, but for quick data science work, or to streamline the development / configuration of visualizations, it can be super useful. [We even have a tutorial on how to use this feature here.](https://core-docs.highchartspython.com/en/latest/tutorials/callbacks.html)\n\n* **Series-first Visualization**\n\n  We no longer have to combine series objects and charts to produce a visualization. Now, we can visualize individual series directly with one method call, no need to assemble them into a chart object.\n\n* **Data and Property Propagation**\n\n  When configuring our data points, we no longer have to adjust each data point individually. To set the same property value on all data points, just set the property on the series and it will get automatically propagated across all data points.\n\n* **Series Type Conversion**\n\n  We can now convert one series to a different series type with one method call.\n\n**Bug Fixes**\n\n* Fixed a bug causing a conflict in certain circumstances where Jupyter Notebook uses RequireJS.\n* Fixed a bug preventing certain chart-specific required Highcharts (JS) modules from loading correctly in Jupyter Notebook/Labs.\n\nWe're already hard at work on the next release, with more improvements coming, but while we work on it, if you're looking for high-end data visualization you'll find the Highcharts for Python Toolkit useful.\n\nHere are all the more detailed links:\n\n* [Highcharts for Python on Github](https://github.com/highcharts-for-python)\n* [Highcharts for Python Website](https://highchartspython.com)\n* Highcharts Core for Python\n\n  * [Source Repo](https://github.com/highcharts-for-python/highcharts-core)\n  * [PyPi](https://pypi.org/project/highcharts-core/)\n  * [Documentation](https://core-docs.highchartspython.com)\n\n* Highcharts Stock for Python\n\n  * [Source Repo](https://github.com/highcharts-for-python/highcharts-stock)\n  * [PyPi](https://pypi.org/project/highcharts-stock/)\n  * [Documentation](https://stock-docs.highchartspython.com)\n\n* Highcharts Maps for Python\n\n  * [Source Repo](https://github.com/highcharts-for-python/highcharts-maps)\n  * [PyPi](https://pypi.org/project/highcharts-maps/)\n  * [Documentation](https://maps-docs.highchartspython.com)\n\n* Highcharts Gantt for Python\n\n  * [Source Repo](https://github.com/highcharts-for-python/highcharts-gantt)\n  * [PyPi](https://pypi.org/project/highcharts-gantt/)\n  * [Documentation](https://gantt-docs.highchartspython.com)\n\nPlease let us know what you think!", "author_fullname": "t2_dq0xmlp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Highcharts for Python v.1.4.0 Released", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174ml3k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696948004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone - Just a quick note to let you know that we just released v.1.4.0 of the &lt;a href=\"https://core-docs.highchartspython.com/\"&gt;Highcharts for Python Toolkit&lt;/a&gt; (Highcharts Core for Python, Highcharts Stock for Python, Highcharts Maps for Python, and Highcharts Gantt for Python).&lt;/p&gt;\n\n&lt;p&gt;While technically this is a minor release since everything remains backwards compatible and new functionality is purely additive, it still brings a ton of significant improvements across all libraries in the toolkit:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;50 - 90% faster when rendering a chart in Jupyter (or when serializing it from Python to JS object literal notation)&lt;/li&gt;\n&lt;li&gt;30 - 90% faster when serializing a chart configuration from Python to JSON&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Both major performance improvements depend somewhat on the chart configuration, but in any case it should be quite significant.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Usability / Quality of Life Improvements&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Support for NumPy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now we can create charts and data series directly from NumPy arrays.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simpler API / Reduced Verbosity&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;While the toolkit still supports the full power of Highcharts (JS), the Python toolkit now supports &amp;quot;naive&amp;quot; usage and smart defaults. The toolkit will attempt to assemble charts and data series for you as best it can based on your data, even without an explicit configuration. Great for quick-and-dirty experimentation!&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python to JavaScript Conversion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now we can write our Highcharts formatter or callback functions in Python, rather than JavaScript. With one method call, we can convert a Python callable/function into its JavaScript equivalent. This relies on integration with either OpenAI&amp;#39;s GPT models or Anthropic&amp;#39;s Claude model, so you will need to have an account with one (or both) of them to use the functionality. Because AI is generating the JavaScript code, best practice is to review the generated JS code before including it in any production application, but for quick data science work, or to streamline the development / configuration of visualizations, it can be super useful. &lt;a href=\"https://core-docs.highchartspython.com/en/latest/tutorials/callbacks.html\"&gt;We even have a tutorial on how to use this feature here.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Series-first Visualization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We no longer have to combine series objects and charts to produce a visualization. Now, we can visualize individual series directly with one method call, no need to assemble them into a chart object.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data and Property Propagation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When configuring our data points, we no longer have to adjust each data point individually. To set the same property value on all data points, just set the property on the series and it will get automatically propagated across all data points.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Series Type Conversion&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We can now convert one series to a different series type with one method call.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fixed a bug causing a conflict in certain circumstances where Jupyter Notebook uses RequireJS.&lt;/li&gt;\n&lt;li&gt;Fixed a bug preventing certain chart-specific required Highcharts (JS) modules from loading correctly in Jupyter Notebook/Labs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;re already hard at work on the next release, with more improvements coming, but while we work on it, if you&amp;#39;re looking for high-end data visualization you&amp;#39;ll find the Highcharts for Python Toolkit useful.&lt;/p&gt;\n\n&lt;p&gt;Here are all the more detailed links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/highcharts-for-python\"&gt;Highcharts for Python on Github&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://highchartspython.com\"&gt;Highcharts for Python Website&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Highcharts Core for Python&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/highcharts-for-python/highcharts-core\"&gt;Source Repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://pypi.org/project/highcharts-core/\"&gt;PyPi&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://core-docs.highchartspython.com\"&gt;Documentation&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Highcharts Stock for Python&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/highcharts-for-python/highcharts-stock\"&gt;Source Repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://pypi.org/project/highcharts-stock/\"&gt;PyPi&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://stock-docs.highchartspython.com\"&gt;Documentation&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Highcharts Maps for Python&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/highcharts-for-python/highcharts-maps\"&gt;Source Repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://pypi.org/project/highcharts-maps/\"&gt;PyPi&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://maps-docs.highchartspython.com\"&gt;Documentation&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Highcharts Gantt for Python&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/highcharts-for-python/highcharts-gantt\"&gt;Source Repo&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://pypi.org/project/highcharts-gantt/\"&gt;PyPi&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://gantt-docs.highchartspython.com\"&gt;Documentation&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please let us know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174ml3k", "is_robot_indexable": true, "report_reasons": null, "author": "InsightIndustry", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174ml3k/highcharts_for_python_v140_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174ml3k/highcharts_for_python_v140_released/", "subreddit_subscribers": 1079570, "created_utc": 1696948004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I mean cities where a data scientist with a few years of experience who lives there would have a hard time NOT finding a new job.\n\nWhat are the top 5 or 10 DS hubs in the US, and then what's 1 or 2 cities near the great lakes that punch well above its weight (besides the obvious answer which I'm assuming is Chicago)?", "author_fullname": "t2_4ckw169q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[US] What are some hubs for data science or data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174sfb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696962814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean cities where a data scientist with a few years of experience who lives there would have a hard time NOT finding a new job.&lt;/p&gt;\n\n&lt;p&gt;What are the top 5 or 10 DS hubs in the US, and then what&amp;#39;s 1 or 2 cities near the great lakes that punch well above its weight (besides the obvious answer which I&amp;#39;m assuming is Chicago)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174sfb2", "is_robot_indexable": true, "report_reasons": null, "author": "valkaress", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174sfb2/us_what_are_some_hubs_for_data_science_or_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174sfb2/us_what_are_some_hubs_for_data_science_or_data/", "subreddit_subscribers": 1079570, "created_utc": 1696962814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi y'all. I'm a fresh graduate working as a data analyst in insurance. I work with lots of health insurance data and use mostly Excel and Power BI to analyze the claims experience of members of occupational healthcare schemes (or in other words, employees). Of course I understand this kind of data can be very sensitive, therefore there a few things I wanted to ask before moving forward in regards to complementing my work with Chat GPT.\n\n* Most of the scheme members data generated is ungendered. Imagine a healthcare scheme (which is basically an employer that has set up medical insurance for its employees) with over 1000 people that have no gender. Chat GPT would pretty much come in clutch here with a simple prompt that could 'gender' for me such data in a table. If I was to do this, I would feed Chat GPT with **first names only** to limit identification.\n* Categorizing ailment data. There are hundreds and hundreds of ailments I encounter in my line of work. Categorizing this data would make analysis easier. For example, ailments such as glaucoma, myopia, hypermetropia and cataracts could be classified under \"Optical/Eye Conditions\". For this I would feed in **solely the ailments** as well as some major groupings then let chat GPT do its thing.\n\nSo in light of GDPR, is this plausible for me to do?", "author_fullname": "t2_70woigu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Question About Chat GPT &amp; Data Protection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174g62t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696926944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y&amp;#39;all. I&amp;#39;m a fresh graduate working as a data analyst in insurance. I work with lots of health insurance data and use mostly Excel and Power BI to analyze the claims experience of members of occupational healthcare schemes (or in other words, employees). Of course I understand this kind of data can be very sensitive, therefore there a few things I wanted to ask before moving forward in regards to complementing my work with Chat GPT.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Most of the scheme members data generated is ungendered. Imagine a healthcare scheme (which is basically an employer that has set up medical insurance for its employees) with over 1000 people that have no gender. Chat GPT would pretty much come in clutch here with a simple prompt that could &amp;#39;gender&amp;#39; for me such data in a table. If I was to do this, I would feed Chat GPT with &lt;strong&gt;first names only&lt;/strong&gt; to limit identification.&lt;/li&gt;\n&lt;li&gt;Categorizing ailment data. There are hundreds and hundreds of ailments I encounter in my line of work. Categorizing this data would make analysis easier. For example, ailments such as glaucoma, myopia, hypermetropia and cataracts could be classified under &amp;quot;Optical/Eye Conditions&amp;quot;. For this I would feed in &lt;strong&gt;solely the ailments&lt;/strong&gt; as well as some major groupings then let chat GPT do its thing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So in light of GDPR, is this plausible for me to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "174g62t", "is_robot_indexable": true, "report_reasons": null, "author": "DonteDante", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/174g62t/a_question_about_chat_gpt_data_protection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/174g62t/a_question_about_chat_gpt_data_protection/", "subreddit_subscribers": 1079570, "created_utc": 1696926944.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}