{"kind": "Listing", "data": {"after": "t3_17401u2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Asset Checks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_173vk4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhiyblumCHOKOoK-lBvHzVvyOgmHNAYhp2z8n_Wp51M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696867500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/dagster-asset-checks", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?auto=webp&amp;s=e030a0c6744899cf318d466d091adc29035e5fb8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=143a51bfaca13836974a1eb8e6901d1948b99659", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c025ff90dc104c5449805749243b8dfb2569d2d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c5d0aaf3289eda4da7f88663372121f48413ccf8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc2bda3089cdbf53d0668586c32c3b345a7baaef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fcd3e79d09c99f8433c137611706c368364eedd", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AaVE9HLbRVcygPJOnjrBGMzDq6VbFKzh2ME6nL4JwI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f23cc1d33afde6adb9d68a9b31c290336a2dce2", "width": 1080, "height": 567}], "variants": {}, "id": "1uxtatdbdRpuzzFcT7KLXP88fRefAJkIBDqgJ_1uvVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "173vk4f", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173vk4f/introducing_asset_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/dagster-asset-checks", "subreddit_subscribers": 133025, "created_utc": 1696867500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.\n\nI then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.\n\nThis has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).\n\nI rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it's 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?\n\nP.S. We are a complete Microsoft shop with Azure databricks.", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I losing My Technical Edge By Just Fixing Mistakes By Contractors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1743emg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696886503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my second data engineering job. The first one involved making pipelines and modelling data yourself and then making the dashboard. Basically wearing many hats. I enjoyed doing that as I was learning a lot in that environment.&lt;/p&gt;\n\n&lt;p&gt;I then jumped to a bigger company with better pay and benefits, and all I do here is fix stuff made by other offshore contractors. There are multiple dashboards which are critical and they break quite often. I then get tickets to fix these issues. I have to read docs about that project, query tables and do debugging to fix issues.&lt;/p&gt;\n\n&lt;p&gt;This has been going on for a year now, and making any greenfield data project seems like not the way I am going with my present role. The previous offshore contractors made a very messy data architecture and my teams budget was being spent just to continuously fix issues related to it (they were basically being milked).&lt;/p&gt;\n\n&lt;p&gt;I rarely engineer a new pipeline other than when when dashboard engineers need a new column or specific KPI. Otherwise it&amp;#39;s 80% debugging. Will I lose my technical skills of integrating data from other platforms by just doing debugging?&lt;/p&gt;\n\n&lt;p&gt;P.S. We are a complete Microsoft shop with Azure databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1743emg", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1743emg/am_i_losing_my_technical_edge_by_just_fixing/", "subreddit_subscribers": 133025, "created_utc": 1696886503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.", "author_fullname": "t2_uqyn3qdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an equivalent of \u201cThe Data Warehouse Toolkit\u201d but for Lakehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173sl3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696860130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173sl3s", "is_robot_indexable": true, "report_reasons": null, "author": "variance-explained", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "subreddit_subscribers": 133025, "created_utc": 1696860130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi there \n\nI\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  \n \n\nIs DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?", "author_fullname": "t2_iweexjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow + DBT - question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17481kb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696898583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there &lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to understand the real reason for using Airflow + DBT, if the first one can connect directly to the database and apply all necessary transformations by using Operators (Postgres, Redshift, etc).  &lt;/p&gt;\n\n&lt;p&gt;Is DBT just adding more complexity to the project, or can it be helpful in other ways that I cannot find out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17481kb", "is_robot_indexable": true, "report_reasons": null, "author": "yeager_doug", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17481kb/airflow_dbt_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17481kb/airflow_dbt_question/", "subreddit_subscribers": 133025, "created_utc": 1696898583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.", "author_fullname": "t2_bfrtvbx9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineer from ETL developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174aeb1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696905373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been etl developer for 15 years(SSIS) and it is time to update skills, I think.\nSearching for suggestions, how can I move to data engineering? Should I just apply to entry position? Market is so bad \ud83d\ude1e.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174aeb1", "is_robot_indexable": true, "report_reasons": null, "author": "Charming_Function_35", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174aeb1/data_engineer_from_etl_developer/", "subreddit_subscribers": 133025, "created_utc": 1696905373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.\n\n&amp;#x200B;\n\nI was having a hard time finding ANY guides about how to set up all the components locally; [so I made a guide](https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;utm_campaign=post&amp;utm_medium=web). It goes over setting up and configuring:\n\n&amp;#x200B;\n\n* Iceberg Rest Catalog\n* Trino\n* Minio (s3 compatible for storage of Iceberg tables)\n* Kafka (3 Node KRaft Mode)\n* Kafka Connect\n* Schema Registry\n* Writing avro encoded data to Kafka in go\n\n&amp;#x200B;\n\nOnce done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  \n\n\nHope this helps :)  \n\n\n&amp;#x200B;", "author_fullname": "t2_31vknkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Data From Kafka into Apache Iceberg + Querying with Trino - Done in Docker Compose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173w6rf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696869032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, a new Kafka Sink Connector was released for writing data from Apache Kafka into Apache Iceberg tables.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was having a hard time finding ANY guides about how to set up all the components locally; &lt;a href=\"https://open.substack.com/pub/hendoxc/p/apache-iceberg-trino-iceberg-kafka?r=2vfgpf&amp;amp;utm_campaign=post&amp;amp;utm_medium=web\"&gt;so I made a guide&lt;/a&gt;. It goes over setting up and configuring:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Iceberg Rest Catalog&lt;/li&gt;\n&lt;li&gt;Trino&lt;/li&gt;\n&lt;li&gt;Minio (s3 compatible for storage of Iceberg tables)&lt;/li&gt;\n&lt;li&gt;Kafka (3 Node KRaft Mode)&lt;/li&gt;\n&lt;li&gt;Kafka Connect&lt;/li&gt;\n&lt;li&gt;Schema Registry&lt;/li&gt;\n&lt;li&gt;Writing avro encoded data to Kafka in go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Once done, running locally,  you will have data being written into an Apache Iceberg table from Kafka, with the underying data stored in minio, that you can query with SQL from your favourite IDE using Trino.  &lt;/p&gt;\n\n&lt;p&gt;Hope this helps :)  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?auto=webp&amp;s=7de03447d3a223deb509f88e091a03da6c93400c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=572c8360ccf24454b4b7b08d84dea4009543f95c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=337134c15aac8e57593f82b672a5fb12a2ef98e7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11c89632662beabc291281fd8ae813b346a65979", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63293a10ef238fbeead34150c54341ea690a80e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1985f6e22592db1ecaafb1908b69602ee237fc61", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/PqXhVVHPP5HVB2Bf4NXIB3qB0JBG5bPouPydGuuPka8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0451f1bef6592955e4a6d41575544c83616059c7", "width": 1080, "height": 540}], "variants": {}, "id": "My3S49gmIim5Z0I62eSqxwTyEEfinz1ddzP28sGEN6k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173w6rf", "is_robot_indexable": true, "report_reasons": null, "author": "thehendoxc", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173w6rf/streaming_data_from_kafka_into_apache_iceberg/", "subreddit_subscribers": 133025, "created_utc": 1696869032.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For background I'm a BI analyst, but helping set up a friend's startup infrastructure. They are early on and want to pull data from CRM's using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don't want to spend much or any money on this initially.\n\nFirst step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?\n\nI was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?\n\nAt first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.", "author_fullname": "t2_4qxl8qxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering pipeline for free or low cost in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173x67u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696871430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For background I&amp;#39;m a BI analyst, but helping set up a friend&amp;#39;s startup infrastructure. They are early on and want to pull data from CRM&amp;#39;s using an api into a database where we can query and visualize metrics based on the data from the API. Problem is they don&amp;#39;t want to spend much or any money on this initially.&lt;/p&gt;\n\n&lt;p&gt;First step is creating a data warehouse to send the data into I believe. Would mysql community edition be good for this and I can create the database on my machine?&lt;/p&gt;\n\n&lt;p&gt;I was looking into airbyte to get the data from the api into the database as the open source version is free. Any other recommendations?&lt;/p&gt;\n\n&lt;p&gt;At first it would just be me and my friend that would need access to the database and have sql privileges. Any and all help is welcome, just a beginner trying to learn the basics.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173x67u", "is_robot_indexable": true, "report_reasons": null, "author": "whittesc", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173x67u/data_engineering_pipeline_for_free_or_low_cost_in/", "subreddit_subscribers": 133025, "created_utc": 1696871430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  \n\nI work for a very old company so we have sales/financial/build records going all the way back to the 1970's....but I have never worked on any projects where anyone looked at anything older than 5 years.    \n\nEvery time I build anything, the business always wants \"everything\" and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn't exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.", "author_fullname": "t2_l5hazyvs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prevent data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173rxlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696858427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  &lt;/p&gt;\n\n&lt;p&gt;I work for a very old company so we have sales/financial/build records going all the way back to the 1970&amp;#39;s....but I have never worked on any projects where anyone looked at anything older than 5 years.    &lt;/p&gt;\n\n&lt;p&gt;Every time I build anything, the business always wants &amp;quot;everything&amp;quot; and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn&amp;#39;t exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173rxlz", "is_robot_indexable": true, "report_reasons": null, "author": "One_Ball8812", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "subreddit_subscribers": 133025, "created_utc": 1696858427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI've put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you're using dbt for your data transformations, this is for you!\n\n\ud83c\udf1f **What's Inside?**\n\n* Specialized dbt functions not in standard Jinja.\n* Unique macros for dbt workflows.\n* Filters, variables, and context methods to elevate your dbt projects.\n\n\ud83d\udd17 [https://datacoves.com/post/dbt-jinja-functions-cheat-sheet](https://datacoves.com/post/dbt-jinja-functions-cheat-sheet)\n\nWould love to hear your feedback and any suggestions for improvement. \ud83d\udca1\n\n\u2764\ufe0f Data-Queen", "author_fullname": "t2_fosm1pwyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hope this helps!: \ud83d\ude80 Ultimate dbt Jinja Functions Cheat Sheet!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173yg5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696874520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve put together a cheat sheet that dives into the Jinja additions to dbt specifically created by dbt Core. If you&amp;#39;re using dbt for your data transformations, this is for you!&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf1f &lt;strong&gt;What&amp;#39;s Inside?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Specialized dbt functions not in standard Jinja.&lt;/li&gt;\n&lt;li&gt;Unique macros for dbt workflows.&lt;/li&gt;\n&lt;li&gt;Filters, variables, and context methods to elevate your dbt projects.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;\ud83d\udd17 &lt;a href=\"https://datacoves.com/post/dbt-jinja-functions-cheat-sheet\"&gt;https://datacoves.com/post/dbt-jinja-functions-cheat-sheet&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your feedback and any suggestions for improvement. \ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;\u2764\ufe0f Data-Queen&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?auto=webp&amp;s=761c2339c1c3a5a98098f2be0b77af737b8c59d2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=659627a31f99370f0cbb33fd4514988c6c50bc57", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6a2a4756bd7205c1f3c0047c8f0b8ec554362b97", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=432641e4b76d230646cffa01cbbc8405158fe8db", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=662713884f50b946f205be681d5e74fe94225d30", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f76c268382b23cb18587bc9c0dfd00422ee1f65", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/AD_jjzPVyfVz37vbl1rQxNSaloB86L01-coWrOF8j7E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ceb922cd1c128faa388775cdb58bcd5948f731d3", "width": 1080, "height": 564}], "variants": {}, "id": "ghkewI0Ac4MJEXxpnppvl085nXvleyOWWENREIl1wOE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173yg5n", "is_robot_indexable": true, "report_reasons": null, "author": "Data-Queen-Mayra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173yg5n/hope_this_helps_ultimate_dbt_jinja_functions/", "subreddit_subscribers": 133025, "created_utc": 1696874520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.\n\nHowever, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don't use it much). The only downside is that the bank doesn't work with cloud solutions, and I'm not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.\n\nOn the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don't want to give up that part of my job), and are willing to provide the necessary training.\n\nThe salaries for both positions are the same, as the company will also give me a raise.\n\nHere's my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that's not true and starting with an on prem solution will be better. What do you think about this?\n\nThanks for all the advice!", "author_fullname": "t2_7jc8db23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173r3am", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696856073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.&lt;/p&gt;\n\n&lt;p&gt;However, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don&amp;#39;t use it much). The only downside is that the bank doesn&amp;#39;t work with cloud solutions, and I&amp;#39;m not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don&amp;#39;t want to give up that part of my job), and are willing to provide the necessary training.&lt;/p&gt;\n\n&lt;p&gt;The salaries for both positions are the same, as the company will also give me a raise.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that&amp;#39;s not true and starting with an on prem solution will be better. What do you think about this?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all the advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173r3am", "is_robot_indexable": true, "report_reasons": null, "author": "eqwlknam", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173r3am/job_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173r3am/job_advice/", "subreddit_subscribers": 133025, "created_utc": 1696856073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.\n\nMy first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.\n\nThe only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.\n\nRepartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.", "author_fullname": "t2_u4zm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark - How to break up extremely compressed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174c1zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696910635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a data stored in a bronze delta that I need to further process. It is only about 2 billion rows and is 20gb of data read from storage. The problem, I think, in short is that the data is highly compressed and while unserialized, my cluster / its settings is not prepared for it and breakdown ensues.&lt;/p&gt;\n\n&lt;p&gt;My first DAG step, mapping partition blocks to RDD, takes an enormously long time (hours). The data is split on my executors in a fairly balanced form, where each gets roughly 1.1 gb of data while my shuffle write is tiny ranging from bytes to kb.&lt;/p&gt;\n\n&lt;p&gt;The only knob I know to turn to alter the input stage is this: spark.sql.files.maxPartitionBytes but it does not help. With a small value, Spark will blow through the first n-thousands but will grind on the last 50-ish blocks of data.&lt;/p&gt;\n\n&lt;p&gt;Repartitioning or coalescing is only an option after my first DAG step. Running OPTIMIZE fails; I get OOM on my executors and I have tried these operations on many configs / cluster sizes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174c1zs", "is_robot_indexable": true, "report_reasons": null, "author": "JohnStud85", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174c1zs/pyspark_how_to_break_up_extremely_compressed_data/", "subreddit_subscribers": 133025, "created_utc": 1696910635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. I have a case of eCommerce data where the transactional database has `orders` and `products`. I'm confused as to how to model this in a star schema.\n\nMy thoughts are that the `orders` should be a fact table (`fct_orders`) containing an aggregate of `products` total value, ie `fct_orders.total_order_value`. However, if an order has multiple products, what would the foreign key look like in `fct_orders`?\n\nI've tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my `products` table the fact table. However, how can I model the aggregate `fct_orders.total_order_value`?\n\nThanks in advance!\n\n&amp;#x200B;\n\nEdit: just learned about bridge tables ([https://www.leapfrogbi.com/bridge-tables/](https://www.leapfrogbi.com/bridge-tables/)), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...", "author_fullname": "t2_9trbw25s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to model one fact table that joins to multiple rows in dimension table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174avck", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696908569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696906843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I have a case of eCommerce data where the transactional database has &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;products&lt;/code&gt;. I&amp;#39;m confused as to how to model this in a star schema.&lt;/p&gt;\n\n&lt;p&gt;My thoughts are that the &lt;code&gt;orders&lt;/code&gt; should be a fact table (&lt;code&gt;fct_orders&lt;/code&gt;) containing an aggregate of &lt;code&gt;products&lt;/code&gt; total value, ie &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;. However, if an order has multiple products, what would the foreign key look like in &lt;code&gt;fct_orders&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried searching online for answers and have read that what defines a fact and dimension table is the one-to-many relationships; there would only be a 1:N relationship between a dimension and fact table, not the other way around - which would make my &lt;code&gt;products&lt;/code&gt; table the fact table. However, how can I model the aggregate &lt;code&gt;fct_orders.total_order_value&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: just learned about bridge tables (&lt;a href=\"https://www.leapfrogbi.com/bridge-tables/\"&gt;https://www.leapfrogbi.com/bridge-tables/&lt;/a&gt;), would this be an appropriate use case of this? It feels like extra compute overhead on the deserialisation and compute though...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?auto=webp&amp;s=736d9650919a08025a3b529a262d040d5eccc7c4", "width": 508, "height": 553}, "resolutions": [{"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=709171c289f304a1417184d7d2ebada30a823909", "width": 108, "height": 117}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c77e4d74b3cfefe997d1d3a271f7c82a6eaa5c5d", "width": 216, "height": 235}, {"url": "https://external-preview.redd.it/M5E3roN5E1_WPpGThJnBk5bFrVOpMN0ED6ABAtD5x5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d6b309475f6fb91e4e9b925f4e3d75edb053c17", "width": 320, "height": 348}], "variants": {}, "id": "bIfqiV67OXElDsn2F_5SezvPv1apC5LPOp1agLPkfo4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174avck", "is_robot_indexable": true, "report_reasons": null, "author": "ternary-thought", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174avck/how_to_model_one_fact_table_that_joins_to/", "subreddit_subscribers": 133025, "created_utc": 1696906843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the difference between Databricks\u2019s Overwatch and System Tables tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_174j9fg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7H7guNkrLzYUixs3AR3Psyd23HlFvAUyaDvJr6r1KQs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696938761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.det.life", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.det.life/whats-the-difference-between-databricks-s-overwatch-and-system-tables-tools-f9d0cd75a2f2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?auto=webp&amp;s=cdf604b54fe85e5e161f564bad80873c3b0b2d73", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=495021a9846e755e37ba06d7fb7b00bdd919d5af", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74fc4c3c060466a5374a28a66f96c5e3a1efaa3e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2f1a08b5cf74574326b34e55fcaa8a7192d42f2", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ae1eaf20860d2e12e3860d79789872c68c554f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8619f09a42879ea79dc90050268fbd11970bf2a0", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/Nw9t-SE-cYa0esSiNqnHZrCn0-HY9eMK0EjI3OZp_rM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfe55cdcb6fd4feaba16b6a4a3591f9e94b9c621", "width": 1080, "height": 720}], "variants": {}, "id": "XTYuj6I-Q2v_xqeIXOzOgfp785bB9G0Zkdojd7TtV_w"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "174j9fg", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174j9fg/whats_the_difference_between_databrickss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.det.life/whats-the-difference-between-databricks-s-overwatch-and-system-tables-tools-f9d0cd75a2f2", "subreddit_subscribers": 133025, "created_utc": 1696938761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the databases, the data reports and do machine learning projects when we have time. The methodology for the last three years, (yes the organization just started to think about data three years ago), has been a low hanging fruit methodology, meaning we start a project, create some value from it, publish it and move on. \n\nNeedless to say that has left a lot of quality control neglected. Name giving is inconsistent, data owners and users are often unknown and some quick fix sh\\*t solutions are still being used like windows scheduler to run some codes. There is hardly any documentation about our data infrastructure to add. \n\nNow I don't think any one is to blame for this as this is a government organization on a budget and the two of us are head over heels in projects but the time has come to tighten loose ends. My question is, has anyone experienced a similar scenario and solved it? How did you solve it? Is there any good literature on the subject or other resources? \n\nFYI we are using Microsoft solutions like Azure and power platform for 90% of what we do. We are also a REIT and construction management type of organization if that is relevant.\n\nThanks a lot in advance for all responses.", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizational documentation for data infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174hlpp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696932690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I work in a two man team for a government organization as an all purpose data engineer. Meaning we set up and maintain all data pipelines, the databases, the data reports and do machine learning projects when we have time. The methodology for the last three years, (yes the organization just started to think about data three years ago), has been a low hanging fruit methodology, meaning we start a project, create some value from it, publish it and move on. &lt;/p&gt;\n\n&lt;p&gt;Needless to say that has left a lot of quality control neglected. Name giving is inconsistent, data owners and users are often unknown and some quick fix sh*t solutions are still being used like windows scheduler to run some codes. There is hardly any documentation about our data infrastructure to add. &lt;/p&gt;\n\n&lt;p&gt;Now I don&amp;#39;t think any one is to blame for this as this is a government organization on a budget and the two of us are head over heels in projects but the time has come to tighten loose ends. My question is, has anyone experienced a similar scenario and solved it? How did you solve it? Is there any good literature on the subject or other resources? &lt;/p&gt;\n\n&lt;p&gt;FYI we are using Microsoft solutions like Azure and power platform for 90% of what we do. We are also a REIT and construction management type of organization if that is relevant.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance for all responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174hlpp", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174hlpp/organizational_documentation_for_data/", "subreddit_subscribers": 133025, "created_utc": 1696932690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, I'm a fresher in DA and DE. I was assigned a task to build a user dimension.\n\n**Purpose**: map user between 3 sources (firebase, MAX, appsflyer) in order to create a complete user journey\n\nIdentifiers that can be used in 3 tables: user\\_id (from firebase), idfa, idfv, appsflyer\\_id\n\nI started to analyze how each identifier would change:\n\n* for user\\_id, it will change after re-installs because my app doesn't require any login so the user\\_id is not consistent \n* idfa will change if a user reset OS or turn on limit ad tracking\n* idfv will change if a user switches to new device\n\nWhat I'm struggling is how i can identify a user and develop general logic, given one of real-life scenarios:\n\n* a user can have multiple devices. how can i identify these devices as 1 particular person?\n* if a user re-installs the app (user\\_id changes), how can i identify him/her as old user coming back?\n* what if a user change platform? (from ios to android) how can i identify this user as old one without treating him/her as new one?\n* not all records in 3 tables can be joined with user\\_id, idfa or idfv (separately); sometimes, user\\_id AND idfa; sometimes, idfa only; sometimes, user\\_id and idfv as i tried to figure out how to layer these ids (trying to put things in a nested field)\n* answering and solving above cases (separately) is not too hard but when those 3 happen at the same time. things starts to become too complex and tangled for me to process (it's demoralizing af :((( )\n\nWrong or missed identification will result in wrong analytics, e.g: wrong CLV as $40K revenue for 10K users is different from $40K revenue for 20K users\n\nIs there any aspects or approaches that i should consider to solve this? Thanks in advanced \\^\\^", "author_fullname": "t2_8wrp4ovv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach user identification when building user dimension?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174eeqe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696919412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;m a fresher in DA and DE. I was assigned a task to build a user dimension.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: map user between 3 sources (firebase, MAX, appsflyer) in order to create a complete user journey&lt;/p&gt;\n\n&lt;p&gt;Identifiers that can be used in 3 tables: user_id (from firebase), idfa, idfv, appsflyer_id&lt;/p&gt;\n\n&lt;p&gt;I started to analyze how each identifier would change:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;for user_id, it will change after re-installs because my app doesn&amp;#39;t require any login so the user_id is not consistent &lt;/li&gt;\n&lt;li&gt;idfa will change if a user reset OS or turn on limit ad tracking&lt;/li&gt;\n&lt;li&gt;idfv will change if a user switches to new device&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What I&amp;#39;m struggling is how i can identify a user and develop general logic, given one of real-life scenarios:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a user can have multiple devices. how can i identify these devices as 1 particular person?&lt;/li&gt;\n&lt;li&gt;if a user re-installs the app (user_id changes), how can i identify him/her as old user coming back?&lt;/li&gt;\n&lt;li&gt;what if a user change platform? (from ios to android) how can i identify this user as old one without treating him/her as new one?&lt;/li&gt;\n&lt;li&gt;not all records in 3 tables can be joined with user_id, idfa or idfv (separately); sometimes, user_id AND idfa; sometimes, idfa only; sometimes, user_id and idfv as i tried to figure out how to layer these ids (trying to put things in a nested field)&lt;/li&gt;\n&lt;li&gt;answering and solving above cases (separately) is not too hard but when those 3 happen at the same time. things starts to become too complex and tangled for me to process (it&amp;#39;s demoralizing af :((( )&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Wrong or missed identification will result in wrong analytics, e.g: wrong CLV as $40K revenue for 10K users is different from $40K revenue for 20K users&lt;/p&gt;\n\n&lt;p&gt;Is there any aspects or approaches that i should consider to solve this? Thanks in advanced ^^&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174eeqe", "is_robot_indexable": true, "report_reasons": null, "author": "girlsyesboysno", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/174eeqe/how_to_approach_user_identification_when_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174eeqe/how_to_approach_user_identification_when_building/", "subreddit_subscribers": 133025, "created_utc": 1696919412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm struggling to efficiently join data when I have multiple failsafe join points.\n\nSpecifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we'll just consider the three utm campaign parameters:\n\n* utm\\_term\n* utm\\_content\n* utm\\_campaign\n\nI want to join my data based on utm\\_term *if that's possible.* But if it's not, I'll fall back on utm\\_content or utm\\_campaign instead.\n\n**The problem** is that any SQL join I'm aware of that uses multiple join points will use every join point possible. So, currently, I'm dealing with this with a two-step process.\n\nFirst, I find the best join point available for each row of data...\n\n    UPDATE session_data a\n    SET a.Join_Type = b.Join_Type\n    FROM (\n        SELECT\n            session_id,\n            CASE\n                WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &gt; 0 THEN 'utm_term'\n                WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &gt; 0 THEN 'utm_content'\n                WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &gt; 0 THEN 'utm_campaign'\n               ELSE 'Channel'\n            END AS Join_Type\n            FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n            LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n            ON ga.channel = ad.channel AND (\n                ga.utm_term = ad.utm_term OR \n                ga.utm_content = ad.utm_content OR \n                ga.utm_campaign = ad.utm_campaign OR \n            GROUP BY session_id\n        )\n    ) b\n    WHERE a.session_id = b.session_id;\n\n... and then I use that label to join by the best join point available only:\n\n    SELECT * \n    FROM `session_data` ga\n    LEFT JOIN `ad_data` ad\n    WHERE \n    CASE\n        WHEN ga.Join_Type = 'utm_term' THEN ga.utm_term = ad.utm_term\n        WHEN ga.Join_Type = 'utm_content' THEN ga.utm_content = ad.utm_content\n        WHEN ga.Join_Type = 'utm_campaign' THEN ga.utm_campaign = ad.utm_campaign\n        WHEN ga.Join_Type = 'Channel' THEN ga.channel = ad.channel\n    END\n\nWhich works! \n\n*(I mean, I'm leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)*\n\nThat first query, in particular, is *super* problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I'm working in), just results in an error message.\n\nThere has got to be a better way to tackle this join. Anyone know of one?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a more efficient way to do a join by multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1742rzg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696884953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m struggling to efficiently join data when I have multiple failsafe join points.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this is for web attribution. When somebody comes to a website, we can figure out which ad campaign they came from based on a lot of clues. My actual model is much more complex than this, but, for here, we&amp;#39;ll just consider the three utm campaign parameters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;utm_term&lt;/li&gt;\n&lt;li&gt;utm_content&lt;/li&gt;\n&lt;li&gt;utm_campaign&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to join my data based on utm_term &lt;em&gt;if that&amp;#39;s possible.&lt;/em&gt; But if it&amp;#39;s not, I&amp;#39;ll fall back on utm_content or utm_campaign instead.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt; is that any SQL join I&amp;#39;m aware of that uses multiple join points will use every join point possible. So, currently, I&amp;#39;m dealing with this with a two-step process.&lt;/p&gt;\n\n&lt;p&gt;First, I find the best join point available for each row of data...&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;UPDATE session_data a\nSET a.Join_Type = b.Join_Type\nFROM (\n    SELECT\n        session_id,\n        CASE\n            WHEN SUM(CASE WHEN ga.utm_term = ad.utm_term THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_term&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_content = ad.utm_content THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_content&amp;#39;\n            WHEN SUM(CASE WHEN ga.utm_campaign = ad.utm_campaign THEN 1 END) &amp;gt; 0 THEN &amp;#39;utm_campaign&amp;#39;\n           ELSE &amp;#39;Channel&amp;#39;\n        END AS Join_Type\n        FROM (SELECT session_id, channel, utm_term, utm_content, utm_campaign FROM `session_data`) ga\n        LEFT JOIN (SELECT channel utm_term, utm_content, utm_campaign FROM `ad_data`) ad\n        ON ga.channel = ad.channel AND (\n            ga.utm_term = ad.utm_term OR \n            ga.utm_content = ad.utm_content OR \n            ga.utm_campaign = ad.utm_campaign OR \n        GROUP BY session_id\n    )\n) b\nWHERE a.session_id = b.session_id;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;... and then I use that label to join by the best join point available only:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * \nFROM `session_data` ga\nLEFT JOIN `ad_data` ad\nWHERE \nCASE\n    WHEN ga.Join_Type = &amp;#39;utm_term&amp;#39; THEN ga.utm_term = ad.utm_term\n    WHEN ga.Join_Type = &amp;#39;utm_content&amp;#39; THEN ga.utm_content = ad.utm_content\n    WHEN ga.Join_Type = &amp;#39;utm_campaign&amp;#39; THEN ga.utm_campaign = ad.utm_campaign\n    WHEN ga.Join_Type = &amp;#39;Channel&amp;#39; THEN ga.channel = ad.channel\nEND\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Which works! &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(I mean, I&amp;#39;m leaving a lot of stuff out -- like the other join clues we use and how we approximate data when there are multiple matches -- but this is where the script really struggles with efficiency issues.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;That first query, in particular, is &lt;em&gt;super&lt;/em&gt; problematic. In some datasets, there are a lot of possible joins that can happen, so it can result in analyzing millions or billions of rows of data -- which, in BigQuery (which I&amp;#39;m working in), just results in an error message.&lt;/p&gt;\n\n&lt;p&gt;There has got to be a better way to tackle this join. Anyone know of one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1742rzg", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1742rzg/is_there_a_more_efficient_way_to_do_a_join_by/", "subreddit_subscribers": 133025, "created_utc": 1696884953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an SWE (**not a data scientist**) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.\n\nI started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don't think they solve the issue of validating *changes in data* (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.\n\nWhat I'm looking for is a tool that validates changes in data by comparing the previous value with the new value.\n\nIn some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there's obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).\n\nThe purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.\n\nThis is just an example, but it would be helpful if we can call an API to do this sort of validation for us.\n\nAnd instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I'm just brainstorming here.\n\nWould highly appreciate some recommendations/tips for tackling this problem. Thank you!", "author_fullname": "t2_mxg44sgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to validate data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_174236b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696883926.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696883258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an SWE (&lt;strong&gt;not a data scientist&lt;/strong&gt;) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.&lt;/p&gt;\n\n&lt;p&gt;I started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don&amp;#39;t think they solve the issue of validating &lt;em&gt;changes in data&lt;/em&gt; (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a tool that validates changes in data by comparing the previous value with the new value.&lt;/p&gt;\n\n&lt;p&gt;In some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there&amp;#39;s obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).&lt;/p&gt;\n\n&lt;p&gt;The purpose of this validation is not to deem the change as 100% invalid/valid but to alert us that there may be a mistake.&lt;/p&gt;\n\n&lt;p&gt;This is just an example, but it would be helpful if we can call an API to do this sort of validation for us.&lt;/p&gt;\n\n&lt;p&gt;And instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I&amp;#39;m just brainstorming here.&lt;/p&gt;\n\n&lt;p&gt;Would highly appreciate some recommendations/tips for tackling this problem. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "174236b", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Main-6700", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174236b/how_to_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174236b/how_to_validate_data/", "subreddit_subscribers": 133025, "created_utc": 1696883258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).\n\nHow do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?\n\nHow do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?\n\nSorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  \n\n\n**NOTE**: when I'm talking about ETL development lifecycle, I actually mean data pipelines in general.", "author_fullname": "t2_21xquoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "E2E ETL lifecycle in multi-environment setup (bonus for integration with E2E analytics)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1740atd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696880710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696878890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! This is a question for teams, that have a multi-environment setup (at least DEV, STAGE/UAT/PRE-PROD, and PROD).&lt;/p&gt;\n\n&lt;p&gt;How do you manage your whole ETL development lifecycle?  Do you develop on DEV, do integrations and other tests on STAGE/UAT, and maintain Data quality checks on PROD? Or do you do the same ETLs for each of the environments? Maybe you do ETLs only on PROD and replicate everything onto other environments?&lt;/p&gt;\n\n&lt;p&gt;How do you integrate E2E analytics or WH development lifecycle? To do the development, you must have PROD-grade source data, how do you ensure that? What does the process look like in your setup?&lt;/p&gt;\n\n&lt;p&gt;Sorry, for being too blunt about the question, but thanks for all of your answers and the time you spent writing :)  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: when I&amp;#39;m talking about ETL development lifecycle, I actually mean data pipelines in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1740atd", "is_robot_indexable": true, "report_reasons": null, "author": "kaributas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1740atd/e2e_etl_lifecycle_in_multienvironment_setup_bonus/", "subreddit_subscribers": 133025, "created_utc": 1696878890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nLet's say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.\n\n&amp;#x200B;\n\nHow do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?\n\n&amp;#x200B;\n\nThank you.", "author_fullname": "t2_a7y0xzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you model customer behavior tracking data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173tosp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696862916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173tosp", "is_robot_indexable": true, "report_reasons": null, "author": "masamibb", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "subreddit_subscribers": 133025, "created_utc": 1696862916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you'd start a career in data engineering  in 2023 ,\n\n&amp; you've exp in python sql  , \n\nwould you pursue learning ssis or choose cloud technology ? \n\nThank you ", "author_fullname": "t2_t3nz93za", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On Premises Vs Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174jezy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696939266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;d start a career in data engineering  in 2023 ,&lt;/p&gt;\n\n&lt;p&gt;&amp;amp; you&amp;#39;ve exp in python sql  , &lt;/p&gt;\n\n&lt;p&gt;would you pursue learning ssis or choose cloud technology ? &lt;/p&gt;\n\n&lt;p&gt;Thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174jezy", "is_robot_indexable": true, "report_reasons": null, "author": "Repulsive-Ad7769", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174jezy/on_premises_vs_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174jezy/on_premises_vs_cloud/", "subreddit_subscribers": 133025, "created_utc": 1696939266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6vz2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Help] Tried highlighting what Databricks does \"in-house\" for a project. Is this accurate?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_174j9ov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/q3Q6PeLx4nf2CkBUckenFrp8x3M87-n6tWdw4vsV20M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696938786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/dqmlofum5dtb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/dqmlofum5dtb1.png?auto=webp&amp;s=934d980ca188a1b4bf1a6c5a211557f9b454fc6e", "width": 2264, "height": 2323}, "resolutions": [{"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3435cde6ec91dfa9ca9f535eaf0a228bc0b461b", "width": 108, "height": 110}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cd06c19a55477daca085fa59cd60b8ce2189463", "width": 216, "height": 221}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7cf468576834af1e6abfe22f983baf6c0683803", "width": 320, "height": 328}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f780866b1d8438d2d36ea255b01eef82974c2e6", "width": 640, "height": 656}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=816f4f3aee433d66db633a010df2efe0a00a3ba1", "width": 960, "height": 985}, {"url": "https://preview.redd.it/dqmlofum5dtb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16fa3fcde7e1fddbff3687503e7403c027f6a376", "width": 1080, "height": 1108}], "variants": {}, "id": "1XiupRIo73tDBkiQdN0J0Hw0qRuljcK-TfvELTnl8Bo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174j9ov", "is_robot_indexable": true, "report_reasons": null, "author": "boulking", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174j9ov/help_tried_highlighting_what_databricks_does/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/dqmlofum5dtb1.png", "subreddit_subscribers": 133025, "created_utc": 1696938786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I created a simple process simulation in Godot game engine which can be controlled with any PLC (with Codesys runtime and TCP/IP compatibility). \n\nThe techninal background is almost ready for this project, but I lack the \"manager story\", like what is the meaning of this simulation, why it could be good in the industry, and so on. \n\nOriginally the idea came from my teaching experience where I wanted to make the process control courses more fun for my students. \n\nSo I need some advice how could I make this project more engineering manager-like?", "author_fullname": "t2_7c2wzv2u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Engineering Manager project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_174j2e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bLBxYv-Qwc3IJrD1AgtylmomU6WpBA44E7T84Jmu9ps.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696938123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created a simple process simulation in Godot game engine which can be controlled with any PLC (with Codesys runtime and TCP/IP compatibility). &lt;/p&gt;\n\n&lt;p&gt;The techninal background is almost ready for this project, but I lack the &amp;quot;manager story&amp;quot;, like what is the meaning of this simulation, why it could be good in the industry, and so on. &lt;/p&gt;\n\n&lt;p&gt;Originally the idea came from my teaching experience where I wanted to make the process control courses more fun for my students. &lt;/p&gt;\n\n&lt;p&gt;So I need some advice how could I make this project more engineering manager-like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ff7v3jy94dtb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?auto=webp&amp;s=97b09b473ea8dd29fbf7061e76a1b5dc20c82a72", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1a810cde4928f805feaf2c03f62223d5a28eded", "width": 108, "height": 60}, {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8638e7bee6e90c68fafcde50c59787bb3d55d0a", "width": 216, "height": 121}, {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c78b7b86cef5edac9069b8a4f3214134f39a0fa7", "width": 320, "height": 180}, {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=616540d108fcd0d07ac42f07e6845d5b7356333e", "width": 640, "height": 360}, {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=45f9cd382ca279a8580d26dcc93d0236f83321d6", "width": 960, "height": 540}, {"url": "https://preview.redd.it/ff7v3jy94dtb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d87880812efe247b3f2ace2157924c3dc59dcd7", "width": 1080, "height": 607}], "variants": {}, "id": "0ePEh19psizZ9hK6-PWwAeQ6Rs0hUQVw_Jqlzoqmnkc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "174j2e4", "is_robot_indexable": true, "report_reasons": null, "author": "SzabiT", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174j2e4/engineering_manager_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ff7v3jy94dtb1.jpg", "subreddit_subscribers": 133025, "created_utc": 1696938123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a consultancy but I've been thinking recently that a lot of SME requirements can really be handled by a solo developer and don't need a whole team or business behind them. \n\nThe biggest obstacles I see are client acquisition and also that a client may not be comfortable hiring a one man team. \n\nDoes anyone have any experience striking out as a solo? I'd love to hear any stories. Or if anyone knows of anything online where someone has documented their own journey.\n\nEven if you haven't done it yourself it'd be interesting to hear your general thoughts on the prospect.", "author_fullname": "t2_6o5du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody here started a solo consultancy (or can share a good resource for it)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_174irny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696937086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a consultancy but I&amp;#39;ve been thinking recently that a lot of SME requirements can really be handled by a solo developer and don&amp;#39;t need a whole team or business behind them. &lt;/p&gt;\n\n&lt;p&gt;The biggest obstacles I see are client acquisition and also that a client may not be comfortable hiring a one man team. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience striking out as a solo? I&amp;#39;d love to hear any stories. Or if anyone knows of anything online where someone has documented their own journey.&lt;/p&gt;\n\n&lt;p&gt;Even if you haven&amp;#39;t done it yourself it&amp;#39;d be interesting to hear your general thoughts on the prospect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "174irny", "is_robot_indexable": true, "report_reasons": null, "author": "Cypher211", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/174irny/anybody_here_started_a_solo_consultancy_or_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/174irny/anybody_here_started_a_solo_consultancy_or_can/", "subreddit_subscribers": 133025, "created_utc": 1696937086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as title", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good place where I can get all 2024 summer data engineering internships ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17403jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696878408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17403jr", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17403jr/any_good_place_where_i_can_get_all_2024_summer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17403jr/any_good_place_where_i_can_get_all_2024_summer/", "subreddit_subscribers": 133025, "created_utc": 1696878408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for working with dbt and Snowflake - A practitioner\u2019s guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17401u2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UzBOjGfWVmMW0PI3xw6ehMGg5y-n8aHgUpGbmnCVs8I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696878298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?auto=webp&amp;s=e7612f0c09356ae4bc3d254edd02e167a83a2733", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0cafd6d1b68fdc7b4bb1b5c215278d61a2577a01", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56902c994507c9bd1a79cb15dea9b39629b5c9d4", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22cc3d82dc59750bc0cc8283f655d26727bd3da0", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aeea40b0ea537d47169953badb79986134a4da7a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20973f9c6ce81bee6ca454d8a853e4101a883306", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/mADNJwLnD39pqbBdzEG_V8luzB_0TSAqh-quI94Z9Qg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6020d7897990fb7542d5ce7123b8536afa9b7fee", "width": 1080, "height": 720}], "variants": {}, "id": "ucEfKV0PvxKYvUs4LpGpFLHRp3wQfXsbHwo066xOa3c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17401u2", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17401u2/best_practices_for_working_with_dbt_and_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/dbt-snowflake-best-practices/", "subreddit_subscribers": 133025, "created_utc": 1696878298.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}