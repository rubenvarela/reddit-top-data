{"kind": "Listing", "data": {"after": "t3_17jubdy", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Pentagram Schema for Evaluating Haunted Data Warehouses**\n\nIn the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the \"Pentagram Schema.\" This schema is not just about data; it's about the eerie, the paranormal, and the mystical.\n\n**Pentagram Schema Overview:**\n\nAt the heart of the Pentagram Schema lies the **\"Haunted Data WareHouse Access\" fact table.** This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.\n\n**The Five Satanic Dimensions:**\n\n1. **Visitor (Dimension 1):** In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.\n2. **Query (Dimension 2):** Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they're regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.\n3. **Outcome (Dimension 3):** The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.\n4. **Tables Hit (Dimension 4):** Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.\n5. **Maintainer (Dimension 5):** In any haunted attraction, there's a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.\n\n&amp;#x200B;\n\n**Why the Pentagram Schema:**\n\nThese five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.\n\nHappy Haunting!\n\n  \n(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haunted data warehouse pentagram schema architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvmmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Pentagram Schema for Evaluating Haunted Data Warehouses&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the &amp;quot;Pentagram Schema.&amp;quot; This schema is not just about data; it&amp;#39;s about the eerie, the paranormal, and the mystical.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pentagram Schema Overview:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At the heart of the Pentagram Schema lies the &lt;strong&gt;&amp;quot;Haunted Data WareHouse Access&amp;quot; fact table.&lt;/strong&gt; This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Five Satanic Dimensions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Visitor (Dimension 1):&lt;/strong&gt; In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Query (Dimension 2):&lt;/strong&gt; Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they&amp;#39;re regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Outcome (Dimension 3):&lt;/strong&gt; The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tables Hit (Dimension 4):&lt;/strong&gt; Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintainer (Dimension 5):&lt;/strong&gt; In any haunted attraction, there&amp;#39;s a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why the Pentagram Schema:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.&lt;/p&gt;\n\n&lt;p&gt;Happy Haunting!&lt;/p&gt;\n\n&lt;p&gt;(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "17jvmmf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "subreddit_subscribers": 137063, "created_utc": 1698678690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some help to in understanding the dbricks cluster loading.\n\nSay I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. \n\nIf I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? \n\nIf I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?\n\nThanks for checking.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens when we submit a lot of spark jobs on a Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvo4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some help to in understanding the dbricks cluster loading.&lt;/p&gt;\n\n&lt;p&gt;Say I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. &lt;/p&gt;\n\n&lt;p&gt;If I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? &lt;/p&gt;\n\n&lt;p&gt;If I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?&lt;/p&gt;\n\n&lt;p&gt;Thanks for checking.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jvo4e", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "subreddit_subscribers": 137063, "created_utc": 1698678796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I just uploaded a Python Pandas course on YouTube. I covered the introduction and installation of pandas, series and series operations, dataframes and basic dataframe creation, creating dataframes from various file formats, dataframe operations, identifying and handling missing data, data manipulation using loc and iloc, sorting and ranking data, combining and merging dataframes, data cleaning techniques, handling categorical data, data transformation techniques, handling date and time data, group by operations, aggregating data using functions, time series data visualization, advanced data manipulation techniques (apply, map, and apply map), data visualization with pandas tools, working with multi-index dataframes and text manipulation methods topics. I am leaving the course link below, have a great day!\n\nhttps://www.youtube.com/watch?v=KvFZf3cL_IY", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I shared a Python Pandas course (1.5 Hrs) on YouTube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvf6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I just uploaded a Python Pandas course on YouTube. I covered the introduction and installation of pandas, series and series operations, dataframes and basic dataframe creation, creating dataframes from various file formats, dataframe operations, identifying and handling missing data, data manipulation using loc and iloc, sorting and ranking data, combining and merging dataframes, data cleaning techniques, handling categorical data, data transformation techniques, handling date and time data, group by operations, aggregating data using functions, time series data visualization, advanced data manipulation techniques (apply, map, and apply map), data visualization with pandas tools, working with multi-index dataframes and text manipulation methods topics. I am leaving the course link below, have a great day!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=KvFZf3cL_IY\"&gt;https://www.youtube.com/watch?v=KvFZf3cL_IY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?auto=webp&amp;s=f1439e666963311860ed2bbc6d18d58138ab20ab", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1160dcf6aa25a9ba5c8b7c522c6001b5a93c5bde", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9047f27c215633f01e07e0e4e2dadf6e306dd0bd", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31180cc898c560b7db600f56a3d2fa228fdaabff", "width": 320, "height": 240}], "variants": {}, "id": "l-wENCMcE_8QSoQ-KSZE0RDTI-ogPC-MOsWTs7k42RI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17jvf6p", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvf6p/i_shared_a_python_pandas_course_15_hrs_on_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvf6p/i_shared_a_python_pandas_course_15_hrs_on_youtube/", "subreddit_subscribers": 137063, "created_utc": 1698678171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI'm a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don't know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere's also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? \n\nHow realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?\n\nCan you recommend some resources or action plan?\nThanks", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to pick up java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k04oj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698690651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;m a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don&amp;#39;t know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere&amp;#39;s also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? &lt;/p&gt;\n\n&lt;p&gt;How realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?&lt;/p&gt;\n\n&lt;p&gt;Can you recommend some resources or action plan?\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17k04oj", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "subreddit_subscribers": 137063, "created_utc": 1698690651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DEs, first of all not to hate on data engineering I\u2019ve been doing this for around 7 years and really like the space and what I do (maybe not so much my current company) but have been approached by a company with a hybrid data/backend engineering role.\n\nThe data engineer title varies a lot between companies but I work a lot with python, building custom tools for everything from custom extraction, schema management, auto DAG generation, Airflow plugins, DBT plugins, etc. \n\nI like what I do but the last year or so I\u2019ve been getting pretty burnt out on the day to day data eng tasks. I actually applied to a backend Scala position internally at my company but that didn\u2019t go through. I think a lot of my frustrations stems from the data org at my current company just being incredibly non-technical and really apathetic around engineering in general.\n\nThis new role is basically someone to own their slim/simple data pipelines and DBT project, but also work on their backend APIs mostly in Golang. Just the thought of working in and learning a new language (I\u2019ve used Go a bit but not extensively) is exciting to me. Plus it seems like Go is being used a lot more on the data processing side, so not a total career flip. \n\nAgain I really enjoy the data eng space but much more so from the eng perspective. Solving \u201cbusiness problems\u201d with data just doesn\u2019t excite me anymore, I don\u2019t wanna use DBT to make your data model I wanna BUIlD DBT, I don\u2019t wanna use airflow run your ETL I wanna BUILD airflow. \n\nAny advice or warnings from engineers who\u2019ve jumped from DE would be appreciated!", "author_fullname": "t2_109762", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to backend engineering from data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kcypl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698726646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DEs, first of all not to hate on data engineering I\u2019ve been doing this for around 7 years and really like the space and what I do (maybe not so much my current company) but have been approached by a company with a hybrid data/backend engineering role.&lt;/p&gt;\n\n&lt;p&gt;The data engineer title varies a lot between companies but I work a lot with python, building custom tools for everything from custom extraction, schema management, auto DAG generation, Airflow plugins, DBT plugins, etc. &lt;/p&gt;\n\n&lt;p&gt;I like what I do but the last year or so I\u2019ve been getting pretty burnt out on the day to day data eng tasks. I actually applied to a backend Scala position internally at my company but that didn\u2019t go through. I think a lot of my frustrations stems from the data org at my current company just being incredibly non-technical and really apathetic around engineering in general.&lt;/p&gt;\n\n&lt;p&gt;This new role is basically someone to own their slim/simple data pipelines and DBT project, but also work on their backend APIs mostly in Golang. Just the thought of working in and learning a new language (I\u2019ve used Go a bit but not extensively) is exciting to me. Plus it seems like Go is being used a lot more on the data processing side, so not a total career flip. &lt;/p&gt;\n\n&lt;p&gt;Again I really enjoy the data eng space but much more so from the eng perspective. Solving \u201cbusiness problems\u201d with data just doesn\u2019t excite me anymore, I don\u2019t wanna use DBT to make your data model I wanna BUIlD DBT, I don\u2019t wanna use airflow run your ETL I wanna BUILD airflow. &lt;/p&gt;\n\n&lt;p&gt;Any advice or warnings from engineers who\u2019ve jumped from DE would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17kcypl", "is_robot_indexable": true, "report_reasons": null, "author": "babyfacebrain666", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kcypl/switching_to_backend_engineering_from_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kcypl/switching_to_backend_engineering_from_data/", "subreddit_subscribers": 137063, "created_utc": 1698726646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "looking for ideas - anything to do with bad data/ data quality/ even regulations etc.", "author_fullname": "t2_j69fw1hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any funny halloween costume ideas related to data quality? \ud83c\udf83", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k3r1v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698700067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;looking for ideas - anything to do with bad data/ data quality/ even regulations etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17k3r1v", "is_robot_indexable": true, "report_reasons": null, "author": "rdtro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k3r1v/any_funny_halloween_costume_ideas_related_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k3r1v/any_funny_halloween_costume_ideas_related_to_data/", "subreddit_subscribers": 137063, "created_utc": 1698700067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serverless Is Not All You Need", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_17k4foj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gFqWPqOOtc9JA7odkNvCO5LKJYM73ibt4UJ-NR9S8LU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698701870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://risingwave.com/blog/serverless-is-not-all-you-need/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?auto=webp&amp;s=04f89ab2d2a41402623fa1bb9a18187db853767b", "width": 6346, "height": 3400}, "resolutions": [{"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=759280e3b45c2a812d67133823836d196855fe44", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d86314d7da3dd7d1b6aef7aebe3c9273fb8ef9f", "width": 216, "height": 115}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad17d68c3cad267b304af6e73e4833f8f6ae8043", "width": 320, "height": 171}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6202570c045f26dc41c53737422a94e8cd834edf", "width": 640, "height": 342}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2449304362e90fdc702f3dea3ed7868b75165", "width": 960, "height": 514}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a584c706e637fbb76c4500d6244af4e3603ddb1", "width": 1080, "height": 578}], "variants": {}, "id": "pF39_BzUSKDtWbiEHONxLPdngWVyglfjSVUqa7wHRPU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17k4foj", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k4foj/serverless_is_not_all_you_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://risingwave.com/blog/serverless-is-not-all-you-need/", "subreddit_subscribers": 137063, "created_utc": 1698701870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I'm curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.\n\nFor some context, I'm looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.", "author_fullname": "t2_gn8s9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse / Lakehouse / Data Set Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jv50c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I&amp;#39;m curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.&lt;/p&gt;\n\n&lt;p&gt;For some context, I&amp;#39;m looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jv50c", "is_robot_indexable": true, "report_reasons": null, "author": "Thiseffingguy2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "subreddit_subscribers": 137063, "created_utc": 1698677418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).\n\nThe dataset is the \"final\" result of a research project and is contained in a SAS dataset:\n\n* The main data file \"data.sas7dat\" comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.\n* The formats file \"formats.sas7bcat\" contains value labels for categorical variables, such as 1 is \"male\" and 2 is \"female\".\n\nChallenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we've actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.\n\nThe main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).\n\nI have attempted the following solutions:\n\n1. Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.\n2. Melting the Dataset: I transformed the columns into rows to create a \"long\" format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.\n\nThe users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all \\~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.\n\nI am considering a star schema but am unsure if that is the most effective approach given the dataset's complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!", "author_fullname": "t2_s3w1zbjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring an extremely wide dataset in an RDBMS: Seeking advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtonq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).&lt;/p&gt;\n\n&lt;p&gt;The dataset is the &amp;quot;final&amp;quot; result of a research project and is contained in a SAS dataset:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The main data file &amp;quot;data.sas7dat&amp;quot; comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.&lt;/li&gt;\n&lt;li&gt;The formats file &amp;quot;formats.sas7bcat&amp;quot; contains value labels for categorical variables, such as 1 is &amp;quot;male&amp;quot; and 2 is &amp;quot;female&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Challenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we&amp;#39;ve actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.&lt;/p&gt;\n\n&lt;p&gt;The main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).&lt;/p&gt;\n\n&lt;p&gt;I have attempted the following solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.&lt;/li&gt;\n&lt;li&gt;Melting the Dataset: I transformed the columns into rows to create a &amp;quot;long&amp;quot; format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all ~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.&lt;/p&gt;\n\n&lt;p&gt;I am considering a star schema but am unsure if that is the most effective approach given the dataset&amp;#39;s complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jtonq", "is_robot_indexable": true, "report_reasons": null, "author": "BudgetAd1030", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "subreddit_subscribers": 137063, "created_utc": 1698673351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "13 Crucial Steps for End-to-End File Testing by iceDQ \ud83d\udcdd\ud83d\ude80", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nlboefgoocxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7ece2ed0cf3a498074596648a071778bf681c2"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ee8561b09888eb5bc2a6593f2ec4373b8f990c"}, {"y": 452, "x": 320, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5957f37ce22e5d778db788c12dc7a7f8cac848b9"}, {"y": 905, "x": 640, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d77c6b9c42c21a0a5e0b3b61e95c074e6e834b"}, {"y": 1357, "x": 960, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd3a1bc09bcf33a0a54c8774b4d9e9d4af04ef4c"}, {"y": 1527, "x": 1080, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c38ef27a54dc9a38c537ba668b899c2976a581ef"}], "s": {"y": 3508, "x": 2480, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390"}, "id": "nlboefgoocxb1"}}, "name": "t3_17juzns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bee6HOYrBoKyF9T9pqtVygDL1FKfzYwDr-vght6rGOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390\"&gt;https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17juzns", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "subreddit_subscribers": 137063, "created_utc": 1698677009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.\n\nWe have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.\n\nOur latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.\n\nSo a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster \"Integrations\" is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don't really know these platforms either. Perhaps we should simply be looking into these?\n\nAre we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?", "author_fullname": "t2_7no9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect&amp;Flask as a Hub and Spoke", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqhvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698671728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698662486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.&lt;/p&gt;\n\n&lt;p&gt;We have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.&lt;/p&gt;\n\n&lt;p&gt;Our latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.&lt;/p&gt;\n\n&lt;p&gt;So a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster &amp;quot;Integrations&amp;quot; is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don&amp;#39;t really know these platforms either. Perhaps we should simply be looking into these?&lt;/p&gt;\n\n&lt;p&gt;Are we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqhvd", "is_robot_indexable": true, "report_reasons": null, "author": "YourOldBuddy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "subreddit_subscribers": 137063, "created_utc": 1698662486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering if any data engineers here have worked for any casino companies or other gambling products. I suspect the data is interesting and I just got a job offer at one. Just curious to hear any feedback.", "author_fullname": "t2_11f1es", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gambling/casino company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k80nr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698711420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if any data engineers here have worked for any casino companies or other gambling products. I suspect the data is interesting and I just got a job offer at one. Just curious to hear any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17k80nr", "is_robot_indexable": true, "report_reasons": null, "author": "ianitic", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k80nr/gamblingcasino_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k80nr/gamblingcasino_company/", "subreddit_subscribers": 137063, "created_utc": 1698711420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on a [personal project](https://github.com/digitalghost-dev/premier-league) where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:\n\n1. Extract data with a Python script.\n2. Load a new row into a PostgreSQL database with zero transformations, just raw.\n3. Use Google Cloud's Datastream to stream the data to BigQuery.\n4. **dbt step** \\- I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.\n\nHere is a flowchart. I'm talking about **Data Pipeline 1**.\n\n[Diagram of my project's data pipeline architecture.](https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833)\n\n&amp;#x200B;\n\nIs this the proper use case and the idea behind dbt?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I using (and understanding) dbt correctly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ua5lnpuetdxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b83e07a70a8b3c2189410caff557ae4aa3799e1"}, {"y": 212, "x": 216, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b6e174c7da8eb4697581acb5c1c80a2ac010cc1"}, {"y": 314, "x": 320, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbbe14518af6f879f0b5253956f5c4fa8a33b3bb"}, {"y": 629, "x": 640, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51e88c787254f71fcd46c39ad9dab95ca43e290e"}, {"y": 943, "x": 960, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5da25a96ef19a0b7d508c5a34fcc014019671da9"}, {"y": 1061, "x": 1080, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3785607b6b617389cc5ca8507935e194c725034"}], "s": {"y": 1846, "x": 1878, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833"}, "id": "ua5lnpuetdxb1"}}, "name": "t3_17jzx1n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/U_Hm0yHJlubQWG9uED_0PP_WpXFeOORip1YigNcAVeM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698690066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;personal project&lt;/a&gt; where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Extract data with a Python script.&lt;/li&gt;\n&lt;li&gt;Load a new row into a PostgreSQL database with zero transformations, just raw.&lt;/li&gt;\n&lt;li&gt;Use Google Cloud&amp;#39;s Datastream to stream the data to BigQuery.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;dbt step&lt;/strong&gt; - I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Here is a flowchart. I&amp;#39;m talking about &lt;strong&gt;Data Pipeline 1&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f406427ef9cdf428514e9be35e56db2922cf833\"&gt;Diagram of my project&amp;#39;s data pipeline architecture.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this the proper use case and the idea behind dbt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?auto=webp&amp;s=e3224c394c241c64ad5e53c355431ac0f8ab8526", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2070931ceff1caacdb448277ad5d4f117bd987", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31fa2166140c915e95065317d8cfb91907b2a034", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13bfb84f156729c537e946581469eae84eeb2d20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=907ff5e42af2eb56467f097761c3511733513afc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fdbc94d7d480cf2be7812bcad28f4cb5c8518ac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=304945d38c5ad77b0f7f149beef56e3dd120bfa5", "width": 1080, "height": 540}], "variants": {}, "id": "KPMQgVo0A9r8AVnlQp0ErSoXWIJswqtQdpS0EBWkAlQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jzx1n", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "subreddit_subscribers": 137063, "created_utc": 1698690066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors, \n\nI am posting this after a failed interview process for Senior Analytics position.\n\nI would like to share with you my situation as I've been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.\n\n\nI worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)\n\nAll of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)\n\n\nAfter that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).\n\nI \"succeeded\" in some of the projects and \"failed\" in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)\n\n\nI don't know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!\n\n\nTL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-\n\n\nPD: wrote this from cellphone, sorry for the grammar and styling", "author_fullname": "t2_13if99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice / rant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jx3of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698682610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors, &lt;/p&gt;\n\n&lt;p&gt;I am posting this after a failed interview process for Senior Analytics position.&lt;/p&gt;\n\n&lt;p&gt;I would like to share with you my situation as I&amp;#39;ve been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.&lt;/p&gt;\n\n&lt;p&gt;I worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)&lt;/p&gt;\n\n&lt;p&gt;All of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)&lt;/p&gt;\n\n&lt;p&gt;After that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).&lt;/p&gt;\n\n&lt;p&gt;I &amp;quot;succeeded&amp;quot; in some of the projects and &amp;quot;failed&amp;quot; in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-&lt;/p&gt;\n\n&lt;p&gt;PD: wrote this from cellphone, sorry for the grammar and styling&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jx3of", "is_robot_indexable": true, "report_reasons": null, "author": "ratacarnic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jx3of/career_advice_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jx3of/career_advice_rant/", "subreddit_subscribers": 137063, "created_utc": 1698682610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).\n\nI've boiled the ocean and have come to the issue is how pyodbc is passing the password.\n\nThe password contains 2 escape characters, 'abc;abc\\[abc', I think the issue is the ; and the \\[. I've tried {} the password and user, I've tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can't get the pw changed sadly because bureaucracy.", "author_fullname": "t2_kg3va7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having issues connecting pyodbc to sql server because escape characters in pw.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jw6q4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698680167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve boiled the ocean and have come to the issue is how pyodbc is passing the password.&lt;/p&gt;\n\n&lt;p&gt;The password contains 2 escape characters, &amp;#39;abc;abc[abc&amp;#39;, I think the issue is the ; and the [. I&amp;#39;ve tried {} the password and user, I&amp;#39;ve tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can&amp;#39;t get the pw changed sadly because bureaucracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jw6q4", "is_robot_indexable": true, "report_reasons": null, "author": "IIndAmendmentJesus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "subreddit_subscribers": 137063, "created_utc": 1698680167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?", "author_fullname": "t2_52cbaf2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Privacy Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtq6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698677124.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jtq6w", "is_robot_indexable": true, "report_reasons": null, "author": "yoquierodata", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "subreddit_subscribers": 137063, "created_utc": 1698673478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAnyone has been involved near or far with the decision of wether or not to build a warehouse in a **nascent** startup that is **just launching its product** ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?", "author_fullname": "t2_8kenyeuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse in a small startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqoei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698663233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Anyone has been involved near or far with the decision of wether or not to build a warehouse in a &lt;strong&gt;nascent&lt;/strong&gt; startup that is &lt;strong&gt;just launching its product&lt;/strong&gt; ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqoei", "is_robot_indexable": true, "report_reasons": null, "author": "btenami", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "subreddit_subscribers": 137063, "created_utc": 1698663233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software engineer trying to learn more about ELT process, I have a personal project in which I am using S3 + Fivetran + Snowflake + DBT, but Fivetran doesn't have a free account, only a 15 days trial and due to life I'm never able to wrap up my project in 15 days. I already created 2 accounts, talked to Fivetran sales people, but the only thing they can do is giving me extra 15 days\ud83d\ude23. So I'm looking for a free alternative to Fivetran, pretty much to connect my S3 bucket to Snowflake. Any tips are appreciated!", "author_fullname": "t2_amvgqfyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran substitute", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kaq90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698720449.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698719352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software engineer trying to learn more about ELT process, I have a personal project in which I am using S3 + Fivetran + Snowflake + DBT, but Fivetran doesn&amp;#39;t have a free account, only a 15 days trial and due to life I&amp;#39;m never able to wrap up my project in 15 days. I already created 2 accounts, talked to Fivetran sales people, but the only thing they can do is giving me extra 15 days\ud83d\ude23. So I&amp;#39;m looking for a free alternative to Fivetran, pretty much to connect my S3 bucket to Snowflake. Any tips are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kaq90", "is_robot_indexable": true, "report_reasons": null, "author": "julinvictus", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kaq90/fivetran_substitute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kaq90/fivetran_substitute/", "subreddit_subscribers": 137063, "created_utc": 1698719352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've switched roles recently from a company where the DE org didn't have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege's, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more \"normal\" end of things, but just curious if others have similar processes.", "author_fullname": "t2_mut7fkytm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does the deploy process look like in your role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jynbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698686684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve switched roles recently from a company where the DE org didn&amp;#39;t have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege&amp;#39;s, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more &amp;quot;normal&amp;quot; end of things, but just curious if others have similar processes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jynbe", "is_robot_indexable": true, "report_reasons": null, "author": "MrCheezle47", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "subreddit_subscribers": 137063, "created_utc": 1698686684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. \n\nWould it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.", "author_fullname": "t2_fd5v0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Preparation with ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jxjbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698683782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. &lt;/p&gt;\n\n&lt;p&gt;Would it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jxjbv", "is_robot_indexable": true, "report_reasons": null, "author": "12Eerc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "subreddit_subscribers": 137063, "created_utc": 1698683782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone take a good online course/mooc for this cert? \n\nAny bit would help! Thanks in advance", "author_fullname": "t2_16k0ev", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake snowpro core cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvbd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone take a good online course/mooc for this cert? &lt;/p&gt;\n\n&lt;p&gt;Any bit would help! Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jvbd6", "is_robot_indexable": true, "report_reasons": null, "author": "jovalabs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "subreddit_subscribers": 137063, "created_utc": 1698677901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a data warehouse solution, is what I mean. Currently looking at user friendly and cost effective options. Ones I like so far are Astera Data Stack, Knime, and Yellowbrick has recently caught my attention. How does it compare to the more popular data warehouse solutions such as Amazon Redshift or Snowflake?", "author_fullname": "t2_5hpq24rk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone here have any experience with Yellowbrick?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kc01w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698723262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a data warehouse solution, is what I mean. Currently looking at user friendly and cost effective options. Ones I like so far are Astera Data Stack, Knime, and Yellowbrick has recently caught my attention. How does it compare to the more popular data warehouse solutions such as Amazon Redshift or Snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kc01w", "is_robot_indexable": true, "report_reasons": null, "author": "letsgopablo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kc01w/does_anyone_here_have_any_experience_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kc01w/does_anyone_here_have_any_experience_with/", "subreddit_subscribers": 137063, "created_utc": 1698723262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys, \n\nI have started my youtube channel to post data engineering related content. Can you guys take a look at the first 2 videos and provide any comments/suggestions.   \n\n\n[https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q](https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q)  \n\n\nAlso, please let me know what kind of content will be helpful , especially for beginners. Thanks!  \n", "author_fullname": "t2_8vjbfemu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Youtube Channel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kb5yh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698720662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys, &lt;/p&gt;\n\n&lt;p&gt;I have started my youtube channel to post data engineering related content. Can you guys take a look at the first 2 videos and provide any comments/suggestions.   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q\"&gt;https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Also, please let me know what kind of content will be helpful , especially for beginners. Thanks!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?auto=webp&amp;s=445df712bc796a463f39ee9e0d9726b82f7e3e40", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf5b1e39d2fe2a3219a2d12fcd254389284fbb26", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8bc9942f642b98fd1554c11cdf72213591d2010", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=071ef7de4178c6ff5f25aa4e0614455d2717e0f9", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c84f62b708161649bc4d2de1a30f015912538df2", "width": 640, "height": 640}], "variants": {}, "id": "genVsnqvFyvAuimvy58zzy4flXtyy4NChxCeDiWi-0o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kb5yh", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Guy81", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kb5yh/data_engineering_youtube_channel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kb5yh/data_engineering_youtube_channel/", "subreddit_subscribers": 137063, "created_utc": 1698720662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog - [https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables](https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables)  \nThis is one of my favorite features that we shipped recently. Building Change Data Capture (CDC) for Postgres Partitioned Tables involves handling various scenarios that need care and emphasis.\n\nWe at [PeerDB](https://peerdb.io) are building a specialized data-movement tool for Postgres. So supporting this feature was a given and a self expectation. This feature was one of the top asks from customers. We will keep adding more such native features as we evolve. \ud83d\udc18 \ud83d\ude0a\n\nAlso, don't forget to see the demo - it was a very candid one. \ud83d\ude0a I enjoyed covering different scenarios incl. adding new partitions, adding new columns, dropping partitions etc and showing how PeerDB handles replication for each of these cases.", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Change Data Capture for Postgres Partitioned Tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k6nqz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698707657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog - &lt;a href=\"https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables\"&gt;https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables&lt;/a&gt;&lt;br/&gt;\nThis is one of my favorite features that we shipped recently. Building Change Data Capture (CDC) for Postgres Partitioned Tables involves handling various scenarios that need care and emphasis.&lt;/p&gt;\n\n&lt;p&gt;We at &lt;a href=\"https://peerdb.io\"&gt;PeerDB&lt;/a&gt; are building a specialized data-movement tool for Postgres. So supporting this feature was a given and a self expectation. This feature was one of the top asks from customers. We will keep adding more such native features as we evolve. \ud83d\udc18 \ud83d\ude0a&lt;/p&gt;\n\n&lt;p&gt;Also, don&amp;#39;t forget to see the demo - it was a very candid one. \ud83d\ude0a I enjoyed covering different scenarios incl. adding new partitions, adding new columns, dropping partitions etc and showing how PeerDB handles replication for each of these cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?auto=webp&amp;s=f15558a233cb041e7cbb936e6d28481da5c669a9", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d42d389c2eaf50f0d59909e9e8c4d27f35090cae", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c12d95bfb8dc5752d80f2e1e8efa12c412fda35", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f8a35b92ef215a27916e04e37db5214df581a13", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8f17a26ccc3c95f27b9b04a213b6b847ac48a368", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d468c005cb1e5297dee15f1136930fb2a6bff9c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/2W-69-HMB1Kln5rBlUsBq7hRSavCiF-sO3S2GWUFvVs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a833b762cb64708495e11e40c4ef4747b2cde979", "width": 1080, "height": 720}], "variants": {}, "id": "cBb2hhJuA-FHaSqfR3Lbm-5Henzo5jLdmsOWfRINUFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17k6nqz", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k6nqz/realtime_change_data_capture_for_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k6nqz/realtime_change_data_capture_for_postgres/", "subreddit_subscribers": 137063, "created_utc": 1698707657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Now, you can streamline data effortlessly and quickly\u00a0regardless of your programming language by simply\u00a0using REST for producing and consuming messages, including straight from your front-end with memphis.dev.\n\nIf you are not familiar with us - Memphis.dev is a highly scalable event streaming and processing engine. Before Memphis came along, handling ingestion and processing of events on a large scale took months to adopt and was a capability reserved for the top 20% of mega-companies. Now, Memphis opens the door for the other 80% to unleash their event and data streaming superpowers quickly, easily, and without breaking the bank.", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We added REST support for our open-source streaming platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jubdy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698675114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now, you can streamline data effortlessly and quickly\u00a0regardless of your programming language by simply\u00a0using REST for producing and consuming messages, including straight from your front-end with memphis.dev.&lt;/p&gt;\n\n&lt;p&gt;If you are not familiar with us - Memphis.dev is a highly scalable event streaming and processing engine. Before Memphis came along, handling ingestion and processing of events on a large scale took months to adopt and was a capability reserved for the top 20% of mega-companies. Now, Memphis opens the door for the other 80% to unleash their event and data streaming superpowers quickly, easily, and without breaking the bank.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17jubdy", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jubdy/we_added_rest_support_for_our_opensource/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jubdy/we_added_rest_support_for_our_opensource/", "subreddit_subscribers": 137063, "created_utc": 1698675114.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}