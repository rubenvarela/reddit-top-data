{"kind": "Listing", "data": {"after": "t3_17kjl8u", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Pentagram Schema for Evaluating Haunted Data Warehouses**\n\nIn the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the \"Pentagram Schema.\" This schema is not just about data; it's about the eerie, the paranormal, and the mystical.\n\n**Pentagram Schema Overview:**\n\nAt the heart of the Pentagram Schema lies the **\"Haunted Data WareHouse Access\" fact table.** This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.\n\n**The Five Satanic Dimensions:**\n\n1. **Visitor (Dimension 1):** In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.\n2. **Query (Dimension 2):** Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they're regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.\n3. **Outcome (Dimension 3):** The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.\n4. **Tables Hit (Dimension 4):** Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.\n5. **Maintainer (Dimension 5):** In any haunted attraction, there's a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.\n\n&amp;#x200B;\n\n**Why the Pentagram Schema:**\n\nThese five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.\n\nHappy Haunting!\n\n  \n(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haunted data warehouse pentagram schema architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvmmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Pentagram Schema for Evaluating Haunted Data Warehouses&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the &amp;quot;Pentagram Schema.&amp;quot; This schema is not just about data; it&amp;#39;s about the eerie, the paranormal, and the mystical.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pentagram Schema Overview:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At the heart of the Pentagram Schema lies the &lt;strong&gt;&amp;quot;Haunted Data WareHouse Access&amp;quot; fact table.&lt;/strong&gt; This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Five Satanic Dimensions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Visitor (Dimension 1):&lt;/strong&gt; In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Query (Dimension 2):&lt;/strong&gt; Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they&amp;#39;re regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Outcome (Dimension 3):&lt;/strong&gt; The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tables Hit (Dimension 4):&lt;/strong&gt; Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintainer (Dimension 5):&lt;/strong&gt; In any haunted attraction, there&amp;#39;s a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why the Pentagram Schema:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.&lt;/p&gt;\n\n&lt;p&gt;Happy Haunting!&lt;/p&gt;\n\n&lt;p&gt;(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "17jvmmf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "subreddit_subscribers": 137121, "created_utc": 1698678690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some help to in understanding the dbricks cluster loading.\n\nSay I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. \n\nIf I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? \n\nIf I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?\n\nThanks for checking.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens when we submit a lot of spark jobs on a Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvo4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some help to in understanding the dbricks cluster loading.&lt;/p&gt;\n\n&lt;p&gt;Say I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. &lt;/p&gt;\n\n&lt;p&gt;If I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? &lt;/p&gt;\n\n&lt;p&gt;If I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?&lt;/p&gt;\n\n&lt;p&gt;Thanks for checking.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jvo4e", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "subreddit_subscribers": 137121, "created_utc": 1698678796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DEs, first of all not to hate on data engineering I\u2019ve been doing this for around 7 years and really like the space and what I do (maybe not so much my current company) but have been approached by a company with a hybrid data/backend engineering role.\n\nThe data engineer title varies a lot between companies but I work a lot with python, building custom tools for everything from custom extraction, schema management, auto DAG generation, Airflow plugins, DBT plugins, etc. \n\nI like what I do but the last year or so I\u2019ve been getting pretty burnt out on the day to day data eng tasks. I actually applied to a backend Scala position internally at my company but that didn\u2019t go through. I think a lot of my frustrations stems from the data org at my current company just being incredibly non-technical and really apathetic around engineering in general.\n\nThis new role is basically someone to own their slim/simple data pipelines and DBT project, but also work on their backend APIs mostly in Golang. Just the thought of working in and learning a new language (I\u2019ve used Go a bit but not extensively) is exciting to me. Plus it seems like Go is being used a lot more on the data processing side, so not a total career flip. \n\nAgain I really enjoy the data eng space but much more so from the eng perspective. Solving \u201cbusiness problems\u201d with data just doesn\u2019t excite me anymore, I don\u2019t wanna use DBT to make your data model I wanna BUIlD DBT, I don\u2019t wanna use airflow run your ETL I wanna BUILD airflow. \n\nAny advice or warnings from engineers who\u2019ve jumped from DE would be appreciated!", "author_fullname": "t2_109762", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to backend engineering from data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kcypl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698726646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DEs, first of all not to hate on data engineering I\u2019ve been doing this for around 7 years and really like the space and what I do (maybe not so much my current company) but have been approached by a company with a hybrid data/backend engineering role.&lt;/p&gt;\n\n&lt;p&gt;The data engineer title varies a lot between companies but I work a lot with python, building custom tools for everything from custom extraction, schema management, auto DAG generation, Airflow plugins, DBT plugins, etc. &lt;/p&gt;\n\n&lt;p&gt;I like what I do but the last year or so I\u2019ve been getting pretty burnt out on the day to day data eng tasks. I actually applied to a backend Scala position internally at my company but that didn\u2019t go through. I think a lot of my frustrations stems from the data org at my current company just being incredibly non-technical and really apathetic around engineering in general.&lt;/p&gt;\n\n&lt;p&gt;This new role is basically someone to own their slim/simple data pipelines and DBT project, but also work on their backend APIs mostly in Golang. Just the thought of working in and learning a new language (I\u2019ve used Go a bit but not extensively) is exciting to me. Plus it seems like Go is being used a lot more on the data processing side, so not a total career flip. &lt;/p&gt;\n\n&lt;p&gt;Again I really enjoy the data eng space but much more so from the eng perspective. Solving \u201cbusiness problems\u201d with data just doesn\u2019t excite me anymore, I don\u2019t wanna use DBT to make your data model I wanna BUIlD DBT, I don\u2019t wanna use airflow run your ETL I wanna BUILD airflow. &lt;/p&gt;\n\n&lt;p&gt;Any advice or warnings from engineers who\u2019ve jumped from DE would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17kcypl", "is_robot_indexable": true, "report_reasons": null, "author": "babyfacebrain666", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kcypl/switching_to_backend_engineering_from_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kcypl/switching_to_backend_engineering_from_data/", "subreddit_subscribers": 137121, "created_utc": 1698726646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI'm a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don't know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere's also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? \n\nHow realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?\n\nCan you recommend some resources or action plan?\nThanks", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to pick up java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k04oj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698690651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;m a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don&amp;#39;t know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere&amp;#39;s also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? &lt;/p&gt;\n\n&lt;p&gt;How realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?&lt;/p&gt;\n\n&lt;p&gt;Can you recommend some resources or action plan?\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17k04oj", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "subreddit_subscribers": 137121, "created_utc": 1698690651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys, \n\nI have started my youtube channel to post data engineering related content. Can you guys take a look at the first 2 videos and provide any comments/suggestions.   \n\n\n[https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q](https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q)  \n\n\nAlso, please let me know what kind of content will be helpful , especially for beginners. Thanks!  \n", "author_fullname": "t2_8vjbfemu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Youtube Channel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kb5yh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698720662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys, &lt;/p&gt;\n\n&lt;p&gt;I have started my youtube channel to post data engineering related content. Can you guys take a look at the first 2 videos and provide any comments/suggestions.   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q\"&gt;https://www.youtube.com/channel/UCwJXNjW9IXa65IVBZEw1G0Q&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Also, please let me know what kind of content will be helpful , especially for beginners. Thanks!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?auto=webp&amp;s=445df712bc796a463f39ee9e0d9726b82f7e3e40", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf5b1e39d2fe2a3219a2d12fcd254389284fbb26", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8bc9942f642b98fd1554c11cdf72213591d2010", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=071ef7de4178c6ff5f25aa4e0614455d2717e0f9", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/MsKu6sL1iTkkef9eWB-2vqxd4C_u3v19icWQus1WzX4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c84f62b708161649bc4d2de1a30f015912538df2", "width": 640, "height": 640}], "variants": {}, "id": "genVsnqvFyvAuimvy58zzy4flXtyy4NChxCeDiWi-0o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kb5yh", "is_robot_indexable": true, "report_reasons": null, "author": "Curious_Guy81", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kb5yh/data_engineering_youtube_channel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kb5yh/data_engineering_youtube_channel/", "subreddit_subscribers": 137121, "created_utc": 1698720662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "looking for ideas - anything to do with bad data/ data quality/ even regulations etc.", "author_fullname": "t2_j69fw1hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any funny halloween costume ideas related to data quality? \ud83c\udf83", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k3r1v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698700067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;looking for ideas - anything to do with bad data/ data quality/ even regulations etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17k3r1v", "is_robot_indexable": true, "report_reasons": null, "author": "rdtro", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k3r1v/any_funny_halloween_costume_ideas_related_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k3r1v/any_funny_halloween_costume_ideas_related_to_data/", "subreddit_subscribers": 137121, "created_utc": 1698700067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on a [personal project](https://github.com/digitalghost-dev/premier-league) where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:\n\n1. Extract data with a Python script.\n2. Load a new row into a PostgreSQL database with zero transformations, just raw.\n3. Use Google Cloud's Datastream to stream the data to BigQuery.\n4. **dbt step** \\- I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.\n\nHere is a flowchart. I'm talking about **Data Pipeline 1**.\n\n[Diagram of my project's data pipeline architecture.](https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833)\n\n&amp;#x200B;\n\nIs this the proper use case and the idea behind dbt?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I using (and understanding) dbt correctly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ua5lnpuetdxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b83e07a70a8b3c2189410caff557ae4aa3799e1"}, {"y": 212, "x": 216, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b6e174c7da8eb4697581acb5c1c80a2ac010cc1"}, {"y": 314, "x": 320, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbbe14518af6f879f0b5253956f5c4fa8a33b3bb"}, {"y": 629, "x": 640, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51e88c787254f71fcd46c39ad9dab95ca43e290e"}, {"y": 943, "x": 960, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5da25a96ef19a0b7d508c5a34fcc014019671da9"}, {"y": 1061, "x": 1080, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3785607b6b617389cc5ca8507935e194c725034"}], "s": {"y": 1846, "x": 1878, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833"}, "id": "ua5lnpuetdxb1"}}, "name": "t3_17jzx1n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/U_Hm0yHJlubQWG9uED_0PP_WpXFeOORip1YigNcAVeM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698690066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;personal project&lt;/a&gt; where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Extract data with a Python script.&lt;/li&gt;\n&lt;li&gt;Load a new row into a PostgreSQL database with zero transformations, just raw.&lt;/li&gt;\n&lt;li&gt;Use Google Cloud&amp;#39;s Datastream to stream the data to BigQuery.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;dbt step&lt;/strong&gt; - I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Here is a flowchart. I&amp;#39;m talking about &lt;strong&gt;Data Pipeline 1&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f406427ef9cdf428514e9be35e56db2922cf833\"&gt;Diagram of my project&amp;#39;s data pipeline architecture.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this the proper use case and the idea behind dbt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?auto=webp&amp;s=e3224c394c241c64ad5e53c355431ac0f8ab8526", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2070931ceff1caacdb448277ad5d4f117bd987", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31fa2166140c915e95065317d8cfb91907b2a034", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13bfb84f156729c537e946581469eae84eeb2d20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=907ff5e42af2eb56467f097761c3511733513afc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fdbc94d7d480cf2be7812bcad28f4cb5c8518ac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=304945d38c5ad77b0f7f149beef56e3dd120bfa5", "width": 1080, "height": 540}], "variants": {}, "id": "KPMQgVo0A9r8AVnlQp0ErSoXWIJswqtQdpS0EBWkAlQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jzx1n", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "subreddit_subscribers": 137121, "created_utc": 1698690066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working with streaming data pipelines for a while now. We use small checks to ensure dirty data is not being written to the serving layer. The way this is done is by filtering data and send it to a layer where all the dirty data resides. We don't use any special library for this. For example, if we are working with Azure Stream Analytics we use a query to do the filters.\n\nI haven't really seen quality checks being done in streaming pipelines by any special library or framework.\n\nSo I was curious... How do other data engineers in this sub handle DQ for streaming?\n\n&amp;#x200B;", "author_fullname": "t2_66knxh65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle data quality in streaming pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kkerp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698757012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working with streaming data pipelines for a while now. We use small checks to ensure dirty data is not being written to the serving layer. The way this is done is by filtering data and send it to a layer where all the dirty data resides. We don&amp;#39;t use any special library for this. For example, if we are working with Azure Stream Analytics we use a query to do the filters.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t really seen quality checks being done in streaming pipelines by any special library or framework.&lt;/p&gt;\n\n&lt;p&gt;So I was curious... How do other data engineers in this sub handle DQ for streaming?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17kkerp", "is_robot_indexable": true, "report_reasons": null, "author": "WarNeverChanges1997", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kkerp/how_do_you_handle_data_quality_in_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kkerp/how_do_you_handle_data_quality_in_streaming/", "subreddit_subscribers": 137121, "created_utc": 1698757012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serverless Is Not All You Need", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_17k4foj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gFqWPqOOtc9JA7odkNvCO5LKJYM73ibt4UJ-NR9S8LU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698701870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://risingwave.com/blog/serverless-is-not-all-you-need/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?auto=webp&amp;s=04f89ab2d2a41402623fa1bb9a18187db853767b", "width": 6346, "height": 3400}, "resolutions": [{"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=759280e3b45c2a812d67133823836d196855fe44", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d86314d7da3dd7d1b6aef7aebe3c9273fb8ef9f", "width": 216, "height": 115}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad17d68c3cad267b304af6e73e4833f8f6ae8043", "width": 320, "height": 171}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6202570c045f26dc41c53737422a94e8cd834edf", "width": 640, "height": 342}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79c2449304362e90fdc702f3dea3ed7868b75165", "width": 960, "height": 514}, {"url": "https://external-preview.redd.it/BjaIuQGBUZVgoauLDw-4RCrdQepMssfz6o3ZCU2buyw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a584c706e637fbb76c4500d6244af4e3603ddb1", "width": 1080, "height": 578}], "variants": {}, "id": "pF39_BzUSKDtWbiEHONxLPdngWVyglfjSVUqa7wHRPU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17k4foj", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k4foj/serverless_is_not_all_you_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://risingwave.com/blog/serverless-is-not-all-you-need/", "subreddit_subscribers": 137121, "created_utc": 1698701870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software engineer trying to learn more about ELT process, I have a personal project in which I am using S3 + Fivetran + Snowflake + DBT, but Fivetran doesn't have a free account, only a 15 days trial and due to life I'm never able to wrap up my project in 15 days. I already created 2 accounts, talked to Fivetran sales people, but the only thing they can do is giving me extra 15 days\ud83d\ude23. So I'm looking for a free alternative to Fivetran, pretty much to connect my S3 bucket to Snowflake. Any tips are appreciated!", "author_fullname": "t2_amvgqfyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran substitute", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kaq90", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698720449.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698719352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software engineer trying to learn more about ELT process, I have a personal project in which I am using S3 + Fivetran + Snowflake + DBT, but Fivetran doesn&amp;#39;t have a free account, only a 15 days trial and due to life I&amp;#39;m never able to wrap up my project in 15 days. I already created 2 accounts, talked to Fivetran sales people, but the only thing they can do is giving me extra 15 days\ud83d\ude23. So I&amp;#39;m looking for a free alternative to Fivetran, pretty much to connect my S3 bucket to Snowflake. Any tips are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kaq90", "is_robot_indexable": true, "report_reasons": null, "author": "julinvictus", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kaq90/fivetran_substitute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kaq90/fivetran_substitute/", "subreddit_subscribers": 137121, "created_utc": 1698719352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We\u2019ve made Data Quality an engineer\u2019s problem. It\u2019s actually a tooling issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_17klp2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3Db5jxn_0xAI4fT11AKSJ0Yo_tyzuFQwh7r0nYp6aUM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698760776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/how-to-improve-data-quality-with-better-data-quality-tools/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?auto=webp&amp;s=c4dc5911e7fb19c1fc99366aa56379a7bb167f41", "width": 1456, "height": 816}, "resolutions": [{"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=abcb39b0564a69a27429f79a8d6e36e918c5e20b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac69a8e8da768eef09b70442f5fbe09c72ed8995", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=739d5674eb2ba4e6621797c680b97a91708d9cda", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c485808d18707b10e548a055ca5538a3b2df40c", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=abd8a022e4d9441f854ece80fb5de794dae60590", "width": 960, "height": 538}, {"url": "https://external-preview.redd.it/hUevSUfoP0jELsOwSYnfPT2WJ5Tz4zjxc1h7ylbqan8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4588bdf1ad9579ba9a7d98c5f9d008974cd60924", "width": 1080, "height": 605}], "variants": {}, "id": "Hl5mXIl8CAHTddaBRAxQHXfbdCpP9xWSNd1SmBkjGoU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17klp2v", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17klp2v/weve_made_data_quality_an_engineers_problem_its/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/how-to-improve-data-quality-with-better-data-quality-tools/", "subreddit_subscribers": 137121, "created_utc": 1698760776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for an SME in the finance industry. We are generally a pretty data immature company, using Excel as a DB in so many places(!) Albeit the data we do have is generally clean (with some funky formatting) and good quality. I want to change this and bring about some systematic ELT so ad-hoc data requests don't take 2-3x longer than they should because data is so disparate. My intention is to use something like Fivetran to build the pipelines into a cloud-based DB. The obstacle I'm running into is my vendors/data generators are wanting to charge 30/40k up front and recurring to have an API set up for us - is this to be expected?\n\nIn Finance a lot of data is transferred by SFTP, which would be an option except my IT won't let us host an FTP server... Any help or insight would be appreciated!", "author_fullname": "t2_4r5q6r38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kfrye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698738747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for an SME in the finance industry. We are generally a pretty data immature company, using Excel as a DB in so many places(!) Albeit the data we do have is generally clean (with some funky formatting) and good quality. I want to change this and bring about some systematic ELT so ad-hoc data requests don&amp;#39;t take 2-3x longer than they should because data is so disparate. My intention is to use something like Fivetran to build the pipelines into a cloud-based DB. The obstacle I&amp;#39;m running into is my vendors/data generators are wanting to charge 30/40k up front and recurring to have an API set up for us - is this to be expected?&lt;/p&gt;\n\n&lt;p&gt;In Finance a lot of data is transferred by SFTP, which would be an option except my IT won&amp;#39;t let us host an FTP server... Any help or insight would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kfrye", "is_robot_indexable": true, "report_reasons": null, "author": "10formicidae", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kfrye/api_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kfrye/api_costs/", "subreddit_subscribers": 137121, "created_utc": 1698738747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was wondering if any data engineers here have worked for any casino companies or other gambling products. I suspect the data is interesting and I just got a job offer at one. Just curious to hear any feedback.", "author_fullname": "t2_11f1es", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gambling/casino company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17k80nr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698711420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if any data engineers here have worked for any casino companies or other gambling products. I suspect the data is interesting and I just got a job offer at one. Just curious to hear any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17k80nr", "is_robot_indexable": true, "report_reasons": null, "author": "ianitic", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k80nr/gamblingcasino_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k80nr/gamblingcasino_company/", "subreddit_subscribers": 137121, "created_utc": 1698711420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've switched roles recently from a company where the DE org didn't have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege's, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more \"normal\" end of things, but just curious if others have similar processes.", "author_fullname": "t2_mut7fkytm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does the deploy process look like in your role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jynbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698686684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve switched roles recently from a company where the DE org didn&amp;#39;t have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege&amp;#39;s, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more &amp;quot;normal&amp;quot; end of things, but just curious if others have similar processes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jynbe", "is_robot_indexable": true, "report_reasons": null, "author": "MrCheezle47", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "subreddit_subscribers": 137121, "created_utc": 1698686684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors, \n\nI am posting this after a failed interview process for Senior Analytics position.\n\nI would like to share with you my situation as I've been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.\n\n\nI worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)\n\nAll of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)\n\n\nAfter that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).\n\nI \"succeeded\" in some of the projects and \"failed\" in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)\n\n\nI don't know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!\n\n\nTL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-\n\n\nPD: wrote this from cellphone, sorry for the grammar and styling", "author_fullname": "t2_13if99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice / rant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jx3of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698682610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors, &lt;/p&gt;\n\n&lt;p&gt;I am posting this after a failed interview process for Senior Analytics position.&lt;/p&gt;\n\n&lt;p&gt;I would like to share with you my situation as I&amp;#39;ve been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.&lt;/p&gt;\n\n&lt;p&gt;I worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)&lt;/p&gt;\n\n&lt;p&gt;All of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)&lt;/p&gt;\n\n&lt;p&gt;After that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).&lt;/p&gt;\n\n&lt;p&gt;I &amp;quot;succeeded&amp;quot; in some of the projects and &amp;quot;failed&amp;quot; in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-&lt;/p&gt;\n\n&lt;p&gt;PD: wrote this from cellphone, sorry for the grammar and styling&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jx3of", "is_robot_indexable": true, "report_reasons": null, "author": "ratacarnic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jx3of/career_advice_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jx3of/career_advice_rant/", "subreddit_subscribers": 137121, "created_utc": 1698682610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).\n\nI've boiled the ocean and have come to the issue is how pyodbc is passing the password.\n\nThe password contains 2 escape characters, 'abc;abc\\[abc', I think the issue is the ; and the \\[. I've tried {} the password and user, I've tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can't get the pw changed sadly because bureaucracy.", "author_fullname": "t2_kg3va7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having issues connecting pyodbc to sql server because escape characters in pw.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jw6q4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698680167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve boiled the ocean and have come to the issue is how pyodbc is passing the password.&lt;/p&gt;\n\n&lt;p&gt;The password contains 2 escape characters, &amp;#39;abc;abc[abc&amp;#39;, I think the issue is the ; and the [. I&amp;#39;ve tried {} the password and user, I&amp;#39;ve tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can&amp;#39;t get the pw changed sadly because bureaucracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jw6q4", "is_robot_indexable": true, "report_reasons": null, "author": "IIndAmendmentJesus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "subreddit_subscribers": 137121, "created_utc": 1698680167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are things that we do that are very infrequent. For example, during our data ingest, sometimes we change the requirements for what exact data are ingested. This changes maybe once a year if that. We insert the requirements into a table and then our ingest service reads from that table.\n\nI've just been inserting the requirements into the table myself with an adhoc script that reads from a YAML config made by our Operations team describing the requirements. They send me the location of the YAML and I just run the script to get it into the table.\n\nBut then I get this wave of shame that this whole process should not be manual with me running the script like this. But the other part of me says this is like a once in a year thing, just run the script. Who cares.\n\nI'm not sure what my question is but do you guys have any thoughts on this? I'm still pretty new and on a small team but have a lot of freedom at my job, so I get this feeling every once in a while about when I should do something more formalized and when I shouldn't since its completely up to me. There are these moments where in a practical real life sense it works and gets the job done, but feels like I'm doing something bad. How much \"adhoc\" / one-off script type of stuff do you guys do?", "author_fullname": "t2_l2cnyx8o0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When should something be made into part of the process versus doing it manually?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17khow9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698747765.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698747382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are things that we do that are very infrequent. For example, during our data ingest, sometimes we change the requirements for what exact data are ingested. This changes maybe once a year if that. We insert the requirements into a table and then our ingest service reads from that table.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just been inserting the requirements into the table myself with an adhoc script that reads from a YAML config made by our Operations team describing the requirements. They send me the location of the YAML and I just run the script to get it into the table.&lt;/p&gt;\n\n&lt;p&gt;But then I get this wave of shame that this whole process should not be manual with me running the script like this. But the other part of me says this is like a once in a year thing, just run the script. Who cares.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what my question is but do you guys have any thoughts on this? I&amp;#39;m still pretty new and on a small team but have a lot of freedom at my job, so I get this feeling every once in a while about when I should do something more formalized and when I shouldn&amp;#39;t since its completely up to me. There are these moments where in a practical real life sense it works and gets the job done, but feels like I&amp;#39;m doing something bad. How much &amp;quot;adhoc&amp;quot; / one-off script type of stuff do you guys do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17khow9", "is_robot_indexable": true, "report_reasons": null, "author": "OctobersOwn2023", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17khow9/when_should_something_be_made_into_part_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17khow9/when_should_something_be_made_into_part_of_the/", "subreddit_subscribers": 137121, "created_utc": 1698747382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. \n\nWould it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.", "author_fullname": "t2_fd5v0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Preparation with ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jxjbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698683782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. &lt;/p&gt;\n\n&lt;p&gt;Would it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jxjbv", "is_robot_indexable": true, "report_reasons": null, "author": "12Eerc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "subreddit_subscribers": 137121, "created_utc": 1698683782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The great philosophical question. I know the \"right\" answer to this question is to give users exactly what they asked for and if they asked for a dumb product, then that's on them and they should have given better requirements... but in my experience, the business doesn't know what to ask for and if you do exactly what they ask, you end up re-inventing excel.\n\nOn the other hand, Henry Ford once said \"If I asked people what they wanted, they would have told me they wanted faster horses.\"  ", "author_fullname": "t2_lp0hc3mff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you give the business what they asked for, or what they want?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17km8af", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698762202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The great philosophical question. I know the &amp;quot;right&amp;quot; answer to this question is to give users exactly what they asked for and if they asked for a dumb product, then that&amp;#39;s on them and they should have given better requirements... but in my experience, the business doesn&amp;#39;t know what to ask for and if you do exactly what they ask, you end up re-inventing excel.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, Henry Ford once said &amp;quot;If I asked people what they wanted, they would have told me they wanted faster horses.&amp;quot;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17km8af", "is_robot_indexable": true, "report_reasons": null, "author": "LopsidedFondant3747", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17km8af/do_you_give_the_business_what_they_asked_for_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17km8af/do_you_give_the_business_what_they_asked_for_or/", "subreddit_subscribers": 137121, "created_utc": 1698762202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been contacted by a recruiter for a local position in a healthcare product company.\nI'm really interested in switching to a product company even through I would prefere abroad in Europe.\n\nHe stated in message that they were going for 70k-90k and then when I talked with him even if with 5 year experience he said that he was going to offer me 45k.\n\nI know, I know, so even through I was maybe interested, I told him that I wouldn't give a range and after the interviews the company would decide the right price to have my skills.\n\nI thought that I was ghosted and I contacted him with updates, he wrote two weeks ago that interviews were halted due to the company manager that had to fly home for family problems.\n\nWhat do I do ?\n\n I don't want to seem desperate but I'm starting to struggle in my role due to\n\n- manager doesn't trust my checks and when we check together, my checks are right\n- extra time for one week, I have severe back pain\n- Thinks data is normalized while is not\n- He checks every day for activities that require a huge technical time (We are loading data from anouther source and sometimes it takes 1 or 2 hours for a lot of months)\n\nManager thinks that asking every moment is gonna make us work faster but we can't haste the process since we're not in charge of that system.\n\nWhat can I write to this recruiter?\nIt would be a good company to change to since they're creating a new company in my country.", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to recontact recruiter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17klz64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698761531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been contacted by a recruiter for a local position in a healthcare product company.\nI&amp;#39;m really interested in switching to a product company even through I would prefere abroad in Europe.&lt;/p&gt;\n\n&lt;p&gt;He stated in message that they were going for 70k-90k and then when I talked with him even if with 5 year experience he said that he was going to offer me 45k.&lt;/p&gt;\n\n&lt;p&gt;I know, I know, so even through I was maybe interested, I told him that I wouldn&amp;#39;t give a range and after the interviews the company would decide the right price to have my skills.&lt;/p&gt;\n\n&lt;p&gt;I thought that I was ghosted and I contacted him with updates, he wrote two weeks ago that interviews were halted due to the company manager that had to fly home for family problems.&lt;/p&gt;\n\n&lt;p&gt;What do I do ?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to seem desperate but I&amp;#39;m starting to struggle in my role due to&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;manager doesn&amp;#39;t trust my checks and when we check together, my checks are right&lt;/li&gt;\n&lt;li&gt;extra time for one week, I have severe back pain&lt;/li&gt;\n&lt;li&gt;Thinks data is normalized while is not&lt;/li&gt;\n&lt;li&gt;He checks every day for activities that require a huge technical time (We are loading data from anouther source and sometimes it takes 1 or 2 hours for a lot of months)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Manager thinks that asking every moment is gonna make us work faster but we can&amp;#39;t haste the process since we&amp;#39;re not in charge of that system.&lt;/p&gt;\n\n&lt;p&gt;What can I write to this recruiter?\nIt would be a good company to change to since they&amp;#39;re creating a new company in my country.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17klz64", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17klz64/how_to_recontact_recruiter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17klz64/how_to_recontact_recruiter/", "subreddit_subscribers": 137121, "created_utc": 1698761531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI obtained various certifications in the last few years but always with vouchers that i got at events, workshop ,etc. now i am trying to get vuochers but i can't get them anywhere, is there still a chance to get them?\n\n&amp;#x200B;", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you get vouchers for certifications?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17klfyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698760045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I obtained various certifications in the last few years but always with vouchers that i got at events, workshop ,etc. now i am trying to get vuochers but i can&amp;#39;t get them anywhere, is there still a chance to get them?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17klfyq", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17klfyq/where_do_you_get_vouchers_for_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17klfyq/where_do_you_get_vouchers_for_certifications/", "subreddit_subscribers": 137121, "created_utc": 1698760045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nI am working on a project for a retail company. The company has different data sources regarding transaction data, which unfortunately also have different granularity. For example, for site A we get all the details regarding the transactions made (sold items), for site B we only get the daily sales. We wanted to build the facts table according to Kimball's Retail schema, but are now wondering how best to solve this due to the different data depth.\n\nDo you have any tips on how best to approach this?", "author_fullname": "t2_3d077o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to harmonize missing data granularity in fact dimension?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kkj2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698757344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I am working on a project for a retail company. The company has different data sources regarding transaction data, which unfortunately also have different granularity. For example, for site A we get all the details regarding the transactions made (sold items), for site B we only get the daily sales. We wanted to build the facts table according to Kimball&amp;#39;s Retail schema, but are now wondering how best to solve this due to the different data depth.&lt;/p&gt;\n\n&lt;p&gt;Do you have any tips on how best to approach this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kkj2s", "is_robot_indexable": true, "report_reasons": null, "author": "axeqizec", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kkj2s/best_way_to_harmonize_missing_data_granularity_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kkj2s/best_way_to_harmonize_missing_data_granularity_in/", "subreddit_subscribers": 137121, "created_utc": 1698757344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To preface, I am not a DE but a DA dabbling in creating data pipelines (mostly within BQ) t o support the data needs of our dept., so that we can have amalgamated figures, ease of building new reports etc.\n\nThe BQ datamart is (in my humble opinion) a big mess: there are thousands of (very wide) views and relevant information is usually scattered across many different views. As the data we have access to are views and not partitioned table, every time we query the data at the source we are querying the whole data, thus increasing costs and processing times.\n\nAs an example, just to get to the amalgamated Sales data for out dept, I need to create a SQL made up of 20 different joins, joining either transactional, dimensional or mapping (thus very few rows) views. This lengthy SQL will easily churn through 200GB worth of data.\n\nI therefore have been busy creating a staging layer, whereby I join a few related views and output the result into a partitioned table contained in our project. Each of these tables is vastly smaller than the source of data as it is made up of far fewer fields, and additionally they are partitioned.\n\nHowever, if for example I combine the 3 tables that  I created (e.g. Sales transaction, customer master, product master) in a join to get the same output of the original SQL code that queries the 20 different views, a higher number of GB will be processed, e.g. 250GB.\n\nI tried experimenting with partitioning on different fields and clustering, but it didn't affect the 250GB figure.\n\nI can imagine it perhaps can be difficult to give some specific advice not having an insight into how the views and the partitioned tables are built.However, generally speaking does anyone have any broad advice on what should I look into to make sure that the number of GB processed will be smaller than the original 200GB, as that was my expectation by using (what I believe are) optimised partitioned/clustered tables?", "author_fullname": "t2_4v8mesko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[BQ] Created a few partitioned staging tables to optimise the raw data, which is all exposed as wide views. The original SQL querying the wide views process fewer GBs than the SQL querying the optimised partitioned tables, which is contrary to expectations. Any pointers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kk214", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698756238.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698755913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To preface, I am not a DE but a DA dabbling in creating data pipelines (mostly within BQ) t o support the data needs of our dept., so that we can have amalgamated figures, ease of building new reports etc.&lt;/p&gt;\n\n&lt;p&gt;The BQ datamart is (in my humble opinion) a big mess: there are thousands of (very wide) views and relevant information is usually scattered across many different views. As the data we have access to are views and not partitioned table, every time we query the data at the source we are querying the whole data, thus increasing costs and processing times.&lt;/p&gt;\n\n&lt;p&gt;As an example, just to get to the amalgamated Sales data for out dept, I need to create a SQL made up of 20 different joins, joining either transactional, dimensional or mapping (thus very few rows) views. This lengthy SQL will easily churn through 200GB worth of data.&lt;/p&gt;\n\n&lt;p&gt;I therefore have been busy creating a staging layer, whereby I join a few related views and output the result into a partitioned table contained in our project. Each of these tables is vastly smaller than the source of data as it is made up of far fewer fields, and additionally they are partitioned.&lt;/p&gt;\n\n&lt;p&gt;However, if for example I combine the 3 tables that  I created (e.g. Sales transaction, customer master, product master) in a join to get the same output of the original SQL code that queries the 20 different views, a higher number of GB will be processed, e.g. 250GB.&lt;/p&gt;\n\n&lt;p&gt;I tried experimenting with partitioning on different fields and clustering, but it didn&amp;#39;t affect the 250GB figure.&lt;/p&gt;\n\n&lt;p&gt;I can imagine it perhaps can be difficult to give some specific advice not having an insight into how the views and the partitioned tables are built.However, generally speaking does anyone have any broad advice on what should I look into to make sure that the number of GB processed will be smaller than the original 200GB, as that was my expectation by using (what I believe are) optimised partitioned/clustered tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17kk214", "is_robot_indexable": true, "report_reasons": null, "author": "_thetrue_SpaceTofu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kk214/bq_created_a_few_partitioned_staging_tables_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kk214/bq_created_a_few_partitioned_staging_tables_to/", "subreddit_subscribers": 137121, "created_utc": 1698755913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone been successful using the meltano target-oracle with an oracle wallet as the login credential? If so, how did you set up your config?  I know it can be done with cx_oracle, but I'm not sure how to set up the tns params in the meltano config.", "author_fullname": "t2_1hb8tav0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano target-oracle with wallet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kjy7e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698755559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been successful using the meltano target-oracle with an oracle wallet as the login credential? If so, how did you set up your config?  I know it can be done with cx_oracle, but I&amp;#39;m not sure how to set up the tns params in the meltano config.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17kjy7e", "is_robot_indexable": true, "report_reasons": null, "author": "FrebTheRat", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kjy7e/meltano_targetoracle_with_wallet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kjy7e/meltano_targetoracle_with_wallet/", "subreddit_subscribers": 137121, "created_utc": 1698755559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey community,\n\nCurrently I\u2019m in the process of building a new data infra at my company, and I always opted using custom DAGs for extracting data from source system, and orchestrating it with Airflow.\n\nCan you please give me some pros and cons of using a tool for this? My line of thinking currently is that a tool would decrease the initial setup time needed, and it is easier to onboard probably, however a custom DAG is not a blackbox, it\u2019s highly customizable for our needs, and allows us to have full ownership.", "author_fullname": "t2_16ts9m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advantage kf using Airbyte/Meltano over custom DAG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kjl8u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698754424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey community,&lt;/p&gt;\n\n&lt;p&gt;Currently I\u2019m in the process of building a new data infra at my company, and I always opted using custom DAGs for extracting data from source system, and orchestrating it with Airflow.&lt;/p&gt;\n\n&lt;p&gt;Can you please give me some pros and cons of using a tool for this? My line of thinking currently is that a tool would decrease the initial setup time needed, and it is easier to onboard probably, however a custom DAG is not a blackbox, it\u2019s highly customizable for our needs, and allows us to have full ownership.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17kjl8u", "is_robot_indexable": true, "report_reasons": null, "author": "pimadd_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17kjl8u/advantage_kf_using_airbytemeltano_over_custom_dag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17kjl8u/advantage_kf_using_airbytemeltano_over_custom_dag/", "subreddit_subscribers": 137121, "created_utc": 1698754424.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}