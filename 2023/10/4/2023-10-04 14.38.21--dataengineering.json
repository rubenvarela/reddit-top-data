{"kind": "Listing", "data": {"after": "t3_16yvknm", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been reading a lot about polars but one thing about the industry I\u2019ve seen is that new technologies come and have a a lot of hype but do not cause any real change to the current accepted stack. \n\nI\u2019m wondering if this subreddit feels the same way about it as me. Obviously it\u2019s good to keep learning but between getting better at pyspark vs Polars I\u2019m wondering which should be the focus", "author_fullname": "t2_xzn0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Polaris worth learning over pyspark? Or is it just hype while the industry has no intention of moving away from a Apache/Cloud framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16yx6f1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696388127.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696352149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been reading a lot about polars but one thing about the industry I\u2019ve seen is that new technologies come and have a a lot of hype but do not cause any real change to the current accepted stack. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering if this subreddit feels the same way about it as me. Obviously it\u2019s good to keep learning but between getting better at pyspark vs Polars I\u2019m wondering which should be the focus&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16yx6f1", "is_robot_indexable": true, "report_reasons": null, "author": "richhoods", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yx6f1/is_polaris_worth_learning_over_pyspark_or_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yx6f1/is_polaris_worth_learning_over_pyspark_or_is_it/", "subreddit_subscribers": 132007, "created_utc": 1696352149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI should preface this by saying I'm a complete noob but I'd like to learn about relational databases and SQL.\n\nI'm playing around with \"historical weather data\" produced by ERA5 which provides e.g. hourly temperatures globally at 0.25 degree resolution. The problem is that the data stretches back to 1940 so that's roughly (83 years) * (24*365 hours per year) * (360/0.25 * 180/0.25 grid points) = 754 billions rows per variable.\n\nI'm finding it very slow to copy the data into Postgres even using: https://www.psycopg.org/psycopg3/docs/basic/copy.html#writing-data-row-by-row\n\nI thought PostgresSQL might be a good option, possibly with PostGIS and/or TimescaleDB, but thought I'd start with just Postgres.\n\nAm I taking a bad approach here? Should I consider another kind of database? Or am I just not loading my data in properly?\n\nI'm also worried Postgres won't compress this data well, but haven't played around with this yet (might be where TimescaleDB helps?).\n\nThank you so much for any advice you guys might have!", "author_fullname": "t2_12i7gz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to efficiently load ~20 TiB of weather data into a new PostgresSQL database? Is PostgresSQL even a good option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z8h6l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696379095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I should preface this by saying I&amp;#39;m a complete noob but I&amp;#39;d like to learn about relational databases and SQL.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m playing around with &amp;quot;historical weather data&amp;quot; produced by ERA5 which provides e.g. hourly temperatures globally at 0.25 degree resolution. The problem is that the data stretches back to 1940 so that&amp;#39;s roughly (83 years) * (24*365 hours per year) * (360/0.25 * 180/0.25 grid points) = 754 billions rows per variable.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m finding it very slow to copy the data into Postgres even using: &lt;a href=\"https://www.psycopg.org/psycopg3/docs/basic/copy.html#writing-data-row-by-row\"&gt;https://www.psycopg.org/psycopg3/docs/basic/copy.html#writing-data-row-by-row&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I thought PostgresSQL might be a good option, possibly with PostGIS and/or TimescaleDB, but thought I&amp;#39;d start with just Postgres.&lt;/p&gt;\n\n&lt;p&gt;Am I taking a bad approach here? Should I consider another kind of database? Or am I just not loading my data in properly?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also worried Postgres won&amp;#39;t compress this data well, but haven&amp;#39;t played around with this yet (might be where TimescaleDB helps?).&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for any advice you guys might have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16z8h6l", "is_robot_indexable": true, "report_reasons": null, "author": "DeadDolphinResearch", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z8h6l/how_to_efficiently_load_20_tib_of_weather_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z8h6l/how_to_efficiently_load_20_tib_of_weather_data/", "subreddit_subscribers": 132007, "created_utc": 1696379095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working as a big data engineer for 5 years, and every day I realize that the topics are too broad and complex. I often get lost and don't know what to study in depth.\n\nDo you have any advice on how to become a good data engineer?\nHow many books do you read in a year?\nHow many hours a day do you study? (Outside of working hours).\n\nAny advice is welcome, thanks in advance", "author_fullname": "t2_6n4s0awu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much time do you study each day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z4nwt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696369828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working as a big data engineer for 5 years, and every day I realize that the topics are too broad and complex. I often get lost and don&amp;#39;t know what to study in depth.&lt;/p&gt;\n\n&lt;p&gt;Do you have any advice on how to become a good data engineer?\nHow many books do you read in a year?\nHow many hours a day do you study? (Outside of working hours).&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome, thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16z4nwt", "is_robot_indexable": true, "report_reasons": null, "author": "el_cortezzz", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z4nwt/how_much_time_do_you_study_each_day/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z4nwt/how_much_time_do_you_study_each_day/", "subreddit_subscribers": 132007, "created_utc": 1696369828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, so ive been on the internship hunt for quite a long time now and 2 of the 3 interviews I've managed to get thus far I've wound up getting surprised with a surprising amount of DS questions (stats, ML modeling experience, prior work with DS/ML tools) along with typical SQL + Data Modeling, Python, Data Integration/Orchestration tooling questions.\n\nThe thing is this hasn't always been very explicit in the job posting and I've only really taken intro-level coursework in stats and ML (almost entirely intro-level supervised learning concepts and models, lightly touched unsupervised learning at the end) so I'm very weak footed when answering DS questions that go much further beyond basic regression in complexity. Im now wondering if I need to sit down and spend time improving that side of my skillset?\n\nIts also extra stressful because doing an internship is a mandatory part of my undergrad program to graduate and with how brutally slim job postings are on average I cant afford to keep slipping on these interviews", "author_fullname": "t2_pfwkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it typical to see Data Engineer and Data Science duties intertwined on Internship roles with smaller companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16yx85g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696352967.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696352268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, so ive been on the internship hunt for quite a long time now and 2 of the 3 interviews I&amp;#39;ve managed to get thus far I&amp;#39;ve wound up getting surprised with a surprising amount of DS questions (stats, ML modeling experience, prior work with DS/ML tools) along with typical SQL + Data Modeling, Python, Data Integration/Orchestration tooling questions.&lt;/p&gt;\n\n&lt;p&gt;The thing is this hasn&amp;#39;t always been very explicit in the job posting and I&amp;#39;ve only really taken intro-level coursework in stats and ML (almost entirely intro-level supervised learning concepts and models, lightly touched unsupervised learning at the end) so I&amp;#39;m very weak footed when answering DS questions that go much further beyond basic regression in complexity. Im now wondering if I need to sit down and spend time improving that side of my skillset?&lt;/p&gt;\n\n&lt;p&gt;Its also extra stressful because doing an internship is a mandatory part of my undergrad program to graduate and with how brutally slim job postings are on average I cant afford to keep slipping on these interviews&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16yx85g", "is_robot_indexable": true, "report_reasons": null, "author": "Anic135", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yx85g/is_it_typical_to_see_data_engineer_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yx85g/is_it_typical_to_see_data_engineer_and_data/", "subreddit_subscribers": 132007, "created_utc": 1696352268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Background:**   \nI am an analyst in an oil &amp; gas company who is interested in data engineering. Been working for about 10 years, most of it not related my current role as an analyst but specific to my industry. My background is not computer science, but chemical engineering. I've been doing a lot more python &amp; SQL. I'm getting much more used to the AWS tech stack (Glue, s3, redshift). \n\nI was in a meeting with him and reached out after the meeting to tell him I am interested in data engineering. I asked if he'd be willing to talk to me. He nicely agreed.\n\n**Why I'm nervous:**\n\nI don't want to waste this opportunity. I am used to the corporate networking, but I'm trying to sell myself and pivot into something quite frankly I don't have a lot of expertise in. One of my concerns is that to get a role I've heard they require testing and a technical interview (coding interview) even for internal. So I'm mostly scared that I won't be able to handle the technical parts!\n\n**Here are some questions I was thinking about**\n\n* Tell me about yourself (always gotta start with the classics)\n* Are there any opportunities to practice or learn more of the DE side?\n* Is there any advice you'd give to someone without a CS background to demonstrate their capability?\n\nAny other advice, questions, or a reminder of \"hey you're overthinking this\"", "author_fullname": "t2_7upjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Speaking with head of data engineering for career pivot, what should I ask?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zafs9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696384323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;br/&gt;\nI am an analyst in an oil &amp;amp; gas company who is interested in data engineering. Been working for about 10 years, most of it not related my current role as an analyst but specific to my industry. My background is not computer science, but chemical engineering. I&amp;#39;ve been doing a lot more python &amp;amp; SQL. I&amp;#39;m getting much more used to the AWS tech stack (Glue, s3, redshift). &lt;/p&gt;\n\n&lt;p&gt;I was in a meeting with him and reached out after the meeting to tell him I am interested in data engineering. I asked if he&amp;#39;d be willing to talk to me. He nicely agreed.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I&amp;#39;m nervous:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to waste this opportunity. I am used to the corporate networking, but I&amp;#39;m trying to sell myself and pivot into something quite frankly I don&amp;#39;t have a lot of expertise in. One of my concerns is that to get a role I&amp;#39;ve heard they require testing and a technical interview (coding interview) even for internal. So I&amp;#39;m mostly scared that I won&amp;#39;t be able to handle the technical parts!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here are some questions I was thinking about&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tell me about yourself (always gotta start with the classics)&lt;/li&gt;\n&lt;li&gt;Are there any opportunities to practice or learn more of the DE side?&lt;/li&gt;\n&lt;li&gt;Is there any advice you&amp;#39;d give to someone without a CS background to demonstrate their capability?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any other advice, questions, or a reminder of &amp;quot;hey you&amp;#39;re overthinking this&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16zafs9", "is_robot_indexable": true, "report_reasons": null, "author": "chlor8", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zafs9/speaking_with_head_of_data_engineering_for_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zafs9/speaking_with_head_of_data_engineering_for_career/", "subreddit_subscribers": 132007, "created_utc": 1696384323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been tasked with centrally documenting a BI product and I am wondering how professionals approach this problem as I am relatively new to this field of work.\n\nThis product in short boils down to: get data from source system --&gt; select and transform data in SQL --&gt; store created tables in data warehouse --&gt; load in Power BI.\n\nOf course there is a number of standard approaches in place such as commenting the SQL queries etc. but I am trying to find the best way to centrally store all underlying relationships in this data product. Such that for every Power BI measure, I store the used data warehouse column(s) and the source column(s) used to generate said data warehouse column(s).\n\nI attached a visual example of the relationships I am trying to centrally document, quickly written up in MS Excel style. In the example *my\\_powerbi\\_measure\\_1* is created using *my\\_datawarehouse\\_column1*/*2*/*3* and *my\\_datawarehouse\\_column1* is created using *my\\_sourcedata\\_column1*.\n\n&amp;#x200B;\n\nWhat are tools or documentation approaches you guys use, or would use, to get to this central documentation of data flows? All ideas and tips are appreciated!\n\n&amp;#x200B;\n\n[Visual example](https://preview.redd.it/gkr7g6cs80sb1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=2d61ee6c27f787bd23ec9074fdff6a4a1488fdc3)", "author_fullname": "t2_3tjzfxzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Centrally documenting data flows of a BI product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 37, "top_awarded_type": null, "hide_score": false, "media_metadata": {"gkr7g6cs80sb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 28, "x": 108, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddacf8e0df5a525fd12de3e7faff4cee91e28338"}, {"y": 57, "x": 216, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e90d278173bca8eef273915d4ec22c16f55f7e15"}, {"y": 85, "x": 320, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b6d7fac4d1bdb877c7b34afab7627f52e9a6a4d"}, {"y": 171, "x": 640, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=587548bb83d6e2ab9a4ff7031ad7b27e5c7720a2"}, {"y": 257, "x": 960, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7857a6bc212c9ce6086927a4113ecaac6ade0622"}], "s": {"y": 274, "x": 1023, "u": "https://preview.redd.it/gkr7g6cs80sb1.png?width=1023&amp;format=png&amp;auto=webp&amp;s=2d61ee6c27f787bd23ec9074fdff6a4a1488fdc3"}, "id": "gkr7g6cs80sb1"}}, "name": "t3_16yuq2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nAeYY4IgGMBgUxkb7oPsBq8oEFo3Uy6L8SJPVkWvKic.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696346440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been tasked with centrally documenting a BI product and I am wondering how professionals approach this problem as I am relatively new to this field of work.&lt;/p&gt;\n\n&lt;p&gt;This product in short boils down to: get data from source system --&amp;gt; select and transform data in SQL --&amp;gt; store created tables in data warehouse --&amp;gt; load in Power BI.&lt;/p&gt;\n\n&lt;p&gt;Of course there is a number of standard approaches in place such as commenting the SQL queries etc. but I am trying to find the best way to centrally store all underlying relationships in this data product. Such that for every Power BI measure, I store the used data warehouse column(s) and the source column(s) used to generate said data warehouse column(s).&lt;/p&gt;\n\n&lt;p&gt;I attached a visual example of the relationships I am trying to centrally document, quickly written up in MS Excel style. In the example &lt;em&gt;my_powerbi_measure_1&lt;/em&gt; is created using &lt;em&gt;my_datawarehouse_column1&lt;/em&gt;/&lt;em&gt;2&lt;/em&gt;/&lt;em&gt;3&lt;/em&gt; and &lt;em&gt;my_datawarehouse_column1&lt;/em&gt; is created using &lt;em&gt;my_sourcedata_column1&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What are tools or documentation approaches you guys use, or would use, to get to this central documentation of data flows? All ideas and tips are appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gkr7g6cs80sb1.png?width=1023&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d61ee6c27f787bd23ec9074fdff6a4a1488fdc3\"&gt;Visual example&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16yuq2k", "is_robot_indexable": true, "report_reasons": null, "author": "basr98", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yuq2k/centrally_documenting_data_flows_of_a_bi_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yuq2k/centrally_documenting_data_flows_of_a_bi_product/", "subreddit_subscribers": 132007, "created_utc": 1696346440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there,\n\nI'm a nurse, so I have a batchelors albeit incredibly unrelated. I'm approaching 30 and I'm so very burnt out.\n\nI want to get into data engineering, it's always interested me and it has a much broader field than pidgeon-hole nursing. (Actually I'm quite downplaying my love of data, design and solutions...), however, I need to find a qualification that fits around my current job.\n\nWhat would you suggest?\n\nI've been looking at online academies and Microsoft certifications etc. Doing a masters isn't financially viable right now. But there's so many pros and cons and all the pathways are a little overwhelming for someone new to the industry. I've been dwelling on this for over a year and I just need to take the plunge.\n\nAlso, my current role is quite well paid so ideally I'd be looking for my first data job to be \u00a340K+ (about $50K+), to cover for mortgage payments etc. Is there a qualification I can do that would help me get there?\n\nPlease help \ud83d\ude05", "author_fullname": "t2_740fnu1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From nurse to data engineer ...send help.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zjdbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696414631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a nurse, so I have a batchelors albeit incredibly unrelated. I&amp;#39;m approaching 30 and I&amp;#39;m so very burnt out.&lt;/p&gt;\n\n&lt;p&gt;I want to get into data engineering, it&amp;#39;s always interested me and it has a much broader field than pidgeon-hole nursing. (Actually I&amp;#39;m quite downplaying my love of data, design and solutions...), however, I need to find a qualification that fits around my current job.&lt;/p&gt;\n\n&lt;p&gt;What would you suggest?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at online academies and Microsoft certifications etc. Doing a masters isn&amp;#39;t financially viable right now. But there&amp;#39;s so many pros and cons and all the pathways are a little overwhelming for someone new to the industry. I&amp;#39;ve been dwelling on this for over a year and I just need to take the plunge.&lt;/p&gt;\n\n&lt;p&gt;Also, my current role is quite well paid so ideally I&amp;#39;d be looking for my first data job to be \u00a340K+ (about $50K+), to cover for mortgage payments etc. Is there a qualification I can do that would help me get there?&lt;/p&gt;\n\n&lt;p&gt;Please help \ud83d\ude05&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16zjdbu", "is_robot_indexable": true, "report_reasons": null, "author": "Lil_Cherry_Beary", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zjdbu/from_nurse_to_data_engineer_send_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zjdbu/from_nurse_to_data_engineer_send_help/", "subreddit_subscribers": 132007, "created_utc": 1696414631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pulling data from a realtime feed at the moment, about 200k records and then pushing them to a Kinesis Data Firehose.  I've experimented with batch sizes and the like, but at the moment I'm seeing that the 200k records when serialised is about 20MB, and the batch and write to disk as parquet via KDF is taking a couple of minutes.  \n\n\nThe realtime feed is updated every 15 seconds, but I'm so far unable to ingest at that rate, so I've dialled it back to every 2 minutes for the time being.\n\nCurrently the task is billed for Lambda, KDF and S3.\n\nI've been experimenting and using Polars to make the 200k records into a dataframe and then writing it directly to S3. It takes about 15 seconds and the parquet file is about 700kb. It's light years faster and cheaper than KDF and I can run it at a faster rate than the KDF.\n\nThe overhead is Lambda and S3.  \n\n\nNow I understand that if you can't afford to drop/loose any of the data, then you need to have a data stream manager of some flavour, but why is it so much slower and so much more expensive?  \n\n\nI'm missing something in regards to the sweet-spot for using the product.", "author_fullname": "t2_ahkdvv1hd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lambda to Firehose vs directly to S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z5j8q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696371824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pulling data from a realtime feed at the moment, about 200k records and then pushing them to a Kinesis Data Firehose.  I&amp;#39;ve experimented with batch sizes and the like, but at the moment I&amp;#39;m seeing that the 200k records when serialised is about 20MB, and the batch and write to disk as parquet via KDF is taking a couple of minutes.  &lt;/p&gt;\n\n&lt;p&gt;The realtime feed is updated every 15 seconds, but I&amp;#39;m so far unable to ingest at that rate, so I&amp;#39;ve dialled it back to every 2 minutes for the time being.&lt;/p&gt;\n\n&lt;p&gt;Currently the task is billed for Lambda, KDF and S3.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been experimenting and using Polars to make the 200k records into a dataframe and then writing it directly to S3. It takes about 15 seconds and the parquet file is about 700kb. It&amp;#39;s light years faster and cheaper than KDF and I can run it at a faster rate than the KDF.&lt;/p&gt;\n\n&lt;p&gt;The overhead is Lambda and S3.  &lt;/p&gt;\n\n&lt;p&gt;Now I understand that if you can&amp;#39;t afford to drop/loose any of the data, then you need to have a data stream manager of some flavour, but why is it so much slower and so much more expensive?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m missing something in regards to the sweet-spot for using the product.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16z5j8q", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Fee5361", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z5j8q/lambda_to_firehose_vs_directly_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z5j8q/lambda_to_firehose_vs_directly_to_s3/", "subreddit_subscribers": 132007, "created_utc": 1696371824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have published a medium article about a dynamic Database CDC Data Processing pipeline in AWS which we designed in one of my earlier projects. Thought it would be useful for someone who faced similar challenges.\n\nPlease take a look and let me know if the logic &amp; wording are clear, as the code is out of scope\n\n[https://medium.com/@sarath.mec/aws-dynamic-cdc-migration-using-dms-kinesis-and-lambda-d0ae8abff76f](https://medium.com/@sarath.mec/aws-dynamic-cdc-migration-using-dms-kinesis-and-lambda-d0ae8abff76f)[https://medium.com/@sarath.mec/kinesis-lambda-trigger-dlq-reprocessing-using-aws-glue-cb0a62710a84](https://medium.com/@sarath.mec/kinesis-lambda-trigger-dlq-reprocessing-using-aws-glue-cb0a62710a84)", "author_fullname": "t2_5rjlld07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS: CDC using DMS, Kinesis &amp; Lambda with Dynamic SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z0rp5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696360702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have published a medium article about a dynamic Database CDC Data Processing pipeline in AWS which we designed in one of my earlier projects. Thought it would be useful for someone who faced similar challenges.&lt;/p&gt;\n\n&lt;p&gt;Please take a look and let me know if the logic &amp;amp; wording are clear, as the code is out of scope&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@sarath.mec/aws-dynamic-cdc-migration-using-dms-kinesis-and-lambda-d0ae8abff76f\"&gt;https://medium.com/@sarath.mec/aws-dynamic-cdc-migration-using-dms-kinesis-and-lambda-d0ae8abff76f&lt;/a&gt;&lt;a href=\"https://medium.com/@sarath.mec/kinesis-lambda-trigger-dlq-reprocessing-using-aws-glue-cb0a62710a84\"&gt;https://medium.com/@sarath.mec/kinesis-lambda-trigger-dlq-reprocessing-using-aws-glue-cb0a62710a84&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rmqzQRBE4DRJbJkIs9qJJ28cyE4nJFodTVD_OVnyy84.jpg?auto=webp&amp;s=0be128e94e89c94c6a4e9171a8a1f391db98af13", "width": 593, "height": 301}, "resolutions": [{"url": "https://external-preview.redd.it/rmqzQRBE4DRJbJkIs9qJJ28cyE4nJFodTVD_OVnyy84.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f76cfe6d6425c1696b32837924e80186df4c47f4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rmqzQRBE4DRJbJkIs9qJJ28cyE4nJFodTVD_OVnyy84.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=707f210772308542a18999572e922b6b2fa5af12", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/rmqzQRBE4DRJbJkIs9qJJ28cyE4nJFodTVD_OVnyy84.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f9fe7a413fd3d9d8d4ce743805bfa9f65149779", "width": 320, "height": 162}], "variants": {}, "id": "twy5yvGGvgN5mlBccSDJ8qoJUJbtAsgtGsXeZM5Pibc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16z0rp5", "is_robot_indexable": true, "report_reasons": null, "author": "DragonflyHumble", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z0rp5/aws_cdc_using_dms_kinesis_lambda_with_dynamic_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z0rp5/aws_cdc_using_dms_kinesis_lambda_with_dynamic_sql/", "subreddit_subscribers": 132007, "created_utc": 1696360702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nJust wondering what data engineering tool(ETL, warehouse, what have you) is most widely used these days. Seems every week i get distracted and try to learn some new tool, and i really want to narrow it down so i can be more focused. \n\nSeems that SQL is the only constant, but i know there's more to that. tia \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_649o8hy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data engineering tools are popular right now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zm47c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696423046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Just wondering what data engineering tool(ETL, warehouse, what have you) is most widely used these days. Seems every week i get distracted and try to learn some new tool, and i really want to narrow it down so i can be more focused. &lt;/p&gt;\n\n&lt;p&gt;Seems that SQL is the only constant, but i know there&amp;#39;s more to that. tia &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16zm47c", "is_robot_indexable": true, "report_reasons": null, "author": "albertcuy", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zm47c/what_data_engineering_tools_are_popular_right_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zm47c/what_data_engineering_tools_are_popular_right_now/", "subreddit_subscribers": 132007, "created_utc": 1696423046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Redditors, I'm facing an issue where I only want to read some data (I used limit) from a folder full of parquet files. The folder has roughly 50 gb of data. \n\nWhen I trigger a read with limit it still reads the entire folder and then applies the limit. This is happening even after providing the schema. Not sure what is happening. \n\nI'm using EMR and data is on S3.", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark reads entire folders parquet data even after limit is applied", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zgzll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696405492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Redditors, I&amp;#39;m facing an issue where I only want to read some data (I used limit) from a folder full of parquet files. The folder has roughly 50 gb of data. &lt;/p&gt;\n\n&lt;p&gt;When I trigger a read with limit it still reads the entire folder and then applies the limit. This is happening even after providing the schema. Not sure what is happening. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using EMR and data is on S3.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16zgzll", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16zgzll/pyspark_reads_entire_folders_parquet_data_even/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zgzll/pyspark_reads_entire_folders_parquet_data_even/", "subreddit_subscribers": 132007, "created_utc": 1696405492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[ADVICE]\nHi All,\n\nI am a data engineer with 5 years of experience. I have got an offer for Senior DataOps and am applying for other Senior DE roles. \n\nThe DataOps role primarily comprises of supporting systems and DE teams and creating alerts for the existing pipelines. It is a support job. \n\nI am confused if this is the right path to take for my next Data Engineering role. Does going ahead with this restrict me to SRE/DevOps role?\n\nIt will be very helpful if you guys can give me some advice on this, if I should take it up or not? And would I get stuck in this and can I, in the future be eligible for Data Engineering roles?", "author_fullname": "t2_5ze1rxlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Next Step as Senior DE vs Senior DataOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16yzsxa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696398337.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696358401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[ADVICE]\nHi All,&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer with 5 years of experience. I have got an offer for Senior DataOps and am applying for other Senior DE roles. &lt;/p&gt;\n\n&lt;p&gt;The DataOps role primarily comprises of supporting systems and DE teams and creating alerts for the existing pipelines. It is a support job. &lt;/p&gt;\n\n&lt;p&gt;I am confused if this is the right path to take for my next Data Engineering role. Does going ahead with this restrict me to SRE/DevOps role?&lt;/p&gt;\n\n&lt;p&gt;It will be very helpful if you guys can give me some advice on this, if I should take it up or not? And would I get stuck in this and can I, in the future be eligible for Data Engineering roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16yzsxa", "is_robot_indexable": true, "report_reasons": null, "author": "ParticularDear5826", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yzsxa/next_step_as_senior_de_vs_senior_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yzsxa/next_step_as_senior_de_vs_senior_dataops/", "subreddit_subscribers": 132007, "created_utc": 1696358401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! \n\nI am relatively new to DE. This is my first job in tech and in DE. Its been 1.5 years into the job now and I just want to take a step back to understand what I have learnt and what I might need to focus on next.\n\nIn current role, I am using fivetran, stitch for data ingestion, dbt for transformation. We are using Snowflake. Mainly I am creating new data pipelines and setting up testing for those. So all I am doing is writing SQL code. In process, I learnt SQL, data engineering and warehousing fundamentals, git, CI/CD. \n\nBut this all involves working with automations and already setup environment. If I were to setup a DE project from scratch, I don't think I will be able to. When I hear about people talking about using python for scripting, S3 for storage and airflow for orchestrating, I understand roughly what they are saying but dont know how to do it technically. \n\nWhat should I do to prepare myself where I might not have all the help available with automation?\n\nThanks!", "author_fullname": "t2_7kdleomd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering with Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zm4ll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696423078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! &lt;/p&gt;\n\n&lt;p&gt;I am relatively new to DE. This is my first job in tech and in DE. Its been 1.5 years into the job now and I just want to take a step back to understand what I have learnt and what I might need to focus on next.&lt;/p&gt;\n\n&lt;p&gt;In current role, I am using fivetran, stitch for data ingestion, dbt for transformation. We are using Snowflake. Mainly I am creating new data pipelines and setting up testing for those. So all I am doing is writing SQL code. In process, I learnt SQL, data engineering and warehousing fundamentals, git, CI/CD. &lt;/p&gt;\n\n&lt;p&gt;But this all involves working with automations and already setup environment. If I were to setup a DE project from scratch, I don&amp;#39;t think I will be able to. When I hear about people talking about using python for scripting, S3 for storage and airflow for orchestrating, I understand roughly what they are saying but dont know how to do it technically. &lt;/p&gt;\n\n&lt;p&gt;What should I do to prepare myself where I might not have all the help available with automation?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16zm4ll", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded-Cod2051", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zm4ll/data_engineering_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zm4ll/data_engineering_with_python/", "subreddit_subscribers": 132007, "created_utc": 1696423078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a table which contains more than 350 Million rows, the DS team needs to use this table for their research. One of their problems (which is understandable) is that querying the data takes too long.   \nCan you refer me to anything that can help?  \nOr have you ever dealt with an issue like that.", "author_fullname": "t2_zbab4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Data Warehouse modeling on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zit1h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696412580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a table which contains more than 350 Million rows, the DS team needs to use this table for their research. One of their problems (which is understandable) is that querying the data takes too long.&lt;br/&gt;\nCan you refer me to anything that can help?&lt;br/&gt;\nOr have you ever dealt with an issue like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16zit1h", "is_robot_indexable": true, "report_reasons": null, "author": "chenvili", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zit1h/optimizing_data_warehouse_modeling_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zit1h/optimizing_data_warehouse_modeling_on_snowflake/", "subreddit_subscribers": 132007, "created_utc": 1696412580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,  \n\n\nCurrently an analyst looking to implement data engineering practices in my work. I've realized within our PowerBI workspace a lot of our reports run duplicate queries against the data warehouse (although sometimes out data isn't in the warehouse and we have to get it from external sources via r scripts). From reading around it seems like PowerBI data flows might be a solution for this but also they seem to scale poorly would it make sense to go about creating a data mart to subset common data sets to improve the workflow for the other analysts on my team? Or does anyone else have idea's for a scalable solution for creating various pipelines from our data warehouse/ external sources (csv's, api's)   \n", "author_fullname": "t2_141785", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to apply data engineering as an analyst", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z668s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696373305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,  &lt;/p&gt;\n\n&lt;p&gt;Currently an analyst looking to implement data engineering practices in my work. I&amp;#39;ve realized within our PowerBI workspace a lot of our reports run duplicate queries against the data warehouse (although sometimes out data isn&amp;#39;t in the warehouse and we have to get it from external sources via r scripts). From reading around it seems like PowerBI data flows might be a solution for this but also they seem to scale poorly would it make sense to go about creating a data mart to subset common data sets to improve the workflow for the other analysts on my team? Or does anyone else have idea&amp;#39;s for a scalable solution for creating various pipelines from our data warehouse/ external sources (csv&amp;#39;s, api&amp;#39;s)   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16z668s", "is_robot_indexable": true, "report_reasons": null, "author": "jotama0121", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z668s/how_to_apply_data_engineering_as_an_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z668s/how_to_apply_data_engineering_as_an_analyst/", "subreddit_subscribers": 132007, "created_utc": 1696373305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " This article will introduce MDX, a language similar to SQL. \n\n [SQL vs. MDX in OLAP](https://medium.com/@hellochenzg/sql-vs-mdx-in-olap-8bec56d9ee9a) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/mw6haqq4u6sb1.png?width=828&amp;format=png&amp;auto=webp&amp;s=7c34e9fe468c3b856160877d72f5e7ce950c7d2e", "author_fullname": "t2_ecwgpeq01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL vs. MDX in OLAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "media_metadata": {"mw6haqq4u6sb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/mw6haqq4u6sb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0e88930bd16513d592b424e26038d1b381fb5bf"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/mw6haqq4u6sb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31e54509a902177959a6e11d9127cffd32bd76b8"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/mw6haqq4u6sb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67cca46f45da7e54fcfc85a2df9cf9979139bd9c"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/mw6haqq4u6sb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55f42e64e119ed420513ac282b2cbbeb963ce682"}], "s": {"y": 462, "x": 828, "u": "https://preview.redd.it/mw6haqq4u6sb1.png?width=828&amp;format=png&amp;auto=webp&amp;s=7c34e9fe468c3b856160877d72f5e7ce950c7d2e"}, "id": "mw6haqq4u6sb1"}}, "name": "t3_16znbth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a1XZtZS8IbQzrz1ss5MEB-lIW-79M7wEIk2Aq4LZ6Vs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696426271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This article will introduce MDX, a language similar to SQL. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@hellochenzg/sql-vs-mdx-in-olap-8bec56d9ee9a\"&gt;SQL vs. MDX in OLAP&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mw6haqq4u6sb1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c34e9fe468c3b856160877d72f5e7ce950c7d2e\"&gt;https://preview.redd.it/mw6haqq4u6sb1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c34e9fe468c3b856160877d72f5e7ce950c7d2e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?auto=webp&amp;s=f8bcaeb67c3c946f1dca414658f2651727cac414", "width": 1093, "height": 609}, "resolutions": [{"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5698403140cd3e23c4bcfbba611d5b3b50e14172", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7deda589389911ea6fe6a96dc5c279d081725971", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d810433da778b697d6adaa944f49b41e96e9115", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=072e7e577f42d9f8a2f5f503e968c99babda51da", "width": 640, "height": 356}, {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d4d3a5c5830b432b0359c368caee0b6215d8afff", "width": 960, "height": 534}, {"url": "https://external-preview.redd.it/3jLGt09XGv-qCYysjsv2nPsAKw8VyMoW3DlMM115E90.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3ee9c5e52bd525114211f13a5ed0e30fc89f8e1d", "width": 1080, "height": 601}], "variants": {}, "id": "n6A9QyQRaeVd8hM9oYzyxXTeaIJ_OS0J08RgakAUbB0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16znbth", "is_robot_indexable": true, "report_reasons": null, "author": "czg715", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16znbth/sql_vs_mdx_in_olap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16znbth/sql_vs_mdx_in_olap/", "subreddit_subscribers": 132007, "created_utc": 1696426271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a school  project  where i designed a power bi dashbord   \nTechnically:\n\n\\- I have created views in BigQuery by consolidating data from various source tables within BigQuery.\n\n\\- I have scheduled queries using BigQuery's query scheduler, although I've been advised that this may not be considered a best practice.\n\n\\- I've connected these views to Power BI to create my dashboard.\n\n&amp;#x200B;\n\nMy question is: How can I professionalize and optimize my Power BI report with best practices? What tools within Google Cloud Platform (GCP) can I use to schedule my queries efficiently (to refresh automatically my dashbord)?\n\n&amp;#x200B;", "author_fullname": "t2_8li8rd9gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Industrialisation of PowerBI report in a full GCP env", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16zmiqf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696424142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a school  project  where i designed a power bi dashbord&lt;br/&gt;\nTechnically:&lt;/p&gt;\n\n&lt;p&gt;- I have created views in BigQuery by consolidating data from various source tables within BigQuery.&lt;/p&gt;\n\n&lt;p&gt;- I have scheduled queries using BigQuery&amp;#39;s query scheduler, although I&amp;#39;ve been advised that this may not be considered a best practice.&lt;/p&gt;\n\n&lt;p&gt;- I&amp;#39;ve connected these views to Power BI to create my dashboard.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is: How can I professionalize and optimize my Power BI report with best practices? What tools within Google Cloud Platform (GCP) can I use to schedule my queries efficiently (to refresh automatically my dashbord)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16zmiqf", "is_robot_indexable": true, "report_reasons": null, "author": "StatisticianFun3709", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zmiqf/industrialisation_of_powerbi_report_in_a_full_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zmiqf/industrialisation_of_powerbi_report_in_a_full_gcp/", "subreddit_subscribers": 132007, "created_utc": 1696424142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI am currently working to help a client migrate from ddb to Snowflake. They have been ingesting data since 4 years from API into ddb which sorta helped them achieve their goals well initially but now it is getting expensive. Essentially it is one huge table which has api data ingested partitioned by the application rule name which is the partition key and the actual api response is dumped as raw json into a map field called payload for that key.\n\nComing in as consultants, we are thinking of exporting ddb to s3 as json and then running a one time pyspark (glue ET) json flatten job that spits out as many number of tables as there are keys in the ddb table and then run SQL queries using Athena to understand their use case better so that we can frame our MVP better. Is this something that seem like a resonable approach?\n\nDdb: 400Gb, 500M records - More than 50 keys. It's just so hard to explore the data as it is right now.  \n\n\nEdit: This is just to explore; Final solution suggested involved API dumps into s3 as json and being picked up via snowpipe into variant column and being flattened there after. ", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explore Big Data From Ddb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zg10z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696401806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am currently working to help a client migrate from ddb to Snowflake. They have been ingesting data since 4 years from API into ddb which sorta helped them achieve their goals well initially but now it is getting expensive. Essentially it is one huge table which has api data ingested partitioned by the application rule name which is the partition key and the actual api response is dumped as raw json into a map field called payload for that key.&lt;/p&gt;\n\n&lt;p&gt;Coming in as consultants, we are thinking of exporting ddb to s3 as json and then running a one time pyspark (glue ET) json flatten job that spits out as many number of tables as there are keys in the ddb table and then run SQL queries using Athena to understand their use case better so that we can frame our MVP better. Is this something that seem like a resonable approach?&lt;/p&gt;\n\n&lt;p&gt;Ddb: 400Gb, 500M records - More than 50 keys. It&amp;#39;s just so hard to explore the data as it is right now.  &lt;/p&gt;\n\n&lt;p&gt;Edit: This is just to explore; Final solution suggested involved API dumps into s3 as json and being picked up via snowpipe into variant column and being flattened there after. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16zg10z", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zg10z/explore_big_data_from_ddb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zg10z/explore_big_data_from_ddb/", "subreddit_subscribers": 132007, "created_utc": 1696401806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently in a small team with the only DE. We have a poor way of version control basically whatever SQL codes and ipynb files is on github and are reuploaded after any changes. I want to implement good version control practices and CI/CD. Would like to know from the community on starting points to implement this", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing CI/CD &amp; Git/version control", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zfxto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696401489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently in a small team with the only DE. We have a poor way of version control basically whatever SQL codes and ipynb files is on github and are reuploaded after any changes. I want to implement good version control practices and CI/CD. Would like to know from the community on starting points to implement this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16zfxto", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zfxto/implementing_cicd_gitversion_control/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zfxto/implementing_cicd_gitversion_control/", "subreddit_subscribers": 132007, "created_utc": 1696401489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we parse a public API, most of data from which is 100% suitable for relation DBs.  Think about this data as user transactions - who, when, how, where send the money. The amount of data is around 1 mb per second.\n\nThe data is already cleaned and we only need to replace the user_id with corresponding primary key in transfer table and add new users to user table.\n\nSo we do that on the fly and store the results in postgres, from which backend grabs info for the FE. As public API support historical queries we don\u2019t need to store the raw data, so we like only doing the last step of ETL pipeline. \n\nNo I\u2019d want to train some logistic regression, make some EDA on that data etc. Tho exporting a lot of data from pg may be very slow. \n\nI\u2019m kind of curious what may be a better way to handle that?", "author_fullname": "t2_3ppayh15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the options for storing 1Mb / s data for latter analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zfc3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696399292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we parse a public API, most of data from which is 100% suitable for relation DBs.  Think about this data as user transactions - who, when, how, where send the money. The amount of data is around 1 mb per second.&lt;/p&gt;\n\n&lt;p&gt;The data is already cleaned and we only need to replace the user_id with corresponding primary key in transfer table and add new users to user table.&lt;/p&gt;\n\n&lt;p&gt;So we do that on the fly and store the results in postgres, from which backend grabs info for the FE. As public API support historical queries we don\u2019t need to store the raw data, so we like only doing the last step of ETL pipeline. &lt;/p&gt;\n\n&lt;p&gt;No I\u2019d want to train some logistic regression, make some EDA on that data etc. Tho exporting a lot of data from pg may be very slow. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m kind of curious what may be a better way to handle that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16zfc3b", "is_robot_indexable": true, "report_reasons": null, "author": "dotaleaker", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zfc3b/what_are_the_options_for_storing_1mb_s_data_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zfc3b/what_are_the_options_for_storing_1mb_s_data_for/", "subreddit_subscribers": 132007, "created_utc": 1696399292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says almost everything my goals are to cover the basics of setting up data streaming using Kafka through confluence, any advice on what exercises would fit this or any tips for setup. I am just starting out and heard Kafka was a pain to work with.", "author_fullname": "t2_d1ss2q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering student looking to setup streaming exercise with Kafka(confluence)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zb1ah", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696385958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says almost everything my goals are to cover the basics of setting up data streaming using Kafka through confluence, any advice on what exercises would fit this or any tips for setup. I am just starting out and heard Kafka was a pain to work with.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16zb1ah", "is_robot_indexable": true, "report_reasons": null, "author": "wolfmaster58", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16zb1ah/data_engineering_student_looking_to_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16zb1ah/data_engineering_student_looking_to_setup/", "subreddit_subscribers": 132007, "created_utc": 1696385958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I\u2019m a recent grad, i\u2019ve been working as a SE for about 4 months now(first career job). The company is good but i dont see myself here long term because of the location, I had to move away. I graduated with a BS in information systems and have knowledge of SQL, tableau, power Bi and most of all python(I use it at work everyday). With SQL, Power Bi and tableau, i had a bunch of data analysis courses. In about 5 months if I wanted to switch jobs to a more entry level Data focused role, could I just apply or would i need something to make me look more attractive to companies? Like maybe get my masters in DS(i want to at some point) or some certificates, is that necessary?\n\nBasically should i just go ahead and apply or am i wasting my time going up that route as i am now?", "author_fullname": "t2_b017ixrv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m a software engineer, I want to be a DS or DE. What should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z055o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696359198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I\u2019m a recent grad, i\u2019ve been working as a SE for about 4 months now(first career job). The company is good but i dont see myself here long term because of the location, I had to move away. I graduated with a BS in information systems and have knowledge of SQL, tableau, power Bi and most of all python(I use it at work everyday). With SQL, Power Bi and tableau, i had a bunch of data analysis courses. In about 5 months if I wanted to switch jobs to a more entry level Data focused role, could I just apply or would i need something to make me look more attractive to companies? Like maybe get my masters in DS(i want to at some point) or some certificates, is that necessary?&lt;/p&gt;\n\n&lt;p&gt;Basically should i just go ahead and apply or am i wasting my time going up that route as i am now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16z055o", "is_robot_indexable": true, "report_reasons": null, "author": "ShowtimeCharles", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16z055o/im_a_software_engineer_i_want_to_be_a_ds_or_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16z055o/im_a_software_engineer_i_want_to_be_a_ds_or_de/", "subreddit_subscribers": 132007, "created_utc": 1696359198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your opinions on how a data engineer position varies between industries? I bet it's hard to generalise, but I'm sure there's some that are more likely to be chill and slow, some that can invest in the shiny tech etc. Where would you never send your resume to? Where would you look if you wanted a career jump? Hope to hear your hypotheses", "author_fullname": "t2_9wfvtd4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the title really industry agnostic?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16yzlo4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696357901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your opinions on how a data engineer position varies between industries? I bet it&amp;#39;s hard to generalise, but I&amp;#39;m sure there&amp;#39;s some that are more likely to be chill and slow, some that can invest in the shiny tech etc. Where would you never send your resume to? Where would you look if you wanted a career jump? Hope to hear your hypotheses&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16yzlo4", "is_robot_indexable": true, "report_reasons": null, "author": "prsrboi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yzlo4/is_the_title_really_industry_agnostic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yzlo4/is_the_title_really_industry_agnostic/", "subreddit_subscribers": 132007, "created_utc": 1696357901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_14v3ms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Completeness: To Stream or Not to Stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ywpin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696351027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/data-completeness.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ywpin", "is_robot_indexable": true, "report_reasons": null, "author": "s0ck_r4w", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ywpin/data_completeness_to_stream_or_not_to_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/data-completeness.html", "subreddit_subscribers": 132007, "created_utc": 1696351027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys! I am working in a field where Pentaho ETL is used. So, i got some curiousity to work in ETL. Now, I want to leave my domain and learn the Powercenter ETL. May I choose a career in Powercenter ETL? How does the future look on this domain? May i know the scope, insights of Powercenter ETL?", "author_fullname": "t2_krnjhopij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Powercenter ETL career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16yvknm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696348474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys! I am working in a field where Pentaho ETL is used. So, i got some curiousity to work in ETL. Now, I want to leave my domain and learn the Powercenter ETL. May I choose a career in Powercenter ETL? How does the future look on this domain? May i know the scope, insights of Powercenter ETL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16yvknm", "is_robot_indexable": true, "report_reasons": null, "author": "tamizhiniyan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16yvknm/powercenter_etl_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16yvknm/powercenter_etl_career/", "subreddit_subscribers": 132007, "created_utc": 1696348474.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}