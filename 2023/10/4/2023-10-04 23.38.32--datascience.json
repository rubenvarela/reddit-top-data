{"kind": "Listing", "data": {"after": "t3_16zbst6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all!  To preface, I started a new job almost a year ago, I graduated with my MBA last year, and I had 2 years of experience as a data analyst prior. \n\nWhen I started my job, I was very excited because of all the glamor and hype that comes along with a space program, but that excitement is gone. My company is a start up, and everyone in my sector has started within the last year or two. A few months in, I realized how poor of processes this company had, if any, along with an evident lack of leadership. We had a major re-organization that left many people without jobs, and on top of that, we let go close to a third of our employees at the same time. We have had drastic funding issues and schedule issues because of that, so overall, the mood has not been good. Additionally, I was put on a \u201cneed to know\u201d program that will potentially take the company down an ethical path that I am not willing to go down. \n\nI kept holding on thinking that maybe just maybe things would get better, but I also had family health issues that required me to leave the state permanently right as my company put in a mandatory work from office policy.  \n\nWith all this being said, I was offered a very nice position at another company that would be a 15% increase in pay in the state I was moving to, but the position was one that I knew that I would be unfulfilled and very unhappy doing. Regardless, I decided that the stability and being able to move for my family was more important than the role, so I turned in my notice at my current job and prepared to take this new one. \n\nOur sector VP, however, made a counter offer for me to stay with the company remote, and they would not only match the opposing company but give me yet an additional 7% raise and a promotion and a sizeable bonus to move. I naively accepted the offer but with the catch that if I left within 12 months, I would have to pay back that bonus in full. \n\nFast forward, things have only gotten worse at this company, and the ethical concern is seeming to become a reality.  I am also in a bad situation in that I am not qualified to be in the promotion I was given, or at least I am not getting the support I need to be successful in this position. My family medical situation has become a major financial burden, and the raise that I was given is not enough to keep up. \n\nI desperately want to leave this company for good, but I feel like there is nothing I can do but wait for my bonus contract obligation to be fulfilled by staying for another year. I also feel like if I were to get another job, I would have to take a pay cut due to my lack of experience, but my current financial situation is already struggling. I don\u2019t know what to do anymore, and every day I work for this company I get more depressed.", "author_fullname": "t2_v54crjii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I accepted my company\u2019s counter offer and it was a mistake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zeenf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 178, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696438694.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696396173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!  To preface, I started a new job almost a year ago, I graduated with my MBA last year, and I had 2 years of experience as a data analyst prior. &lt;/p&gt;\n\n&lt;p&gt;When I started my job, I was very excited because of all the glamor and hype that comes along with a space program, but that excitement is gone. My company is a start up, and everyone in my sector has started within the last year or two. A few months in, I realized how poor of processes this company had, if any, along with an evident lack of leadership. We had a major re-organization that left many people without jobs, and on top of that, we let go close to a third of our employees at the same time. We have had drastic funding issues and schedule issues because of that, so overall, the mood has not been good. Additionally, I was put on a \u201cneed to know\u201d program that will potentially take the company down an ethical path that I am not willing to go down. &lt;/p&gt;\n\n&lt;p&gt;I kept holding on thinking that maybe just maybe things would get better, but I also had family health issues that required me to leave the state permanently right as my company put in a mandatory work from office policy.  &lt;/p&gt;\n\n&lt;p&gt;With all this being said, I was offered a very nice position at another company that would be a 15% increase in pay in the state I was moving to, but the position was one that I knew that I would be unfulfilled and very unhappy doing. Regardless, I decided that the stability and being able to move for my family was more important than the role, so I turned in my notice at my current job and prepared to take this new one. &lt;/p&gt;\n\n&lt;p&gt;Our sector VP, however, made a counter offer for me to stay with the company remote, and they would not only match the opposing company but give me yet an additional 7% raise and a promotion and a sizeable bonus to move. I naively accepted the offer but with the catch that if I left within 12 months, I would have to pay back that bonus in full. &lt;/p&gt;\n\n&lt;p&gt;Fast forward, things have only gotten worse at this company, and the ethical concern is seeming to become a reality.  I am also in a bad situation in that I am not qualified to be in the promotion I was given, or at least I am not getting the support I need to be successful in this position. My family medical situation has become a major financial burden, and the raise that I was given is not enough to keep up. &lt;/p&gt;\n\n&lt;p&gt;I desperately want to leave this company for good, but I feel like there is nothing I can do but wait for my bonus contract obligation to be fulfilled by staying for another year. I also feel like if I were to get another job, I would have to take a pay cut due to my lack of experience, but my current financial situation is already struggling. I don\u2019t know what to do anymore, and every day I work for this company I get more depressed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zeenf", "is_robot_indexable": true, "report_reasons": null, "author": "college_geek1", "discussion_type": null, "num_comments": 77, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zeenf/i_accepted_my_companys_counter_offer_and_it_was_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zeenf/i_accepted_my_companys_counter_offer_and_it_was_a/", "subreddit_subscribers": 1072105, "created_utc": 1696396173.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As a data scientist, if you could let someone else solve something for you what would it be?\n\nI was curious to know the problems data scientists face. This can be anywhere from collecting data and cleaning data to making and deploying machine learning models.", "author_fullname": "t2_l10ic37ez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do corporate data scientists struggle with the most at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z8pez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 113, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 113, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696379698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a data scientist, if you could let someone else solve something for you what would it be?&lt;/p&gt;\n\n&lt;p&gt;I was curious to know the problems data scientists face. This can be anywhere from collecting data and cleaning data to making and deploying machine learning models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16z8pez", "is_robot_indexable": true, "report_reasons": null, "author": "Potanee", "discussion_type": null, "num_comments": 106, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16z8pez/what_do_corporate_data_scientists_struggle_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16z8pez/what_do_corporate_data_scientists_struggle_with/", "subreddit_subscribers": 1072105, "created_utc": 1696379698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am lead of a new Data Science Division. The management team at our company is insistent that Data Sciences in the plural is a better fit. On my team we have statisticians, database managers, geospatial geographers, programmers, and data scientists. We are also incorporating machine learning as well. Google searches almost exclusively mention Data Science in the singular. Does anyone have any opinions or suggestions? Should I bow down and embrace the plural or should I be adamant about the norm of the singular?", "author_fullname": "t2_iwmayqjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science(s) in the plural", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zw1dz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696447263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am lead of a new Data Science Division. The management team at our company is insistent that Data Sciences in the plural is a better fit. On my team we have statisticians, database managers, geospatial geographers, programmers, and data scientists. We are also incorporating machine learning as well. Google searches almost exclusively mention Data Science in the singular. Does anyone have any opinions or suggestions? Should I bow down and embrace the plural or should I be adamant about the norm of the singular?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zw1dz", "is_robot_indexable": true, "report_reasons": null, "author": "EcoNerd007", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zw1dz/data_sciences_in_the_plural/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zw1dz/data_sciences_in_the_plural/", "subreddit_subscribers": 1072105, "created_utc": 1696447263.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm playing around with adapting matrix profiles to my time series data and I want to ensure that the data and parameters are set correctly.\n\nI'm working with a month's worth of data placed into 5-minute bins (288 samples/day). I initially rebinned to 1-hour but I wasn't sure whether or not that might hide certain higher frequency patterns or if it would just make it too noisy.\n\nI'm also attempting to tune the window_size parameter used by the STUMPY python library (stumpy.stumpy function). This adjusts the length of the segments that the matrix profile algorithm uses to compare to measure similarity. If the window size is too small (fewer points), you are more likely to get incidental matches. If it's too large, you're less likely to appropriately match similar patterns.\n\nThere is seasonality in the series that reflects diurnal patters (activity spikes during peak operational hours and drops out during off-peak hours). Because of this, I wanted a window size of at least half a day (144 for 5m bins, 12 for 1h bins) or a third of a day (96 for 5m bins, 8 for 1h bins) to achieve the Nyquist frequency of the seasonality, if that makes sense.\n\nAre there any tests that I could run to help identify and optimize both the size of the time bins and the window size?\n\nOne thing I noticed is that when adjusting the window size, the rate of change of the number of detected motifs isn't linear. I have a hunch that I could probably plot it out and use the elbow method but I need a sanity check before I try it out.\n\nFor the bin size, I usually use a Power Spectral Density plot to identify dominant frequencies (I mostly use that for selecting seasonality parameters for decompositions). For the 1h bucket series, there are 1-3 dominant frequencies well above the noise floor, which is good. However, when I use the 5m bucket series, there doesn't appear to be any dominant frequencies, just noise. Would that suggest that the 5m bucket series is suboptimal in terms of bin size compared to the 1h series?\n\nI just need a second set of eyes on it to make sure I'm not misinterpreting or misunderstanding something. I'm also open to suggestions or ideas, if any are available.", "author_fullname": "t2_131bi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When building out a matrix profile for a time series, what tests can be used to determine that both the bin size and window size are optimal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zh6dt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696406203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m playing around with adapting matrix profiles to my time series data and I want to ensure that the data and parameters are set correctly.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working with a month&amp;#39;s worth of data placed into 5-minute bins (288 samples/day). I initially rebinned to 1-hour but I wasn&amp;#39;t sure whether or not that might hide certain higher frequency patterns or if it would just make it too noisy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also attempting to tune the window_size parameter used by the STUMPY python library (stumpy.stumpy function). This adjusts the length of the segments that the matrix profile algorithm uses to compare to measure similarity. If the window size is too small (fewer points), you are more likely to get incidental matches. If it&amp;#39;s too large, you&amp;#39;re less likely to appropriately match similar patterns.&lt;/p&gt;\n\n&lt;p&gt;There is seasonality in the series that reflects diurnal patters (activity spikes during peak operational hours and drops out during off-peak hours). Because of this, I wanted a window size of at least half a day (144 for 5m bins, 12 for 1h bins) or a third of a day (96 for 5m bins, 8 for 1h bins) to achieve the Nyquist frequency of the seasonality, if that makes sense.&lt;/p&gt;\n\n&lt;p&gt;Are there any tests that I could run to help identify and optimize both the size of the time bins and the window size?&lt;/p&gt;\n\n&lt;p&gt;One thing I noticed is that when adjusting the window size, the rate of change of the number of detected motifs isn&amp;#39;t linear. I have a hunch that I could probably plot it out and use the elbow method but I need a sanity check before I try it out.&lt;/p&gt;\n\n&lt;p&gt;For the bin size, I usually use a Power Spectral Density plot to identify dominant frequencies (I mostly use that for selecting seasonality parameters for decompositions). For the 1h bucket series, there are 1-3 dominant frequencies well above the noise floor, which is good. However, when I use the 5m bucket series, there doesn&amp;#39;t appear to be any dominant frequencies, just noise. Would that suggest that the 5m bucket series is suboptimal in terms of bin size compared to the 1h series?&lt;/p&gt;\n\n&lt;p&gt;I just need a second set of eyes on it to make sure I&amp;#39;m not misinterpreting or misunderstanding something. I&amp;#39;m also open to suggestions or ideas, if any are available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zh6dt", "is_robot_indexable": true, "report_reasons": null, "author": "WadeEffingWilson", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zh6dt/when_building_out_a_matrix_profile_for_a_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zh6dt/when_building_out_a_matrix_profile_for_a_time/", "subreddit_subscribers": 1072105, "created_utc": 1696406203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I graduated this May, and after applying for positions for the past three months now, I'm so happy to be able to say that I finally got hired as a Junior Supply Chain Analyst! The company is amazing as well so I couldn't have asked for a better opportunity. During that time, I lurked a lot in this subreddit for advice and motivation, so I just wanted to share my success story and thank everyone that takes the time to offer their wisdom to those who are just starting out.\n\nThese past five months have been incredibly arduous as it goes without saying that the job market isn't the best right now. However, I'd like to dedicate this win towards communities such as these that allowed me to surround myself with those who are much more experienced than I am.\n\nMuch love! \ud83c\udf7b\ud83c\udf7b\ud83c\udf7b", "author_fullname": "t2_5iv264h3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recent grad finally hired!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17004s9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696457039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I graduated this May, and after applying for positions for the past three months now, I&amp;#39;m so happy to be able to say that I finally got hired as a Junior Supply Chain Analyst! The company is amazing as well so I couldn&amp;#39;t have asked for a better opportunity. During that time, I lurked a lot in this subreddit for advice and motivation, so I just wanted to share my success story and thank everyone that takes the time to offer their wisdom to those who are just starting out.&lt;/p&gt;\n\n&lt;p&gt;These past five months have been incredibly arduous as it goes without saying that the job market isn&amp;#39;t the best right now. However, I&amp;#39;d like to dedicate this win towards communities such as these that allowed me to surround myself with those who are much more experienced than I am.&lt;/p&gt;\n\n&lt;p&gt;Much love! \ud83c\udf7b\ud83c\udf7b\ud83c\udf7b&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17004s9", "is_robot_indexable": true, "report_reasons": null, "author": "slashded", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17004s9/recent_grad_finally_hired/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17004s9/recent_grad_finally_hired/", "subreddit_subscribers": 1072105, "created_utc": 1696457039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am considering comparing mutual information scores, but I also don't think I understand MI well enough. \n\nFor example, I(X;Y) = H(X) + H(Y) - H(X,Y). To me, visualizing H(X) and H(Y) as venn diagrams and H(X,Y) as the information from both X, Y (like an overlapping venn diagram) makes me think that when X, Y are disjoint, then MI is 0 and when X, Y overlap completely, then the MI score will be high. So, I'm thinking that a high MI value is \"bad\" since this means X, Y would be redundant. I am not sure if my understanding here is correct. \n\nAnother method I have tried is to binarize the data for each feature (represented as rows in my dataset) using \"present\" (1) and \"absent\" (0). The main issue I have run into doing this is that I am trying to then create a **distribution** to compare the features (such as seeing what percent of 1s and 0s I find in each feature), but here is the issue: \n\nLet's say that feature A has 50% 1s and 50% 0s, and feature B also has 50% 1s and 50% 0s. So, it will look as if the distribution of their values is identical, though it could be that feature A and B are \"opposites\":\n\nFeat. A: [0, 0, 1, 1]\n\nFeat. B: [1, 1, 0, 0]\n\nSo, I wonder if there is a better way to compare the distributions of the features once I have made the data \"present\" (1) and \"absent\" (0). \n\nI am also looking at making a Probability Density Function for each feature to compare them, but it's not clear to me how I would go about creating such a PDF for each feature given that I don't know what the probabilities associated actually are. Should I be binning the data then finding what percentage falls in these intervals?\n\n______\n\nOverall, I am looking for advice on where to find useful information on how to compare features for **unsupervised** feature selection, particularly in regards to how to use and compare mutual information scores, how to create PDFs for features, and how to compare distributions between features after they have been binned to avoid the problem I mentioned (with how [0, 0, 1, 1] and [1, 1, 0, 0] would appear to have the same distribution). \n\nRelevant textbook resources and other reliable source recommendations would be much appreciated. \n\nThank you.", "author_fullname": "t2_efotpocwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some effective dimensionality reduction (unsupervised feature selection) techniques for a high dimensional, sparse dataset?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z8v18", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696380115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am considering comparing mutual information scores, but I also don&amp;#39;t think I understand MI well enough. &lt;/p&gt;\n\n&lt;p&gt;For example, I(X;Y) = H(X) + H(Y) - H(X,Y). To me, visualizing H(X) and H(Y) as venn diagrams and H(X,Y) as the information from both X, Y (like an overlapping venn diagram) makes me think that when X, Y are disjoint, then MI is 0 and when X, Y overlap completely, then the MI score will be high. So, I&amp;#39;m thinking that a high MI value is &amp;quot;bad&amp;quot; since this means X, Y would be redundant. I am not sure if my understanding here is correct. &lt;/p&gt;\n\n&lt;p&gt;Another method I have tried is to binarize the data for each feature (represented as rows in my dataset) using &amp;quot;present&amp;quot; (1) and &amp;quot;absent&amp;quot; (0). The main issue I have run into doing this is that I am trying to then create a &lt;strong&gt;distribution&lt;/strong&gt; to compare the features (such as seeing what percent of 1s and 0s I find in each feature), but here is the issue: &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that feature A has 50% 1s and 50% 0s, and feature B also has 50% 1s and 50% 0s. So, it will look as if the distribution of their values is identical, though it could be that feature A and B are &amp;quot;opposites&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;Feat. A: [0, 0, 1, 1]&lt;/p&gt;\n\n&lt;p&gt;Feat. B: [1, 1, 0, 0]&lt;/p&gt;\n\n&lt;p&gt;So, I wonder if there is a better way to compare the distributions of the features once I have made the data &amp;quot;present&amp;quot; (1) and &amp;quot;absent&amp;quot; (0). &lt;/p&gt;\n\n&lt;p&gt;I am also looking at making a Probability Density Function for each feature to compare them, but it&amp;#39;s not clear to me how I would go about creating such a PDF for each feature given that I don&amp;#39;t know what the probabilities associated actually are. Should I be binning the data then finding what percentage falls in these intervals?&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Overall, I am looking for advice on where to find useful information on how to compare features for &lt;strong&gt;unsupervised&lt;/strong&gt; feature selection, particularly in regards to how to use and compare mutual information scores, how to create PDFs for features, and how to compare distributions between features after they have been binned to avoid the problem I mentioned (with how [0, 0, 1, 1] and [1, 1, 0, 0] would appear to have the same distribution). &lt;/p&gt;\n\n&lt;p&gt;Relevant textbook resources and other reliable source recommendations would be much appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16z8v18", "is_robot_indexable": true, "report_reasons": null, "author": "MLquestionAccount", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16z8v18/what_are_some_effective_dimensionality_reduction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16z8v18/what_are_some_effective_dimensionality_reduction/", "subreddit_subscribers": 1072105, "created_utc": 1696380115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When my team mate used the probabilities from one model and used as feature to the other model the probabilities from first model was highest on the feature importance map for the second model.\n\nIs this an example of stacked model or is it better to have trained both models with additional features and compare the accuracy of both models rather than reporting the accuracy of the linked model in step 1. \n\nPlease share your experience. Thanks", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we take the probabilities from one model and make it a feature to the other model along with additional features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zwzv6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696449630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When my team mate used the probabilities from one model and used as feature to the other model the probabilities from first model was highest on the feature importance map for the second model.&lt;/p&gt;\n\n&lt;p&gt;Is this an example of stacked model or is it better to have trained both models with additional features and compare the accuracy of both models rather than reporting the accuracy of the linked model in step 1. &lt;/p&gt;\n\n&lt;p&gt;Please share your experience. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zwzv6", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zwzv6/can_we_take_the_probabilities_from_one_model_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zwzv6/can_we_take_the_probabilities_from_one_model_and/", "subreddit_subscribers": 1072105, "created_utc": 1696449630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I find it pretty interesting so for those who did it, do you mind telling us your experience, how you did it, and if it's something you'd recommend?", "author_fullname": "t2_k9cm6k85o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can those who uses Data Analytics to get into Data Science shed some light about your experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1700545", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696457059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find it pretty interesting so for those who did it, do you mind telling us your experience, how you did it, and if it&amp;#39;s something you&amp;#39;d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1700545", "is_robot_indexable": true, "report_reasons": null, "author": "Jaded_Masterpiece_12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1700545/can_those_who_uses_data_analytics_to_get_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1700545/can_those_who_uses_data_analytics_to_get_into/", "subreddit_subscribers": 1072105, "created_utc": 1696457059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "suppose that i have 1000 sites that i need to build a script to extract individually and need the data to be refreshed weekly, what are some tools/software that can help me to automate such task?", "author_fullname": "t2_8es0n3vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good scraping software to use for task automation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zldu3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696420998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;suppose that i have 1000 sites that i need to build a script to extract individually and need the data to be refreshed weekly, what are some tools/software that can help me to automate such task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zldu3", "is_robot_indexable": true, "report_reasons": null, "author": "Breadskinjinhojiak", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zldu3/what_are_some_good_scraping_software_to_use_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zldu3/what_are_some_good_scraping_software_to_use_for/", "subreddit_subscribers": 1072105, "created_utc": 1696420998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey everyone!\n\nCurrently,  I am working on a project around music emotion classifcation/regression  model. Basically I am trying to predict a score to each emotion on a  given song.\n\nThe problem is that my  dataset has quite imbalanced scores (y). Most scores are centered  around a certain score range. Therefore, having difficulties predicting  scores that are further away of the mean values.\n\nI  had this idea to bring in pre-trained (on other datasets and problems)  audio classification models into this as there are a bunch of good  performing pre-trained classification models out there already. The  prediction of these pre-trained models should be used as features (e.g.  prediction of genre, instrument etc) beside the original spectorgram in  my model.\n\nI know this won't solve  the problem of imbalances in the scores but I thought maybe this could  improve the performance as the model would have more features to work  with.\n\nDoes this make sense?\n\nI appreciate any input.", "author_fullname": "t2_8puue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using pre-trained models as features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zi4jk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696409924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;Currently,  I am working on a project around music emotion classifcation/regression  model. Basically I am trying to predict a score to each emotion on a  given song.&lt;/p&gt;\n\n&lt;p&gt;The problem is that my  dataset has quite imbalanced scores (y). Most scores are centered  around a certain score range. Therefore, having difficulties predicting  scores that are further away of the mean values.&lt;/p&gt;\n\n&lt;p&gt;I  had this idea to bring in pre-trained (on other datasets and problems)  audio classification models into this as there are a bunch of good  performing pre-trained classification models out there already. The  prediction of these pre-trained models should be used as features (e.g.  prediction of genre, instrument etc) beside the original spectorgram in  my model.&lt;/p&gt;\n\n&lt;p&gt;I know this won&amp;#39;t solve  the problem of imbalances in the scores but I thought maybe this could  improve the performance as the model would have more features to work  with.&lt;/p&gt;\n\n&lt;p&gt;Does this make sense?&lt;/p&gt;\n\n&lt;p&gt;I appreciate any input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zi4jk", "is_robot_indexable": true, "report_reasons": null, "author": "Kniggi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zi4jk/using_pretrained_models_as_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zi4jk/using_pretrained_models_as_features/", "subreddit_subscribers": 1072105, "created_utc": 1696409924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys\nI\u2019m looking for a project idea in Computer Vision. Been browsing through multiple datasets but haven\u2019t been able to think of an idea that has not been implemented before. Can you guys help me out with some ideas? I\u2019m a grad student. Thanks!", "author_fullname": "t2_6hsosf1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Ideas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zcw19", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696391265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys\nI\u2019m looking for a project idea in Computer Vision. Been browsing through multiple datasets but haven\u2019t been able to think of an idea that has not been implemented before. Can you guys help me out with some ideas? I\u2019m a grad student. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zcw19", "is_robot_indexable": true, "report_reasons": null, "author": "Libran10", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zcw19/project_ideas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zcw19/project_ideas/", "subreddit_subscribers": 1072105, "created_utc": 1696391265.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it worth to get a Python certification, e.g. Python Institute, Anaconda, etc, to start in the data science career?", "author_fullname": "t2_dzepa4wng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1701142", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696459162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it worth to get a Python certification, e.g. Python Institute, Anaconda, etc, to start in the data science career?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1701142", "is_robot_indexable": true, "report_reasons": null, "author": "Classic_Impress4680", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1701142/python_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1701142/python_certification/", "subreddit_subscribers": 1072105, "created_utc": 1696459162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I'm finishing up my last term of school in a data science masters program. I have the chance to choose between a Data Warehousing course and an Advanced Big Data Management Course. \n\nData Warehousing uses SQL server for various ETL and data governance matters, and Advanced Big Data Mangement uses Hadoop, PySpark, MongoDB, Kafka, and noSQL.\n\nMy skillset is really Python-heavy right now, so I'm looking to one of these classes to give me a little more practice on the SQL or SQL-ish side of things. Also want to round myself out and make myself the most viable work candidate possible. But also am exhausted and just want to get over the finish line of my masters in one piece. \n\nAny recommendations?", "author_fullname": "t2_ivq62s7r5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grad School Course Selection Advice: Data Warehousing vs Advanced Big Data Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1700rwg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696458560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m finishing up my last term of school in a data science masters program. I have the chance to choose between a Data Warehousing course and an Advanced Big Data Management Course. &lt;/p&gt;\n\n&lt;p&gt;Data Warehousing uses SQL server for various ETL and data governance matters, and Advanced Big Data Mangement uses Hadoop, PySpark, MongoDB, Kafka, and noSQL.&lt;/p&gt;\n\n&lt;p&gt;My skillset is really Python-heavy right now, so I&amp;#39;m looking to one of these classes to give me a little more practice on the SQL or SQL-ish side of things. Also want to round myself out and make myself the most viable work candidate possible. But also am exhausted and just want to get over the finish line of my masters in one piece. &lt;/p&gt;\n\n&lt;p&gt;Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1700rwg", "is_robot_indexable": true, "report_reasons": null, "author": "37thAndOStreet", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1700rwg/grad_school_course_selection_advice_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1700rwg/grad_school_course_selection_advice_data/", "subreddit_subscribers": 1072105, "created_utc": 1696458560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have two relational databases with \\~30 tables each. While they both hold essentially the same data, the schema for each is wildly different. I eventually need to migrate the data so I'd like to build a good 1-for-1 schema map for each column from the origin database to where that would go in the destination database (or to note that it's data that doesn't need to move, for one reason or another).\n\nI could certainly just manually build this all in Excel but that's boring and a time drain. Any good tools, preferably but not necessarily visual, that folks know that might work for this project? \n\nI've seen lots of good schema mapping tools online, but unclear that any of them are well suited for connecting the dots between two different database schemas.", "author_fullname": "t2_729g2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a tool to help map two databases schemas against each other", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16zzta7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696456285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two relational databases with ~30 tables each. While they both hold essentially the same data, the schema for each is wildly different. I eventually need to migrate the data so I&amp;#39;d like to build a good 1-for-1 schema map for each column from the origin database to where that would go in the destination database (or to note that it&amp;#39;s data that doesn&amp;#39;t need to move, for one reason or another).&lt;/p&gt;\n\n&lt;p&gt;I could certainly just manually build this all in Excel but that&amp;#39;s boring and a time drain. Any good tools, preferably but not necessarily visual, that folks know that might work for this project? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen lots of good schema mapping tools online, but unclear that any of them are well suited for connecting the dots between two different database schemas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zzta7", "is_robot_indexable": true, "report_reasons": null, "author": "KillingTimeSince99", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zzta7/looking_for_a_tool_to_help_map_two_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zzta7/looking_for_a_tool_to_help_map_two_databases/", "subreddit_subscribers": 1072105, "created_utc": 1696456285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey friends ,i hope u are well. I was thinking about  applying to this NYU phd - a data science program.i know its going to be competitive but willing to give it a huge shot.\n\n- i studied comp scie. In an american Uni graduated this summer\n- had a 3.05 gpa\n-  have had one internship in a big 4 consulting\n- currently working part time with a prof ,he is writing a book . I am mostly helping in the dataviz part.\n- the prof may write me a strong recommendation letter.\n\n(Please be brutal and kind at the same time lol)", "author_fullname": "t2_91mnxwws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some little help here", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16zzjei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696455654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey friends ,i hope u are well. I was thinking about  applying to this NYU phd - a data science program.i know its going to be competitive but willing to give it a huge shot.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;i studied comp scie. In an american Uni graduated this summer&lt;/li&gt;\n&lt;li&gt;had a 3.05 gpa&lt;/li&gt;\n&lt;li&gt; have had one internship in a big 4 consulting&lt;/li&gt;\n&lt;li&gt;currently working part time with a prof ,he is writing a book . I am mostly helping in the dataviz part.&lt;/li&gt;\n&lt;li&gt;the prof may write me a strong recommendation letter.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(Please be brutal and kind at the same time lol)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zzjei", "is_robot_indexable": true, "report_reasons": null, "author": "Rude-Paper2845", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zzjei/some_little_help_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zzjei/some_little_help_here/", "subreddit_subscribers": 1072105, "created_utc": 1696455654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a dataset of a year daily sales of an e-commerce. This e-commerce is closed on saturdays and certain holidays.\n\nThe data is non-stationary, but differencing would not be possible as specific days have 0 sales. Any tips on how to make it stationary?", "author_fullname": "t2_5tyo1mcq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Irregular Time Series Stationarity Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zyfvt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696453101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a dataset of a year daily sales of an e-commerce. This e-commerce is closed on saturdays and certain holidays.&lt;/p&gt;\n\n&lt;p&gt;The data is non-stationary, but differencing would not be possible as specific days have 0 sales. Any tips on how to make it stationary?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zyfvt", "is_robot_indexable": true, "report_reasons": null, "author": "biofrik", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zyfvt/irregular_time_series_stationarity_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zyfvt/irregular_time_series_stationarity_question/", "subreddit_subscribers": 1072105, "created_utc": 1696453101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_5if554f5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web Scraping using ChatGPT - Complete Guide with Examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zvf0e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696445738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "proxiesapi.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://proxiesapi.com/articles/web-scraping-using-chatgpt-complete-guide-with-examples", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zvf0e", "is_robot_indexable": true, "report_reasons": null, "author": "mohan-thatguy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zvf0e/web_scraping_using_chatgpt_complete_guide_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://proxiesapi.com/articles/web-scraping-using-chatgpt-complete-guide-with-examples", "subreddit_subscribers": 1072105, "created_utc": 1696445738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ywp5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Push notifications for your Jupyter Notebooks: Anyone would use this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zvcgb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1696445564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "notebookpush.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://notebookpush.com/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zvcgb", "is_robot_indexable": true, "report_reasons": null, "author": "mandrade2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zvcgb/push_notifications_for_your_jupyter_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://notebookpush.com/", "subreddit_subscribers": 1072105, "created_utc": 1696445564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In one of my courses (I am about to graduate in a few months) we have an end project where we are given a dataset and a question, that we have to answer using at least 3 classifiers. We were told that we \"may need to do a proper EDA otherwise won't get good results\". Thing is, in none of our earlier courses we were explained how to perform EDA. We did univariate/multivariate statistics, have seen a few techniques for projection / clustering / feature selection ...etc... and I think we have a few building blocks for everything, but we never were told (or given ressources) telling us what actually was EDA: which tasks are supposed to be done with the dataset before we are \"sure\" (I think we can never be fully sure but let's say reasonably sure) that it is ready for choosing a model, fitting it, and then interpreting the results.  \n\nWhen looking online, I could only find these shitty blog articles saying \"we'll explain all what EDA is\" and in the end only telling you to \"check the means and the standard deviations\", \"look at correlations\", \"make graphics\" alongside 3 lines of code, but which don't actually explain what needs to be done for a dataset to be ready for use. If anyone could share a good article which actually explains with enough depth what needs to be done (I don't need code or examples, I just need a structure).  \n\nI already tried to do my homework by compiling some stuff I found online + gathering some lonely notes scattered around in my courses + trying to use some logic in order to have a list of what I think could potentially be a list of things to do for EDA. I'll put it next so that you can see where I'm at right now and the extent to which I know nothing ( :'( )   \n\n--------------------------------------\n## Data loading and cleaning  \n- understand what each feature is and its format. Potentially reencode it or transform it (either inplace or adding a column) (example: income -&gt; log(income) in some cases).  \n- find missing/corrupted/duplicated data. Try to understand why this data is like that. Use this understanding to decide if we need to do something about it and if so what to do about it.  \n- try to understand the data collection process in general as it may help for decisions about what to do with weird things.  \n\n## Univariate analysis  \nFor each feature:\n- find the units. Also check their ranges, and decide whether it needs to be standardized, and how (squash between a and b, normalize, other?). May depend on the model to be used so maybe we shouldn't think about this now but later?  \n- visualize the distribution. If anything special about it (multimode, skewness, kurtosis...) understand what it means for the data, and whether transforming the data so get a \"nicer\" distribution is a good idea. Maybe transform and put in an extra column.  \n- identify imbalances (can be seen from the distribution already). Try to understand if it comes from a bias in the dataset, where/what this bias is, if we need to do something about this bias, and if yes what approaches may be suitable.  \n- identify outliers -&gt; do we do something with them? if yes, what?  \n\n## Multivariate analysis  \n- check correlation between features (like a correlation matrix for example)  \n- check interactions between features (because correlation only shows linear relationship. But I don't know any technique for checking nonlinear interactions?)  \n- investigate global relationships:  \n  - PCA with projections (how much inertia explained by each axis? does projection helps separate end classes? is there good potential for dimensionality reduction with PCA on this dataset?)  \n  - Clustering (can clusters be used an extra synthetic feature, for example if some clusters isolate well some labels? Using them for dimensionality reduction?)  \n\n## Name for the next phase? Modeling?\n- Translate the initial question into a goal that is compatible with how the data is structured and which information is available  \n- Choose a performance metric + methodology (including feature selection)  \n\n------------------------------------  \n\nIf anything is plain stupid in what I wrote above please feel free to correct. As I said I never had any ressource give me a proper explanation of how to perform EDA. If you have a good ressource on the subject that would be gold to me! (but I'm poor, sorry)  \n\nThanks in advance.  \n\nEdit: No I didn't ask chatGPT. Because I'm not asking chatGPT for things I can't check if it is bullshitting me.", "author_fullname": "t2_5kjf2ti0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EDA: what should I do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zuiwf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696443599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In one of my courses (I am about to graduate in a few months) we have an end project where we are given a dataset and a question, that we have to answer using at least 3 classifiers. We were told that we &amp;quot;may need to do a proper EDA otherwise won&amp;#39;t get good results&amp;quot;. Thing is, in none of our earlier courses we were explained how to perform EDA. We did univariate/multivariate statistics, have seen a few techniques for projection / clustering / feature selection ...etc... and I think we have a few building blocks for everything, but we never were told (or given ressources) telling us what actually was EDA: which tasks are supposed to be done with the dataset before we are &amp;quot;sure&amp;quot; (I think we can never be fully sure but let&amp;#39;s say reasonably sure) that it is ready for choosing a model, fitting it, and then interpreting the results.  &lt;/p&gt;\n\n&lt;p&gt;When looking online, I could only find these shitty blog articles saying &amp;quot;we&amp;#39;ll explain all what EDA is&amp;quot; and in the end only telling you to &amp;quot;check the means and the standard deviations&amp;quot;, &amp;quot;look at correlations&amp;quot;, &amp;quot;make graphics&amp;quot; alongside 3 lines of code, but which don&amp;#39;t actually explain what needs to be done for a dataset to be ready for use. If anyone could share a good article which actually explains with enough depth what needs to be done (I don&amp;#39;t need code or examples, I just need a structure).  &lt;/p&gt;\n\n&lt;p&gt;I already tried to do my homework by compiling some stuff I found online + gathering some lonely notes scattered around in my courses + trying to use some logic in order to have a list of what I think could potentially be a list of things to do for EDA. I&amp;#39;ll put it next so that you can see where I&amp;#39;m at right now and the extent to which I know nothing ( :&amp;#39;( )   &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Data loading and cleaning&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;understand what each feature is and its format. Potentially reencode it or transform it (either inplace or adding a column) (example: income -&amp;gt; log(income) in some cases).&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;find missing/corrupted/duplicated data. Try to understand why this data is like that. Use this understanding to decide if we need to do something about it and if so what to do about it.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;try to understand the data collection process in general as it may help for decisions about what to do with weird things.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Univariate analysis&lt;/h2&gt;\n\n&lt;p&gt;For each feature:\n- find the units. Also check their ranges, and decide whether it needs to be standardized, and how (squash between a and b, normalize, other?). May depend on the model to be used so maybe we shouldn&amp;#39;t think about this now but later?&lt;br/&gt;\n- visualize the distribution. If anything special about it (multimode, skewness, kurtosis...) understand what it means for the data, and whether transforming the data so get a &amp;quot;nicer&amp;quot; distribution is a good idea. Maybe transform and put in an extra column.&lt;br/&gt;\n- identify imbalances (can be seen from the distribution already). Try to understand if it comes from a bias in the dataset, where/what this bias is, if we need to do something about this bias, and if yes what approaches may be suitable.&lt;br/&gt;\n- identify outliers -&amp;gt; do we do something with them? if yes, what?  &lt;/p&gt;\n\n&lt;h2&gt;Multivariate analysis&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;check correlation between features (like a correlation matrix for example)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;check interactions between features (because correlation only shows linear relationship. But I don&amp;#39;t know any technique for checking nonlinear interactions?)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;investigate global relationships:&lt;br/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PCA with projections (how much inertia explained by each axis? does projection helps separate end classes? is there good potential for dimensionality reduction with PCA on this dataset?)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Clustering (can clusters be used an extra synthetic feature, for example if some clusters isolate well some labels? Using them for dimensionality reduction?)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Name for the next phase? Modeling?&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Translate the initial question into a goal that is compatible with how the data is structured and which information is available&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Choose a performance metric + methodology (including feature selection)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;If anything is plain stupid in what I wrote above please feel free to correct. As I said I never had any ressource give me a proper explanation of how to perform EDA. If you have a good ressource on the subject that would be gold to me! (but I&amp;#39;m poor, sorry)  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.  &lt;/p&gt;\n\n&lt;p&gt;Edit: No I didn&amp;#39;t ask chatGPT. Because I&amp;#39;m not asking chatGPT for things I can&amp;#39;t check if it is bullshitting me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zuiwf", "is_robot_indexable": true, "report_reasons": null, "author": "DoctorFuu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zuiwf/eda_what_should_i_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zuiwf/eda_what_should_i_do/", "subreddit_subscribers": 1072105, "created_utc": 1696443599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I've been working as an ML Engineer/Researcher intern for last three months first several months were cool, been writing scripts for cropping images, etc. everything went well. Not so long ago I've started working on NLP projects, and been asked to checkout how possible it is to fine-tune sota models on low-end hardware. \n\nI've started going through sota models and found some of them which would fit on gpu. However, what I came into is that for 10 different models you usually have 10 different APIs because of quantisation methods, chatlikenes, languages weights etc... \n\nI have to go through TONS of trial and error WAITING for model weights to load on my pc just to learn that I need to wait MORE to find out that it works at 1tok/eternity or doesn't work because of mismatch of versions, then you go to Collab go through the same process, get crashes, timeouts, etc., you get the idea. \n\nWhat frustrates me the most is the WAITING and NO results. I usually have 27281 pages opened to find the right guide (because half of them result in error and others are outdated because they are 2 seconds old). \n\nHow do I deal with such kind of frustration and not leave state of flow?\n\n It doesn't seem like I don't understand something during the job, I use as much ready solutions as I can, however it feels like poking a black box and waiting 30 minutes and trying again to make it work in comparison to more \"developmenty\" tasks when you just write the code and everything works as you suppose and it's easy to track you progress and you \"feel\" the progress or at least constantly in a flow state not waiting for something to happen.", "author_fullname": "t2_2hdao3wq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to do with frustrating tasks when you don't feel the progress or it's very slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ztxgl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696442124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been working as an ML Engineer/Researcher intern for last three months first several months were cool, been writing scripts for cropping images, etc. everything went well. Not so long ago I&amp;#39;ve started working on NLP projects, and been asked to checkout how possible it is to fine-tune sota models on low-end hardware. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve started going through sota models and found some of them which would fit on gpu. However, what I came into is that for 10 different models you usually have 10 different APIs because of quantisation methods, chatlikenes, languages weights etc... &lt;/p&gt;\n\n&lt;p&gt;I have to go through TONS of trial and error WAITING for model weights to load on my pc just to learn that I need to wait MORE to find out that it works at 1tok/eternity or doesn&amp;#39;t work because of mismatch of versions, then you go to Collab go through the same process, get crashes, timeouts, etc., you get the idea. &lt;/p&gt;\n\n&lt;p&gt;What frustrates me the most is the WAITING and NO results. I usually have 27281 pages opened to find the right guide (because half of them result in error and others are outdated because they are 2 seconds old). &lt;/p&gt;\n\n&lt;p&gt;How do I deal with such kind of frustration and not leave state of flow?&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem like I don&amp;#39;t understand something during the job, I use as much ready solutions as I can, however it feels like poking a black box and waiting 30 minutes and trying again to make it work in comparison to more &amp;quot;developmenty&amp;quot; tasks when you just write the code and everything works as you suppose and it&amp;#39;s easy to track you progress and you &amp;quot;feel&amp;quot; the progress or at least constantly in a flow state not waiting for something to happen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ztxgl", "is_robot_indexable": true, "report_reasons": null, "author": "gdl68", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ztxgl/what_to_do_with_frustrating_tasks_when_you_dont/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ztxgl/what_to_do_with_frustrating_tasks_when_you_dont/", "subreddit_subscribers": 1072105, "created_utc": 1696442124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So, a few months ago, I started developing this deep learning model, which was made purely to differentiate whether the input image is driftwood floating in water or a crocodile. To my knowledge, I leveraged the resnet50 pre-trained SoTA model to train my deep learning model, and for that, I downloaded almost 5k images of driftwood and crocodiles for my model training. Once the training was complete, I took the next step and deployed my model on the Hugging Face Spaces app, allowing my friends to put it to the test. But here's where I ran into a significant challenge: users could even upload their own selfies, and my model would attempt to predict whether they were a crocodile or a piece of driftwood!\n\nSo how can I leverage object detection or the image segmentation pipeline so that when the user inputs their image, it tries to detect the object from the photo and then detect whether the detected object from the given image contains a crocodile or not? If the crocodile or driftwood is not found, then it should return \"No object found\" or like that.", "author_fullname": "t2_iygdn4q4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I apply object detection and image segmentation functionality to my current custom-trained Image Classification model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zsikw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696438757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, a few months ago, I started developing this deep learning model, which was made purely to differentiate whether the input image is driftwood floating in water or a crocodile. To my knowledge, I leveraged the resnet50 pre-trained SoTA model to train my deep learning model, and for that, I downloaded almost 5k images of driftwood and crocodiles for my model training. Once the training was complete, I took the next step and deployed my model on the Hugging Face Spaces app, allowing my friends to put it to the test. But here&amp;#39;s where I ran into a significant challenge: users could even upload their own selfies, and my model would attempt to predict whether they were a crocodile or a piece of driftwood!&lt;/p&gt;\n\n&lt;p&gt;So how can I leverage object detection or the image segmentation pipeline so that when the user inputs their image, it tries to detect the object from the photo and then detect whether the detected object from the given image contains a crocodile or not? If the crocodile or driftwood is not found, then it should return &amp;quot;No object found&amp;quot; or like that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zsikw", "is_robot_indexable": true, "report_reasons": null, "author": "meWhoObserves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zsikw/how_can_i_apply_object_detection_and_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zsikw/how_can_i_apply_object_detection_and_image/", "subreddit_subscribers": 1072105, "created_utc": 1696438757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work in consultancy but I'm considering a pivot into data analysis. However, I am worried that companies can easily hire data analysts and scientists in other countries for a lot cheaper whereas consultancy is better protected against this due to the importance of face to face meetings, on site work and local knowledge.\n\nDue to Covid, many companies have learnt how to create remote teams, which may accelerate this change further. \n\nIs this a major risk over the next five to 10 years? Can we expect fewer jobs in The West and lower wages due to outsourcing to other countries and remote working?", "author_fullname": "t2_d3239yyip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you worry that outsourcing will take your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16z8d2e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696378790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in consultancy but I&amp;#39;m considering a pivot into data analysis. However, I am worried that companies can easily hire data analysts and scientists in other countries for a lot cheaper whereas consultancy is better protected against this due to the importance of face to face meetings, on site work and local knowledge.&lt;/p&gt;\n\n&lt;p&gt;Due to Covid, many companies have learnt how to create remote teams, which may accelerate this change further. &lt;/p&gt;\n\n&lt;p&gt;Is this a major risk over the next five to 10 years? Can we expect fewer jobs in The West and lower wages due to outsourcing to other countries and remote working?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16z8d2e", "is_robot_indexable": true, "report_reasons": null, "author": "John198777", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16z8d2e/do_you_worry_that_outsourcing_will_take_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16z8d2e/do_you_worry_that_outsourcing_will_take_your_job/", "subreddit_subscribers": 1072105, "created_utc": 1696378790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Greetings fellow Data Scientists !  \n\n\nI've been finding myself more inclined towards writing lately and I wanted to share something I've been working on for the past weeks.\n\nI recently updated the look and feel of my personal space: **ConsciousML Blog !**\n\nSome updates about the blog:  \n\u25cf Fresh, minimalist design.  \n\u25cf Focusing more on consciousness and productivity - topics close to my heart.  \n\u25cf Starting a newsletter for updates.  \n\u25cf Planning to release one AI or productivity article every week.\n\nThe core purpose is still the same: A space to share my thoughts and my experiences in building machine learning solutions.\n\nCheck it out here: [https://blog.axelmendoza.fr/](https://blog.axelmendoza.fr/)\n\nAppreciate everyone who takes a moment to read. Any feedback to improve the medium would be greatly appreciated !", "author_fullname": "t2_2g16z8fd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ConsciousML Blog - Mindful AI meets Productive Living", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zsvcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696439610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings fellow Data Scientists !  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been finding myself more inclined towards writing lately and I wanted to share something I&amp;#39;ve been working on for the past weeks.&lt;/p&gt;\n\n&lt;p&gt;I recently updated the look and feel of my personal space: &lt;strong&gt;ConsciousML Blog !&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Some updates about the blog:&lt;br/&gt;\n\u25cf Fresh, minimalist design.&lt;br/&gt;\n\u25cf Focusing more on consciousness and productivity - topics close to my heart.&lt;br/&gt;\n\u25cf Starting a newsletter for updates.&lt;br/&gt;\n\u25cf Planning to release one AI or productivity article every week.&lt;/p&gt;\n\n&lt;p&gt;The core purpose is still the same: A space to share my thoughts and my experiences in building machine learning solutions.&lt;/p&gt;\n\n&lt;p&gt;Check it out here: &lt;a href=\"https://blog.axelmendoza.fr/\"&gt;https://blog.axelmendoza.fr/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Appreciate everyone who takes a moment to read. Any feedback to improve the medium would be greatly appreciated !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?auto=webp&amp;s=a4611ff5b3c9f2f57d91203d72196e2292fa2f76", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a7f48b1973a5fc2026b09d80f5bd9c5204cd1c37", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=699419753c65a606178ee7e406e9cb01a1caab93", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60ec9896bb9fa996f5d53e35a4b7f8cafc07b2aa", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4268e1134add01fcd119b076be18ee8170d0adcf", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=64a57403e31a1edc4a7a5e849e049dc974a71c96", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/bq5kmCMB9ofoZ1V-9eqsfPQzLxA8boDfTu3_FhslsNU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fa1de61a612edafa2079254ed9bfb749426972a", "width": 1080, "height": 567}], "variants": {}, "id": "W_XA0S42nt_vH48VBqD6UiKnaAO2GCWnT_-19scpUjE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zsvcp", "is_robot_indexable": true, "report_reasons": null, "author": "GrixisNow", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zsvcp/consciousml_blog_mindful_ai_meets_productive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zsvcp/consciousml_blog_mindful_ai_meets_productive/", "subreddit_subscribers": 1072105, "created_utc": 1696439610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We are in the process of developing a data analytics platform for our client. This platform is primarily built using Python and Dash. We're exploring options to allow our clients to provide comments on each section of the analytics platform containing multiple pages.\n\nDoes anyone know of any methods or tools that would facilitate this interactive feedback mechanism.\n\n&amp;#x200B;\n\nIt would be better if we could track individual user comments. ", "author_fullname": "t2_m0r3pnug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Feedback Mechanism for Our Python/Dash Analytics Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zi29z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696410240.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696409667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are in the process of developing a data analytics platform for our client. This platform is primarily built using Python and Dash. We&amp;#39;re exploring options to allow our clients to provide comments on each section of the analytics platform containing multiple pages.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any methods or tools that would facilitate this interactive feedback mechanism.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It would be better if we could track individual user comments. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zi29z", "is_robot_indexable": true, "report_reasons": null, "author": "Alertt_53", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zi29z/seeking_feedback_mechanism_for_our_pythondash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zi29z/seeking_feedback_mechanism_for_our_pythondash/", "subreddit_subscribers": 1072105, "created_utc": 1696409667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Everyone has problems related to building AI/ML models with data. Maybe it\u2019s collecting data, synthesizing data, training a model, etc.\n\nWhich part of the workflow sucks the most? What solution would you propose?", "author_fullname": "t2_l10ic37ez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a dream solution (software) to fix your problems at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16zbst6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696388038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone has problems related to building AI/ML models with data. Maybe it\u2019s collecting data, synthesizing data, training a model, etc.&lt;/p&gt;\n\n&lt;p&gt;Which part of the workflow sucks the most? What solution would you propose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16zbst6", "is_robot_indexable": true, "report_reasons": null, "author": "Potanee", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16zbst6/what_is_a_dream_solution_software_to_fix_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16zbst6/what_is_a_dream_solution_software_to_fix_your/", "subreddit_subscribers": 1072105, "created_utc": 1696388038.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}