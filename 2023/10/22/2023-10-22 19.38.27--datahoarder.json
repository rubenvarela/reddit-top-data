{"kind": "Listing", "data": {"after": "t3_17dv6sb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_w2ys9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital and Kioxia to Announce Merge This Month: Report", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwqq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_HjNwUP_ErZdeswEWVIkT9veOtbwLgjUkMrqlIofI6E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697990951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?auto=webp&amp;s=0a039feef35030b1ed90389fb053276881b1f6eb", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8045b31dfe1fca2d265b727545f0599704f7dd0", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d282422022ced40625bd6fc43b3653b626c5bdc", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5079d484539d9125775eae8c1c43c7f62583f050", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a01dea74e9b5552cb000be63f5f62341ffc2652e", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=370b433d16b978661a0e2cff60c015bcda96410b", "width": 960, "height": 640}], "variants": {}, "id": "i4fwqt6AascksQGPmda9WPjiAdzc2Ub9m3c54W9xlPQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwqq4", "is_robot_indexable": true, "report_reasons": null, "author": "777fer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwqq4/western_digital_and_kioxia_to_announce_merge_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "subreddit_subscribers": 708000, "created_utc": 1697990951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sry for my english but I am so amazed of how much data we can store.\n\nI feel like, that I can store everything.\n\nBut then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)\n\nLike imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.", "author_fullname": "t2_oq2e3f8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much does it theoretically cost to record your WHOLE life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dtp2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697982502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sry for my english but I am so amazed of how much data we can store.&lt;/p&gt;\n\n&lt;p&gt;I feel like, that I can store everything.&lt;/p&gt;\n\n&lt;p&gt;But then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)&lt;/p&gt;\n\n&lt;p&gt;Like imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dtp2m", "is_robot_indexable": true, "report_reasons": null, "author": "Sorita_", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "subreddit_subscribers": 708000, "created_utc": 1697982502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am basically planning to use a mini pc like an optiplex to use as a media server for Plex and running other stuff to go with it for home media. \n\nI already have a 16TB Ironwolf which I am currently transferring my contents to another 32 OTW. \n\nNow the issue I am facing is since it being a mini pc there is no way to directly attach the drives to the system. \n\nSomeone suggested to buy a Nas, but the Nas prices even 2nd hand here (Thailand) are exceptionally high. Is there anyway else one can connect these drives to build such a home media system? I saw some Orico enclosures but I heard HDDs over USB is a bad idea so I am dropping that thought. \n\nMaybe building a mini Nas? Or some kind of home storage. I am quite lost.", "author_fullname": "t2_155d4nhk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long time (data)hoarder on here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dpof4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697967948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am basically planning to use a mini pc like an optiplex to use as a media server for Plex and running other stuff to go with it for home media. &lt;/p&gt;\n\n&lt;p&gt;I already have a 16TB Ironwolf which I am currently transferring my contents to another 32 OTW. &lt;/p&gt;\n\n&lt;p&gt;Now the issue I am facing is since it being a mini pc there is no way to directly attach the drives to the system. &lt;/p&gt;\n\n&lt;p&gt;Someone suggested to buy a Nas, but the Nas prices even 2nd hand here (Thailand) are exceptionally high. Is there anyway else one can connect these drives to build such a home media system? I saw some Orico enclosures but I heard HDDs over USB is a bad idea so I am dropping that thought. &lt;/p&gt;\n\n&lt;p&gt;Maybe building a mini Nas? Or some kind of home storage. I am quite lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dpof4", "is_robot_indexable": true, "report_reasons": null, "author": "qwertyuiop1158", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dpof4/long_time_datahoarder_on_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dpof4/long_time_datahoarder_on_here/", "subreddit_subscribers": 708000, "created_utc": 1697967948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://github.com/Obscurely/Pbthal-Archive-Manager\n\nI've made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.\n\nAny Instructions you need are in the repo's readme. If you have any questions let me know.\n\nThis is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.", "author_fullname": "t2_38ga09q5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python CLI to make downloading music from PBTHAL's archive easier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dskl2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Obscurely/Pbthal-Archive-Manager\"&gt;https://github.com/Obscurely/Pbthal-Archive-Manager&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.&lt;/p&gt;\n\n&lt;p&gt;Any Instructions you need are in the repo&amp;#39;s readme. If you have any questions let me know.&lt;/p&gt;\n\n&lt;p&gt;This is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?auto=webp&amp;s=9c1d67d21f9fbd196762f514de41d499aec9608b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b1e81a571b06809f8210158292ba9ccebcdd459", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b02a07f7ba574446ff90564afab245b5538e04c7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff993a9b399bad4efecf21deaf72738c6f0d2361", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d6fd13ae82424b7d6ba1481d34196f9ab0c4c14", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b0add97f962e11c0e295390342fd18ecb30321b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56881814dbdb3162653e1bac7374b40d666eb710", "width": 1080, "height": 540}], "variants": {}, "id": "Polxcak_Uw013SprAbMfapj9eIxjp1_aTOvkDEj48iM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dskl2", "is_robot_indexable": true, "report_reasons": null, "author": "CrismarucAdrian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "subreddit_subscribers": 708000, "created_utc": 1697979072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a little under 3tb of porn across 90k+ files. Realistically I've been using a basic folder system to organize things but it makes it difficult to really differentiate or find stuff beyond a certain point. Like I have to make decisions on do I sort things by the website they came from, the performers in question, specific acts/scenes I like in that clip, or what?\n\nI'm already planning on going through and seeing what I can cut out (because, realistically, I don't need this much porn and there's large swathes that I don't even watch) which will help, but previous attempts to get it organized have proven difficult and fruitless. \n\nI've tried using Pornganizer (which, is that even around anymore?) as well as Picasa and also just plain redoing the folder structure. But none of these feel quite as clean or easy to use as I'd like. I do often find myself looking for something specific and not being able to find it, even when I know I have it on my computer.\n\nSo, what do y'all do?", "author_fullname": "t2_mbeizvm17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing porn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17dz7r2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697997703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a little under 3tb of porn across 90k+ files. Realistically I&amp;#39;ve been using a basic folder system to organize things but it makes it difficult to really differentiate or find stuff beyond a certain point. Like I have to make decisions on do I sort things by the website they came from, the performers in question, specific acts/scenes I like in that clip, or what?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already planning on going through and seeing what I can cut out (because, realistically, I don&amp;#39;t need this much porn and there&amp;#39;s large swathes that I don&amp;#39;t even watch) which will help, but previous attempts to get it organized have proven difficult and fruitless. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using Pornganizer (which, is that even around anymore?) as well as Picasa and also just plain redoing the folder structure. But none of these feel quite as clean or easy to use as I&amp;#39;d like. I do often find myself looking for something specific and not being able to find it, even when I know I have it on my computer.&lt;/p&gt;\n\n&lt;p&gt;So, what do y&amp;#39;all do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dz7r2", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantArmy6030", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dz7r2/organizing_porn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dz7r2/organizing_porn/", "subreddit_subscribers": 708000, "created_utc": 1697997703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt's currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don't currently have a backup. I've been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.\n\nI want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I'll have once the hard drives are all upgraded. I don't need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.\n\n...please help!...", "author_fullname": "t2_coe35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading 10Tb NAS to 30Tb, need backup solution.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17du9xi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697984201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt&amp;#39;s currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don&amp;#39;t currently have a backup. I&amp;#39;ve been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I&amp;#39;ll have once the hard drives are all upgraded. I don&amp;#39;t need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.&lt;/p&gt;\n\n&lt;p&gt;...please help!...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17du9xi", "is_robot_indexable": true, "report_reasons": null, "author": "braedan51", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "subreddit_subscribers": 708000, "created_utc": 1697984201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have read at several boards (even here), that people had bad experience with BD-Rs:\n\n\" I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.\n\nOn  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?\"\n\n\"  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  \"\n\netc...\n\nI personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.\n\nI am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!", "author_fullname": "t2_c0w7qv19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does BD-R really have shorter lifespan than DVD-R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dseim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697978519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have read at several boards (even here), that people had bad experience with BD-Rs:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot; I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.&lt;/p&gt;\n\n&lt;p&gt;On  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;etc...&lt;/p&gt;\n\n&lt;p&gt;I personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dseim", "is_robot_indexable": true, "report_reasons": null, "author": "neidhardtzx", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "subreddit_subscribers": 708000, "created_utc": 1697978519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 3TB Internal HDD, and I would like to use my 3TB External HDD as a backup drive.\n\nFor example, I could connect the external hdd once per month, or something like that, and backup the changes. Is there a good tool to do this? I am using Windows.", "author_fullname": "t2_2xftem1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD as Backup for my Internal HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17drevb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697975029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3TB Internal HDD, and I would like to use my 3TB External HDD as a backup drive.&lt;/p&gt;\n\n&lt;p&gt;For example, I could connect the external hdd once per month, or something like that, and backup the changes. Is there a good tool to do this? I am using Windows.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17drevb", "is_robot_indexable": true, "report_reasons": null, "author": "Leonhardt90", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17drevb/external_hdd_as_backup_for_my_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17drevb/external_hdd_as_backup_for_my_internal_hdd/", "subreddit_subscribers": 708000, "created_utc": 1697975029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "While running this:\n\n    # badblocks -b 512 -wsv -o /root/badblocks.txt /dev/sdb\n    Checking for bad blocks in read-write mode\n    From block 0 to 1953525167\n    Testing with pattern 0xaa: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0x55: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0xff: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0x00: done                                                 \n    Reading and comparing: done                                                 \n    Pass completed, 0 bad blocks found. (0/0/0 errors)\n\nThis appears in dmesg:\n\n    [19837.677357] ata5.00: exception Emask 0x0 SAct 0x4000 SErr 0x0 action 0x0\n    [19837.677367] ata5.00: irq_stat 0x40000008\n    [19837.677372] ata5.00: failed command: READ FPDMA QUEUED\n    [19837.677374] ata5.00: cmd 60/00:70:00:2a:11/02:00:38:00:00/40 tag 14 ncq dma 262144 in\n                            res 41/40:00:01:2a:11/00:00:38:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [19837.677383] ata5.00: status: { DRDY ERR }\n    [19837.677386] ata5.00: error: { UNC }\n    [19837.681847] ata5.00: configured for UDMA/133\n    [19837.681867] sd 4:0:0:0: [sdb] tag#14 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [19837.681872] sd 4:0:0:0: [sdb] tag#14 Sense Key : Medium Error [current] \n    [19837.681875] sd 4:0:0:0: [sdb] tag#14 Add. Sense: Unrecovered read error - auto reallocate failed\n    [19837.681879] sd 4:0:0:0: [sdb] tag#14 CDB: Read(10) 28 00 38 11 2a 00 00 02 00 00\n    [19837.681881] I/O error, dev sdb, sector 940648961 op 0x0:(READ) flags 0x800 phys_seg 64 prio class 2\n    [19837.681893] ata5: EH complete\n    [82326.997668] ata5.00: exception Emask 0x0 SAct 0x40000 SErr 0x0 action 0x0\n    [82326.997678] ata5.00: irq_stat 0x40000008\n    [82326.997684] ata5.00: failed command: READ FPDMA QUEUED\n    [82326.997686] ata5.00: cmd 60/40:90:c0:9d:46/00:00:4a:00:00/40 tag 18 ncq dma 32768 in\n                            res 41/40:00:fd:9d:46/00:00:4a:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [82326.997695] ata5.00: status: { DRDY ERR }\n    [82326.997698] ata5.00: error: { UNC }\n    [82327.003076] ata5.00: configured for UDMA/133\n    [82327.003096] sd 4:0:0:0: [sdb] tag#18 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [82327.003101] sd 4:0:0:0: [sdb] tag#18 Sense Key : Medium Error [current] \n    [82327.003104] sd 4:0:0:0: [sdb] tag#18 Add. Sense: Unrecovered read error - auto reallocate failed\n    [82327.003108] sd 4:0:0:0: [sdb] tag#18 CDB: Read(10) 28 00 4a 46 9d c0 00 00 40 00\n    [82327.003110] I/O error, dev sdb, sector 1246141949 op 0x0:(READ) flags 0x800 phys_seg 1 prio class 2\n    [82327.003125] ata5: EH complete\n    [82419.252924] ata5.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x0\n    [82419.252934] ata5.00: irq_stat 0x40000008\n    [82419.252940] ata5.00: failed command: READ FPDMA QUEUED\n    [82419.252942] ata5.00: cmd 60/40:98:c0:92:94/00:00:4a:00:00/40 tag 19 ncq dma 32768 in\n                            res 41/40:00:c3:92:94/00:00:4a:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [82419.252951] ata5.00: status: { DRDY ERR }\n    [82419.252954] ata5.00: error: { UNC }\n    [82419.258166] ata5.00: configured for UDMA/133\n    [82419.258185] sd 4:0:0:0: [sdb] tag#19 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [82419.258190] sd 4:0:0:0: [sdb] tag#19 Sense Key : Medium Error [current] \n    [82419.258193] sd 4:0:0:0: [sdb] tag#19 Add. Sense: Unrecovered read error - auto reallocate failed\n    [82419.258197] sd 4:0:0:0: [sdb] tag#19 CDB: Read(10) 28 00 4a 94 92 c0 00 00 40 00\n    [82419.258199] I/O error, dev sdb, sector 1251250883 op 0x0:(READ) flags 0x800 phys_seg 8 prio class 2\n    [82419.258213] ata5: EH complete\n\nI don't know who to trust.", "author_fullname": "t2_aox1z74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This isn't making any sense. (badblocks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17djsic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697944186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While running this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# badblocks -b 512 -wsv -o /root/badblocks.txt /dev/sdb\nChecking for bad blocks in read-write mode\nFrom block 0 to 1953525167\nTesting with pattern 0xaa: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0x55: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0xff: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0x00: done                                                 \nReading and comparing: done                                                 \nPass completed, 0 bad blocks found. (0/0/0 errors)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This appears in dmesg:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[19837.677357] ata5.00: exception Emask 0x0 SAct 0x4000 SErr 0x0 action 0x0\n[19837.677367] ata5.00: irq_stat 0x40000008\n[19837.677372] ata5.00: failed command: READ FPDMA QUEUED\n[19837.677374] ata5.00: cmd 60/00:70:00:2a:11/02:00:38:00:00/40 tag 14 ncq dma 262144 in\n                        res 41/40:00:01:2a:11/00:00:38:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[19837.677383] ata5.00: status: { DRDY ERR }\n[19837.677386] ata5.00: error: { UNC }\n[19837.681847] ata5.00: configured for UDMA/133\n[19837.681867] sd 4:0:0:0: [sdb] tag#14 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[19837.681872] sd 4:0:0:0: [sdb] tag#14 Sense Key : Medium Error [current] \n[19837.681875] sd 4:0:0:0: [sdb] tag#14 Add. Sense: Unrecovered read error - auto reallocate failed\n[19837.681879] sd 4:0:0:0: [sdb] tag#14 CDB: Read(10) 28 00 38 11 2a 00 00 02 00 00\n[19837.681881] I/O error, dev sdb, sector 940648961 op 0x0:(READ) flags 0x800 phys_seg 64 prio class 2\n[19837.681893] ata5: EH complete\n[82326.997668] ata5.00: exception Emask 0x0 SAct 0x40000 SErr 0x0 action 0x0\n[82326.997678] ata5.00: irq_stat 0x40000008\n[82326.997684] ata5.00: failed command: READ FPDMA QUEUED\n[82326.997686] ata5.00: cmd 60/40:90:c0:9d:46/00:00:4a:00:00/40 tag 18 ncq dma 32768 in\n                        res 41/40:00:fd:9d:46/00:00:4a:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[82326.997695] ata5.00: status: { DRDY ERR }\n[82326.997698] ata5.00: error: { UNC }\n[82327.003076] ata5.00: configured for UDMA/133\n[82327.003096] sd 4:0:0:0: [sdb] tag#18 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[82327.003101] sd 4:0:0:0: [sdb] tag#18 Sense Key : Medium Error [current] \n[82327.003104] sd 4:0:0:0: [sdb] tag#18 Add. Sense: Unrecovered read error - auto reallocate failed\n[82327.003108] sd 4:0:0:0: [sdb] tag#18 CDB: Read(10) 28 00 4a 46 9d c0 00 00 40 00\n[82327.003110] I/O error, dev sdb, sector 1246141949 op 0x0:(READ) flags 0x800 phys_seg 1 prio class 2\n[82327.003125] ata5: EH complete\n[82419.252924] ata5.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x0\n[82419.252934] ata5.00: irq_stat 0x40000008\n[82419.252940] ata5.00: failed command: READ FPDMA QUEUED\n[82419.252942] ata5.00: cmd 60/40:98:c0:92:94/00:00:4a:00:00/40 tag 19 ncq dma 32768 in\n                        res 41/40:00:c3:92:94/00:00:4a:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[82419.252951] ata5.00: status: { DRDY ERR }\n[82419.252954] ata5.00: error: { UNC }\n[82419.258166] ata5.00: configured for UDMA/133\n[82419.258185] sd 4:0:0:0: [sdb] tag#19 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[82419.258190] sd 4:0:0:0: [sdb] tag#19 Sense Key : Medium Error [current] \n[82419.258193] sd 4:0:0:0: [sdb] tag#19 Add. Sense: Unrecovered read error - auto reallocate failed\n[82419.258197] sd 4:0:0:0: [sdb] tag#19 CDB: Read(10) 28 00 4a 94 92 c0 00 00 40 00\n[82419.258199] I/O error, dev sdb, sector 1251250883 op 0x0:(READ) flags 0x800 phys_seg 8 prio class 2\n[82419.258213] ata5: EH complete\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I don&amp;#39;t know who to trust.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17djsic", "is_robot_indexable": true, "report_reasons": null, "author": "ppw0", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17djsic/this_isnt_making_any_sense_badblocks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17djsic/this_isnt_making_any_sense_badblocks/", "subreddit_subscribers": 708000, "created_utc": 1697944186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the place to ask. I am building a raid 5 array 4x8tb drives, for photo and original video clip storage. \nI have this yottomaster enclosure and segate drives.\n\nNeed Mac compatible, usb type c \n\nhttps://a.co/d/jlIF2bv\n\nhttps://a.co/d/4r9ppFN\n\nIs this a solid build for the price?", "author_fullname": "t2_ehdh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware recs for a 4 Drive raid 5 Array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17dc32r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/eCjWpwfol_zWFHa5hAxo39bI9ZWvlloblxnIlf5eZf4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697921408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the place to ask. I am building a raid 5 array 4x8tb drives, for photo and original video clip storage. \nI have this yottomaster enclosure and segate drives.&lt;/p&gt;\n\n&lt;p&gt;Need Mac compatible, usb type c &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://a.co/d/jlIF2bv\"&gt;https://a.co/d/jlIF2bv&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://a.co/d/4r9ppFN\"&gt;https://a.co/d/4r9ppFN&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is this a solid build for the price?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jq25ay13cmvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?auto=webp&amp;s=e71d8664b9a4b66c4c3294a668e61608f24568fc", "width": 1169, "height": 1608}, "resolutions": [{"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a84b9bf2d547eb8b3fb1d4437671e4dc65283982", "width": 108, "height": 148}, {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c09d290338c143d7bfca62b1e459ba1ca8a5b09d", "width": 216, "height": 297}, {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6b9dc359563148c92ef706e380b8565e6ae8190", "width": 320, "height": 440}, {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82e22999ef67f460b89ee89bfb86224f06b2e60a", "width": 640, "height": 880}, {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb57b1db853cae2186e5996341a31ff8981a2c21", "width": 960, "height": 1320}, {"url": "https://preview.redd.it/jq25ay13cmvb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d55d34a8b8795591af9842fe3e0aa451e7eb799", "width": 1080, "height": 1485}], "variants": {}, "id": "_nscSB9-JOuDiY1YCGzBtz2rcRw7nWpKNEAX4IndTGg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dc32r", "is_robot_indexable": true, "report_reasons": null, "author": "brendan2015", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dc32r/hardware_recs_for_a_4_drive_raid_5_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jq25ay13cmvb1.jpg", "subreddit_subscribers": 708000, "created_utc": 1697921408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nI recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  \n\n\nIn specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.\n\n&amp;#x200B;\n\nNow my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don't touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?\n\n&amp;#x200B;\n\nResources for SSD data loss:\n\n[https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will\\_ssd\\_lose\\_data\\_if\\_left\\_unpowered\\_for\\_extended/](https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/)\n\n[https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd\\_lose\\_data\\_wo\\_power\\_in\\_a\\_year\\_myth\\_or\\_truth/](https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/)\n\n[https://www.reddit.com/r/DataHoarder/comments/10fn86x/do\\_ssd\\_drives\\_really\\_lose\\_data\\_if\\_left\\_unpowered/](https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/)\n\n  \nThanks a lot for your expert opinion.", "author_fullname": "t2_491r3ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you mitigate/avoid SSD data loss?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17dzg27", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697998306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  &lt;/p&gt;\n\n&lt;p&gt;In specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don&amp;#39;t touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Resources for SSD data loss:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/\"&gt;https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/\"&gt;https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your expert opinion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dzg27", "is_robot_indexable": true, "report_reasons": null, "author": "ghac101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "subreddit_subscribers": 708000, "created_utc": 1697998306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.\n\nI initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.", "author_fullname": "t2_1gvgwz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video files on USB not opening", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dy2zb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697994645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dy2zb", "is_robot_indexable": true, "report_reasons": null, "author": "Parker813", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "subreddit_subscribers": 708000, "created_utc": 1697994645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "tl;dr is in the title already...\n\n- My family has Windows machines and I need to backup them. \n- The most frequently recommended tool is Veeam.\n- The backup target I am trying to use is a Hetzner storage box\n- Veeam \"free community edition for Windows\" can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.\n- However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). \n\nSeems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.\n\nAm I missing something? Has Samba become more secure in the recent years?", "author_fullname": "t2_vqur7zf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Machines Backup - everyone recommends Veeam - but the only protocol available is Samba, should I use this over the open internet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwmpz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697990645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr is in the title already...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My family has Windows machines and I need to backup them. &lt;/li&gt;\n&lt;li&gt;The most frequently recommended tool is Veeam.&lt;/li&gt;\n&lt;li&gt;The backup target I am trying to use is a Hetzner storage box&lt;/li&gt;\n&lt;li&gt;Veeam &amp;quot;free community edition for Windows&amp;quot; can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.&lt;/li&gt;\n&lt;li&gt;However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Seems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Has Samba become more secure in the recent years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwmpz", "is_robot_indexable": true, "report_reasons": null, "author": "AlpineGuy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "subreddit_subscribers": 708000, "created_utc": 1697990645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# TLDR:\n\n***I have a python program for downloading twitch streams and I'm looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!***\n\n...\n\nBack when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.^(1)\n\nThis program was created solely to archive a single streamer's content and, if i wanted to archive someone else's, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I've been trying to get back into archiving Twitch streams again, but now there's more than one streamer I'd like an archive for.\n\nWhen I searched on Google, however, it seems as though yt-dlp doesn't support this via normal command line usage and so I'll have to come up with a solution in python instead.\n\nCurrently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.\n\n**I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here's those solutions:**\n\n* **Brute force:** just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I'd like to avoid it at all cost.\n* **Add multi-threading to current script:** while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven't done any testing, but I assume I'd run into some kind of issue here.\n* **Add multi-threading + a ffmpeg library:** while this also seems like a good solution, I'm once again unsure if the ffmpeg library would solve those issues with these threads all using it.\n\n...\n\n1. *My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.*", "author_fullname": "t2_vjs2k4um", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using yt-dlp + Python to Archive Multiple Twitch Streams Simultaneously", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dv9a9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697986939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;TLDR:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I have a python program for downloading twitch streams and I&amp;#39;m looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;p&gt;Back when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;This program was created solely to archive a single streamer&amp;#39;s content and, if i wanted to archive someone else&amp;#39;s, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I&amp;#39;ve been trying to get back into archiving Twitch streams again, but now there&amp;#39;s more than one streamer I&amp;#39;d like an archive for.&lt;/p&gt;\n\n&lt;p&gt;When I searched on Google, however, it seems as though yt-dlp doesn&amp;#39;t support this via normal command line usage and so I&amp;#39;ll have to come up with a solution in python instead.&lt;/p&gt;\n\n&lt;p&gt;Currently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here&amp;#39;s those solutions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Brute force:&lt;/strong&gt; just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I&amp;#39;d like to avoid it at all cost.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading to current script:&lt;/strong&gt; while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven&amp;#39;t done any testing, but I assume I&amp;#39;d run into some kind of issue here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading + a ffmpeg library:&lt;/strong&gt; while this also seems like a good solution, I&amp;#39;m once again unsure if the ffmpeg library would solve those issues with these threads all using it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dv9a9", "is_robot_indexable": true, "report_reasons": null, "author": "GothicMutt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "subreddit_subscribers": 708000, "created_utc": 1697986939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I'm looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a \"drive\" or a \"sync\" feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?", "author_fullname": "t2_sh0pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Storage Service with Shared Access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dss1a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I&amp;#39;m looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a &amp;quot;drive&amp;quot; or a &amp;quot;sync&amp;quot; feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dss1a", "is_robot_indexable": true, "report_reasons": null, "author": "WeWillFigureItOut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "subreddit_subscribers": 708000, "created_utc": 1697979728.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I have been figuring out for hours how to mirror the entirety of some very complicated websites such as [this](https://www.aten7.com/),  [this](https://www.sprite.com/zerolimits), and [this](https://kprverse.com/). I tried to use wget and HTTrack and obviously is not working.\n\nI have heard about [Offline Explorer](https://metaproducts.com/products/offline-explorer) but it is pricey. How should I go about this or is it just impossible since it is too complicated?", "author_fullname": "t2_e3bkxcn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring a very complex website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17drfx8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697975142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been figuring out for hours how to mirror the entirety of some very complicated websites such as &lt;a href=\"https://www.aten7.com/\"&gt;this&lt;/a&gt;,  &lt;a href=\"https://www.sprite.com/zerolimits\"&gt;this&lt;/a&gt;, and &lt;a href=\"https://kprverse.com/\"&gt;this&lt;/a&gt;. I tried to use wget and HTTrack and obviously is not working.&lt;/p&gt;\n\n&lt;p&gt;I have heard about &lt;a href=\"https://metaproducts.com/products/offline-explorer\"&gt;Offline Explorer&lt;/a&gt; but it is pricey. How should I go about this or is it just impossible since it is too complicated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?auto=webp&amp;s=019fe2a6bebec7b7501a65412fc5a85171d4770c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87b01fedeea8b795318a4882c2a4f6970676f5e2", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b0680511a543384ddb6d1cf5b7292e7d25796e0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a44c9d2b1df85a9da679573c4111606cf093fb83", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=53f0b20d451ec30506a6dc85f293ce6a39d2ed0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b09996b49469e80858cae90db5ebf424c56f779c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d98591ca3ed0593040f1869fd4a0881e6631e8b", "width": 1080, "height": 567}], "variants": {}, "id": "Ri93iwPEsuQ9QqVgtArPKG-9kfxAbbyT82muhhicR04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17drfx8", "is_robot_indexable": true, "report_reasons": null, "author": "-Meme-Lord-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17drfx8/mirroring_a_very_complex_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17drfx8/mirroring_a_very_complex_website/", "subreddit_subscribers": 708000, "created_utc": 1697975142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The title. But it's showing up on YouTube. But when it comes to Facebook videos, it's not showing up. Can anyone help?", "author_fullname": "t2_6hchm85h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Idm download panel not showing up in facebook.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17do6wn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697961395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title. But it&amp;#39;s showing up on YouTube. But when it comes to Facebook videos, it&amp;#39;s not showing up. Can anyone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17do6wn", "is_robot_indexable": true, "report_reasons": null, "author": "abrisham200", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17do6wn/idm_download_panel_not_showing_up_in_facebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17do6wn/idm_download_panel_not_showing_up_in_facebook/", "subreddit_subscribers": 708000, "created_utc": 1697961395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've picked up a couple of SAS drives for pretty cheap, but don't really know the most effective way to use them. I don't have any spare PCIe slots on my machine, and tbh no real idea what I'm doing. I kinda know about HBAs and I could buy a PCIe splitter, or are there cables for mini-SBA to the SATA ports on my motherboard?", "author_fullname": "t2_q13ihcw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest way to use SAS drives in a personal machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dijos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697940160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve picked up a couple of SAS drives for pretty cheap, but don&amp;#39;t really know the most effective way to use them. I don&amp;#39;t have any spare PCIe slots on my machine, and tbh no real idea what I&amp;#39;m doing. I kinda know about HBAs and I could buy a PCIe splitter, or are there cables for mini-SBA to the SATA ports on my motherboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dijos", "is_robot_indexable": true, "report_reasons": null, "author": "DirpyToes1315", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dijos/cheapest_way_to_use_sas_drives_in_a_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dijos/cheapest_way_to_use_sas_drives_in_a_personal/", "subreddit_subscribers": 708000, "created_utc": 1697940160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! \n\nI'm currently recovering from two failed hard drives (I'll have trust issues with Seagate going forward). Over the years I have a bunch of items scattered about between Google Drive, Dropbox, other non-failed hard drives, and currently a 10 year-old Macbook Pro that's at capacity. Basically, I need to offload and organize my stuff ASAP. \n\nI'm thinking both short and long-term coming up with a solution for my personal storage. I hardly use my personal computer for anything besides personal travel photo-editing. I'll log on a couple times a year to download vacation photos onto my computer, edit, usually print a few or make into a photo book, and be done for the remaining 98% of the year. I shoot my photos in RAW so they're larger file sizes. I also take a good amount of photos on my iPhone. Between the hard drive I'm attempting to restore and my Macbook, I'm probably sitting at around 2 Tb. Hard drive was 2Tb and maybe 50% full, Macbook is 500 gb. I have about 18,000 personal photos from my camera and at least 6,000 photos on my phone (that I should definitely go through)\n\nFor home/personal use, I'm pretty ingrained into the Apple ecosystem (Macbook, iPhone, iPad) so iCloud+/drive seems a natural choice. However, most of my personal storage over the years from college to present has been through Google and Google Drive. I've also read mixed reviews on iCloud so this casts some doubt for me.\n\nShort-term: I'm thinking of just data dumping almost everything onto the cloud to organize and temporarily secure my important stuff. Right now my laptop is my only source of archive for a lot of stuff and it's giving me anxiety. \n\nLong-term: I'd like to use the cloud as an archive where I can drop my photos in addition to copies on two local hard drives. I've done some investigating into NAS systems and it's just feeling a little overkill for my personal needs. I'll be looking into getting a couple of physical hard drives over black Friday sales for archiving.\n\nAnyway, my head is swimming and I've been over-researching for a week now and I need to just make a decision to secure my stuff. There probably isn't a wrong answer when you're desperate, but I want to make sure I set up/invest/organize for the long term. \n\nMost of what I've seen is for professionals or people with tons of data and millions of photos and I'm just not there and would probably spend way too much in short and long-term operating costs. \n\nWhat are your opinions? Thank you so much in advance!", "author_fullname": "t2_45fjfbk4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage for Personal Use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dgvbg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697934864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently recovering from two failed hard drives (I&amp;#39;ll have trust issues with Seagate going forward). Over the years I have a bunch of items scattered about between Google Drive, Dropbox, other non-failed hard drives, and currently a 10 year-old Macbook Pro that&amp;#39;s at capacity. Basically, I need to offload and organize my stuff ASAP. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking both short and long-term coming up with a solution for my personal storage. I hardly use my personal computer for anything besides personal travel photo-editing. I&amp;#39;ll log on a couple times a year to download vacation photos onto my computer, edit, usually print a few or make into a photo book, and be done for the remaining 98% of the year. I shoot my photos in RAW so they&amp;#39;re larger file sizes. I also take a good amount of photos on my iPhone. Between the hard drive I&amp;#39;m attempting to restore and my Macbook, I&amp;#39;m probably sitting at around 2 Tb. Hard drive was 2Tb and maybe 50% full, Macbook is 500 gb. I have about 18,000 personal photos from my camera and at least 6,000 photos on my phone (that I should definitely go through)&lt;/p&gt;\n\n&lt;p&gt;For home/personal use, I&amp;#39;m pretty ingrained into the Apple ecosystem (Macbook, iPhone, iPad) so iCloud+/drive seems a natural choice. However, most of my personal storage over the years from college to present has been through Google and Google Drive. I&amp;#39;ve also read mixed reviews on iCloud so this casts some doubt for me.&lt;/p&gt;\n\n&lt;p&gt;Short-term: I&amp;#39;m thinking of just data dumping almost everything onto the cloud to organize and temporarily secure my important stuff. Right now my laptop is my only source of archive for a lot of stuff and it&amp;#39;s giving me anxiety. &lt;/p&gt;\n\n&lt;p&gt;Long-term: I&amp;#39;d like to use the cloud as an archive where I can drop my photos in addition to copies on two local hard drives. I&amp;#39;ve done some investigating into NAS systems and it&amp;#39;s just feeling a little overkill for my personal needs. I&amp;#39;ll be looking into getting a couple of physical hard drives over black Friday sales for archiving.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my head is swimming and I&amp;#39;ve been over-researching for a week now and I need to just make a decision to secure my stuff. There probably isn&amp;#39;t a wrong answer when you&amp;#39;re desperate, but I want to make sure I set up/invest/organize for the long term. &lt;/p&gt;\n\n&lt;p&gt;Most of what I&amp;#39;ve seen is for professionals or people with tons of data and millions of photos and I&amp;#39;m just not there and would probably spend way too much in short and long-term operating costs. &lt;/p&gt;\n\n&lt;p&gt;What are your opinions? Thank you so much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dgvbg", "is_robot_indexable": true, "report_reasons": null, "author": "rampaige0191", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dgvbg/cloud_storage_for_personal_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dgvbg/cloud_storage_for_personal_use/", "subreddit_subscribers": 708000, "created_utc": 1697934864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was trying to move some files from older hdd ended up losing some existing data on my newer hdd. How do I know which files got deleted? \n\nTeracopy replace older files option apparently deleted my existing file because I didn't have much space but I accidentally clicked teracopy replace older files instead of the teracopy only option", "author_fullname": "t2_sp80qr89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accidentally lost some data using Teracopy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dgkcr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697933915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to move some files from older hdd ended up losing some existing data on my newer hdd. How do I know which files got deleted? &lt;/p&gt;\n\n&lt;p&gt;Teracopy replace older files option apparently deleted my existing file because I didn&amp;#39;t have much space but I accidentally clicked teracopy replace older files instead of the teracopy only option&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dgkcr", "is_robot_indexable": true, "report_reasons": null, "author": "friesianic", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dgkcr/accidentally_lost_some_data_using_teracopy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dgkcr/accidentally_lost_some_data_using_teracopy/", "subreddit_subscribers": 708000, "created_utc": 1697933915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Folks,\n\n  \nSorry if this is an answered question but searching for it yields lots of similar but not the same queries. When friends and I go on group holidays or trips we setup a shared album and everyone throws stuff in there as we go. Works great. Two of us carry full bodies and those shots go in to. \n\nSo in the end I'll have a big album that's about 1/3 my shots and 2/3 others but using google's compression option for the higher res shots. I can download all of them and dedupe them on the basis of filename (as they wont be bit for bit copies) but that seems crude. \n\nAny way to download just other accounts contributed content? Dont want to trust Google long term ([https://killedbygoogle.com/](https://killedbygoogle.com/)) \n\nThanks", "author_fullname": "t2_hb8i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pulling the photos I didn't contribute to a shared GPhoto album", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17deir1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697927972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is an answered question but searching for it yields lots of similar but not the same queries. When friends and I go on group holidays or trips we setup a shared album and everyone throws stuff in there as we go. Works great. Two of us carry full bodies and those shots go in to. &lt;/p&gt;\n\n&lt;p&gt;So in the end I&amp;#39;ll have a big album that&amp;#39;s about 1/3 my shots and 2/3 others but using google&amp;#39;s compression option for the higher res shots. I can download all of them and dedupe them on the basis of filename (as they wont be bit for bit copies) but that seems crude. &lt;/p&gt;\n\n&lt;p&gt;Any way to download just other accounts contributed content? Dont want to trust Google long term (&lt;a href=\"https://killedbygoogle.com/\"&gt;https://killedbygoogle.com/&lt;/a&gt;) &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?auto=webp&amp;s=089c2cb85db8b48f1d11ebe90af233289ae75ffb", "width": 2588, "height": 1358}, "resolutions": [{"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53fc68e79ef5b2b6cf9467c5635f95adb6b2510e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce7f5ae18b6d282aea9cc333bdc493a4c7007660", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f119a807be9f773fc21f5df6620ea43d6373307f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=357506973b827d3ceabb04b3d1cca52d2f5d93eb", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b82ea2dca2b2a5f28d8a1be851465a4add79a0e", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/UVA4Cef4bqC2qzs4Pwx-Q4Ja8Pd3WrnykIbD4uOsvMI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=180a926785cc4b16a9829cb2f777f2952dc5bf39", "width": 1080, "height": 566}], "variants": {}, "id": "dljkuLtHskZlCoKMzIVxP6pF4MrWsBb0rmz1ZL7TKwA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "17TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17deir1", "is_robot_indexable": true, "report_reasons": null, "author": "linef4ult", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17deir1/pulling_the_photos_i_didnt_contribute_to_a_shared/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17deir1/pulling_the_photos_i_didnt_contribute_to_a_shared/", "subreddit_subscribers": 708000, "created_utc": 1697927972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm just testing idrive service and maybe I'm missing the point but it looks that anyone who gain access to [idrive.com](https://idrive.com) is able to get any file from any local drive, I mean any file. And even there're no notification that I should enable 2FA. And this mean that some employees could get access as well\n\nEdit. I've Looked deeper, ok yet the attacker need access to email account to read verification code. I thought it would be possible to restore data to Cloud but it my ip is trusted for such operation", "author_fullname": "t2_1saeau6c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "idrive security concern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dyi5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697997476.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697995802.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just testing idrive service and maybe I&amp;#39;m missing the point but it looks that anyone who gain access to &lt;a href=\"https://idrive.com\"&gt;idrive.com&lt;/a&gt; is able to get any file from any local drive, I mean any file. And even there&amp;#39;re no notification that I should enable 2FA. And this mean that some employees could get access as well&lt;/p&gt;\n\n&lt;p&gt;Edit. I&amp;#39;ve Looked deeper, ok yet the attacker need access to email account to read verification code. I thought it would be possible to restore data to Cloud but it my ip is trusted for such operation&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/h6uJt0evpKOhl3WemqCG2vhuXH6Jm3NpY0pphUEj6pI.jpg?auto=webp&amp;s=791d1351cf4c3b29ed0f66dbbbd51abb55e3c695", "width": 250, "height": 250}, "resolutions": [{"url": "https://external-preview.redd.it/h6uJt0evpKOhl3WemqCG2vhuXH6Jm3NpY0pphUEj6pI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11f90548a78f58828523a0aded4a09ff305313ae", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/h6uJt0evpKOhl3WemqCG2vhuXH6Jm3NpY0pphUEj6pI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef3bdeaa2f490bd0b2cc9909bc65584a596db1a1", "width": 216, "height": 216}], "variants": {}, "id": "fUEa_soLuQhREMOv0chqm8-KuP0XPZiDd61vlfwDZVo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dyi5b", "is_robot_indexable": true, "report_reasons": null, "author": "FarBuffalo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dyi5b/idrive_security_concern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dyi5b/idrive_security_concern/", "subreddit_subscribers": 708000, "created_utc": 1697995802.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\nThe conflict being what it is over there has the US media in a frenzy. There's a lot of pointing fingers and little video evidence. I believe it was on this sub that someone posted a link to a bunch of videos from Jan 6... Does someone have a hoard of videos from Israel/Gaza/West Bank?", "author_fullname": "t2_7xicfc09", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Israel Conflict Videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dy6y4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697994941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;The conflict being what it is over there has the US media in a frenzy. There&amp;#39;s a lot of pointing fingers and little video evidence. I believe it was on this sub that someone posted a link to a bunch of videos from Jan 6... Does someone have a hoard of videos from Israel/Gaza/West Bank?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dy6y4", "is_robot_indexable": true, "report_reasons": null, "author": "FringeActual", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dy6y4/israel_conflict_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dy6y4/israel_conflict_videos/", "subreddit_subscribers": 708000, "created_utc": 1697994941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, sorry if this breaks any rules, joined this sub just now to try help my wife. \n\nMy wife has found some old dvds and cds with family photos on but our laptop doesn't recognise most of the disks.  They look clean and scratch free but they're from 2007/2008 so I'm wondering if it would work on an older machine/ older version of Windows. \n\nDoes anyone here have experience getting photos off old discs like this and have any advice of how to get this sorted?  Again sorry if this breaks any rules but I would appreciate any help. \n\nThanks", "author_fullname": "t2_ba95dv7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get photos off really old dvds and cds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dx6wx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697992186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, sorry if this breaks any rules, joined this sub just now to try help my wife. &lt;/p&gt;\n\n&lt;p&gt;My wife has found some old dvds and cds with family photos on but our laptop doesn&amp;#39;t recognise most of the disks.  They look clean and scratch free but they&amp;#39;re from 2007/2008 so I&amp;#39;m wondering if it would work on an older machine/ older version of Windows. &lt;/p&gt;\n\n&lt;p&gt;Does anyone here have experience getting photos off old discs like this and have any advice of how to get this sorted?  Again sorry if this breaks any rules but I would appreciate any help. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dx6wx", "is_robot_indexable": true, "report_reasons": null, "author": "CMSeddon", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dx6wx/how_to_get_photos_off_really_old_dvds_and_cds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dx6wx/how_to_get_photos_off_really_old_dvds_and_cds/", "subreddit_subscribers": 708000, "created_utc": 1697992186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried Googling and not much can be seen on the ES18's passthrough capability.", "author_fullname": "t2_2ufw7ltl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the DMR-ES18 similar to the ES15?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dv6sb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697986768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried Googling and not much can be seen on the ES18&amp;#39;s passthrough capability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dv6sb", "is_robot_indexable": true, "report_reasons": null, "author": "Pandaemonae0n", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dv6sb/is_the_dmres18_similar_to_the_es15/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dv6sb/is_the_dmres18_similar_to_the_es15/", "subreddit_subscribers": 708000, "created_utc": 1697986768.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}