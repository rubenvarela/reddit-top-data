{"kind": "Listing", "data": {"after": "t3_171qzke", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it simply because it's easier to write:\ns.column\nthan it is to write:\nsomeDescriptiveTableAlias.column\n\nI mean haven't you ever heard of multiline editing... or tab...\n\nYesterday I spent like 20 minutes trying to figure out what \"wom\" meant. Turns out it stood for weekOfMonth... why not just use weekOfMonth??", "author_fullname": "t2_1r24entl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do data engineers use such short and ambiguous variable/alias names in SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171pbro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696630203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it simply because it&amp;#39;s easier to write:\ns.column\nthan it is to write:\nsomeDescriptiveTableAlias.column&lt;/p&gt;\n\n&lt;p&gt;I mean haven&amp;#39;t you ever heard of multiline editing... or tab...&lt;/p&gt;\n\n&lt;p&gt;Yesterday I spent like 20 minutes trying to figure out what &amp;quot;wom&amp;quot; meant. Turns out it stood for weekOfMonth... why not just use weekOfMonth??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171pbro", "is_robot_indexable": true, "report_reasons": null, "author": "AncientElevator9", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171pbro/why_do_data_engineers_use_such_short_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171pbro/why_do_data_engineers_use_such_short_and/", "subreddit_subscribers": 132509, "created_utc": 1696630203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a senior DE at a major media streaming company, and am being put up for promotion to become a staff DE.\n\nStarted my career as an accountant and in accounting, the progression is staff accountant -&gt; senior accountant -&gt; accounting manager. There are several other professions that follow that path as well. \n\nWhenever I tell my family \u201cI might get promoted to staff engineer!\u201d They act confused and say it sounds like a demotion \ud83d\ude02\n\nDoes anyone have any idea how/why staff is higher than senior for engineering roles??", "author_fullname": "t2_25bhk0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is staff higher than senior?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171h49k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696610377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a senior DE at a major media streaming company, and am being put up for promotion to become a staff DE.&lt;/p&gt;\n\n&lt;p&gt;Started my career as an accountant and in accounting, the progression is staff accountant -&amp;gt; senior accountant -&amp;gt; accounting manager. There are several other professions that follow that path as well. &lt;/p&gt;\n\n&lt;p&gt;Whenever I tell my family \u201cI might get promoted to staff engineer!\u201d They act confused and say it sounds like a demotion \ud83d\ude02&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any idea how/why staff is higher than senior for engineering roles??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171h49k", "is_robot_indexable": true, "report_reasons": null, "author": "scranice3", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171h49k/why_is_staff_higher_than_senior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171h49k/why_is_staff_higher_than_senior/", "subreddit_subscribers": 132509, "created_utc": 1696610377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m thinking of taking part of a Data Engineering boot camp. I work as a BI developer and have a pretty solid foundation in relational databases, specifically SQL Server.\n\nI\u2019m hoping practicing Data Engineer\u2019s will provide thoughts/ a review of the boot camp based on the syllabus image I\u2019ve attached.\n\nImmediate questions that come to mind:\n\n* Is the tech stack covered in high demand?\n* Are you currently working with any of these tools or would you advise that it\u2019s for a niche role? Maybe the tech stack is too general.\n\nAny thoughts, feedback, or advice is appreciated.", "author_fullname": "t2_vjz1hul6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking input on a DE boot camp from practicing DE\u2019s.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_171eoye", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tP4XyebbNWuwtZDXXfk3QtpoiJIeq4E1V6JUbV12jdU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696604708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m thinking of taking part of a Data Engineering boot camp. I work as a BI developer and have a pretty solid foundation in relational databases, specifically SQL Server.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m hoping practicing Data Engineer\u2019s will provide thoughts/ a review of the boot camp based on the syllabus image I\u2019ve attached.&lt;/p&gt;\n\n&lt;p&gt;Immediate questions that come to mind:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is the tech stack covered in high demand?&lt;/li&gt;\n&lt;li&gt;Are you currently working with any of these tools or would you advise that it\u2019s for a niche role? Maybe the tech stack is too general.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any thoughts, feedback, or advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fb1b310vklsb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?auto=webp&amp;s=504df79d4330bd3a494efc5354b254ff9cad5b4d", "width": 795, "height": 1442}, "resolutions": [{"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad0c25dc490e8f029d618fdcc4bd974092e8577e", "width": 108, "height": 195}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bde5d0f22aae297620fe32a65071ca11f093370", "width": 216, "height": 391}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de747a7722aff5905fe4de274296a552c4369a98", "width": 320, "height": 580}, {"url": "https://preview.redd.it/fb1b310vklsb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cd56a2014272e1d42f57d7a10c73f58899ed8c1", "width": 640, "height": 1160}], "variants": {}, "id": "YHgSUNNGdKFvbZ8I4CujIeJaw2aO9a12Ztld64uBplc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171eoye", "is_robot_indexable": true, "report_reasons": null, "author": "FisticuffMetal", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171eoye/seeking_input_on_a_de_boot_camp_from_practicing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fb1b310vklsb1.jpg", "subreddit_subscribers": 132509, "created_utc": 1696604708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "That's it really. It sucks and makes deploying resources to terraform a giant pain in the ass. I know they only took the provider over from chanzuckerberg foundation relatively recently, and some things seem better since then, but so much is a miss. A few specific gripes:\n\n- Cant use dynamic blocks for columns in a table, which means no creating a table as part of a module. \n- Changing how grants are structured in a minor release was very frustrating.\n- We can create database roles, but can't grant any permissions on them. So effectively can't use TF to create shares.\n- Creating a SQL UDF is broken because the provider inserts `CALLED ON NULL INPUT` which is invalid.\n\nThis makes defining a pipeline with IaC pretty difficult. \n\nWhat other options are out there for deploying things like storage integrations, stages, pipes etc.?", "author_fullname": "t2_atygu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is the snowflake_terraform provider so terrible?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171t8yl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696642191.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696640597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s it really. It sucks and makes deploying resources to terraform a giant pain in the ass. I know they only took the provider over from chanzuckerberg foundation relatively recently, and some things seem better since then, but so much is a miss. A few specific gripes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cant use dynamic blocks for columns in a table, which means no creating a table as part of a module. &lt;/li&gt;\n&lt;li&gt;Changing how grants are structured in a minor release was very frustrating.&lt;/li&gt;\n&lt;li&gt;We can create database roles, but can&amp;#39;t grant any permissions on them. So effectively can&amp;#39;t use TF to create shares.&lt;/li&gt;\n&lt;li&gt;Creating a SQL UDF is broken because the provider inserts &lt;code&gt;CALLED ON NULL INPUT&lt;/code&gt; which is invalid.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This makes defining a pipeline with IaC pretty difficult. &lt;/p&gt;\n\n&lt;p&gt;What other options are out there for deploying things like storage integrations, stages, pipes etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171t8yl", "is_robot_indexable": true, "report_reasons": null, "author": "pixlPirate", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171t8yl/why_is_the_snowflake_terraform_provider_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171t8yl/why_is_the_snowflake_terraform_provider_so/", "subreddit_subscribers": 132509, "created_utc": 1696640597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On the surface it's a basic ETL problem but with some additional complications. I'm curious how other experienced engineers would approach this problem to see if I'm overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.\n\nContext\n\n* 200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they're not NDJSON.\n* The files have to parsed so we can transform them into a unified schema and remove the data we're not interested in.\n* Four of the fields have to be enriched.  \n\n   * We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.\n   * There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.\n   * There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don't already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party\n   * Last we need to call another internal api that can scale to any rps.\n* Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it's accessible but not searchable.\n* We'll have to run this process monthly.", "author_fullname": "t2_42grp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach parsing, enriching, and storing 200M JSON records?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a00g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696592257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On the surface it&amp;#39;s a basic ETL problem but with some additional complications. I&amp;#39;m curious how other experienced engineers would approach this problem to see if I&amp;#39;m overlooking things. No tech is off limits, consider costs and scale (time) but again, nothing is off the table.&lt;/p&gt;\n\n&lt;p&gt;Context&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;200M records come in individual json files with 10k records in each. Files range in size depending on the size of the records in the file but most are under 60mb each. Files are stored on cloud storage. Each json file is a list of records so to parse you have to read the entire file in, they&amp;#39;re not NDJSON.&lt;/li&gt;\n&lt;li&gt;The files have to parsed so we can transform them into a unified schema and remove the data we&amp;#39;re not interested in.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Four of the fields have to be enriched.  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We need to map external ids in the files with internal ids if we already have them (dedupe) so we have to compare the existing id with our internal id and if it exists already in our system add this id to the json record.&lt;/li&gt;\n&lt;li&gt;There is a string location, we need to geocode this using an internal system. This system can only handle around 500rps.&lt;/li&gt;\n&lt;li&gt;There is another string we need to pass to a third-party api to get a normalized version. This api can handle 50rps but we only need to call this external api if we don&amp;#39;t already have this string stored in our internal table. So, we looking if we have this normalized, if not we need to call the third party&lt;/li&gt;\n&lt;li&gt;Last we need to call another internal api that can scale to any rps.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Once this enrichment process is done we store all the records as individual rows in postgres. We only pull out a few columns we want to index and the rest is stored in a jsonb field so it&amp;#39;s accessible but not searchable.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ll have to run this process monthly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171a00g", "is_robot_indexable": true, "report_reasons": null, "author": "Detz", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a00g/how_would_you_approach_parsing_enriching_and/", "subreddit_subscribers": 132509, "created_utc": 1696592257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:\n\n\\- Airbyte for extracting data and insert into my data warehouse( Snowflake)\n\n\\- DBT Cloud for transformation \n\n\\- A PostgresDB for OLTP, which will be accessed by the user facing API and UI\n\nData freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say `A` and `B` transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don't have to worry about write.\n\nI can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?\n\n&amp;#x200B;", "author_fullname": "t2_ap7kqtwu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools to use for extracting data from OLAP back to OLTP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171dzqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696603059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to bring some data from data warehouse, into the user-facing database. Currently I have a pipeline like this:&lt;/p&gt;\n\n&lt;p&gt;- Airbyte for extracting data and insert into my data warehouse( Snowflake)&lt;/p&gt;\n\n&lt;p&gt;- DBT Cloud for transformation &lt;/p&gt;\n\n&lt;p&gt;- A PostgresDB for OLTP, which will be accessed by the user facing API and UI&lt;/p&gt;\n\n&lt;p&gt;Data freshness is not really critical, and current the job for updating data in snowflake will run daily. If I need to copy two tables, let say &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; transformed by DBT into that PostgresDB, is there any specific tool for that? These two tables will be read-only, so we don&amp;#39;t have to worry about write.&lt;/p&gt;\n\n&lt;p&gt;I can think of a cron job that select those data from snowflake, and then insert back to postgres. This will definitely work, but I wonder is there any solution out there, that will make my life easier?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171dzqk", "is_robot_indexable": true, "report_reasons": null, "author": "hksparrowboy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dzqk/what_tools_to_use_for_extracting_data_from_olap/", "subreddit_subscribers": 132509, "created_utc": 1696603059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i do know there is a subreddit here with yellow 'personal project' label , but they are kinda beginner-ish imho. can u recommend some more complex advanced-looking projects? or maybe suggest some idea beyond reddit comments stream analysis pipeline?\n\n&amp;#x200B;\n\ni did look into github and still not satisfied", "author_fullname": "t2_e6wts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what are other more impressive data eng projects beside personals in this forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171q2hh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696633600.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696632063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i do know there is a subreddit here with yellow &amp;#39;personal project&amp;#39; label , but they are kinda beginner-ish imho. can u recommend some more complex advanced-looking projects? or maybe suggest some idea beyond reddit comments stream analysis pipeline?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;i did look into github and still not satisfied&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171q2hh", "is_robot_indexable": true, "report_reasons": null, "author": "erjcan", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171q2hh/what_are_other_more_impressive_data_eng_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171q2hh/what_are_other_more_impressive_data_eng_projects/", "subreddit_subscribers": 132509, "created_utc": 1696632063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: [Enabling static analysis of SQL queries at Meta - (fb.com)](https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/)\n\nAlso would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?", "author_fullname": "t2_jx4zrwe0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Static analysers for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1719j9d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696590729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any (freely available or open source) static analysers available for SQL, somethings along the lines of: &lt;a href=\"https://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\"&gt;Enabling static analysis of SQL queries at Meta - (fb.com)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Also would such analyser need access to metastore of the table, for example whether a certain table contains particular column or not ( I guess that would not be part of static analysis?), I am more concerned with using it in CTEs where original column names can be taken care of manually, but the subsequent part would need static analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?auto=webp&amp;s=909387532145f6d6f4319d06cb1d6208fd43c145", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0efd775c5705334b59afbea64c2e78caed9ad31e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe42988ed379e8ee52fae618a3da0840033d3a52", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7c6656435d7d9d9f8c61b4b73fc8c8c24758a35", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e0ded8d78913a04e500afd810ac273773bb3f4a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b3a60647f47ba03c6686405d656724bf8664f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/rJxQsFD1_p_Cp0cVypSQgAMfM4IsFHpsp6PSXCrAwb0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13f763cc226b0b456ab56df4143da9da2d4db438", "width": 1080, "height": 607}], "variants": {}, "id": "Ven08R7rW3VUewldA00PYwtDrsf5TziOv0R5SeV6HyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1719j9d", "is_robot_indexable": true, "report_reasons": null, "author": "sjdevelop", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1719j9d/static_analysers_for_sql/", "subreddit_subscribers": 132509, "created_utc": 1696590729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for an ecommerce company and ever since I started working here about a year ago, I've seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it's customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what's worst is that backend team is so distant to data engineers, that we can't even access their repos or confluence pages. \n\nBecause of this, my new team built (many years ago) a customer identifier, let's call it customer\\_id, which is basically just a hashed value of a bunch of other ids (like user\\_checkout\\_id or anonymous\\_checkout\\_id). This customer\\_id is what's used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer\\_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer\\_id has also left the company years ago and the code is extremely complex.\n\nWe are planning to do a refactor soon but don't really know what's the best practice here. I'm guessing you should ignore some errors based on some threshold and have some uniformity. \n\nhas anyone got any experience on this? any books or blogs? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to deal with complex user_id modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1717jgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696583310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for an ecommerce company and ever since I started working here about a year ago, I&amp;#39;ve seen how no one knows exactly which customer identifier to use and how they work. since the company has more than 20 years of existence, it&amp;#39;s customer identifier has changed many times throughout the year and everyone who made those changes are long gone and there is barely any documentation, and what&amp;#39;s worst is that backend team is so distant to data engineers, that we can&amp;#39;t even access their repos or confluence pages. &lt;/p&gt;\n\n&lt;p&gt;Because of this, my new team built (many years ago) a customer identifier, let&amp;#39;s call it customer_id, which is basically just a hashed value of a bunch of other ids (like user_checkout_id or anonymous_checkout_id). This customer_id is what&amp;#39;s used by all the analytics department and for marketing campaigns and ML models, etc. Problem here is that backend team has changed some definitions of customer (and anonymous customers) and we have new tools such as Segment which gives us new ids. Now, our customer_id is experiencing many inconsistencies, like duplicates emails or missing emails, and no one knows where the problem lies since the creator of the customer_id has also left the company years ago and the code is extremely complex.&lt;/p&gt;\n\n&lt;p&gt;We are planning to do a refactor soon but don&amp;#39;t really know what&amp;#39;s the best practice here. I&amp;#39;m guessing you should ignore some errors based on some threshold and have some uniformity. &lt;/p&gt;\n\n&lt;p&gt;has anyone got any experience on this? any books or blogs? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1717jgi", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1717jgi/how_to_deal_with_complex_user_id_modelling/", "subreddit_subscribers": 132509, "created_utc": 1696583310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those of you who have implemented Databricks serveless have you seen aggregated compute costs in SQL warehouse reduce/increase/remain flat vs standing up a an \u201calways on\u201d cluster (or cluster that\u2019s on most of the day). \n\nMy org never got on the SQL warehouse bandwagon because of the potential costs of \u201calways on\u201d clusters, but even with the larger dbu/hr cost with serverless Databricks is pushing serveless as a cheaper alternative since you are using their own cloud hardware (i.e. not additional ec2 costs on AWS), no spinnup time and no idol time when no one is using the SQL warehouse. The logic makes sense to me, but I\u2019m wondering if orgs are seeing these results in the real world? What\u2019s your take?", "author_fullname": "t2_8y2a4hfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Serverless Costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171tjwn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696641508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who have implemented Databricks serveless have you seen aggregated compute costs in SQL warehouse reduce/increase/remain flat vs standing up a an \u201calways on\u201d cluster (or cluster that\u2019s on most of the day). &lt;/p&gt;\n\n&lt;p&gt;My org never got on the SQL warehouse bandwagon because of the potential costs of \u201calways on\u201d clusters, but even with the larger dbu/hr cost with serverless Databricks is pushing serveless as a cheaper alternative since you are using their own cloud hardware (i.e. not additional ec2 costs on AWS), no spinnup time and no idol time when no one is using the SQL warehouse. The logic makes sense to me, but I\u2019m wondering if orgs are seeing these results in the real world? What\u2019s your take?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171tjwn", "is_robot_indexable": true, "report_reasons": null, "author": "Known-Delay7227", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/171tjwn/databricks_serverless_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171tjwn/databricks_serverless_costs/", "subreddit_subscribers": 132509, "created_utc": 1696641508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're are using EXT tables to read from external stage with storage and notification integrations, as prescribed, no problem. \n\nRecently I found that I can also define a view to read from external stage with storage integration and it is working without the use of notification integration.\n\nDid any of you try this? Are there any advantages or disadvantages using one over the other?", "author_fullname": "t2_f88oipy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake - why not use views instead of EXT tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171r0xl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696634514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re are using EXT tables to read from external stage with storage and notification integrations, as prescribed, no problem. &lt;/p&gt;\n\n&lt;p&gt;Recently I found that I can also define a view to read from external stage with storage integration and it is working without the use of notification integration.&lt;/p&gt;\n\n&lt;p&gt;Did any of you try this? Are there any advantages or disadvantages using one over the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171r0xl", "is_robot_indexable": true, "report_reasons": null, "author": "Ring_Lo_Finger", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171r0xl/snowflake_why_not_use_views_instead_of_ext_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171r0xl/snowflake_why_not_use_views_instead_of_ext_tables/", "subreddit_subscribers": 132509, "created_utc": 1696634514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.\n\n1. I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)\n\n2. We use Data integration (Informatica) to load the files to Snowflake. \n\nAs you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to collect data via API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171dfvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696601730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The processes I am collecting data through API is a not quite ideal I believe. I wonder what is everyone doing out there and hope to get some insights to improve the processes.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I run Python script to collect data from Facebook/Google API and write the responses to Flat Files. (This stage somehow fails of ERROR 400 or ERROR 500, it happens from time to time. I hate it that I have to wake up early to fix it even though it\u2019s on the weekend.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;We use Data integration (Informatica) to load the files to Snowflake. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As you can see, we are doing a two-step process which is quite unnecessary. We used to have a process to use Informatica to collect data and write it into Snowflake (no FF), however we are trying to move to Python. There should be a way to write the responses to Snowflake I believe, but what happens if the process failed while collecting the responses? Don\u2019t we have bad data in snowflake then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171dfvx", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171dfvx/whats_the_best_way_to_collect_data_via_api/", "subreddit_subscribers": 132509, "created_utc": 1696601730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.\n\nThe strings always have the same format, such as \"MM2525 6562451 Some text\". So from this string I would only want the \"6562451\" bit. I tried to find regular expressions like python but got no luck. Thanks in advance!\n\nP.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting digits from a string column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171a9o6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696593077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a table with a column that has some strings. In that string there are sometime a set of 7 numbers. I want to transform the column such that if there are a set of 7 digits that will be the new value, otherwise null.&lt;/p&gt;\n\n&lt;p&gt;The strings always have the same format, such as &amp;quot;MM2525 6562451 Some text&amp;quot;. So from this string I would only want the &amp;quot;6562451&amp;quot; bit. I tried to find regular expressions like python but got no luck. Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;P.S. The plan was to use this method to create a view for the table with this transformed column. Now I wonder, is it better to use data factory with azure function and python regular expressions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171a9o6", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171a9o6/extracting_digits_from_a_string_column/", "subreddit_subscribers": 132509, "created_utc": 1696593077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mxj2oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Reduce Your PostgreSQL Database Size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_171ksrz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xK2orDB1GA0YMsy7kJ4ISKK6VirJ1GgxsoHsB8NGrzk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696619109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "timescale.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.timescale.com/blog/how-to-reduce-your-postgresql-database-size/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?auto=webp&amp;s=b4d5c66e00840f7247f701154f5d99e5e9f07ccd", "width": 2000, "height": 1116}, "resolutions": [{"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ea41183365123c1bc38bd8daca21a2051b39b51", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5302a209df1f865ca97031e474d761139dabc346", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90f9965a0825982debbb6e7fe570422827115f51", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85de90bbbe2d045e41b86338bcf08dfd25b15131", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=14a5ee9cb9316497f4121dd7313527d03fd59ec1", "width": 960, "height": 535}, {"url": "https://external-preview.redd.it/Xkgag5xyv7drMsO-A3LDlhPcf_ck7V-QwCaFZ-Z0v-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe28c99650e4dc1b7929314d68a226a7dceb7dde", "width": 1080, "height": 602}], "variants": {}, "id": "EETO4C2vVBM_qywvuyxOBGV6RIRTzyu81pMJzEXVkrU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "171ksrz", "is_robot_indexable": true, "report_reasons": null, "author": "carlotasoto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171ksrz/how_to_reduce_your_postgresql_database_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.timescale.com/blog/how-to-reduce-your-postgresql-database-size/", "subreddit_subscribers": 132509, "created_utc": 1696619109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working with data for around 4 years now. I started in business intelligence using AWS. At that time, I was the only data analyst, so I had to learn and do everything by myself. This included tasks such as acquiring the data, gaining some knowledge of S3 and Glue for processing and storage, and building dashboards.\n\nIn the last year, I moved to a new company as an \"analytics engineer.\" It started similarly, working end-to-end but with a different tech stack (Power BI, dbt, PostgreSQL, etc.) and with colleagues on my side (rather than being alone).\n\nRecently, our department's structure changed to specialized roles. Now, there are \"data analysts\" for each stream/area building the BIs , and I have taken on the role of the company's \"data engineer.\" I'm not entirely sure what this means.\n\nAnalysts will still be querying in dbt and creating their dashboards, but I don't have many tasks to handle on a regular basis, aside from occasional activities like API integrations or preparing staging models. My boss tells me that data engineering is a \"proactive role\". The thing is that I feel I lack the knowledge and expertise to know new things to implement.\n\nSo, that's was a long way of asking: \"How can I be a good data engineer? For a new starter\"", "author_fullname": "t2_126mha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I be a good data engineer? And what does it means?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171hpr8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696611743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working with data for around 4 years now. I started in business intelligence using AWS. At that time, I was the only data analyst, so I had to learn and do everything by myself. This included tasks such as acquiring the data, gaining some knowledge of S3 and Glue for processing and storage, and building dashboards.&lt;/p&gt;\n\n&lt;p&gt;In the last year, I moved to a new company as an &amp;quot;analytics engineer.&amp;quot; It started similarly, working end-to-end but with a different tech stack (Power BI, dbt, PostgreSQL, etc.) and with colleagues on my side (rather than being alone).&lt;/p&gt;\n\n&lt;p&gt;Recently, our department&amp;#39;s structure changed to specialized roles. Now, there are &amp;quot;data analysts&amp;quot; for each stream/area building the BIs , and I have taken on the role of the company&amp;#39;s &amp;quot;data engineer.&amp;quot; I&amp;#39;m not entirely sure what this means.&lt;/p&gt;\n\n&lt;p&gt;Analysts will still be querying in dbt and creating their dashboards, but I don&amp;#39;t have many tasks to handle on a regular basis, aside from occasional activities like API integrations or preparing staging models. My boss tells me that data engineering is a &amp;quot;proactive role&amp;quot;. The thing is that I feel I lack the knowledge and expertise to know new things to implement.&lt;/p&gt;\n\n&lt;p&gt;So, that&amp;#39;s was a long way of asking: &amp;quot;How can I be a good data engineer? For a new starter&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171hpr8", "is_robot_indexable": true, "report_reasons": null, "author": "Peivol", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171hpr8/how_can_i_be_a_good_data_engineer_and_what_does/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171hpr8/how_can_i_be_a_good_data_engineer_and_what_does/", "subreddit_subscribers": 132509, "created_utc": 1696611743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.", "author_fullname": "t2_jvtay3bqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meaning of title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171d26s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696600808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! I have set my focus on working more with data wrangling and applied for a job in a company that needs someone to work with their databricks/ADF/database solutions. I was eager to switch and accepted the offer that was put forward. The contract was fine but I was expecting that the title would be data engineer but they put technical architect. Can someone enlighten me as to what the distinction is (if any)? As long as I get to work with interesting problem-sets and with technology like databricks, python, ADF and SQL I\u2019m a happy fish, but a little concerned that the title implies some other tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "171d26s", "is_robot_indexable": true, "report_reasons": null, "author": "Southern_Version2681", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171d26s/meaning_of_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171d26s/meaning_of_title/", "subreddit_subscribers": 132509, "created_utc": 1696600808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?", "author_fullname": "t2_5n2f6pea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing ETL config", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171c29z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696598198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, has anyone come across any tooling for housing config? We\u2019ve abstracted away a lot of the parameters that feed into our ETLs, but end up with a bunch of really complicated yaml files or spreadsheets we have to manage. This results in a lot of untracked changes, poor validation and makes simple updates pretty onerous. I\u2019m picturing some bare bones front end like a Google form with version control we can configure to intake config - does that exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171c29z", "is_robot_indexable": true, "report_reasons": null, "author": "sosa_12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171c29z/managing_etl_config/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171c29z/managing_etl_config/", "subreddit_subscribers": 132509, "created_utc": 1696598198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? ", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using python azure function for data transformation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171b9qn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696595996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, this is maybe a dumb question. But is it feasible at all to use python azure function to do data transformations in azure data factory as there are other tools available. If so, in what scenario would you use it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "171b9qn", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171b9qn/using_python_azure_function_for_data/", "subreddit_subscribers": 132509, "created_utc": 1696595996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nJust a quick rant of how much time I spend on following up on bad requirements/requests from non-developers at my company. \n\nFeels like something that should be clear to spec out comes in the form of incomprehensible jibberish that forces me to spend the next couple of days just following up with questions until I understand it.\n\nAnyone has any tips for dealing with non-devs submitting feature requests?\n\nFor context I\u2019m at a fairly small company so structure isn\u2019t exactly abundant.", "author_fullname": "t2_vkw5iqf2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Requirements Gathering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1720e5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696664111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Just a quick rant of how much time I spend on following up on bad requirements/requests from non-developers at my company. &lt;/p&gt;\n\n&lt;p&gt;Feels like something that should be clear to spec out comes in the form of incomprehensible jibberish that forces me to spend the next couple of days just following up with questions until I understand it.&lt;/p&gt;\n\n&lt;p&gt;Anyone has any tips for dealing with non-devs submitting feature requests?&lt;/p&gt;\n\n&lt;p&gt;For context I\u2019m at a fairly small company so structure isn\u2019t exactly abundant.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1720e5a", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial_West_8337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1720e5a/requirements_gathering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1720e5a/requirements_gathering/", "subreddit_subscribers": 132509, "created_utc": 1696664111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It looks like many companies (in Europe) are now hiring MLE to do lots of DE tasks.\n\nHas anyone transitioned to this role from DE? If yes, how did you do it? What did you study?", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone shifted to Machine Learning Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171woch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696650892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like many companies (in Europe) are now hiring MLE to do lots of DE tasks.&lt;/p&gt;\n\n&lt;p&gt;Has anyone transitioned to this role from DE? If yes, how did you do it? What did you study?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "171woch", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/171woch/has_anyone_shifted_to_machine_learning_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171woch/has_anyone_shifted_to_machine_learning_engineering/", "subreddit_subscribers": 132509, "created_utc": 1696650892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as title", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any free tool to create ER diagram online and generate respective SQL statements? Same as title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171vy34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696648552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171vy34", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171vy34/any_free_tool_to_create_er_diagram_online_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171vy34/any_free_tool_to_create_er_diagram_online_and/", "subreddit_subscribers": 132509, "created_utc": 1696648552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have started a new position and one of the first projects I will be working on involves creating a data observability platform. One of the things we need to build is the ability to discover duplicate tables.\n\nThere are close to 85000 tables. My first thought is to segment tables by size, #rows, #columns. And within each segment compare table on hash values.\n\nJust curious if someone else has worked on similar problem and what approach they used.", "author_fullname": "t2_3o3jbkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding duplicate tables on a datalake.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171v6eb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696646239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have started a new position and one of the first projects I will be working on involves creating a data observability platform. One of the things we need to build is the ability to discover duplicate tables.&lt;/p&gt;\n\n&lt;p&gt;There are close to 85000 tables. My first thought is to segment tables by size, #rows, #columns. And within each segment compare table on hash values.&lt;/p&gt;\n\n&lt;p&gt;Just curious if someone else has worked on similar problem and what approach they used.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171v6eb", "is_robot_indexable": true, "report_reasons": null, "author": "gunnvant", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171v6eb/finding_duplicate_tables_on_a_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171v6eb/finding_duplicate_tables_on_a_datalake/", "subreddit_subscribers": 132509, "created_utc": 1696646239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Only DE in a small team and we have built a DW on AWS. Using GUI ETL tool and python for ingestion and SQL and Python/pandas for transformations to create datasets for tableau reporting. I am tasked to provide what the team will be doing from the data engg front.\nI am seeking suggestions on what to build. Scale of the data is really small and itll be a shame to call it big data and hence not thinking of incorporating spark. Manager is non technical so will accept anything I suggest as I will be the one doing it. Two things I have thought of:\nAlerting systems\nAdding new data sources (lol) \n\nI want to transition to SWE or infra/dataops. Keeping that in mind what else I can suggest which will be beneficial for both me &amp; the team?", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions on building DE assets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171uoxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696644805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Only DE in a small team and we have built a DW on AWS. Using GUI ETL tool and python for ingestion and SQL and Python/pandas for transformations to create datasets for tableau reporting. I am tasked to provide what the team will be doing from the data engg front.\nI am seeking suggestions on what to build. Scale of the data is really small and itll be a shame to call it big data and hence not thinking of incorporating spark. Manager is non technical so will accept anything I suggest as I will be the one doing it. Two things I have thought of:\nAlerting systems\nAdding new data sources (lol) &lt;/p&gt;\n\n&lt;p&gt;I want to transition to SWE or infra/dataops. Keeping that in mind what else I can suggest which will be beneficial for both me &amp;amp; the team?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171uoxt", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171uoxt/suggestions_on_building_de_assets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171uoxt/suggestions_on_building_de_assets/", "subreddit_subscribers": 132509, "created_utc": 1696644805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, say I have a bus matric that says the FactVist table has relationship to DimContact, and the Mother, Father, Grandpa and Aunty should all connect with the DimContact table. Is it possible to make a relationship for each of them? My tool doesnt let me create more than one relationship. \n\n&amp;#x200B;\n\n|FactVist|DimContact|\n|:-|:-|\n|MotherKey|Key|\n|FatherKey|Name|\n|GrandpaKey|Address|\n|AuntyKey||\n\nThank you!", "author_fullname": "t2_8igg91b5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating multiple relation from one fact table to one dim table.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171u4ww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696643174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, say I have a bus matric that says the FactVist table has relationship to DimContact, and the Mother, Father, Grandpa and Aunty should all connect with the DimContact table. Is it possible to make a relationship for each of them? My tool doesnt let me create more than one relationship. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;FactVist&lt;/th&gt;\n&lt;th align=\"left\"&gt;DimContact&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;MotherKey&lt;/td&gt;\n&lt;td align=\"left\"&gt;Key&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;FatherKey&lt;/td&gt;\n&lt;td align=\"left\"&gt;Name&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GrandpaKey&lt;/td&gt;\n&lt;td align=\"left\"&gt;Address&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AuntyKey&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171u4ww", "is_robot_indexable": true, "report_reasons": null, "author": "Evening-Mousse-1812", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171u4ww/creating_multiple_relation_from_one_fact_table_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171u4ww/creating_multiple_relation_from_one_fact_table_to/", "subreddit_subscribers": 132509, "created_utc": 1696643174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good afternoon guys,\n\nI have been working for a small floral company for a few weeks now. Family owned and super traditional. They are doing nothing with their data.\n\nThey use an old built-in-house program to input their production orders, purchase orders, invoice information, etc. That program is hosted on-premises, and the data is saved in an MS SQL Server right there.\n\nSales information comes from various other systems and can be downloaded manually as .csv or Excel tables.\n\nFor example, if Walmart orders a PO of 50 boxes for a certain location, this is inputted into the program. Then, if one wanted to see what the sell through of those 500 boxes was, you can download it from the Walmart system and then do the cross-checking.\n\nI already managed to make an ETL process using Mage that incrementally takes, once a day, the on-premises information and takes it to BigQuery. Then, I take those staging tables to dbt to develop the data models that I finally inject back into BigQuery. So I can, for example, develop dashboards and reports.\n\nI was planning to use Retool to create internal-facing and external-facing apps. For example, to track shipments using UPS API and the tracking numbers we store.\n\nHow can I handle the Sales information that needs to be downloaded manually? Does that mean that I need some kind of Lakehouse?\n\nWhat am I missing? If you were in my shoes, how would yo do this?\n\nThanks", "author_fullname": "t2_il87ibi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data stack for a SMB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_171qzke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696634415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon guys,&lt;/p&gt;\n\n&lt;p&gt;I have been working for a small floral company for a few weeks now. Family owned and super traditional. They are doing nothing with their data.&lt;/p&gt;\n\n&lt;p&gt;They use an old built-in-house program to input their production orders, purchase orders, invoice information, etc. That program is hosted on-premises, and the data is saved in an MS SQL Server right there.&lt;/p&gt;\n\n&lt;p&gt;Sales information comes from various other systems and can be downloaded manually as .csv or Excel tables.&lt;/p&gt;\n\n&lt;p&gt;For example, if Walmart orders a PO of 50 boxes for a certain location, this is inputted into the program. Then, if one wanted to see what the sell through of those 500 boxes was, you can download it from the Walmart system and then do the cross-checking.&lt;/p&gt;\n\n&lt;p&gt;I already managed to make an ETL process using Mage that incrementally takes, once a day, the on-premises information and takes it to BigQuery. Then, I take those staging tables to dbt to develop the data models that I finally inject back into BigQuery. So I can, for example, develop dashboards and reports.&lt;/p&gt;\n\n&lt;p&gt;I was planning to use Retool to create internal-facing and external-facing apps. For example, to track shipments using UPS API and the tracking numbers we store.&lt;/p&gt;\n\n&lt;p&gt;How can I handle the Sales information that needs to be downloaded manually? Does that mean that I need some kind of Lakehouse?&lt;/p&gt;\n\n&lt;p&gt;What am I missing? If you were in my shoes, how would yo do this?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "171qzke", "is_robot_indexable": true, "report_reasons": null, "author": "thevangea", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/171qzke/data_stack_for_a_smb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/171qzke/data_stack_for_a_smb/", "subreddit_subscribers": 132509, "created_utc": 1696634415.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}