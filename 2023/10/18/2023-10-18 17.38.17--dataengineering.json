{"kind": "Listing", "data": {"after": "t3_17aq6b0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do \"big data\" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). \n\nI'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?\n\nMaybe I'm reading too much into this, and their description was just a weird way of saying \"expect standard data structures leetcode\"?", "author_fullname": "t2_mz663ln7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've got a DE Interview, but they're not letting me use libraries or SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aadnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697580962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they&amp;#39;ll have me do &amp;quot;big data&amp;quot; code challenges, and explicitly states that SQL, Pandas, etc. won&amp;#39;t be allowed (only default Python). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m honestly confused as most interviews I&amp;#39;ve had expected me to use data-related technologies, I find it odd that they&amp;#39;d explicitly exclude them. Has anyone encountered a similar situation?&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m reading too much into this, and their description was just a weird way of saying &amp;quot;expect standard data structures leetcode&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17aadnb", "is_robot_indexable": true, "report_reasons": null, "author": "arkoftheconvenient", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "subreddit_subscribers": 134662, "created_utc": 1697580962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?\n\nWould love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.\n\nTrying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you seen any examples of \u201cserious\u201d companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17asnwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from other DEs that serve data to &lt;strong&gt;pro-code&lt;/strong&gt; visualization tools like Shiny, Dash, or D3.js.&lt;/p&gt;\n\n&lt;p&gt;Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17asnwh", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "subreddit_subscribers": 134662, "created_utc": 1697640732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI work for a fintech startup which is a small business where I'm the sole individual responsible for building data applications to inform the business. I'm looking for the best data infrastructure for a small business using a modern data stack.\n\nTo date, I've addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I'm pondering about the next professional step, especially as we anticipate our system to grow.\n\nOur primary data sources are:\n\n* **RDS (MySQL)**: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there's a hefty table with several JSONs weighing 14GB.\n* **DynamoDB**: A table for financial decisions with 2 million rows, taking up 3GB.\n* **Google Sheets**: Tables with fewer than a thousand rows.\n* **Bank API for fetching rates**: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.\n\nThe solution I'm considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I'd connect Power BI with Amazon Athena to these transformed databases in S3.\n\nDoes this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I'm leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company's size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.\n\nWould you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?\n\nI'm grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.\n\nThank you in advance for your advice and expertise!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Infrastructure on AWS without Redshift for a small business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a6g6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697570897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I work for a fintech startup which is a small business where I&amp;#39;m the sole individual responsible for building data applications to inform the business. I&amp;#39;m looking for the best data infrastructure for a small business using a modern data stack.&lt;/p&gt;\n\n&lt;p&gt;To date, I&amp;#39;ve addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I&amp;#39;m pondering about the next professional step, especially as we anticipate our system to grow.&lt;/p&gt;\n\n&lt;p&gt;Our primary data sources are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;RDS (MySQL)&lt;/strong&gt;: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there&amp;#39;s a hefty table with several JSONs weighing 14GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt;: A table for financial decisions with 2 million rows, taking up 3GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Sheets&lt;/strong&gt;: Tables with fewer than a thousand rows.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bank API for fetching rates&lt;/strong&gt;: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The solution I&amp;#39;m considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I&amp;#39;d connect Power BI with Amazon Athena to these transformed databases in S3.&lt;/p&gt;\n\n&lt;p&gt;Does this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I&amp;#39;m leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company&amp;#39;s size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.&lt;/p&gt;\n\n&lt;p&gt;Would you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your advice and expertise!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a6g6i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "subreddit_subscribers": 134662, "created_utc": 1697570897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?", "author_fullname": "t2_si3ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best boot camp for novice/intermediate data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ae5be", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697591224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ae5be", "is_robot_indexable": true, "report_reasons": null, "author": "Romarros", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "subreddit_subscribers": 134662, "created_utc": 1697591224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog Post from OpenMetadata team [https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364](https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364) on how to build/deploy , capture results and set alerts all via OpenMetadata. ", "author_fullname": "t2_4h8ymgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplifying Data Quality using OpenMetadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3so0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog Post from OpenMetadata team &lt;a href=\"https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364\"&gt;https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364&lt;/a&gt; on how to build/deploy , capture results and set alerts all via OpenMetadata. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?auto=webp&amp;s=11b603c7839cb0bcfb1836e0d02264077be4df18", "width": 1200, "height": 483}, "resolutions": [{"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d9a2f96a33e3f9f02be6785888e32ab79cf764b", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2870e974d4c22835d1ce3c0cf7dbda2104e5d1bb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46355b53cdbb9c4e84d01e385b56b914cf89f122", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ef23face4242dad2d4237e7ce5853283b426da", "width": 640, "height": 257}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b9d85a129f9b34ef5cb4608e8fba45664753524", "width": 960, "height": 386}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9159457240321cfbc20d4b8e633a605abdc1b31a", "width": 1080, "height": 434}], "variants": {}, "id": "jibYnJc1eg8fLQihM9kKlMVQV76i3ag7MfEojzuA7Ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17a3so0", "is_robot_indexable": true, "report_reasons": null, "author": "d3fmacro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "subreddit_subscribers": 134662, "created_utc": 1697563984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ifpztnvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aotyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697629743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aotyk", "is_robot_indexable": true, "report_reasons": null, "author": "getriglad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aotyk/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing/", "subreddit_subscribers": 134662, "created_utc": 1697629743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE's) into a false sense of reliability, but will ultimately be flawed and become a liability.\n\nI am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I've been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.\n\nIn some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd\n\nI want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting\n\nI've googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):\n\n1. Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?\n2. Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022\n3. Data Pipelines Pocket Reference -  James Densmore, 2021, again isn't it too old?\n4. Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?\n5. Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?\n\nI would like the books to be as close to the real world scenarios, production data engineering.\n\nHow would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Close the gap and consolidate fundamentals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a900f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697577679.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697577490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE&amp;#39;s) into a false sense of reliability, but will ultimately be flawed and become a liability.&lt;/p&gt;\n\n&lt;p&gt;I am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I&amp;#39;ve been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.&lt;/p&gt;\n\n&lt;p&gt;In some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd&lt;/p&gt;\n\n&lt;p&gt;I want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?&lt;/li&gt;\n&lt;li&gt;Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022&lt;/li&gt;\n&lt;li&gt;Data Pipelines Pocket Reference -  James Densmore, 2021, again isn&amp;#39;t it too old?&lt;/li&gt;\n&lt;li&gt;Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?&lt;/li&gt;\n&lt;li&gt;Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would like the books to be as close to the real world scenarios, production data engineering.&lt;/p&gt;\n\n&lt;p&gt;How would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a900f", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "subreddit_subscribers": 134662, "created_utc": 1697577490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. \n\nWe are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. \n\nWe are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.   \nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.   \nOur goal is to work on anonymized data in the DEV &amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. \n\nI have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it's not even necessary (for at least the next 6 months).\n\nNow we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. \n\nI am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn't seem required to us). \n\nI understand that the Unity Catalog is the \"new hot thing\" that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? ", "author_fullname": "t2_5iao6str", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Unity Catalog worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17app3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697632428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. &lt;/p&gt;\n\n&lt;p&gt;We are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. &lt;/p&gt;\n\n&lt;p&gt;We are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.&lt;br/&gt;\nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.&lt;br/&gt;\nOur goal is to work on anonymized data in the DEV &amp;amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. &lt;/p&gt;\n\n&lt;p&gt;I have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it&amp;#39;s not even necessary (for at least the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;Now we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;I am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn&amp;#39;t seem required to us). &lt;/p&gt;\n\n&lt;p&gt;I understand that the Unity Catalog is the &amp;quot;new hot thing&amp;quot; that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17app3b", "is_robot_indexable": true, "report_reasons": null, "author": "Far-String829", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "subreddit_subscribers": 134662, "created_utc": 1697632428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! I've decided to ask this here as I've been lurking for a while and the community feels very approachable. I've googled this question and searched the Kimball's forum but to no avail. I also don't remember this from Kimball's book.\n\n&amp;#x200B;\n\nI'm creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these 'sub-dimensions'. The thing is... It's getting too wide? \n\nThat's the question I can't really answer. When is a dimension table too wide? Right now I'm around 100 columns, which feels excessive and might be hard to use for reporting. \n\nI'm thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_l38l5aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is a dimension table too denormalized? | Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aaznc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697582566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I&amp;#39;ve decided to ask this here as I&amp;#39;ve been lurking for a while and the community feels very approachable. I&amp;#39;ve googled this question and searched the Kimball&amp;#39;s forum but to no avail. I also don&amp;#39;t remember this from Kimball&amp;#39;s book.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these &amp;#39;sub-dimensions&amp;#39;. The thing is... It&amp;#39;s getting too wide? &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s the question I can&amp;#39;t really answer. When is a dimension table too wide? Right now I&amp;#39;m around 100 columns, which feels excessive and might be hard to use for reporting. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aaznc", "is_robot_indexable": true, "report_reasons": null, "author": "ArgenEgo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "subreddit_subscribers": 134662, "created_utc": 1697582566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.  \nIn my experience, 90% of the code and effort seems to be data plumbing.\n\nHow do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?\n\nI\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.\n\n[https://github.com/DataSQRL/sqrl](https://github.com/DataSQRL/sqrl)\n\nWhat do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution *could* exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!", "author_fullname": "t2_ihfn9f9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you solve data plumbing? Can we compile it away?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ahqib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697602012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.&lt;br/&gt;\nIn my experience, 90% of the code and effort seems to be data plumbing.&lt;/p&gt;\n\n&lt;p&gt;How do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/DataSQRL/sqrl\"&gt;https://github.com/DataSQRL/sqrl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution &lt;em&gt;could&lt;/em&gt; exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?auto=webp&amp;s=fa781d1ddcfbca87a6a2b23d5a68793504c49497", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c436bb0beceacd0ad7cfa97946894b6a4b7c9f97", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f989c23a59c0c3fee7040000bab38ffb717b6b0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93d69e2944ffed693f0ff4ed0d5f90dd756528b5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d23b6b8613519343928733fe218625dd049fda91", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e47dca88094c9601ef51964fd1d3b49b94d6343", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bf29af59fa7323cce2efdadd72bc6fb0d1ac9f", "width": 1080, "height": 540}], "variants": {}, "id": "45fS3FFg9knjZShinUTVGtiesnQZRpUhGztbG1rpTBs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ahqib", "is_robot_indexable": true, "report_reasons": null, "author": "matthiasBcom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "subreddit_subscribers": 134662, "created_utc": 1697602012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.\n\nhttps://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;utm_campaign=post&amp;utm_medium=reddit", "author_fullname": "t2_a6t9ksvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From pipelines to platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17amyoo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit\"&gt;https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?auto=webp&amp;s=9c8e6e3536f0f4c65db581de64e64e99bab1b192", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aba92d06a8e7cbfde84a8a2608f9bb6b49e0f5fc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e1cd58976fb4b92ebbdb1950cd2e99e543ec074", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a11481ef0a5ec5245fc53e0770ef8c3883f2936d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9abc0eafe4bf2c1277f5b457028ec55073f86f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1cef300cde287850f63f6d5a3c52b7bb2fe92f2e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62146cd257889560244beacc968e6ce8235fe121", "width": 1080, "height": 540}], "variants": {}, "id": "qBpJuY4j45_ONii6LIPwu5GEADbSiI3mqZIgn1TG_4g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17amyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_End_979", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "subreddit_subscribers": 134662, "created_utc": 1697623030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently architecting a Azure solution for a DWH  that will integrate data extracted from \\~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.\n\nFor the extraction part, I'm leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered 'externally' via a call from ADF, Synapse pipelines or Airflow.\n\nI would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!", "author_fullname": "t2_lvjbzi2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting an Azure Solution for DWH Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aqwt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697635964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently architecting a Azure solution for a DWH  that will integrate data extracted from ~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.&lt;/p&gt;\n\n&lt;p&gt;For the extraction part, I&amp;#39;m leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered &amp;#39;externally&amp;#39; via a call from ADF, Synapse pipelines or Airflow.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aqwt9", "is_robot_indexable": true, "report_reasons": null, "author": "cloclosh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "subreddit_subscribers": 134662, "created_utc": 1697635964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have recently started working at a mid-company as a DE but apparently there's not a lot of work on DE involved around production for it. Since I just started, there's not a lot of work around the corner for me. I thought that since I'm not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I've thought of trying kaggle. Do you guys recommend of any other way to do so?", "author_fullname": "t2_8laf2pzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upskilling myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697608918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have recently started working at a mid-company as a DE but apparently there&amp;#39;s not a lot of work on DE involved around production for it. Since I just started, there&amp;#39;s not a lot of work around the corner for me. I thought that since I&amp;#39;m not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I&amp;#39;ve thought of trying kaggle. Do you guys recommend of any other way to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ajlft", "is_robot_indexable": true, "report_reasons": null, "author": "Key_Consideration385", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "subreddit_subscribers": 134662, "created_utc": 1697608918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm seriously confused about this.\n\nfrom what i gather. it's like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.\n\ni'm trying to use pyiceberg to implement this.\n\ni created an iceberg table in s3. but now how do i actually point it to parquet files?\n\ni tried putting them in the same directory, that didn't do it.\n\ni tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.\n\nupdate:\n\nok this makes sense.\nhttps://github.com/apache/iceberg-python/issues/23\n\nthis library does not have the capability yet, but they are working on it.\n\nguess ill wait", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can anyone explain apache iceberg better than chatgpt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a8dfa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697595187.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697575896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m seriously confused about this.&lt;/p&gt;\n\n&lt;p&gt;from what i gather. it&amp;#39;s like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m trying to use pyiceberg to implement this.&lt;/p&gt;\n\n&lt;p&gt;i created an iceberg table in s3. but now how do i actually point it to parquet files?&lt;/p&gt;\n\n&lt;p&gt;i tried putting them in the same directory, that didn&amp;#39;t do it.&lt;/p&gt;\n\n&lt;p&gt;i tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.&lt;/p&gt;\n\n&lt;p&gt;update:&lt;/p&gt;\n\n&lt;p&gt;ok this makes sense.\n&lt;a href=\"https://github.com/apache/iceberg-python/issues/23\"&gt;https://github.com/apache/iceberg-python/issues/23&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;this library does not have the capability yet, but they are working on it.&lt;/p&gt;\n\n&lt;p&gt;guess ill wait&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?auto=webp&amp;s=31e34155cd137a7525a7e5f701d7e8e192b92605", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a98431ffefe10eff79cebd3d42a62cf257f44c4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d5bb50f74032c8aa522452daeceadde8e247ec7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=086894491b879a6c7ea93a033e9f29d5da55aafd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf9f50d600924f27002673ade980086d2ada9579", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=192625faf66eef19687259ca7059fbf0e13890ea", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3ca73d8340497056b29c1d919ea28e39221e151", "width": 1080, "height": 540}], "variants": {}, "id": "i4ID1QjNU5CbbrmzIy_Hv86E7TVuRnrDtybuA_z6Lu0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a8dfa", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "subreddit_subscribers": 134662, "created_utc": 1697575896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?\n\nTrying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise\n\nThanks in advance!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3k4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?&lt;/p&gt;\n\n&lt;p&gt;Trying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17a3k4m", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3k4m/question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3k4m/question/", "subreddit_subscribers": 134662, "created_utc": 1697563339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.\n\n&amp;#x200B;\n\nI frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.\n\n&amp;#x200B;\n\nAfter conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.\n\n&amp;#x200B;\n\nYou can try it out at [http://sqlvisual.net](http://sqlvisual.net) without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_la0ljviui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make SQL easy to understand with sqlvisual", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17au1n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697644390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can try it out at &lt;a href=\"http://sqlvisual.net\"&gt;http://sqlvisual.net&lt;/a&gt; without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17au1n7", "is_robot_indexable": true, "report_reasons": null, "author": "glxrun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "subreddit_subscribers": 134662, "created_utc": 1697644390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine you're in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.\n\nHow would you architect the analytics portion of this system? Where would you store the data?\n\nAssume we ideally want the data available in real-time, but some delay is acceptable\n\nHow would that architecture change depending on traffic needs? I'm curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that's scaling up to support tens or hundreds of millions of impressions in traffic per month.\n\n\\---\n\nThe most dead simple way would be to just store event data in a SQL database, but I assume that's bad practice as it's not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?\n\nSorry if these are dumb questions. I'm a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don't understand on a practical level how and when to actually implement this stuff.", "author_fullname": "t2_7h4yync7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to architect an analytics system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17atuwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine you&amp;#39;re in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.&lt;/p&gt;\n\n&lt;p&gt;How would you architect the analytics portion of this system? Where would you store the data?&lt;/p&gt;\n\n&lt;p&gt;Assume we ideally want the data available in real-time, but some delay is acceptable&lt;/p&gt;\n\n&lt;p&gt;How would that architecture change depending on traffic needs? I&amp;#39;m curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that&amp;#39;s scaling up to support tens or hundreds of millions of impressions in traffic per month.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The most dead simple way would be to just store event data in a SQL database, but I assume that&amp;#39;s bad practice as it&amp;#39;s not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?&lt;/p&gt;\n\n&lt;p&gt;Sorry if these are dumb questions. I&amp;#39;m a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don&amp;#39;t understand on a practical level how and when to actually implement this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17atuwj", "is_robot_indexable": true, "report_reasons": null, "author": "techworker716", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "subreddit_subscribers": 134662, "created_utc": 1697643908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.\n\nSo far I've been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.\n\nAfter assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn't be using Debezium as, according to him and another expert, it doesn't simply read the db's logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db's log without needing a trigger from it.\n\nI've been doing some research and it turns out that there aren't many options on the table, especially if we consider that everything has to be **on premise** and according [to this paper from Netflix](https://arxiv.org/pdf/2010.12597.pdf) Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they're quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.\n\nSo I'm wondering whether the project is actually viable or if I'm headed into a dead end.\n\nI case it hasn't already transpired I'm not really a data engineer so I'm learning as much as possible during the process.", "author_fullname": "t2_79y3zlzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Viability of a CDC project paired with Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ash74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.&lt;/p&gt;\n\n&lt;p&gt;After assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn&amp;#39;t be using Debezium as, according to him and another expert, it doesn&amp;#39;t simply read the db&amp;#39;s logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db&amp;#39;s log without needing a trigger from it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing some research and it turns out that there aren&amp;#39;t many options on the table, especially if we consider that everything has to be &lt;strong&gt;on premise&lt;/strong&gt; and according &lt;a href=\"https://arxiv.org/pdf/2010.12597.pdf\"&gt;to this paper from Netflix&lt;/a&gt; Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they&amp;#39;re quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering whether the project is actually viable or if I&amp;#39;m headed into a dead end.&lt;/p&gt;\n\n&lt;p&gt;I case it hasn&amp;#39;t already transpired I&amp;#39;m not really a data engineer so I&amp;#39;m learning as much as possible during the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ash74", "is_robot_indexable": true, "report_reasons": null, "author": "ExactTreat593", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "subreddit_subscribers": 134662, "created_utc": 1697640225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.\n\nGiving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar\n\nI know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).\n\nAlso what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: what software should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ap7is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.&lt;/p&gt;\n\n&lt;p&gt;Giving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar&lt;/p&gt;\n\n&lt;p&gt;I know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).&lt;/p&gt;\n\n&lt;p&gt;Also what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ap7is", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "subreddit_subscribers": 134662, "created_utc": 1697630909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use python and sql mostly for my job. I want to learn another language to have more competency. I touched c++ a bit when working on my esp32, but realised that the use of c++ in data engineering space is very niche and isolated to iot. If I can choose between java and javascript, thinking kotlin vs typescript to learn, which language will be more useful in general for data engineering?", "author_fullname": "t2_86pd6cgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "java vs javascript as an additional language to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17afqeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697595729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use python and sql mostly for my job. I want to learn another language to have more competency. I touched c++ a bit when working on my esp32, but realised that the use of c++ in data engineering space is very niche and isolated to iot. If I can choose between java and javascript, thinking kotlin vs typescript to learn, which language will be more useful in general for data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17afqeu", "is_robot_indexable": true, "report_reasons": null, "author": "EmploymentMammoth659", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17afqeu/java_vs_javascript_as_an_additional_language_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17afqeu/java_vs_javascript_as_an_additional_language_to/", "subreddit_subscribers": 134662, "created_utc": 1697595729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was looking to extract a Windows log file in csv format with Pandas.   \nI'm trying to plot a graph with the UtcTime on the x-axis(in seconds) and the number of events/occurences on the y-axis(at the second when they occurred)  \n\n\nAny suggestion?  \n\n\nEvents are stored in this csv format:\n\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\\ProperTreeModuleInner\n    Details: Binary Data\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,12,Registry object added or deleted (rule: RegistryEvent),Registry object added or deleted:\n    RuleName: -\n    EventType: CreateKey\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\FFlags\n    Details: DWORD (0x00000001)\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\Mode\n    Details: DWORD (0x00000004)\n    User: DESKTOP-D505GS3\\Tester", "author_fullname": "t2_dtz2djqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract a log file with Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17avd2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697647765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was looking to extract a Windows log file in csv format with Pandas.&lt;br/&gt;\nI&amp;#39;m trying to plot a graph with the UtcTime on the x-axis(in seconds) and the number of events/occurences on the y-axis(at the second when they occurred)  &lt;/p&gt;\n\n&lt;p&gt;Any suggestion?  &lt;/p&gt;\n\n&lt;p&gt;Events are stored in this csv format:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\\ProperTreeModuleInner\nDetails: Binary Data\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,12,Registry object added or deleted (rule: RegistryEvent),Registry object added or deleted:\nRuleName: -\nEventType: CreateKey\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\FFlags\nDetails: DWORD (0x00000001)\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\Mode\nDetails: DWORD (0x00000004)\nUser: DESKTOP-D505GS3\\Tester\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17avd2w", "is_robot_indexable": true, "report_reasons": null, "author": "Otherwise_Virus_722", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17avd2w/extract_a_log_file_with_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17avd2w/extract_a_log_file_with_pandas/", "subreddit_subscribers": 134662, "created_utc": 1697647765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the top improvements Databricks can have within notebooks or cluster management?", "author_fullname": "t2_7qtt69k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improvements in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17atqkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the top improvements Databricks can have within notebooks or cluster management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17atqkd", "is_robot_indexable": true, "report_reasons": null, "author": "Mevirr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atqkd/improvements_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atqkd/improvements_in_databricks/", "subreddit_subscribers": 134662, "created_utc": 1697643585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Denormalisation in Streams\n\nI work with a platform team that manages receipt of event streams from various sources, followed by correction and 'enrichment with meta (ex: product details) ' and conversion of payload into internal proto contracts. The output of my team is a raw stream of event stream for rest of org to consume. The challenge that has been is that there is no end to requests for enrichments. Most consumers want the enrichment to be done in the platform. Till date we have pushed back any requests coming from a single consumer for meta enrichments, but we strongly feel push back is not the solution. We seem to be failing to build a consensus on right place for any given enrichment. At its core the argument is around 'why are we not allowing denormalisation as early into event flow as possible'. I feel this is not a new problem. I am looking for some advice/implementations that this group has come across. \ud83d\ude42\n\nA quite note - we have a fair number of event streams - ~400+", "author_fullname": "t2_ug6a7t7s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalisation in Streams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17atfyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697642790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Denormalisation in Streams&lt;/p&gt;\n\n&lt;p&gt;I work with a platform team that manages receipt of event streams from various sources, followed by correction and &amp;#39;enrichment with meta (ex: product details) &amp;#39; and conversion of payload into internal proto contracts. The output of my team is a raw stream of event stream for rest of org to consume. The challenge that has been is that there is no end to requests for enrichments. Most consumers want the enrichment to be done in the platform. Till date we have pushed back any requests coming from a single consumer for meta enrichments, but we strongly feel push back is not the solution. We seem to be failing to build a consensus on right place for any given enrichment. At its core the argument is around &amp;#39;why are we not allowing denormalisation as early into event flow as possible&amp;#39;. I feel this is not a new problem. I am looking for some advice/implementations that this group has come across. \ud83d\ude42&lt;/p&gt;\n\n&lt;p&gt;A quite note - we have a fair number of event streams - ~400+&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17atfyp", "is_robot_indexable": true, "report_reasons": null, "author": "that-pipe-dream", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atfyp/denormalisation_in_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atfyp/denormalisation_in_streams/", "subreddit_subscribers": 134662, "created_utc": 1697642790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:\n\n*We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json\\_normalize to flatten the json into a tabular format) However we've had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)*\n\n**What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?**\n\nOur current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can't directly load data that doesn't match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.\n\nWe want to try and keep this as close to realtime as possible and preferably serverless", "author_fullname": "t2_48vnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Problem: help with inconsistent day to day parquet schemas (pandas, AWS lambda)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq76s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json_normalize to flatten the json into a tabular format) However we&amp;#39;ve had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can&amp;#39;t directly load data that doesn&amp;#39;t match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.&lt;/p&gt;\n\n&lt;p&gt;We want to try and keep this as close to realtime as possible and preferably serverless&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aq76s", "is_robot_indexable": true, "report_reasons": null, "author": "freerangetrousers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "subreddit_subscribers": 134662, "created_utc": 1697633939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6y0b4txf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we built a Streaming SQL Engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq6b0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JZpOkQZB5tMoMElRL2wme009VaJvENXW1wgEJj845dw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697633860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.epsio.io/blog/how-to-create-a-streaming-sql-engine", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?auto=webp&amp;s=bde1bb439f8042c2066e539de311e97a9b34f68c", "width": 1376, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eac5d4e9cb92dce7b0b63f164195fd8d9934b59c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41f34043110161b56e0b27712a353a975d56bf20", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b52001b9486b04f0742e761902c5cfcd8cbbf0dc", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a259d069e4ab50668673eb345d4d655ffd6c5cb5", "width": 640, "height": 332}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fa737b762cbbbf12c1acfcccbaca8eb8785678f", "width": 960, "height": 498}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=213bb55bc131cf226f4518542ac26d7f7de4a0fa", "width": 1080, "height": 560}], "variants": {}, "id": "L6vsGJtlTpCBQYIHfzVB4U_eHzCDAFcguQ6gS8SW1ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aq6b0", "is_robot_indexable": true, "report_reasons": null, "author": "Giladkl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq6b0/how_we_built_a_streaming_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.epsio.io/blog/how-to-create-a-streaming-sql-engine", "subreddit_subscribers": 134662, "created_utc": 1697633860.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}