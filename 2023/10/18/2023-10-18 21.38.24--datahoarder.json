{"kind": "Listing", "data": {"after": "t3_17b09wc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I heard it better if it HTL, they are the only one available on Amazon.", "author_fullname": "t2_b8epvoau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these Blu-ray good for long-term storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 58, "top_awarded_type": null, "hide_score": false, "name": "t3_17ad9wm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 118, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/uSdUPzB_ltMsRGmnSdlFQ5vFwoPmXNfXZ8ChGSHhal0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697588797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I heard it better if it HTL, they are the only one available on Amazon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/7qbt4z72vuub1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?auto=webp&amp;s=6e0b0b453bec63fe77352ab892c656d5bfa30955", "width": 1157, "height": 484}, "resolutions": [{"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bce3363d1f6551b6b1de0a273eda35937a798d26", "width": 108, "height": 45}, {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=71a3350272ca11289c9843721b84da40851f829e", "width": 216, "height": 90}, {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=494ad329bd2458866588bf81db6de141b6c5fa07", "width": 320, "height": 133}, {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8837835bc2548e493a782e86e89dabd17311422b", "width": 640, "height": 267}, {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d25a18425fc145f531eb6db8423464ae42ff2e70", "width": 960, "height": 401}, {"url": "https://preview.redd.it/7qbt4z72vuub1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30ee16d3b48d738f9e1ae960bcca97294931b33c", "width": 1080, "height": 451}], "variants": {}, "id": "SKHan-4NxlkdiDWU6PN_tyARu-pMwZpIj56LDPUHM4o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ad9wm", "is_robot_indexable": true, "report_reasons": null, "author": "Capitaine_IC", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ad9wm/are_these_bluray_good_for_longterm_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/7qbt4z72vuub1.jpg", "subreddit_subscribers": 707440, "created_utc": 1697588797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1749ew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A flaw in Synology DiskStation Manager allows admin account takeover", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17at9pg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/abWXiMdI-z52O3C1FzdfJMi1a-5ygN1BOyaIrPX7JH4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697642342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "securityaffairs.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://securityaffairs.com/152645/hacking/synology-diskstation-manager-admin-account-takeover.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LNUaToxX33_YlPTXO77zBU64yNAoYigXInZZXXLO-V8.jpg?auto=webp&amp;s=990a082c18da9b187e027847821cf69d81e65679", "width": 570, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/LNUaToxX33_YlPTXO77zBU64yNAoYigXInZZXXLO-V8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ba3ea5fb8426066e3ed2774818dfd90c2929883", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/LNUaToxX33_YlPTXO77zBU64yNAoYigXInZZXXLO-V8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=759c25c2da0166ba328937c96d14436fc6374cb5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/LNUaToxX33_YlPTXO77zBU64yNAoYigXInZZXXLO-V8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=79c356e9156607f17f124002f075212798320c0a", "width": 320, "height": 168}], "variants": {}, "id": "8nEDb5M2B0sVNfkWpHifwYz_1NddFDRw5IrHFoTpLVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17at9pg", "is_robot_indexable": true, "report_reasons": null, "author": "Behinddasticks", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17at9pg/a_flaw_in_synology_diskstation_manager_allows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://securityaffairs.com/152645/hacking/synology-diskstation-manager-admin-account-takeover.html", "subreddit_subscribers": 707440, "created_utc": 1697642342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been considering going the LTO route more and more as my collection grows in size. \n\nI am using Enterprise HDD's with parity for primary storage. And equal sized HDD's in a DAS for external cold storage (off-site).\n\nDue to limitations on upload speeds I do not have a cloud backup solution at present time. Therefore I am considering implementing LTO tape as part of my backup strategy.\n\nI've read posts from others who recommend data thresholds where LTO starts to make sense, or becomes cost-effective.\n\nMy current data-set is 40TB, growing by about 10-15TB annually.\n\nMy PC case doesn't have 5.25\" bays, so I am looking for an external SAS drive. I already have a SAS card (9207-8e) and am trying to determine which generation is most cost-effective and obtainable at present day in 2023. \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dare I say \"cost-effective\" Tape Backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ap4g9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been considering going the LTO route more and more as my collection grows in size. &lt;/p&gt;\n\n&lt;p&gt;I am using Enterprise HDD&amp;#39;s with parity for primary storage. And equal sized HDD&amp;#39;s in a DAS for external cold storage (off-site).&lt;/p&gt;\n\n&lt;p&gt;Due to limitations on upload speeds I do not have a cloud backup solution at present time. Therefore I am considering implementing LTO tape as part of my backup strategy.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read posts from others who recommend data thresholds where LTO starts to make sense, or becomes cost-effective.&lt;/p&gt;\n\n&lt;p&gt;My current data-set is 40TB, growing by about 10-15TB annually.&lt;/p&gt;\n\n&lt;p&gt;My PC case doesn&amp;#39;t have 5.25&amp;quot; bays, so I am looking for an external SAS drive. I already have a SAS card (9207-8e) and am trying to determine which generation is most cost-effective and obtainable at present day in 2023. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ap4g9", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17ap4g9/dare_i_say_costeffective_tape_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ap4g9/dare_i_say_costeffective_tape_backup/", "subreddit_subscribers": 707440, "created_utc": 1697630634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, so long story short, due to a system error I got 5 x 4TBs of new external-HDD storage for 100$ ( were supposed to be sold for 400$ ).\nAny ideas how to use them?\n\nEdit: Total price was 100$ to be exact.", "author_fullname": "t2_2hn1nnjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20TB of HDD for 110$", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17art7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697638581.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697638395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, so long story short, due to a system error I got 5 x 4TBs of new external-HDD storage for 100$ ( were supposed to be sold for 400$ ).\nAny ideas how to use them?&lt;/p&gt;\n\n&lt;p&gt;Edit: Total price was 100$ to be exact.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17art7y", "is_robot_indexable": true, "report_reasons": null, "author": "JustHadros", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17art7y/20tb_of_hdd_for_110/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17art7y/20tb_of_hdd_for_110/", "subreddit_subscribers": 707440, "created_utc": 1697638395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm a storage server noob on their quest to build my first storage build. I have acquired all the major parts (all the relevant parts for the question being a Meshify 2xl, evga p3 supernova 1000w 80+ Plat, 6 Ironwolf pro 16tb and 2 sata SSDs) and all that remains for me to acquire are splitters or whatever other cables I'd need to actually power all the drives. Every video I watch on YouTube seems to gloss right over how people actually wire up their drives to power and I want to make sure I a) do it most efficiently to help with cable management, and b) not burn my system and house down. I also want to plan for expansion of adding more HDDs in the future since I have a bunch laying around in smaller &lt; 2TB quantities that I may want to add at some point or just do more NAS/enterprise drives if I feel like spending more money.\n\nTL;DR: How do I most efficiently wire up at least 8 SATA (but probably more) drives to power, when my power supply only supports 4 SATA power connections.", "author_fullname": "t2_168acv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to power all the drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq56u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m a storage server noob on their quest to build my first storage build. I have acquired all the major parts (all the relevant parts for the question being a Meshify 2xl, evga p3 supernova 1000w 80+ Plat, 6 Ironwolf pro 16tb and 2 sata SSDs) and all that remains for me to acquire are splitters or whatever other cables I&amp;#39;d need to actually power all the drives. Every video I watch on YouTube seems to gloss right over how people actually wire up their drives to power and I want to make sure I a) do it most efficiently to help with cable management, and b) not burn my system and house down. I also want to plan for expansion of adding more HDDs in the future since I have a bunch laying around in smaller &amp;lt; 2TB quantities that I may want to add at some point or just do more NAS/enterprise drives if I feel like spending more money.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: How do I most efficiently wire up at least 8 SATA (but probably more) drives to power, when my power supply only supports 4 SATA power connections.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aq56u", "is_robot_indexable": true, "report_reasons": null, "author": "rubikso", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aq56u/how_to_power_all_the_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aq56u/how_to_power_all_the_drives/", "subreddit_subscribers": 707440, "created_utc": 1697633764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using the Pioneer BDR-XD05S Drive to rip blu-rays using MakeMKV for about 7 years now. In the last few months, it's been struggling with this task. It still rips and reads regular DVD's just fine. When I put in a pristine blu-ray, sometimes it will just spin for a few mins and then eject it. Other times it will take a while, finally read it and then when I try to rip, it will eventually fail. Do these drives age out or is there some sort of issue with blu-rays nowadays that this technique doesn't work anymore?\n\nI didn't realize the drive was so old until I started writing this post. Am I being a cheapskate and it's just time to buy a new drive? If so, any recommendations?", "author_fullname": "t2_126ikl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BluRay burner won't rip blu-rays anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ads6e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697590203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using the Pioneer BDR-XD05S Drive to rip blu-rays using MakeMKV for about 7 years now. In the last few months, it&amp;#39;s been struggling with this task. It still rips and reads regular DVD&amp;#39;s just fine. When I put in a pristine blu-ray, sometimes it will just spin for a few mins and then eject it. Other times it will take a while, finally read it and then when I try to rip, it will eventually fail. Do these drives age out or is there some sort of issue with blu-rays nowadays that this technique doesn&amp;#39;t work anymore?&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t realize the drive was so old until I started writing this post. Am I being a cheapskate and it&amp;#39;s just time to buy a new drive? If so, any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ads6e", "is_robot_indexable": true, "report_reasons": null, "author": "whiteymcgroovenhaven", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ads6e/bluray_burner_wont_rip_blurays_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ads6e/bluray_burner_wont_rip_blurays_anymore/", "subreddit_subscribers": 707440, "created_utc": 1697590203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Everyone. \n\nMy macrium reflect image is corrupted and can't be read from, explored or restored. The image is a full image of a partition(no incremental or differential images) that was made on the free version of reflect 8. The entire file is intact on file explorer, although when i highlight the file it says it's a, \"Macrium Reflect unknown file\". It doesn't appear on the drop down menu for the restore section in the app either.\n\nI've tried booting into the Macrium reflect winPE rescue menu to explore it but it keeps giving me an error saying unable to load image.  Also I don't know how to access the Diskrestorex64 command prompts that are mentioned in the link below:\n\n[https://knowledgebase.macrium.com/display/KNOW7/Running+DiskRestore+from+Windows+PE](https://knowledgebase.macrium.com/display/KNOW7/Running+DiskRestore+from+Windows+PE)\n\nThis image is not on the device it was created on but it's on an external HDD. Furthermore the original device has been formatted.\n\nIs there anything i can do or should i just delete this image?\n\nSorry if this post isn't right for this sub. I've used so many links and pages as well as subreddits but I've made zero progress.\n\nHere are the images of the errors:\n\n&amp;#x200B;\n\n[429gb file is unknown](https://preview.redd.it/nozc86ri4zub1.png?width=766&amp;format=png&amp;auto=webp&amp;s=90da30961057510f7fc9365aa6a14ee4a5f50c57)\n\n[File not appearing in menu](https://preview.redd.it/vawqpari4zub1.png?width=1420&amp;format=png&amp;auto=webp&amp;s=dc33e1668a3ee773907d6895d8697486c947143a)\n\nhttps://preview.redd.it/pbz969ri4zub1.png?width=972&amp;format=png&amp;auto=webp&amp;s=4ae2ba8926849eeca180a448a31d28edb9b4621f\n\nhttps://preview.redd.it/8bgfh7ri4zub1.png?width=571&amp;format=png&amp;auto=webp&amp;s=97883d661a2862787f932fd053bd954056a68ec7", "author_fullname": "t2_ci4gerpo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recovering a corrupt macrium reflect image", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 25, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vawqpari4zub1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/vawqpari4zub1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=56a0792877d88af7e671f5b35076e44e747ebe97"}, {"y": 105, "x": 216, "u": "https://preview.redd.it/vawqpari4zub1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fadddb2fdbed5bba233d6aed4f05c2dfb057dea"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/vawqpari4zub1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=491d44e7e884705df8658d323e1f4f720f623f94"}, {"y": 311, "x": 640, "u": "https://preview.redd.it/vawqpari4zub1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb8b8b21fd6f28f65a26c0ca0349032d946629b0"}, {"y": 467, "x": 960, "u": "https://preview.redd.it/vawqpari4zub1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=199484ed977b926cd58d3a146330bdb876ed1b3f"}, {"y": 525, "x": 1080, "u": "https://preview.redd.it/vawqpari4zub1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61e2f44c4630cee1237e5c8291a338a8a7971ae7"}], "s": {"y": 691, "x": 1420, "u": "https://preview.redd.it/vawqpari4zub1.png?width=1420&amp;format=png&amp;auto=webp&amp;s=dc33e1668a3ee773907d6895d8697486c947143a"}, "id": "vawqpari4zub1"}, "8bgfh7ri4zub1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 51, "x": 108, "u": "https://preview.redd.it/8bgfh7ri4zub1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=babea435fa41d1f7e7dc0092a3d8efe88ad498e1"}, {"y": 102, "x": 216, "u": "https://preview.redd.it/8bgfh7ri4zub1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e84760fae306f7f9f54b89fdc8087cbc9d22c6f"}, {"y": 151, "x": 320, "u": "https://preview.redd.it/8bgfh7ri4zub1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1914decda83bc1af644f3c405e7b4a0551390a72"}], "s": {"y": 270, "x": 571, "u": "https://preview.redd.it/8bgfh7ri4zub1.png?width=571&amp;format=png&amp;auto=webp&amp;s=97883d661a2862787f932fd053bd954056a68ec7"}, "id": "8bgfh7ri4zub1"}, "pbz969ri4zub1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=250ada96343f131b4c621180d27e3b31c5f3cf56"}, {"y": 38, "x": 216, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ed0cd72f7865b3312c9ea2e232ad0c9e99bb759"}, {"y": 56, "x": 320, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5dceb7c8dcc45c4c66121aa224fd375ea8c7b79f"}, {"y": 113, "x": 640, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8084352e29f87d27ade11dccb8ec73ce01d7d29e"}, {"y": 169, "x": 960, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fdad7f2f1b2a38be4ef49f41e3989fa19d5b65a3"}], "s": {"y": 172, "x": 972, "u": "https://preview.redd.it/pbz969ri4zub1.png?width=972&amp;format=png&amp;auto=webp&amp;s=4ae2ba8926849eeca180a448a31d28edb9b4621f"}, "id": "pbz969ri4zub1"}, "nozc86ri4zub1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/nozc86ri4zub1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=322fb0930a12f52514625e35a303319c61a7b364"}, {"y": 38, "x": 216, "u": "https://preview.redd.it/nozc86ri4zub1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14de6586baa09427d87071038c7934a022665a9f"}, {"y": 57, "x": 320, "u": "https://preview.redd.it/nozc86ri4zub1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=06fa7b1eb09f4decaac8479000b6c93d6069fdec"}, {"y": 115, "x": 640, "u": "https://preview.redd.it/nozc86ri4zub1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7332cef22dbe51b403ea7f9a262a0418dd445158"}], "s": {"y": 138, "x": 766, "u": "https://preview.redd.it/nozc86ri4zub1.png?width=766&amp;format=png&amp;auto=webp&amp;s=90da30961057510f7fc9365aa6a14ee4a5f50c57"}, "id": "nozc86ri4zub1"}}, "name": "t3_17askgq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sYR4jdKO6klEZ_KZ84s_-FnZ64O2cIkc9jLKbnSOzCI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone. &lt;/p&gt;\n\n&lt;p&gt;My macrium reflect image is corrupted and can&amp;#39;t be read from, explored or restored. The image is a full image of a partition(no incremental or differential images) that was made on the free version of reflect 8. The entire file is intact on file explorer, although when i highlight the file it says it&amp;#39;s a, &amp;quot;Macrium Reflect unknown file&amp;quot;. It doesn&amp;#39;t appear on the drop down menu for the restore section in the app either.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried booting into the Macrium reflect winPE rescue menu to explore it but it keeps giving me an error saying unable to load image.  Also I don&amp;#39;t know how to access the Diskrestorex64 command prompts that are mentioned in the link below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://knowledgebase.macrium.com/display/KNOW7/Running+DiskRestore+from+Windows+PE\"&gt;https://knowledgebase.macrium.com/display/KNOW7/Running+DiskRestore+from+Windows+PE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This image is not on the device it was created on but it&amp;#39;s on an external HDD. Furthermore the original device has been formatted.&lt;/p&gt;\n\n&lt;p&gt;Is there anything i can do or should i just delete this image?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this post isn&amp;#39;t right for this sub. I&amp;#39;ve used so many links and pages as well as subreddits but I&amp;#39;ve made zero progress.&lt;/p&gt;\n\n&lt;p&gt;Here are the images of the errors:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nozc86ri4zub1.png?width=766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90da30961057510f7fc9365aa6a14ee4a5f50c57\"&gt;429gb file is unknown&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vawqpari4zub1.png?width=1420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc33e1668a3ee773907d6895d8697486c947143a\"&gt;File not appearing in menu&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pbz969ri4zub1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ae2ba8926849eeca180a448a31d28edb9b4621f\"&gt;https://preview.redd.it/pbz969ri4zub1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ae2ba8926849eeca180a448a31d28edb9b4621f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8bgfh7ri4zub1.png?width=571&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97883d661a2862787f932fd053bd954056a68ec7\"&gt;https://preview.redd.it/8bgfh7ri4zub1.png?width=571&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97883d661a2862787f932fd053bd954056a68ec7&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17askgq", "is_robot_indexable": true, "report_reasons": null, "author": "RandomAccountno6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17askgq/recovering_a_corrupt_macrium_reflect_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17askgq/recovering_a_corrupt_macrium_reflect_image/", "subreddit_subscribers": 707440, "created_utc": 1697640473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A friend of mine is a total data hog to the point that he wants copy his home cctv video files to the cloud so he can, if required view footage should something happen at home when he\u2019s not there. \n\nI\u2019ve looked at a few solutions such as backblaze but I\u2019m not sure that their \u201ccomputer backup\u201d offering is right for him as it\u2019s not technically a backup. Their \u201cB2 cloud storage\u201d probably is the right fit but can get expensive. \n\nDoes anyone know of a solution that could fit the bill?", "author_fullname": "t2_h08x725u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Online data storeage, around 8tb-10tb per month.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aagy6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697581207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine is a total data hog to the point that he wants copy his home cctv video files to the cloud so he can, if required view footage should something happen at home when he\u2019s not there. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked at a few solutions such as backblaze but I\u2019m not sure that their \u201ccomputer backup\u201d offering is right for him as it\u2019s not technically a backup. Their \u201cB2 cloud storage\u201d probably is the right fit but can get expensive. &lt;/p&gt;\n\n&lt;p&gt;Does anyone know of a solution that could fit the bill?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aagy6", "is_robot_indexable": true, "report_reasons": null, "author": "oichie_uk", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aagy6/online_data_storeage_around_8tb10tb_per_month/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aagy6/online_data_storeage_around_8tb10tb_per_month/", "subreddit_subscribers": 707440, "created_utc": 1697581207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a cloud storage solutions that allows users to\n\n1. read/download all files \n2. upload their own files (anywhere, not just in a specific folder)\n3. but only edit/delete their own uploaded files.\n\nI would love if there was a cloud storage provider that allows me to just setup those permissions and share a link to the storage. If that's not a thing cloud storage providers offer I would setup my own NAS but I would need some guidance there too on how to set those permissions up on a NAS.", "author_fullname": "t2_1ng5ht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud storage with read permissions for all files AND edit permissions for your own files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17apa9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697631157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a cloud storage solutions that allows users to&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;read/download all files &lt;/li&gt;\n&lt;li&gt;upload their own files (anywhere, not just in a specific folder)&lt;/li&gt;\n&lt;li&gt;but only edit/delete their own uploaded files.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would love if there was a cloud storage provider that allows me to just setup those permissions and share a link to the storage. If that&amp;#39;s not a thing cloud storage providers offer I would setup my own NAS but I would need some guidance there too on how to set those permissions up on a NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17apa9v", "is_robot_indexable": true, "report_reasons": null, "author": "R0tum", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17apa9v/cloud_storage_with_read_permissions_for_all_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17apa9v/cloud_storage_with_read_permissions_for_all_files/", "subreddit_subscribers": 707440, "created_utc": 1697631157.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am asking this here because I want an NVMe SSD with more than 4 TB of capacity but at a low price. There are cheap 7.68 TB and larger U.2/U.3 drives on the second-hand market, so I think this could be useful to many people that also only have access to M.2 slots. \n\nI have an itx system with only M.2 slots that I can use to connect to U.2/U.3 drives. I was wondering if anyone has successfully confirmed any M.2 SFF-8643 to SFF-8639 adapters to properly work at PCIe 4.0 speed without any CRC errors? That's something I'm worried about after reading [this ServeTheHome thread](https://forums.servethehome.com/index.php?threads/m-2-to-u-2-3-pcie4x4.31193/).\n\nI would imagine that the M.2 to SFF-8643 adapter is the easy part in terms of signal integrity, so something like [this adapter from StarTech](https://www.startech.com/en-us/hdd/m2e4sff8643) should work. The hard part is probably getting a quality and short-enough SFF-8643 to SFF-8639 cable to run at PCIe 4.0 speeds without CRC errors and whatever else. Two cables that might work based off their descriptions and reviews are [this one from LINKUP](https://www.amazon.com/LINKUP-Internal-PCIE3-0-SFF-8643-SFF-8639/dp/B07KQPLZ23) and [this one from 10Gtek](https://www.amazon.com/10Gtek-Internal-SFF-8643-SFF-8639-Connector/dp/B095SDQWTY). They're probably random Chinese cables, but I suppose that's what you get for a niche cable like this.\n\nAll that being said, can anyone confirm if this is something that can be done, or if I should give up on this idea? I could *potentially* buy all the parts to try myself if there's enough confidence that it should work. If this isn't something that can be done, then I'd have to settle for a 4 TB M.2 because 8 TB M.2 drives haven't become cheap yet.", "author_fullname": "t2_7osthc1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any M.2 SFF-8643 to SFF-8639 U.2/U.3 Adapters Capable of PCIe 4.0 Speed without CRC Errors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aktmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697613950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am asking this here because I want an NVMe SSD with more than 4 TB of capacity but at a low price. There are cheap 7.68 TB and larger U.2/U.3 drives on the second-hand market, so I think this could be useful to many people that also only have access to M.2 slots. &lt;/p&gt;\n\n&lt;p&gt;I have an itx system with only M.2 slots that I can use to connect to U.2/U.3 drives. I was wondering if anyone has successfully confirmed any M.2 SFF-8643 to SFF-8639 adapters to properly work at PCIe 4.0 speed without any CRC errors? That&amp;#39;s something I&amp;#39;m worried about after reading &lt;a href=\"https://forums.servethehome.com/index.php?threads/m-2-to-u-2-3-pcie4x4.31193/\"&gt;this ServeTheHome thread&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I would imagine that the M.2 to SFF-8643 adapter is the easy part in terms of signal integrity, so something like &lt;a href=\"https://www.startech.com/en-us/hdd/m2e4sff8643\"&gt;this adapter from StarTech&lt;/a&gt; should work. The hard part is probably getting a quality and short-enough SFF-8643 to SFF-8639 cable to run at PCIe 4.0 speeds without CRC errors and whatever else. Two cables that might work based off their descriptions and reviews are &lt;a href=\"https://www.amazon.com/LINKUP-Internal-PCIE3-0-SFF-8643-SFF-8639/dp/B07KQPLZ23\"&gt;this one from LINKUP&lt;/a&gt; and &lt;a href=\"https://www.amazon.com/10Gtek-Internal-SFF-8643-SFF-8639-Connector/dp/B095SDQWTY\"&gt;this one from 10Gtek&lt;/a&gt;. They&amp;#39;re probably random Chinese cables, but I suppose that&amp;#39;s what you get for a niche cable like this.&lt;/p&gt;\n\n&lt;p&gt;All that being said, can anyone confirm if this is something that can be done, or if I should give up on this idea? I could &lt;em&gt;potentially&lt;/em&gt; buy all the parts to try myself if there&amp;#39;s enough confidence that it should work. If this isn&amp;#39;t something that can be done, then I&amp;#39;d have to settle for a 4 TB M.2 because 8 TB M.2 drives haven&amp;#39;t become cheap yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aktmu", "is_robot_indexable": true, "report_reasons": null, "author": "pullupsNpushups", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aktmu/any_m2_sff8643_to_sff8639_u2u3_adapters_capable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aktmu/any_m2_sff8643_to_sff8639_u2u3_adapters_capable/", "subreddit_subscribers": 707440, "created_utc": 1697613950.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9xfcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Had a drive fail, replaced it and had another drive fail during resilver. All fixed up now and scrubs almost done. Fingers crossed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 115, "top_awarded_type": null, "hide_score": true, "name": "t3_17azgy2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Di19kgK_baicGcdGAXlCSiRTiII9CH5mWBMbOKRbbms.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697658258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/v7n98apel0vb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/v7n98apel0vb1.png?auto=webp&amp;s=e8dbc441d5ee097a3813f80fa2d83f6d21ef54b7", "width": 704, "height": 579}, "resolutions": [{"url": "https://preview.redd.it/v7n98apel0vb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6097fbf96996acfa2c88ff8dc5616d518cb4ddfc", "width": 108, "height": 88}, {"url": "https://preview.redd.it/v7n98apel0vb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c00f47403da24dbeb31cb00d192d74b64377b86e", "width": 216, "height": 177}, {"url": "https://preview.redd.it/v7n98apel0vb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b392212b8a1a2847792adfe787fca526bc50ccb", "width": 320, "height": 263}, {"url": "https://preview.redd.it/v7n98apel0vb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd88b9c262276d2b5fdee1ed7517f9ea502352d0", "width": 640, "height": 526}], "variants": {}, "id": "iDnR-YmJnNqobaLfQlxXBpdJJsLMnk_SwTbXL_Pb8P4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "233TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17azgy2", "is_robot_indexable": true, "report_reasons": null, "author": "Neathh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17azgy2/had_a_drive_fail_replaced_it_and_had_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/v7n98apel0vb1.png", "subreddit_subscribers": 707440, "created_utc": 1697658258.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm asking bc there was this other software I used a while ago that automatically named it like S01D02 or something and it's so nice for organization so I was just wondering if there's a way for makemkv?", "author_fullname": "t2_k457l2o98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When I use MakeMKV to backup my blurays is there a way for them to name the folder better? Like name the season and disc it is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17axkl6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697653392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m asking bc there was this other software I used a while ago that automatically named it like S01D02 or something and it&amp;#39;s so nice for organization so I was just wondering if there&amp;#39;s a way for makemkv?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17axkl6", "is_robot_indexable": true, "report_reasons": null, "author": "OnlyMyOpinions", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17axkl6/when_i_use_makemkv_to_backup_my_blurays_is_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17axkl6/when_i_use_makemkv_to_backup_my_blurays_is_there/", "subreddit_subscribers": 707440, "created_utc": 1697653392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " The repos haven't been updated to support the newest matterport softwares and I am trying to archive a tour of my house before it is being sold. ", "author_fullname": "t2_dh4hjpj4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to hoard 3d house tours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aqpof", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697635392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The repos haven&amp;#39;t been updated to support the newest matterport softwares and I am trying to archive a tour of my house before it is being sold. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aqpof", "is_robot_indexable": true, "report_reasons": null, "author": "__Acedia_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aqpof/how_to_hoard_3d_house_tours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aqpof/how_to_hoard_3d_house_tours/", "subreddit_subscribers": 707440, "created_utc": 1697635392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently downloaded Filen.io as a backup for my OneDrive files and I wanted to know what was the recommended amount of cloud storage sites I should have for back up. Realistically, how many should I have and how can I better backup my stuff?", "author_fullname": "t2_2atcwqee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many cloud storage services should i have as backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17b14v1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697662555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently downloaded Filen.io as a backup for my OneDrive files and I wanted to know what was the recommended amount of cloud storage sites I should have for back up. Realistically, how many should I have and how can I better backup my stuff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b14v1", "is_robot_indexable": true, "report_reasons": null, "author": "Coolkatisa2511", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b14v1/how_many_cloud_storage_services_should_i_have_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b14v1/how_many_cloud_storage_services_should_i_have_as/", "subreddit_subscribers": 707440, "created_utc": 1697662555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For those who scan stuff, do you add metadata or information about the scans?  If so, what info and where do you store it?\n\nFor example: scanner used, scanning profile (color, grayscale), name of your cat, software used, etc.\n\nI've always wondered how best to do this.  I routinely go back to my scanned grayscale documents and wonder whether the document itself was printed in grayscale or whether it was a color document that I had just decided to scan to grayscale to save space.  It'd be nice to have some indication of this but I'm not sure where you'd typically store this type of information (metadata, separate file, or something else).", "author_fullname": "t2_ctpeck6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you add metadata to stuff you scan?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17azcbx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697657921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who scan stuff, do you add metadata or information about the scans?  If so, what info and where do you store it?&lt;/p&gt;\n\n&lt;p&gt;For example: scanner used, scanning profile (color, grayscale), name of your cat, software used, etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always wondered how best to do this.  I routinely go back to my scanned grayscale documents and wonder whether the document itself was printed in grayscale or whether it was a color document that I had just decided to scan to grayscale to save space.  It&amp;#39;d be nice to have some indication of this but I&amp;#39;m not sure where you&amp;#39;d typically store this type of information (metadata, separate file, or something else).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17azcbx", "is_robot_indexable": true, "report_reasons": null, "author": "DingusKhan62", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17azcbx/do_you_add_metadata_to_stuff_you_scan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17azcbx/do_you_add_metadata_to_stuff_you_scan/", "subreddit_subscribers": 707440, "created_utc": 1697657921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a drive which had windows installed on which starting playing up (boot looping) but it still has all my work files on it if I create an image of the hard drive using macrium reflect can I then reformat that hard drive to reinstall windows back on to it", "author_fullname": "t2_2edsbiiz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick question about reformatting a hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17auea5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697645304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a drive which had windows installed on which starting playing up (boot looping) but it still has all my work files on it if I create an image of the hard drive using macrium reflect can I then reformat that hard drive to reinstall windows back on to it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17auea5", "is_robot_indexable": true, "report_reasons": null, "author": "greenduckracing", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17auea5/quick_question_about_reformatting_a_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17auea5/quick_question_about_reformatting_a_hard_drive/", "subreddit_subscribers": 707440, "created_utc": 1697645304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a a list of 5000 urls to twitter images in a word doc, and I want to download them all. I presume I need a crawler (that goes down one degree), or is there any browser extension that can download them if I convert doc as into a HTML file? \n\n[https://pbs.twimg.com/profile\\_images/472358907734523904/M5CB9jSP\\_normal.jpeg](https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg)\n\n[https://pbs.twimg.com/profile\\_images/472358907734523904/M5CB9jSP\\_normal.jpeg](https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_11vkze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading direct twitter links to images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17attuz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a a list of 5000 urls to twitter images in a word doc, and I want to download them all. I presume I need a crawler (that goes down one degree), or is there any browser extension that can download them if I convert doc as into a HTML file? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg\"&gt;https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg\"&gt;https://pbs.twimg.com/profile_images/472358907734523904/M5CB9jSP_normal.jpeg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17attuz", "is_robot_indexable": true, "report_reasons": null, "author": "subaculture", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17attuz/downloading_direct_twitter_links_to_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17attuz/downloading_direct_twitter_links_to_images/", "subreddit_subscribers": 707440, "created_utc": 1697643829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, firstly wanna preface this by saying I got majority of my hardware cheap and it worked for me at the time but looking for a solution now.\n\nCurrently I have a R730 unfortunately the 8 bay SFF so I'm limited to 2.5\" drives this runs ESXI for any VMs I wanna spin up for school or testing projects. \n\nSecondly I have a day old Dell optiplex which essentially functions as my seed box running plex and the Arr suite but it can also only fit one drive and I have a Sabrent 2 bay drive enclosure attached to it through USB with 2 20TB WD Red Pro drives. \n\nThis is all in a makeshift rack with a cheap 2.5G switch and it works for majority of things but it just isnt optimal, especially if plex is doing anything substantial. I would like to consolidate this all into one server. \n\nI had preordered the HL15 from 45Drvies but being over $3000 Cad for me I think maybe I'm better off getting a R730XD with 16 drives? Is this overkill? Am I better of getting the HL15 in the long run, with the power cost of old hardware? Or should I be looking at something else? I am lucky enough to have a place to buy repurposed enterprise hardware for cheap so shipping wouldn't be a issue there.", "author_fullname": "t2_h47h5de", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help a Beginner Future Proof", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aro4t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697638004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, firstly wanna preface this by saying I got majority of my hardware cheap and it worked for me at the time but looking for a solution now.&lt;/p&gt;\n\n&lt;p&gt;Currently I have a R730 unfortunately the 8 bay SFF so I&amp;#39;m limited to 2.5&amp;quot; drives this runs ESXI for any VMs I wanna spin up for school or testing projects. &lt;/p&gt;\n\n&lt;p&gt;Secondly I have a day old Dell optiplex which essentially functions as my seed box running plex and the Arr suite but it can also only fit one drive and I have a Sabrent 2 bay drive enclosure attached to it through USB with 2 20TB WD Red Pro drives. &lt;/p&gt;\n\n&lt;p&gt;This is all in a makeshift rack with a cheap 2.5G switch and it works for majority of things but it just isnt optimal, especially if plex is doing anything substantial. I would like to consolidate this all into one server. &lt;/p&gt;\n\n&lt;p&gt;I had preordered the HL15 from 45Drvies but being over $3000 Cad for me I think maybe I&amp;#39;m better off getting a R730XD with 16 drives? Is this overkill? Am I better of getting the HL15 in the long run, with the power cost of old hardware? Or should I be looking at something else? I am lucky enough to have a place to buy repurposed enterprise hardware for cheap so shipping wouldn&amp;#39;t be a issue there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aro4t", "is_robot_indexable": true, "report_reasons": null, "author": "Ironrevo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aro4t/help_a_beginner_future_proof/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aro4t/help_a_beginner_future_proof/", "subreddit_subscribers": 707440, "created_utc": 1697638004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking into Amazon's refurbished drives, such as [this one](https://www.amazon.de/dp/B08Z77QT43?tag=synack0f0e-21&amp;linkCode=osi&amp;th=1&amp;psc=1). However, I am wondering if these drives are actually recertified (similar to those on ServerPartDeals; basically a customer returns) or if they have a decent number of work hours behind them. \n\nA product description does mention that:\n\n&gt;\\- This product will have a battery that exceeds 80% capacity relative to the new.\n\nBut I am not sure whether this is just a generic sentence or if it means that the product is at 80% of the designated MTBF (mean time between failures).", "author_fullname": "t2_ii3t82nb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Refurbished Drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17arf8l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697637368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking into Amazon&amp;#39;s refurbished drives, such as &lt;a href=\"https://www.amazon.de/dp/B08Z77QT43?tag=synack0f0e-21&amp;amp;linkCode=osi&amp;amp;th=1&amp;amp;psc=1\"&gt;this one&lt;/a&gt;. However, I am wondering if these drives are actually recertified (similar to those on ServerPartDeals; basically a customer returns) or if they have a decent number of work hours behind them. &lt;/p&gt;\n\n&lt;p&gt;A product description does mention that:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;- This product will have a battery that exceeds 80% capacity relative to the new.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But I am not sure whether this is just a generic sentence or if it means that the product is at 80% of the designated MTBF (mean time between failures).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17arf8l", "is_robot_indexable": true, "report_reasons": null, "author": "ExNihilo___", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17arf8l/amazon_refurbished_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17arf8l/amazon_refurbished_drives/", "subreddit_subscribers": 707440, "created_utc": 1697637368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not any kind of politician myself, but I'm sick and tired of the short-term memory of the electorate in my country (and city especially), and most of the news outlets and \"\"journalists\"\" around here a so sub-par that they basically just quote politicians all day without double-checking what they said before or giving some context to any promise, declaration, or accusation they throw around.\n\n&amp;#x200B;\n\nSo, while I'm also not a journalist either, I did study Law, and I've a few connections with a local investigative journalism team, and I'd like to help them scrape, store, and analyze as much data about \"\"old news\"\" (if that makes sense) as we can. I'd mostly be helping with analyzing the then-current legislation, how the \"\"promises\"\" evolved over the years, and what the actual results of their work are (government officials, mayors, and other local elected officials).\n\n&amp;#x200B;\n\nBasically, while I'll leave most of the journalism to the experts, I'd like to know the fastest ways to scrape through Google (or anything else you can suggest) for specific names and filter by keywords and time. Then, I'd also be grateful for tips on how to collect and organize these news articles/quotes/photos/videos/tweets/Facebook posts, etc, so they're easy to look through and store. It's a bit tougher to find these for my country with less international relevance, but I know it's all still around online, at least dating back to 2005-2008 when a lot of the current political forces \"\"began.\"\"\n\n&amp;#x200B;\n\nI've started working on a way to organize and categorize the articles I find, and I've already begun work on a couple of these politicians just by Googling, but I need a faster and more effective way of doing the same with at least two dozen people. I've also found a way to [download a video from a tweet](https://twtrvideodownloader.com/), which is easier than I expected, and some of these go back as far as 2012, so it'll be helpful. I'll also try to use graphs, timelines, and other visual aids to make the analysis more understandable and engaging.\n\n&amp;#x200B;\n\nTL;DR: Finding, storing, organizing, and analyzing old news articles and data - what's the fastest way to do it properly? Thanks!!", "author_fullname": "t2_ht74yc1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I store and analyze old news articles about local politics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aopsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697629367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not any kind of politician myself, but I&amp;#39;m sick and tired of the short-term memory of the electorate in my country (and city especially), and most of the news outlets and &amp;quot;&amp;quot;journalists&amp;quot;&amp;quot; around here a so sub-par that they basically just quote politicians all day without double-checking what they said before or giving some context to any promise, declaration, or accusation they throw around.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, while I&amp;#39;m also not a journalist either, I did study Law, and I&amp;#39;ve a few connections with a local investigative journalism team, and I&amp;#39;d like to help them scrape, store, and analyze as much data about &amp;quot;&amp;quot;old news&amp;quot;&amp;quot; (if that makes sense) as we can. I&amp;#39;d mostly be helping with analyzing the then-current legislation, how the &amp;quot;&amp;quot;promises&amp;quot;&amp;quot; evolved over the years, and what the actual results of their work are (government officials, mayors, and other local elected officials).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, while I&amp;#39;ll leave most of the journalism to the experts, I&amp;#39;d like to know the fastest ways to scrape through Google (or anything else you can suggest) for specific names and filter by keywords and time. Then, I&amp;#39;d also be grateful for tips on how to collect and organize these news articles/quotes/photos/videos/tweets/Facebook posts, etc, so they&amp;#39;re easy to look through and store. It&amp;#39;s a bit tougher to find these for my country with less international relevance, but I know it&amp;#39;s all still around online, at least dating back to 2005-2008 when a lot of the current political forces &amp;quot;&amp;quot;began.&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve started working on a way to organize and categorize the articles I find, and I&amp;#39;ve already begun work on a couple of these politicians just by Googling, but I need a faster and more effective way of doing the same with at least two dozen people. I&amp;#39;ve also found a way to &lt;a href=\"https://twtrvideodownloader.com/\"&gt;download a video from a tweet&lt;/a&gt;, which is easier than I expected, and some of these go back as far as 2012, so it&amp;#39;ll be helpful. I&amp;#39;ll also try to use graphs, timelines, and other visual aids to make the analysis more understandable and engaging.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: Finding, storing, organizing, and analyzing old news articles and data - what&amp;#39;s the fastest way to do it properly? Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aopsg", "is_robot_indexable": true, "report_reasons": null, "author": "acutelycrab27", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aopsg/how_do_i_store_and_analyze_old_news_articles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aopsg/how_do_i_store_and_analyze_old_news_articles/", "subreddit_subscribers": 707440, "created_utc": 1697629367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using official API to download data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajirr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_8dpo59j5", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Setting up", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Instagram", "selftext": "I am a normal user. Is there any idea how to use Instagram's Basic Display API to download Stories from people I follow ?\n\nI need to use the Official API to download them daily and archive it. So, I see it at a later time I wish rather than being forced to come daily to the app to check out the stories.\n\nThere are python scripts such as Instaloader but they are unofficial. I don't wanna use that.", "author_fullname": "t2_8dpo59j5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using official API to download data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Instagram", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajdb1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697608040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Instagram", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a normal user. Is there any idea how to use Instagram&amp;#39;s Basic Display API to download Stories from people I follow ?&lt;/p&gt;\n\n&lt;p&gt;I need to use the Official API to download them daily and archive it. So, I see it at a later time I wish rather than being forced to come daily to the app to check out the stories.&lt;/p&gt;\n\n&lt;p&gt;There are python scripts such as Instaloader but they are unofficial. I don&amp;#39;t wanna use that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3e572f16-c895-11e3-a1c9-12313d056e4a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2seh9", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17ajdb1", "is_robot_indexable": true, "report_reasons": null, "author": "Mc_King_95", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Instagram/comments/17ajdb1/using_official_api_to_download_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Instagram/comments/17ajdb1/using_official_api_to_download_data/", "subreddit_subscribers": 475210, "created_utc": 1697608040.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1697608644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Instagram", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Instagram/comments/17ajdb1/using_official_api_to_download_data/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17ajirr", "is_robot_indexable": true, "report_reasons": null, "author": "Mc_King_95", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_17ajdb1", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ajirr/using_official_api_to_download_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Instagram/comments/17ajdb1/using_official_api_to_download_data/", "subreddit_subscribers": 707440, "created_utc": 1697608644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I'm in a bit of a copyright problem where someone is copying my old animations. This is a reoccurring problem with this person.\nI'm pretty sure they credited me in the description when they were first uploaded, but since then the description has been edited and I'm no longer credited. \nIs there any site or way I can look at the old/original video description?", "author_fullname": "t2_lnse5pni8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to see old YouTube descriptions after they were edited?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ago7l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697598567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I&amp;#39;m in a bit of a copyright problem where someone is copying my old animations. This is a reoccurring problem with this person.\nI&amp;#39;m pretty sure they credited me in the description when they were first uploaded, but since then the description has been edited and I&amp;#39;m no longer credited. \nIs there any site or way I can look at the old/original video description?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ago7l", "is_robot_indexable": true, "report_reasons": null, "author": "FreeFromUsernames", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ago7l/is_there_a_way_to_see_old_youtube_descriptions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ago7l/is_there_a_way_to_see_old_youtube_descriptions/", "subreddit_subscribers": 707440, "created_utc": 1697598567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like the BBC, CNN, and the likes?", "author_fullname": "t2_8docc5x4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a project that preserves push notifications from popular news apps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ab87s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697583178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the BBC, CNN, and the likes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ab87s", "is_robot_indexable": true, "report_reasons": null, "author": "just73in", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ab87s/is_there_a_project_that_preserves_push/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ab87s/is_there_a_project_that_preserves_push/", "subreddit_subscribers": 707440, "created_utc": 1697583178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any suggestions on a plugin or tool that can help me download thousands of lightbox style gallery images? \n\nImage Grabber in Pale Moon always serves me well, but since these images have to be clicked to open the full size media it's not able to process them. \n\nOther extensions only download the thumbnail \"preview\" image on the main gallery page and not the actual full size media from the lightbox. \n\nAny suggestions are much appreciated!", "author_fullname": "t2_1qmujwbj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading lightbox gallery images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aa8rv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697580613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions on a plugin or tool that can help me download thousands of lightbox style gallery images? &lt;/p&gt;\n\n&lt;p&gt;Image Grabber in Pale Moon always serves me well, but since these images have to be clicked to open the full size media it&amp;#39;s not able to process them. &lt;/p&gt;\n\n&lt;p&gt;Other extensions only download the thumbnail &amp;quot;preview&amp;quot; image on the main gallery page and not the actual full size media from the lightbox. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions are much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17aa8rv", "is_robot_indexable": true, "report_reasons": null, "author": "twy-ster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17aa8rv/downloading_lightbox_gallery_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17aa8rv/downloading_lightbox_gallery_images/", "subreddit_subscribers": 707440, "created_utc": 1697580613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've used Auslogics for duplicate file finding &amp; I've found it's done a decent job. My issue is I've just discovered it doesn't find duplicates when the file names are different. I literally just made a copy of an image file &amp; renamed the new file - Auslogics didn't detect it.\n\nAs I'll have many files with different names that are actually the same file (let's say an audio file with a featuring artist - one file may say \"featuring XYZ\" whereas the other file doesn't for example) - I would like to know if there's a (preferably free) way to do this, or can Auslogics do it &amp; I've just not used it properly?", "author_fullname": "t2_q0mux6g8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Duplicate file finder - with different names?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17b09wc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697660314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used Auslogics for duplicate file finding &amp;amp; I&amp;#39;ve found it&amp;#39;s done a decent job. My issue is I&amp;#39;ve just discovered it doesn&amp;#39;t find duplicates when the file names are different. I literally just made a copy of an image file &amp;amp; renamed the new file - Auslogics didn&amp;#39;t detect it.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ll have many files with different names that are actually the same file (let&amp;#39;s say an audio file with a featuring artist - one file may say &amp;quot;featuring XYZ&amp;quot; whereas the other file doesn&amp;#39;t for example) - I would like to know if there&amp;#39;s a (preferably free) way to do this, or can Auslogics do it &amp;amp; I&amp;#39;ve just not used it properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b09wc", "is_robot_indexable": true, "report_reasons": null, "author": "Clive1792", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b09wc/duplicate_file_finder_with_different_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b09wc/duplicate_file_finder_with_different_names/", "subreddit_subscribers": 707440, "created_utc": 1697660314.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}