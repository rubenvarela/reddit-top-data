{"kind": "Listing", "data": {"after": "t3_17avd2w", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do \"big data\" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). \n\nI'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?\n\nMaybe I'm reading too much into this, and their description was just a weird way of saying \"expect standard data structures leetcode\"?", "author_fullname": "t2_mz663ln7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've got a DE Interview, but they're not letting me use libraries or SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aadnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697580962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they&amp;#39;ll have me do &amp;quot;big data&amp;quot; code challenges, and explicitly states that SQL, Pandas, etc. won&amp;#39;t be allowed (only default Python). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m honestly confused as most interviews I&amp;#39;ve had expected me to use data-related technologies, I find it odd that they&amp;#39;d explicitly exclude them. Has anyone encountered a similar situation?&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m reading too much into this, and their description was just a weird way of saying &amp;quot;expect standard data structures leetcode&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17aadnb", "is_robot_indexable": true, "report_reasons": null, "author": "arkoftheconvenient", "discussion_type": null, "num_comments": 100, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "subreddit_subscribers": 134712, "created_utc": 1697580962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?\n\nWould love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.\n\nTrying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you seen any examples of \u201cserious\u201d companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17asnwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from other DEs that serve data to &lt;strong&gt;pro-code&lt;/strong&gt; visualization tools like Shiny, Dash, or D3.js.&lt;/p&gt;\n\n&lt;p&gt;Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17asnwh", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 110, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "subreddit_subscribers": 134712, "created_utc": 1697640732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.\n\n&amp;#x200B;\n\nI frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.\n\n&amp;#x200B;\n\nAfter conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.\n\n&amp;#x200B;\n\nYou can try it out at [http://sqlvisual.net](http://sqlvisual.net) without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_la0ljviui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make SQL easy to understand with sqlvisual", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17au1n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697644390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can try it out at &lt;a href=\"http://sqlvisual.net\"&gt;http://sqlvisual.net&lt;/a&gt; without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17au1n7", "is_robot_indexable": true, "report_reasons": null, "author": "glxrun", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "subreddit_subscribers": 134712, "created_utc": 1697644390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?", "author_fullname": "t2_si3ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best boot camp for novice/intermediate data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ae5be", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697591224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ae5be", "is_robot_indexable": true, "report_reasons": null, "author": "Romarros", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "subreddit_subscribers": 134712, "created_utc": 1697591224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. \n\nWe are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. \n\nWe are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.   \nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.   \nOur goal is to work on anonymized data in the DEV &amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. \n\nI have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it's not even necessary (for at least the next 6 months).\n\nNow we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. \n\nI am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn't seem required to us). \n\nI understand that the Unity Catalog is the \"new hot thing\" that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? ", "author_fullname": "t2_5iao6str", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Unity Catalog worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17app3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697632428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. &lt;/p&gt;\n\n&lt;p&gt;We are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. &lt;/p&gt;\n\n&lt;p&gt;We are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.&lt;br/&gt;\nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.&lt;br/&gt;\nOur goal is to work on anonymized data in the DEV &amp;amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. &lt;/p&gt;\n\n&lt;p&gt;I have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it&amp;#39;s not even necessary (for at least the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;Now we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;I am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn&amp;#39;t seem required to us). &lt;/p&gt;\n\n&lt;p&gt;I understand that the Unity Catalog is the &amp;quot;new hot thing&amp;quot; that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17app3b", "is_robot_indexable": true, "report_reasons": null, "author": "Far-String829", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "subreddit_subscribers": 134712, "created_utc": 1697632428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ifpztnvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aotyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697629743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aotyk", "is_robot_indexable": true, "report_reasons": null, "author": "getriglad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aotyk/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing/", "subreddit_subscribers": 134712, "created_utc": 1697629743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! I've decided to ask this here as I've been lurking for a while and the community feels very approachable. I've googled this question and searched the Kimball's forum but to no avail. I also don't remember this from Kimball's book.\n\n&amp;#x200B;\n\nI'm creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these 'sub-dimensions'. The thing is... It's getting too wide? \n\nThat's the question I can't really answer. When is a dimension table too wide? Right now I'm around 100 columns, which feels excessive and might be hard to use for reporting. \n\nI'm thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_l38l5aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is a dimension table too denormalized? | Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aaznc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697582566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I&amp;#39;ve decided to ask this here as I&amp;#39;ve been lurking for a while and the community feels very approachable. I&amp;#39;ve googled this question and searched the Kimball&amp;#39;s forum but to no avail. I also don&amp;#39;t remember this from Kimball&amp;#39;s book.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these &amp;#39;sub-dimensions&amp;#39;. The thing is... It&amp;#39;s getting too wide? &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s the question I can&amp;#39;t really answer. When is a dimension table too wide? Right now I&amp;#39;m around 100 columns, which feels excessive and might be hard to use for reporting. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aaznc", "is_robot_indexable": true, "report_reasons": null, "author": "ArgenEgo", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "subreddit_subscribers": 134712, "created_utc": 1697582566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine you're in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.\n\nHow would you architect the analytics portion of this system? Where would you store the data?\n\nAssume we ideally want the data available in real-time, but some delay is acceptable\n\nHow would that architecture change depending on traffic needs? I'm curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that's scaling up to support tens or hundreds of millions of impressions in traffic per month.\n\n\\---\n\nThe most dead simple way would be to just store event data in a SQL database, but I assume that's bad practice as it's not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?\n\nSorry if these are dumb questions. I'm a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don't understand on a practical level how and when to actually implement this stuff.", "author_fullname": "t2_7h4yync7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to architect an analytics system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17atuwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine you&amp;#39;re in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.&lt;/p&gt;\n\n&lt;p&gt;How would you architect the analytics portion of this system? Where would you store the data?&lt;/p&gt;\n\n&lt;p&gt;Assume we ideally want the data available in real-time, but some delay is acceptable&lt;/p&gt;\n\n&lt;p&gt;How would that architecture change depending on traffic needs? I&amp;#39;m curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that&amp;#39;s scaling up to support tens or hundreds of millions of impressions in traffic per month.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The most dead simple way would be to just store event data in a SQL database, but I assume that&amp;#39;s bad practice as it&amp;#39;s not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?&lt;/p&gt;\n\n&lt;p&gt;Sorry if these are dumb questions. I&amp;#39;m a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don&amp;#39;t understand on a practical level how and when to actually implement this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17atuwj", "is_robot_indexable": true, "report_reasons": null, "author": "techworker716", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "subreddit_subscribers": 134712, "created_utc": 1697643908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.\n\nhttps://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;utm_campaign=post&amp;utm_medium=reddit", "author_fullname": "t2_a6t9ksvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From pipelines to platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17amyoo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit\"&gt;https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?auto=webp&amp;s=9c8e6e3536f0f4c65db581de64e64e99bab1b192", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aba92d06a8e7cbfde84a8a2608f9bb6b49e0f5fc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e1cd58976fb4b92ebbdb1950cd2e99e543ec074", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a11481ef0a5ec5245fc53e0770ef8c3883f2936d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9abc0eafe4bf2c1277f5b457028ec55073f86f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1cef300cde287850f63f6d5a3c52b7bb2fe92f2e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62146cd257889560244beacc968e6ce8235fe121", "width": 1080, "height": 540}], "variants": {}, "id": "qBpJuY4j45_ONii6LIPwu5GEADbSiI3mqZIgn1TG_4g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17amyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_End_979", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "subreddit_subscribers": 134712, "created_utc": 1697623030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have recently started working at a mid-company as a DE but apparently there's not a lot of work on DE involved around production for it. Since I just started, there's not a lot of work around the corner for me. I thought that since I'm not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I've thought of trying kaggle. Do you guys recommend of any other way to do so?", "author_fullname": "t2_8laf2pzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upskilling myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697608918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have recently started working at a mid-company as a DE but apparently there&amp;#39;s not a lot of work on DE involved around production for it. Since I just started, there&amp;#39;s not a lot of work around the corner for me. I thought that since I&amp;#39;m not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I&amp;#39;ve thought of trying kaggle. Do you guys recommend of any other way to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ajlft", "is_robot_indexable": true, "report_reasons": null, "author": "Key_Consideration385", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "subreddit_subscribers": 134712, "created_utc": 1697608918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.  \nIn my experience, 90% of the code and effort seems to be data plumbing.\n\nHow do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?\n\nI\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.\n\n[https://github.com/DataSQRL/sqrl](https://github.com/DataSQRL/sqrl)\n\nWhat do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution *could* exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!", "author_fullname": "t2_ihfn9f9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you solve data plumbing? Can we compile it away?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ahqib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697602012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.&lt;br/&gt;\nIn my experience, 90% of the code and effort seems to be data plumbing.&lt;/p&gt;\n\n&lt;p&gt;How do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/DataSQRL/sqrl\"&gt;https://github.com/DataSQRL/sqrl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution &lt;em&gt;could&lt;/em&gt; exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?auto=webp&amp;s=fa781d1ddcfbca87a6a2b23d5a68793504c49497", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c436bb0beceacd0ad7cfa97946894b6a4b7c9f97", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f989c23a59c0c3fee7040000bab38ffb717b6b0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93d69e2944ffed693f0ff4ed0d5f90dd756528b5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d23b6b8613519343928733fe218625dd049fda91", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e47dca88094c9601ef51964fd1d3b49b94d6343", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bf29af59fa7323cce2efdadd72bc6fb0d1ac9f", "width": 1080, "height": 540}], "variants": {}, "id": "45fS3FFg9knjZShinUTVGtiesnQZRpUhGztbG1rpTBs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ahqib", "is_robot_indexable": true, "report_reasons": null, "author": "matthiasBcom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "subreddit_subscribers": 134712, "created_utc": 1697602012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.\n\nSo far I've been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.\n\nAfter assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn't be using Debezium as, according to him and another expert, it doesn't simply read the db's logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db's log without needing a trigger from it.\n\nI've been doing some research and it turns out that there aren't many options on the table, especially if we consider that everything has to be **on premise** and according [to this paper from Netflix](https://arxiv.org/pdf/2010.12597.pdf) Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they're quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.\n\nSo I'm wondering whether the project is actually viable or if I'm headed into a dead end.\n\nI case it hasn't already transpired I'm not really a data engineer so I'm learning as much as possible during the process.", "author_fullname": "t2_79y3zlzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Viability of a CDC project paired with Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ash74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.&lt;/p&gt;\n\n&lt;p&gt;After assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn&amp;#39;t be using Debezium as, according to him and another expert, it doesn&amp;#39;t simply read the db&amp;#39;s logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db&amp;#39;s log without needing a trigger from it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing some research and it turns out that there aren&amp;#39;t many options on the table, especially if we consider that everything has to be &lt;strong&gt;on premise&lt;/strong&gt; and according &lt;a href=\"https://arxiv.org/pdf/2010.12597.pdf\"&gt;to this paper from Netflix&lt;/a&gt; Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they&amp;#39;re quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering whether the project is actually viable or if I&amp;#39;m headed into a dead end.&lt;/p&gt;\n\n&lt;p&gt;I case it hasn&amp;#39;t already transpired I&amp;#39;m not really a data engineer so I&amp;#39;m learning as much as possible during the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ash74", "is_robot_indexable": true, "report_reasons": null, "author": "ExactTreat593", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "subreddit_subscribers": 134712, "created_utc": 1697640225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently architecting a Azure solution for a DWH  that will integrate data extracted from \\~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.\n\nFor the extraction part, I'm leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered 'externally' via a call from ADF, Synapse pipelines or Airflow.\n\nI would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!", "author_fullname": "t2_lvjbzi2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting an Azure Solution for DWH Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aqwt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697635964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently architecting a Azure solution for a DWH  that will integrate data extracted from ~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.&lt;/p&gt;\n\n&lt;p&gt;For the extraction part, I&amp;#39;m leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered &amp;#39;externally&amp;#39; via a call from ADF, Synapse pipelines or Airflow.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aqwt9", "is_robot_indexable": true, "report_reasons": null, "author": "cloclosh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "subreddit_subscribers": 134712, "created_utc": 1697635964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.\n\nThey announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d\n\nNot sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).", "author_fullname": "t2_nexbbb26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company is implementing stack ranking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17b1zaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697664710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.&lt;/p&gt;\n\n&lt;p&gt;They announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d&lt;/p&gt;\n\n&lt;p&gt;Not sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b1zaw", "is_robot_indexable": true, "report_reasons": null, "author": "keep_it_professional", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "subreddit_subscribers": 134712, "created_utc": 1697664710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn't pricey, since it's not something I make money off of lol. ", "author_fullname": "t2_k4oyb5z58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Servers for Personal Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17azvdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697659289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn&amp;#39;t pricey, since it&amp;#39;s not something I make money off of lol. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "17azvdl", "is_robot_indexable": true, "report_reasons": null, "author": "SoccerFilmNerd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "subreddit_subscribers": 134712, "created_utc": 1697659289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I'm looking for help on a seemingly simple task that has me confused.\n\nThe vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.\n\nWe would hit the APIs about once a day to get updates on these conversions. Here's my idea so far:\n\n1. Our data isn't massive, so I'm ok with sticking with python+postgres and cron.\n\n2. Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.\n\n3. I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.\n\n4. Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.\n\n5. Use the LAG function to compare the status on this run to the status from the last run.\n\n6. If the status change is relevant to us, capture that conversion and forward it.\n\nThis seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.\n\nIs there a better way to do this that is standard practice?", "author_fullname": "t2_d8wq0wkd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for capturing changes in column values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17avpdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697648615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I&amp;#39;m looking for help on a seemingly simple task that has me confused.&lt;/p&gt;\n\n&lt;p&gt;The vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.&lt;/p&gt;\n\n&lt;p&gt;We would hit the APIs about once a day to get updates on these conversions. Here&amp;#39;s my idea so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Our data isn&amp;#39;t massive, so I&amp;#39;m ok with sticking with python+postgres and cron.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use the LAG function to compare the status on this run to the status from the last run.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If the status change is relevant to us, capture that conversion and forward it.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.&lt;/p&gt;\n\n&lt;p&gt;Is there a better way to do this that is standard practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17avpdg", "is_robot_indexable": true, "report_reasons": null, "author": "fabricmoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "subreddit_subscribers": 134712, "created_utc": 1697648615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:\n\n*We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json\\_normalize to flatten the json into a tabular format) However we've had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)*\n\n**What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?**\n\nOur current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can't directly load data that doesn't match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.\n\nWe want to try and keep this as close to realtime as possible and preferably serverless", "author_fullname": "t2_48vnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Problem: help with inconsistent day to day parquet schemas (pandas, AWS lambda)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq76s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json_normalize to flatten the json into a tabular format) However we&amp;#39;ve had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can&amp;#39;t directly load data that doesn&amp;#39;t match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.&lt;/p&gt;\n\n&lt;p&gt;We want to try and keep this as close to realtime as possible and preferably serverless&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aq76s", "is_robot_indexable": true, "report_reasons": null, "author": "freerangetrousers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "subreddit_subscribers": 134712, "created_utc": 1697633939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.\n\nGiving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar\n\nI know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).\n\nAlso what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: what software should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ap7is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.&lt;/p&gt;\n\n&lt;p&gt;Giving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar&lt;/p&gt;\n\n&lt;p&gt;I know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).&lt;/p&gt;\n\n&lt;p&gt;Also what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ap7is", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "subreddit_subscribers": 134712, "created_utc": 1697630909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Referring to taking data from the transaction database to a warehouse. What ETL process did you run?", "author_fullname": "t2_legyu6zou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone had any experience modelling data from a Netsuite database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17an2k7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Referring to taking data from the transaction database to a warehouse. What ETL process did you run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17an2k7", "is_robot_indexable": true, "report_reasons": null, "author": "anotherwetsock", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "subreddit_subscribers": 134712, "created_utc": 1697623431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use python and sql mostly for my job. I want to learn another language to have more competency. I touched c++ a bit when working on my esp32, but realised that the use of c++ in data engineering space is very niche and isolated to iot. If I can choose between java and javascript, thinking kotlin vs typescript to learn, which language will be more useful in general for data engineering?", "author_fullname": "t2_86pd6cgg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "java vs javascript as an additional language to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17afqeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697595729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use python and sql mostly for my job. I want to learn another language to have more competency. I touched c++ a bit when working on my esp32, but realised that the use of c++ in data engineering space is very niche and isolated to iot. If I can choose between java and javascript, thinking kotlin vs typescript to learn, which language will be more useful in general for data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17afqeu", "is_robot_indexable": true, "report_reasons": null, "author": "EmploymentMammoth659", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17afqeu/java_vs_javascript_as_an_additional_language_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17afqeu/java_vs_javascript_as_an_additional_language_to/", "subreddit_subscribers": 134712, "created_utc": 1697595729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.\n\nMy primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? \n\nAppreciate any insight.", "author_fullname": "t2_3auynywj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance on how event data is stored in a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17b2d47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.&lt;/p&gt;\n\n&lt;p&gt;My primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? &lt;/p&gt;\n\n&lt;p&gt;Appreciate any insight.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b2d47", "is_robot_indexable": true, "report_reasons": null, "author": "currentlyreadingit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "subreddit_subscribers": 134712, "created_utc": 1697665706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI have several parquet files in a datalake on premise. I would like to describe those datas and shemas using for  instance a json file as bellow. Does a standard format already exists for data catalog ? \n\n    {\n      \"url\":  \"albums.parquet\",\n      \"description\" : \"list of albums\" ,\n      \"schema\" : { \n       {\"name\": {\"string\", \"name of album\"},\n       {\"author\": {string, \"name of author\n      }\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_gdtn2w15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we have a standard format for describing a data catalog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17b2445", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I have several parquet files in a datalake on premise. I would like to describe those datas and shemas using for  instance a json file as bellow. Does a standard format already exists for data catalog ? &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;url&amp;quot;:  &amp;quot;albums.parquet&amp;quot;,\n  &amp;quot;description&amp;quot; : &amp;quot;list of albums&amp;quot; ,\n  &amp;quot;schema&amp;quot; : { \n   {&amp;quot;name&amp;quot;: {&amp;quot;string&amp;quot;, &amp;quot;name of album&amp;quot;},\n   {&amp;quot;author&amp;quot;: {string, &amp;quot;name of author\n  }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b2445", "is_robot_indexable": true, "report_reasons": null, "author": "TargetDangerous2216", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b2445/do_we_have_a_standard_format_for_describing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b2445/do_we_have_a_standard_format_for_describing_a/", "subreddit_subscribers": 134712, "created_utc": 1697665062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some money I can use for a short outsourced project (~$180k)\n\nCompany profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now\n\nI started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it's time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of \"important spreadsheets\" scattered around the place. Once we have the core data stack in place, I hope we'll be able to manage better and deliver more as a very small team\n\nSAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...\n\nWhat I'm thinking is that we build a short 3-5 page request for proposal (RFP) to \"build our data stack\" without actually specifying the tech stack we want, with some high level requirements of\n\n* Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure\n* Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting \n* Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too\n* We have a preference for popular and open tools with minimal maintenance\n\nAnother direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack\n\nThe main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don't know what it is\n\nWe've just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it's not a full enough platform yet, so it's a risk for us to dive into that\n\nThoughts? Anything I can add that would be helpful?\n\nThanks very much", "author_fullname": "t2_4t8zz1xo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you spend $180k on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b0jhs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697661031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some money I can use for a short outsourced project (~$180k)&lt;/p&gt;\n\n&lt;p&gt;Company profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now&lt;/p&gt;\n\n&lt;p&gt;I started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&amp;gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it&amp;#39;s time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of &amp;quot;important spreadsheets&amp;quot; scattered around the place. Once we have the core data stack in place, I hope we&amp;#39;ll be able to manage better and deliver more as a very small team&lt;/p&gt;\n\n&lt;p&gt;SAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m thinking is that we build a short 3-5 page request for proposal (RFP) to &amp;quot;build our data stack&amp;quot; without actually specifying the tech stack we want, with some high level requirements of&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure&lt;/li&gt;\n&lt;li&gt;Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting &lt;/li&gt;\n&lt;li&gt;Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too&lt;/li&gt;\n&lt;li&gt;We have a preference for popular and open tools with minimal maintenance&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Another direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack&lt;/p&gt;\n\n&lt;p&gt;The main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don&amp;#39;t know what it is&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it&amp;#39;s not a full enough platform yet, so it&amp;#39;s a risk for us to dive into that&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Anything I can add that would be helpful?&lt;/p&gt;\n\n&lt;p&gt;Thanks very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b0jhs", "is_robot_indexable": true, "report_reasons": null, "author": "c3f7", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "subreddit_subscribers": 134712, "created_utc": 1697661031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, is Dr. Fred Baptiste course good to build a solid knowledge on python for data engineering? Also, what sql recommendations can you guys give me? Thanks!", "author_fullname": "t2_6g45dtzy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ax69l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697652358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is Dr. Fred Baptiste course good to build a solid knowledge on python for data engineering? Also, what sql recommendations can you guys give me? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ax69l", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering-Rope-3210", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ax69l/python_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ax69l/python_course/", "subreddit_subscribers": 134712, "created_utc": 1697652358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was looking to extract a Windows log file in csv format with Pandas.   \nI'm trying to plot a graph with the UtcTime on the x-axis(in seconds) and the number of events/occurences on the y-axis(at the second when they occurred)  \n\n\nAny suggestion?  \n\n\nEvents are stored in this csv format:\n\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\\ProperTreeModuleInner\n    Details: Binary Data\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,12,Registry object added or deleted (rule: RegistryEvent),Registry object added or deleted:\n    RuleName: -\n    EventType: CreateKey\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\FFlags\n    Details: DWORD (0x00000001)\n    User: DESKTOP-D505GS3\\Tester\n    Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\n    RuleName: -\n    EventType: SetValue\n    UtcTime: 2023-10-09 18:07:25.965\n    ProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\n    ProcessId: 10144\n    Image: C:\\Windows\\system32\\mmc.exe\n    TargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\Mode\n    Details: DWORD (0x00000004)\n    User: DESKTOP-D505GS3\\Tester", "author_fullname": "t2_dtz2djqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract a log file with Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17avd2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697647765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was looking to extract a Windows log file in csv format with Pandas.&lt;br/&gt;\nI&amp;#39;m trying to plot a graph with the UtcTime on the x-axis(in seconds) and the number of events/occurences on the y-axis(at the second when they occurred)  &lt;/p&gt;\n\n&lt;p&gt;Any suggestion?  &lt;/p&gt;\n\n&lt;p&gt;Events are stored in this csv format:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Information,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\\ProperTreeModuleInner\nDetails: Binary Data\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,12,Registry object added or deleted (rule: RegistryEvent),Registry object added or deleted:\nRuleName: -\nEventType: CreateKey\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\CIDSave\\Modules\\GlobalSettings\\ProperTreeModuleInner\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\FFlags\nDetails: DWORD (0x00000001)\nUser: DESKTOP-D505GS3\\Tester\nInformation,09/10/2023 20:07:26,Microsoft-Windows-Sysmon,13,Registry value set (rule: RegistryEvent),Registry value set:\nRuleName: -\nEventType: SetValue\nUtcTime: 2023-10-09 18:07:25.965\nProcessGuid: {f3ae464f-9b50-6508-d00a-000000000e00}\nProcessId: 10144\nImage: C:\\Windows\\system32\\mmc.exe\nTargetObject: HKU\\S-1-5-21-3061039920-3460183204-410001764-1001_Classes\\Local Settings\\Software\\Microsoft\\Windows\\Shell\\Bags\\69\\ComDlg\\{5C4F28B5-F869-4E84-8E60-F11DB97C5CC7}\\Mode\nDetails: DWORD (0x00000004)\nUser: DESKTOP-D505GS3\\Tester\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17avd2w", "is_robot_indexable": true, "report_reasons": null, "author": "Otherwise_Virus_722", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17avd2w/extract_a_log_file_with_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17avd2w/extract_a_log_file_with_pandas/", "subreddit_subscribers": 134712, "created_utc": 1697647765.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}