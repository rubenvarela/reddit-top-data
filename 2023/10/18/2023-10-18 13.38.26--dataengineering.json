{"kind": "Listing", "data": {"after": "t3_17an2k7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do \"big data\" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). \n\nI'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?\n\nMaybe I'm reading too much into this, and their description was just a weird way of saying \"expect standard data structures leetcode\"?", "author_fullname": "t2_mz663ln7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've got a DE Interview, but they're not letting me use libraries or SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aadnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697580962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they&amp;#39;ll have me do &amp;quot;big data&amp;quot; code challenges, and explicitly states that SQL, Pandas, etc. won&amp;#39;t be allowed (only default Python). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m honestly confused as most interviews I&amp;#39;ve had expected me to use data-related technologies, I find it odd that they&amp;#39;d explicitly exclude them. Has anyone encountered a similar situation?&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m reading too much into this, and their description was just a weird way of saying &amp;quot;expect standard data structures leetcode&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17aadnb", "is_robot_indexable": true, "report_reasons": null, "author": "arkoftheconvenient", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "subreddit_subscribers": 134627, "created_utc": 1697580962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI could really use some guidance and advice on breaking into the field at an entry-level position. \n\nI don\u2019t have any peers or mentors so this subreddit is really my only connection to other data engineers.\n\n**TLDR**\n\n* I've learned programming and data engineering principles but struggle with confidence in my own ability to complete a project that would impress employers.\n* Burned out from tutorials and overwhelmed by the scope of some projects, I need guidance on narrowing down technologies and finding a realistic project.\n* I have a narrow time frame - I really need a job.\n* Advice on building a straightforward yet practical and comprehensive ETL/ELT pipeline (*asking for the impossible, I know..*) or recommendations for a reproducible project that would be appealing to employers would be greatly appreciated!\n\n**Full context**\n\nAfter spending almost a year learning programming and data engineering principles, I'm struggling to take the next step and become job-ready (I\u2019m applying to a few jobs here and there, but don\u2019t feel confident in my abilities to do more than that). While I have a good grasp of Python, SQL, and Relational Databases, I'm finding it difficult to complete a full project that would impress potential employers.\n\nI'm experiencing burnout from tutorials and feeling overwhelmed when trying to combine different technologies into a finished project. I understand the conceptual aspects, but the scope of work required to create a satisfactory end product is daunting.\n\nNarrowing down which technologies to use from the many available options is also proving challenging. I understand how to build rudimentary ETL scripts in Bash and Python, schedule using cron, connect to a relational database and insert data using SQL, but this doesn\u2019t feel like enough when I try to construct a practical or comprehensive data pipeline.\n\nWhen I try to learn and implement additional components like orchestration frameworks, transformation workflows, APIs, cloud computing, CI/CD workflows, monitoring procedures, UI interfaces, and deployment, it becomes too much to handle all at once. I\u2019m sure with more time, as well as renewed energy and motivation I will get to a point where I will be able to work with all of these technologies simultaneously but right now I am stressed out trying to just build *something* that resembles best practices, that would be appealing to a potential employer.\n\nVideo explanations and reverse engineering example repositories have been frustrating and often either don't provide the depth of knowledge I need, will lack clear explanations, or are out of date. (If I hear one more person describe something as a *simple* process while mumbling a half-baked explanation across a 3-hour unedited \"tutorial\" I might scream).\n\nAdditionally, having ADHD makes it difficult to stay focused and finish tasks without getting easily distracted by the feeling that I need to learn something else in order to move forward.\n\nI guess I\u2019m looking for guidance from someone who has successfully built a project that realistically covers the requirements of an entry-level position. I would really appreciate any advice or recommendations on how to produce a project that I can showcase and discuss during job interviews. Ideally, this would be a reproducible ETL/ELT pipeline following best practices, with a dashboard and data visualization component.\n\n*Sorry for the long post, thanks for reading and please don't hesitate to share any thoughts and/or advice!*", "author_fullname": "t2_dih4xe7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on breaking into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a1htx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697557861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I could really use some guidance and advice on breaking into the field at an entry-level position. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have any peers or mentors so this subreddit is really my only connection to other data engineers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve learned programming and data engineering principles but struggle with confidence in my own ability to complete a project that would impress employers.&lt;/li&gt;\n&lt;li&gt;Burned out from tutorials and overwhelmed by the scope of some projects, I need guidance on narrowing down technologies and finding a realistic project.&lt;/li&gt;\n&lt;li&gt;I have a narrow time frame - I really need a job.&lt;/li&gt;\n&lt;li&gt;Advice on building a straightforward yet practical and comprehensive ETL/ELT pipeline (&lt;em&gt;asking for the impossible, I know..&lt;/em&gt;) or recommendations for a reproducible project that would be appealing to employers would be greatly appreciated!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Full context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;After spending almost a year learning programming and data engineering principles, I&amp;#39;m struggling to take the next step and become job-ready (I\u2019m applying to a few jobs here and there, but don\u2019t feel confident in my abilities to do more than that). While I have a good grasp of Python, SQL, and Relational Databases, I&amp;#39;m finding it difficult to complete a full project that would impress potential employers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m experiencing burnout from tutorials and feeling overwhelmed when trying to combine different technologies into a finished project. I understand the conceptual aspects, but the scope of work required to create a satisfactory end product is daunting.&lt;/p&gt;\n\n&lt;p&gt;Narrowing down which technologies to use from the many available options is also proving challenging. I understand how to build rudimentary ETL scripts in Bash and Python, schedule using cron, connect to a relational database and insert data using SQL, but this doesn\u2019t feel like enough when I try to construct a practical or comprehensive data pipeline.&lt;/p&gt;\n\n&lt;p&gt;When I try to learn and implement additional components like orchestration frameworks, transformation workflows, APIs, cloud computing, CI/CD workflows, monitoring procedures, UI interfaces, and deployment, it becomes too much to handle all at once. I\u2019m sure with more time, as well as renewed energy and motivation I will get to a point where I will be able to work with all of these technologies simultaneously but right now I am stressed out trying to just build &lt;em&gt;something&lt;/em&gt; that resembles best practices, that would be appealing to a potential employer.&lt;/p&gt;\n\n&lt;p&gt;Video explanations and reverse engineering example repositories have been frustrating and often either don&amp;#39;t provide the depth of knowledge I need, will lack clear explanations, or are out of date. (If I hear one more person describe something as a &lt;em&gt;simple&lt;/em&gt; process while mumbling a half-baked explanation across a 3-hour unedited &amp;quot;tutorial&amp;quot; I might scream).&lt;/p&gt;\n\n&lt;p&gt;Additionally, having ADHD makes it difficult to stay focused and finish tasks without getting easily distracted by the feeling that I need to learn something else in order to move forward.&lt;/p&gt;\n\n&lt;p&gt;I guess I\u2019m looking for guidance from someone who has successfully built a project that realistically covers the requirements of an entry-level position. I would really appreciate any advice or recommendations on how to produce a project that I can showcase and discuss during job interviews. Ideally, this would be a reproducible ETL/ELT pipeline following best practices, with a dashboard and data visualization component.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Sorry for the long post, thanks for reading and please don&amp;#39;t hesitate to share any thoughts and/or advice!&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a1htx", "is_robot_indexable": true, "report_reasons": null, "author": "Weekly_Yellow1256", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a1htx/seeking_advice_on_breaking_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a1htx/seeking_advice_on_breaking_into_data_engineering/", "subreddit_subscribers": 134627, "created_utc": 1697557861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI work for a fintech startup which is a small business where I'm the sole individual responsible for building data applications to inform the business. I'm looking for the best data infrastructure for a small business using a modern data stack.\n\nTo date, I've addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I'm pondering about the next professional step, especially as we anticipate our system to grow.\n\nOur primary data sources are:\n\n* **RDS (MySQL)**: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there's a hefty table with several JSONs weighing 14GB.\n* **DynamoDB**: A table for financial decisions with 2 million rows, taking up 3GB.\n* **Google Sheets**: Tables with fewer than a thousand rows.\n* **Bank API for fetching rates**: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.\n\nThe solution I'm considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I'd connect Power BI with Amazon Athena to these transformed databases in S3.\n\nDoes this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I'm leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company's size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.\n\nWould you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?\n\nI'm grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.\n\nThank you in advance for your advice and expertise!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Infrastructure on AWS without Redshift for a small business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a6g6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697570897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I work for a fintech startup which is a small business where I&amp;#39;m the sole individual responsible for building data applications to inform the business. I&amp;#39;m looking for the best data infrastructure for a small business using a modern data stack.&lt;/p&gt;\n\n&lt;p&gt;To date, I&amp;#39;ve addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I&amp;#39;m pondering about the next professional step, especially as we anticipate our system to grow.&lt;/p&gt;\n\n&lt;p&gt;Our primary data sources are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;RDS (MySQL)&lt;/strong&gt;: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there&amp;#39;s a hefty table with several JSONs weighing 14GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt;: A table for financial decisions with 2 million rows, taking up 3GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Sheets&lt;/strong&gt;: Tables with fewer than a thousand rows.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bank API for fetching rates&lt;/strong&gt;: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The solution I&amp;#39;m considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I&amp;#39;d connect Power BI with Amazon Athena to these transformed databases in S3.&lt;/p&gt;\n\n&lt;p&gt;Does this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I&amp;#39;m leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company&amp;#39;s size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.&lt;/p&gt;\n\n&lt;p&gt;Would you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your advice and expertise!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a6g6i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "subreddit_subscribers": 134627, "created_utc": 1697570897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?", "author_fullname": "t2_si3ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best boot camp for novice/intermediate data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ae5be", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697591224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a data analyst (4 years exp) turned data scientist (2 years exp). The last 10 months I\u2019ve been working as a de facto analytics engineer but I\u2019m wondering what\u2019s the best way to break into a full-time data engineer role and build a good foundation. Should I do a boot camp? If so, what are some good ones you\u2019d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ae5be", "is_robot_indexable": true, "report_reasons": null, "author": "Romarros", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ae5be/best_boot_camp_for_noviceintermediate_data/", "subreddit_subscribers": 134627, "created_utc": 1697591224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE's) into a false sense of reliability, but will ultimately be flawed and become a liability.\n\nI am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I've been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.\n\nIn some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd\n\nI want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting\n\nI've googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):\n\n1. Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?\n2. Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022\n3. Data Pipelines Pocket Reference -  James Densmore, 2021, again isn't it too old?\n4. Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?\n5. Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?\n\nI would like the books to be as close to the real world scenarios, production data engineering.\n\nHow would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Close the gap and consolidate fundamentals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a900f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697577679.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697577490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE&amp;#39;s) into a false sense of reliability, but will ultimately be flawed and become a liability.&lt;/p&gt;\n\n&lt;p&gt;I am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I&amp;#39;ve been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.&lt;/p&gt;\n\n&lt;p&gt;In some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd&lt;/p&gt;\n\n&lt;p&gt;I want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?&lt;/li&gt;\n&lt;li&gt;Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022&lt;/li&gt;\n&lt;li&gt;Data Pipelines Pocket Reference -  James Densmore, 2021, again isn&amp;#39;t it too old?&lt;/li&gt;\n&lt;li&gt;Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?&lt;/li&gt;\n&lt;li&gt;Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would like the books to be as close to the real world scenarios, production data engineering.&lt;/p&gt;\n\n&lt;p&gt;How would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a900f", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "subreddit_subscribers": 134627, "created_utc": 1697577490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.\n\nhttps://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;utm_campaign=post&amp;utm_medium=reddit", "author_fullname": "t2_a6t9ksvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From pipelines to platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17amyoo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit\"&gt;https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?auto=webp&amp;s=9c8e6e3536f0f4c65db581de64e64e99bab1b192", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aba92d06a8e7cbfde84a8a2608f9bb6b49e0f5fc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e1cd58976fb4b92ebbdb1950cd2e99e543ec074", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a11481ef0a5ec5245fc53e0770ef8c3883f2936d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9abc0eafe4bf2c1277f5b457028ec55073f86f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1cef300cde287850f63f6d5a3c52b7bb2fe92f2e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62146cd257889560244beacc968e6ce8235fe121", "width": 1080, "height": 540}], "variants": {}, "id": "qBpJuY4j45_ONii6LIPwu5GEADbSiI3mqZIgn1TG_4g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17amyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_End_979", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "subreddit_subscribers": 134627, "created_utc": 1697623030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! I've decided to ask this here as I've been lurking for a while and the community feels very approachable. I've googled this question and searched the Kimball's forum but to no avail. I also don't remember this from Kimball's book.\n\n&amp;#x200B;\n\nI'm creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these 'sub-dimensions'. The thing is... It's getting too wide? \n\nThat's the question I can't really answer. When is a dimension table too wide? Right now I'm around 100 columns, which feels excessive and might be hard to use for reporting. \n\nI'm thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_l38l5aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is a dimension table too denormalized? | Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aaznc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697582566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I&amp;#39;ve decided to ask this here as I&amp;#39;ve been lurking for a while and the community feels very approachable. I&amp;#39;ve googled this question and searched the Kimball&amp;#39;s forum but to no avail. I also don&amp;#39;t remember this from Kimball&amp;#39;s book.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these &amp;#39;sub-dimensions&amp;#39;. The thing is... It&amp;#39;s getting too wide? &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s the question I can&amp;#39;t really answer. When is a dimension table too wide? Right now I&amp;#39;m around 100 columns, which feels excessive and might be hard to use for reporting. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aaznc", "is_robot_indexable": true, "report_reasons": null, "author": "ArgenEgo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "subreddit_subscribers": 134627, "created_utc": 1697582566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog Post from OpenMetadata team [https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364](https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364) on how to build/deploy , capture results and set alerts all via OpenMetadata. ", "author_fullname": "t2_4h8ymgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplifying Data Quality using OpenMetadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3so0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog Post from OpenMetadata team &lt;a href=\"https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364\"&gt;https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364&lt;/a&gt; on how to build/deploy , capture results and set alerts all via OpenMetadata. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?auto=webp&amp;s=11b603c7839cb0bcfb1836e0d02264077be4df18", "width": 1200, "height": 483}, "resolutions": [{"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d9a2f96a33e3f9f02be6785888e32ab79cf764b", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2870e974d4c22835d1ce3c0cf7dbda2104e5d1bb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46355b53cdbb9c4e84d01e385b56b914cf89f122", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ef23face4242dad2d4237e7ce5853283b426da", "width": 640, "height": 257}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b9d85a129f9b34ef5cb4608e8fba45664753524", "width": 960, "height": 386}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9159457240321cfbc20d4b8e633a605abdc1b31a", "width": 1080, "height": 434}], "variants": {}, "id": "jibYnJc1eg8fLQihM9kKlMVQV76i3ag7MfEojzuA7Ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17a3so0", "is_robot_indexable": true, "report_reasons": null, "author": "d3fmacro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "subreddit_subscribers": 134627, "created_utc": 1697563984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am very new to aws so sorry for the dumb question but I was hoping for some advice on the best way to do this. I am pretty sure that snowball is not an option here too. I was considering the following two options but I am unsure which would be faster:\n\n1. use either `aws s3 cp --recursive` or `aws s3 sync` and move all the files to a s3 bucket and then once all the files are in the bucket, I will do the same and move them from the bucket to the volume.\n\n2. again use either `aws s3 cp --recursive` or `aws s3 sync` to move the files to the bucket but then have a trigger set so that all incoming files to the s3 bucket automatically get copied into the EBS volume.\n\nWhich of these ways would be faster and are there better ways that I should consider to do all of this? Thanks!\n\nSome edits:\n* I should be more clear on the data. I have not exactly seen that data yet but I do know that it is 8TiB of images but I am unsure of the file type.", "author_fullname": "t2_3h5b6a2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some ways I can move 8TiB of data from my local machine to an EBS volume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2zbn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697580680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697561856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very new to aws so sorry for the dumb question but I was hoping for some advice on the best way to do this. I am pretty sure that snowball is not an option here too. I was considering the following two options but I am unsure which would be faster:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;use either &lt;code&gt;aws s3 cp --recursive&lt;/code&gt; or &lt;code&gt;aws s3 sync&lt;/code&gt; and move all the files to a s3 bucket and then once all the files are in the bucket, I will do the same and move them from the bucket to the volume.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;again use either &lt;code&gt;aws s3 cp --recursive&lt;/code&gt; or &lt;code&gt;aws s3 sync&lt;/code&gt; to move the files to the bucket but then have a trigger set so that all incoming files to the s3 bucket automatically get copied into the EBS volume.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Which of these ways would be faster and are there better ways that I should consider to do all of this? Thanks!&lt;/p&gt;\n\n&lt;p&gt;Some edits:\n* I should be more clear on the data. I have not exactly seen that data yet but I do know that it is 8TiB of images but I am unsure of the file type.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a2zbn", "is_robot_indexable": true, "report_reasons": null, "author": "psssat", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2zbn/what_are_some_ways_i_can_move_8tib_of_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2zbn/what_are_some_ways_i_can_move_8tib_of_data_from/", "subreddit_subscribers": 134627, "created_utc": 1697561856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm seriously confused about this.\n\nfrom what i gather. it's like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.\n\ni'm trying to use pyiceberg to implement this.\n\ni created an iceberg table in s3. but now how do i actually point it to parquet files?\n\ni tried putting them in the same directory, that didn't do it.\n\ni tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.\n\nupdate:\n\nok this makes sense.\nhttps://github.com/apache/iceberg-python/issues/23\n\nthis library does not have the capability yet, but they are working on it.\n\nguess ill wait", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can anyone explain apache iceberg better than chatgpt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a8dfa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697595187.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697575896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m seriously confused about this.&lt;/p&gt;\n\n&lt;p&gt;from what i gather. it&amp;#39;s like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m trying to use pyiceberg to implement this.&lt;/p&gt;\n\n&lt;p&gt;i created an iceberg table in s3. but now how do i actually point it to parquet files?&lt;/p&gt;\n\n&lt;p&gt;i tried putting them in the same directory, that didn&amp;#39;t do it.&lt;/p&gt;\n\n&lt;p&gt;i tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.&lt;/p&gt;\n\n&lt;p&gt;update:&lt;/p&gt;\n\n&lt;p&gt;ok this makes sense.\n&lt;a href=\"https://github.com/apache/iceberg-python/issues/23\"&gt;https://github.com/apache/iceberg-python/issues/23&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;this library does not have the capability yet, but they are working on it.&lt;/p&gt;\n\n&lt;p&gt;guess ill wait&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?auto=webp&amp;s=31e34155cd137a7525a7e5f701d7e8e192b92605", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a98431ffefe10eff79cebd3d42a62cf257f44c4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d5bb50f74032c8aa522452daeceadde8e247ec7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=086894491b879a6c7ea93a033e9f29d5da55aafd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf9f50d600924f27002673ade980086d2ada9579", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=192625faf66eef19687259ca7059fbf0e13890ea", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qLKC30zsBHXRW0vVqtv1K4nVvtLk99bFB01NKCapHUA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3ca73d8340497056b29c1d919ea28e39221e151", "width": 1080, "height": 540}], "variants": {}, "id": "i4ID1QjNU5CbbrmzIy_Hv86E7TVuRnrDtybuA_z6Lu0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a8dfa", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "subreddit_subscribers": 134627, "created_utc": 1697575896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have recently started working at a mid-company as a DE but apparently there's not a lot of work on DE involved around production for it. Since I just started, there's not a lot of work around the corner for me. I thought that since I'm not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I've thought of trying kaggle. Do you guys recommend of any other way to do so?", "author_fullname": "t2_8laf2pzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upskilling myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697608918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have recently started working at a mid-company as a DE but apparently there&amp;#39;s not a lot of work on DE involved around production for it. Since I just started, there&amp;#39;s not a lot of work around the corner for me. I thought that since I&amp;#39;m not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I&amp;#39;ve thought of trying kaggle. Do you guys recommend of any other way to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ajlft", "is_robot_indexable": true, "report_reasons": null, "author": "Key_Consideration385", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "subreddit_subscribers": 134627, "created_utc": 1697608918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.  \nIn my experience, 90% of the code and effort seems to be data plumbing.\n\nHow do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?\n\nI\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.\n\n[https://github.com/DataSQRL/sqrl](https://github.com/DataSQRL/sqrl)\n\nWhat do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution *could* exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!", "author_fullname": "t2_ihfn9f9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you solve data plumbing? Can we compile it away?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ahqib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697602012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.&lt;br/&gt;\nIn my experience, 90% of the code and effort seems to be data plumbing.&lt;/p&gt;\n\n&lt;p&gt;How do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/DataSQRL/sqrl\"&gt;https://github.com/DataSQRL/sqrl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution &lt;em&gt;could&lt;/em&gt; exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?auto=webp&amp;s=fa781d1ddcfbca87a6a2b23d5a68793504c49497", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c436bb0beceacd0ad7cfa97946894b6a4b7c9f97", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f989c23a59c0c3fee7040000bab38ffb717b6b0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93d69e2944ffed693f0ff4ed0d5f90dd756528b5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d23b6b8613519343928733fe218625dd049fda91", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e47dca88094c9601ef51964fd1d3b49b94d6343", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bf29af59fa7323cce2efdadd72bc6fb0d1ac9f", "width": 1080, "height": 540}], "variants": {}, "id": "45fS3FFg9knjZShinUTVGtiesnQZRpUhGztbG1rpTBs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ahqib", "is_robot_indexable": true, "report_reasons": null, "author": "matthiasBcom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "subreddit_subscribers": 134627, "created_utc": 1697602012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?\n\nTrying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise\n\nThanks in advance!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3k4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?&lt;/p&gt;\n\n&lt;p&gt;Trying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17a3k4m", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3k4m/question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3k4m/question/", "subreddit_subscribers": 134627, "created_utc": 1697563339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "## Introduction\n\nHey guys, I am a solo data engineer at a small-medium sized manufacturing company. Something that I run into pretty often at my current company is the need for a solution for data capture of some manual process in order to create a KPI. These scenarios don't typically merit the construction of a full application around the process, whose data can then be tapped to report on the KPI of \\_\\_\\_\\_. Most stakeholder's simply want a place to enter their hand collected weekly data, and so the idea that I have come up with is using a cloud Excel workbook with a table where the stakeholder can manually enter their weekly data. I'm not satisfied with this solution, but I'm having trouble thinking of alternatives that don't require more work on my end than the KPI seems to even be worth.\n\n## Question\n\nI was wondering if there is a best-practice approach in the industry to this sort of situation.", "author_fullname": "t2_fjnfv117", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom solutions for data capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179zkaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697552670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Introduction&lt;/h2&gt;\n\n&lt;p&gt;Hey guys, I am a solo data engineer at a small-medium sized manufacturing company. Something that I run into pretty often at my current company is the need for a solution for data capture of some manual process in order to create a KPI. These scenarios don&amp;#39;t typically merit the construction of a full application around the process, whose data can then be tapped to report on the KPI of ____. Most stakeholder&amp;#39;s simply want a place to enter their hand collected weekly data, and so the idea that I have come up with is using a cloud Excel workbook with a table where the stakeholder can manually enter their weekly data. I&amp;#39;m not satisfied with this solution, but I&amp;#39;m having trouble thinking of alternatives that don&amp;#39;t require more work on my end than the KPI seems to even be worth.&lt;/p&gt;\n\n&lt;h2&gt;Question&lt;/h2&gt;\n\n&lt;p&gt;I was wondering if there is a best-practice approach in the industry to this sort of situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179zkaa", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Wear3951", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179zkaa/custom_solutions_for_data_capture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179zkaa/custom_solutions_for_data_capture/", "subreddit_subscribers": 134627, "created_utc": 1697552670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ifpztnvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17aotyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697629743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aotyk", "is_robot_indexable": true, "report_reasons": null, "author": "getriglad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aotyk/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing/", "subreddit_subscribers": 134627, "created_utc": 1697629743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nWe are looking to modernize our ETL infrastructure for one of our business processes that leverages informatica. I don't have any experience with the backend of informatica nor a modern comparator like databrics. But I know we should push towards it. Can anyone give a strong technical rationale why this is the way to go?", "author_fullname": "t2_4oldr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modernizing from informatica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2tlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697589736.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697561442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;We are looking to modernize our ETL infrastructure for one of our business processes that leverages informatica. I don&amp;#39;t have any experience with the backend of informatica nor a modern comparator like databrics. But I know we should push towards it. Can anyone give a strong technical rationale why this is the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17a2tlf", "is_robot_indexable": true, "report_reasons": null, "author": "lysis_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2tlf/modernizing_from_informatica/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2tlf/modernizing_from_informatica/", "subreddit_subscribers": 134627, "created_utc": 1697561442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "background :  started my current data role about a year ago. I had minimal azure skills but was able to pickup quite a bit as I previously worked on a similar stack. I\u2019ve mainly been doing Data factory pipeline and SQL for data loads. As an org we are still maturing in data practices and i introduced new process like automated deployment release and Incremental data loads (and received great feedback from manager)\n\norg issues :\n 1 - my team lead is seems overbearing and wants to rehash things a few times. So basically whenever I work on something new, he wants me go over things atleast 4-5 time. I\u2019ve noticed that he does that with 2 other ETL developers in the team. He does minimal hands on work but still wants everything to be explained to him until he gets it. I find that quite draining and many times I feel like I could use my time better working on stuff. \n\n2- second issue I am dealing with is the devs who\u2019ve been around there 2-3 year longer. They usually don\u2019t support my ideas even though it could be better for the team. I have resolved code issues in the past that they weren\u2019t able to and have generally noticed a slower pace of development on their side (these folks are contractors , unfortunately my org is slightly old school , contractors are paid quite high and expected to implement a lot of stuff quickly) \n\nAnyone navigated similar issues ? How do you boost morale and continue with growth in your organization? And perhaps gain more autonomy to implement new ideas in midst of pushy/adamant co workers ?", "author_fullname": "t2_d90jcqqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to gain more autonomy as a new-ish employee?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2njr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697560995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;background :  started my current data role about a year ago. I had minimal azure skills but was able to pickup quite a bit as I previously worked on a similar stack. I\u2019ve mainly been doing Data factory pipeline and SQL for data loads. As an org we are still maturing in data practices and i introduced new process like automated deployment release and Incremental data loads (and received great feedback from manager)&lt;/p&gt;\n\n&lt;p&gt;org issues :\n 1 - my team lead is seems overbearing and wants to rehash things a few times. So basically whenever I work on something new, he wants me go over things atleast 4-5 time. I\u2019ve noticed that he does that with 2 other ETL developers in the team. He does minimal hands on work but still wants everything to be explained to him until he gets it. I find that quite draining and many times I feel like I could use my time better working on stuff. &lt;/p&gt;\n\n&lt;p&gt;2- second issue I am dealing with is the devs who\u2019ve been around there 2-3 year longer. They usually don\u2019t support my ideas even though it could be better for the team. I have resolved code issues in the past that they weren\u2019t able to and have generally noticed a slower pace of development on their side (these folks are contractors , unfortunately my org is slightly old school , contractors are paid quite high and expected to implement a lot of stuff quickly) &lt;/p&gt;\n\n&lt;p&gt;Anyone navigated similar issues ? How do you boost morale and continue with growth in your organization? And perhaps gain more autonomy to implement new ideas in midst of pushy/adamant co workers ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17a2njr", "is_robot_indexable": true, "report_reasons": null, "author": "Proof-Yellow6362", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2njr/how_to_gain_more_autonomy_as_a_newish_employee/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2njr/how_to_gain_more_autonomy_as_a_newish_employee/", "subreddit_subscribers": 134627, "created_utc": 1697560995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I(M23) have just started an internship in an organization under the Data Engineering section. I have a background in Statistics. I am currently looking for resources, Big data, Apache Nifi and Hadoop, both on premise and cloud(AWS) to facilitate an easy transition for me in the team. Any and all learning materials you may have are welcome, Thank you.", "author_fullname": "t2_cxzdcift", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do I start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179yvgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697550770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I(M23) have just started an internship in an organization under the Data Engineering section. I have a background in Statistics. I am currently looking for resources, Big data, Apache Nifi and Hadoop, both on premise and cloud(AWS) to facilitate an easy transition for me in the team. Any and all learning materials you may have are welcome, Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "179yvgb", "is_robot_indexable": true, "report_reasons": null, "author": "Lazy_Secret8626", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179yvgb/where_do_i_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179yvgb/where_do_i_start/", "subreddit_subscribers": 134627, "created_utc": 1697550770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products? Deploy: Part 3/4 - Doubling down on the power of Unified Experiences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_179ycsl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fIAPxsCawPBu-N75Vvoyg_keOmRvMjS4W4feJpzH2AE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697549245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?auto=webp&amp;s=ea619a7c524f8c5536cc06f8b13ba792caddd5b5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66add8fac528d26de7feccdd0c5022d39fd89850", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=85446cd271d715b3865f735be872ce747366b093", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=929b190abb183f4784a7d725a150dbb2083ba894", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e4c400d8251d52d7421fc6640499a17ea524db3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e716ed51284478d9d2a2569fdf859fe895fdf91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5219b40b806ce0099e4650da1d92ec438c0e19d8", "width": 1080, "height": 540}], "variants": {}, "id": "_WJ_OVW7dv_Q8Y2muH3UxiO88vjiOO6xTcwW1zV8-tA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "179ycsl", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179ycsl/how_to_build_data_products_deploy_part_34/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "subreddit_subscribers": 134627, "created_utc": 1697549245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:\n\n*We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json\\_normalize to flatten the json into a tabular format) However we've had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)*\n\n**What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?**\n\nOur current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can't directly load data that doesn't match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.\n\nWe want to try and keep this as close to realtime as possible and preferably serverless", "author_fullname": "t2_48vnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Problem: help with inconsistent day to day parquet schemas (pandas, AWS lambda)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17aq76s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json_normalize to flatten the json into a tabular format) However we&amp;#39;ve had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can&amp;#39;t directly load data that doesn&amp;#39;t match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.&lt;/p&gt;\n\n&lt;p&gt;We want to try and keep this as close to realtime as possible and preferably serverless&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aq76s", "is_robot_indexable": true, "report_reasons": null, "author": "freerangetrousers", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "subreddit_subscribers": 134627, "created_utc": 1697633939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6y0b4txf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How we built a Streaming SQL Engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": true, "name": "t3_17aq6b0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JZpOkQZB5tMoMElRL2wme009VaJvENXW1wgEJj845dw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697633860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.epsio.io/blog/how-to-create-a-streaming-sql-engine", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?auto=webp&amp;s=bde1bb439f8042c2066e539de311e97a9b34f68c", "width": 1376, "height": 714}, "resolutions": [{"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eac5d4e9cb92dce7b0b63f164195fd8d9934b59c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41f34043110161b56e0b27712a353a975d56bf20", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b52001b9486b04f0742e761902c5cfcd8cbbf0dc", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a259d069e4ab50668673eb345d4d655ffd6c5cb5", "width": 640, "height": 332}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fa737b762cbbbf12c1acfcccbaca8eb8785678f", "width": 960, "height": 498}, {"url": "https://external-preview.redd.it/FnoSiaDUngR1tdl1ppiLK9jThDecKTz8uqybcSLwtIk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=213bb55bc131cf226f4518542ac26d7f7de4a0fa", "width": 1080, "height": 560}], "variants": {}, "id": "L6vsGJtlTpCBQYIHfzVB4U_eHzCDAFcguQ6gS8SW1ns"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aq6b0", "is_robot_indexable": true, "report_reasons": null, "author": "Giladkl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq6b0/how_we_built_a_streaming_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.epsio.io/blog/how-to-create-a-streaming-sql-engine", "subreddit_subscribers": 134627, "created_utc": 1697633860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "5 Lessons Learned from Testing Databricks SQL Serverless + DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_17aq0oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LUm9ZURK_0PH1PMAhrfpTQguZmjXp-qXZPATFx1v0dQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697633406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/5-lessons-learned-from-testing-databricks-sql-serverless-dbt-1d85bc861358", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?auto=webp&amp;s=fbd4d0e53addbafbec476ab24f5df1855572e966", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d221ad91c4d1384b0c2374cbffc6fb6bd3056310", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23081a22464bd8b9d13bf43bffdddf281813af6b", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47600e587c6102981e3ded3b58b45b9950126fad", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b12a28ea63767e02f04e3967995c275a085fc9dd", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3192037d68c2b34a1a2cbdd8180adb74febc08a4", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/-Dl97pW1sO-v0TWx7FHqx6r8xqJUkb23ayTtvMRdrPk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=559ca48cd60d19d0015f0c7e4d8c28646e1a5a23", "width": 1080, "height": 720}], "variants": {}, "id": "rLL7K_wv2XyPOl8O44e_zqRofCeR-oU1uTCFVuuMs3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aq0oe", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq0oe/5_lessons_learned_from_testing_databricks_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/5-lessons-learned-from-testing-databricks-sql-serverless-dbt-1d85bc861358", "subreddit_subscribers": 134627, "created_utc": 1697633406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. \n\nWe are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. \n\nWe are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.   \nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.   \nOur goal is to work on anonymized data in the DEV &amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. \n\nI have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it's not even necessary (for at least the next 6 months).\n\nNow we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. \n\nI am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn't seem required to us). \n\nI understand that the Unity Catalog is the \"new hot thing\" that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? ", "author_fullname": "t2_5iao6str", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Unity Catalog worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17app3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697632428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. &lt;/p&gt;\n\n&lt;p&gt;We are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. &lt;/p&gt;\n\n&lt;p&gt;We are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.&lt;br/&gt;\nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.&lt;br/&gt;\nOur goal is to work on anonymized data in the DEV &amp;amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. &lt;/p&gt;\n\n&lt;p&gt;I have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it&amp;#39;s not even necessary (for at least the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;Now we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;I am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn&amp;#39;t seem required to us). &lt;/p&gt;\n\n&lt;p&gt;I understand that the Unity Catalog is the &amp;quot;new hot thing&amp;quot; that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17app3b", "is_robot_indexable": true, "report_reasons": null, "author": "Far-String829", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "subreddit_subscribers": 134627, "created_utc": 1697632428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.\n\nGiving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar\n\nI know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).\n\nAlso what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: what software should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17ap7is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.&lt;/p&gt;\n\n&lt;p&gt;Giving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar&lt;/p&gt;\n\n&lt;p&gt;I know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).&lt;/p&gt;\n\n&lt;p&gt;Also what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ap7is", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "subreddit_subscribers": 134627, "created_utc": 1697630909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Referring to taking data from the transaction database to a warehouse. What ETL process did you run?", "author_fullname": "t2_legyu6zou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone had any experience modelling data from a Netsuite database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17an2k7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Referring to taking data from the transaction database to a warehouse. What ETL process did you run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17an2k7", "is_robot_indexable": true, "report_reasons": null, "author": "anotherwetsock", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "subreddit_subscribers": 134627, "created_utc": 1697623431.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}