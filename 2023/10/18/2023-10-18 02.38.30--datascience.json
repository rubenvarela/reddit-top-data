{"kind": "Listing", "data": {"after": "t3_17a9bma", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Folks\n\nI am currently working as  Senior ML Engineer on a startup in Dubai. I am 28 years old and getting 120k USD yearly. (no tax).\n\nActually, I am living a good life here, its close to my home country (Turkey), so I can see my family easily and we are almost in same timezone. But the quality of things we are doing here not that good, and I am not sure can I grow in my career here in long run. I don't want to move to Europe because as I can see salaries are very low . If I stay in Dubai getting a salary around 150k in 2 years in here is very doable for me now.\n\nI started considering to move to USA. I found hybrid master programs (first year is remote second year is USA and I can OPT). So I don't have to sacrifice my 2 years to go USA. Probably I will stay without a job just one year.\n\nDo you have any advice for me? Is moving to USA changing my lifestyle, and making sacrifices (time, masters, moving to USA, money etc) worth it?  \n\n\n&amp;#x200B;", "author_fullname": "t2_auxp06r9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving to USA or Staying in UAE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179uwm2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697537235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks&lt;/p&gt;\n\n&lt;p&gt;I am currently working as  Senior ML Engineer on a startup in Dubai. I am 28 years old and getting 120k USD yearly. (no tax).&lt;/p&gt;\n\n&lt;p&gt;Actually, I am living a good life here, its close to my home country (Turkey), so I can see my family easily and we are almost in same timezone. But the quality of things we are doing here not that good, and I am not sure can I grow in my career here in long run. I don&amp;#39;t want to move to Europe because as I can see salaries are very low . If I stay in Dubai getting a salary around 150k in 2 years in here is very doable for me now.&lt;/p&gt;\n\n&lt;p&gt;I started considering to move to USA. I found hybrid master programs (first year is remote second year is USA and I can OPT). So I don&amp;#39;t have to sacrifice my 2 years to go USA. Probably I will stay without a job just one year.&lt;/p&gt;\n\n&lt;p&gt;Do you have any advice for me? Is moving to USA changing my lifestyle, and making sacrifices (time, masters, moving to USA, money etc) worth it?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179uwm2", "is_robot_indexable": true, "report_reasons": null, "author": "Round_Inflation_2199", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179uwm2/moving_to_usa_or_staying_in_uae/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179uwm2/moving_to_usa_or_staying_in_uae/", "subreddit_subscribers": 1088794, "created_utc": 1697537235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am finishing my my masters degree in data analytics. Previously I've worked as a business analyst for three years. I just had an interview for a data analyst position and I was asked to complete a take home assignment with two parts: a written analysis, and an R project that included a business report with a summary and discussion for recommendations on improving the data reporting. I had 24 from after my interview to return the assignment. I got the exam at 2pm yesterday, so I had until 2pm today. \n\nI got home at 3pm and got the first written portion done yesterday. It involved some simple excel manipulations. Then I had to go to class at 5. Didn't get home till 10pm.\n\nFast forward this morning. I wake up at 8.i get started on the R project at 9am.\n\nThe data was some of the messiness I've seen, and cleaning and transforming the data took four hours. The analysis and visualizations took about one. I know there were some mistakes, and I got the written summary done. But I could not submit the discussion on recommendations. \n\nI'm not here to ask about my likelihood of getting the job. But this task seemed monumental for just 24 hours (i have other obligations like class and a family). Even my worst professors haven't asked me to do anything like that in such a short time. Is this to be expected going forward?", "author_fullname": "t2_fypd1w6xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Take Home task seemed unreasonable", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a79kw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697572997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am finishing my my masters degree in data analytics. Previously I&amp;#39;ve worked as a business analyst for three years. I just had an interview for a data analyst position and I was asked to complete a take home assignment with two parts: a written analysis, and an R project that included a business report with a summary and discussion for recommendations on improving the data reporting. I had 24 from after my interview to return the assignment. I got the exam at 2pm yesterday, so I had until 2pm today. &lt;/p&gt;\n\n&lt;p&gt;I got home at 3pm and got the first written portion done yesterday. It involved some simple excel manipulations. Then I had to go to class at 5. Didn&amp;#39;t get home till 10pm.&lt;/p&gt;\n\n&lt;p&gt;Fast forward this morning. I wake up at 8.i get started on the R project at 9am.&lt;/p&gt;\n\n&lt;p&gt;The data was some of the messiness I&amp;#39;ve seen, and cleaning and transforming the data took four hours. The analysis and visualizations took about one. I know there were some mistakes, and I got the written summary done. But I could not submit the discussion on recommendations. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not here to ask about my likelihood of getting the job. But this task seemed monumental for just 24 hours (i have other obligations like class and a family). Even my worst professors haven&amp;#39;t asked me to do anything like that in such a short time. Is this to be expected going forward?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a79kw", "is_robot_indexable": true, "report_reasons": null, "author": "wildwildwildebeast", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a79kw/interview_take_home_task_seemed_unreasonable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a79kw/interview_take_home_task_seemed_unreasonable/", "subreddit_subscribers": 1088794, "created_utc": 1697572997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just recently graduated in May with a BS statistics from texas a&amp;m with a specialization in GIS. I have a good knowledge of statistics, not a slacker in the academic sense. 3.5 gpa. One semester of research, no internship experience Edit: Passed two preliminary actuarial exams P and FM early on in university. Since then, I got a contracting gig at apple as GIS editor/mapper, maybe I can market it off as an analyst), I was training there for a month and got laid off, can def get a good ref letter though. I have a decent capstone project from university, shiny app utilizing exploratory methods for points patterns. I'm almost done with the meta coursera front-end prof certificate and I'm gonna do the back-end version, because I want to know how to deploy a shiny app with all the bells and whistles using the rhino framework, connected to a database, testing, user feedback, hosted on the cloud. Maybe then if can have a little web app on my resume that also makes peoples lives a little easier. I've thought about it, looks like I have a lot to learn, ux/ui design, marketing the web-app somewhere, even if it doesn't get any traffic, maybe it'll look good on a resume.\n\nI'm disenchanted with it all, I'm hearing a person with a PhD in a quantitative field hardly ever needs PhD level knowledge in their work, unless they are in academia or industry doing research, do you even need a masters? I mean, doesn't a bachelors in statistics, especially coupled with a few graduate level stacked courses in statistics, basically qualify you, as \"pretty much as knowledgeable as a masters in statistics with no undergrad statistics related coursework, in terms of theoretical knowledge of probability, regression, inference\", I'm not asking any questions. It's just that a person with connections and a bachelors in english, can get a job in analytics, and I am having trouble. I call it how I see it, my knowledge of statistics is not nearly as important, as having something tangible, that says \"I'm of value\" and \"people can rely on me\", and knowing people. Especially when employers aren't going to ask my professors how I was like,  I imagine I'll have a better chance getting into a PhD program. Even if you get a PhD, you still need to fight for job, learn new skills, deal with layoffs, and probably, continue the wage-slave life like most people in America (which is a good life I admit for most people). No question here, I'm just saying how I feel at the moment, making no implicit claims. I can get good rec letters from my professors, I'm pretty sure, lol, ya think I can get it at places like texas tech, iowa state, or kansas state?  Open to conversation about anything. So plan is, get a PhD, because I can't get a job now, maybe yeet out with a masters, but honestly, I like teaching, I like learning, I don't mind taking tests, and I know how to live with a low overhead (I don't buy stuff I don't need). I know what really matters, your basic needs, family, a few good friends. Ok, that's not all that matters, there have been many people who have excelled in their fields, sacrificing their time with their family and friends, in order to do things that everybody would agree matters. Some I know regret it, others don't.", "author_fullname": "t2_qorbr809", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm giving up finding a job. I feel like I'll have better luck applying to PhD programs. This is a rant.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aba8a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697588397.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697583329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently graduated in May with a BS statistics from texas a&amp;amp;m with a specialization in GIS. I have a good knowledge of statistics, not a slacker in the academic sense. 3.5 gpa. One semester of research, no internship experience Edit: Passed two preliminary actuarial exams P and FM early on in university. Since then, I got a contracting gig at apple as GIS editor/mapper, maybe I can market it off as an analyst), I was training there for a month and got laid off, can def get a good ref letter though. I have a decent capstone project from university, shiny app utilizing exploratory methods for points patterns. I&amp;#39;m almost done with the meta coursera front-end prof certificate and I&amp;#39;m gonna do the back-end version, because I want to know how to deploy a shiny app with all the bells and whistles using the rhino framework, connected to a database, testing, user feedback, hosted on the cloud. Maybe then if can have a little web app on my resume that also makes peoples lives a little easier. I&amp;#39;ve thought about it, looks like I have a lot to learn, ux/ui design, marketing the web-app somewhere, even if it doesn&amp;#39;t get any traffic, maybe it&amp;#39;ll look good on a resume.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m disenchanted with it all, I&amp;#39;m hearing a person with a PhD in a quantitative field hardly ever needs PhD level knowledge in their work, unless they are in academia or industry doing research, do you even need a masters? I mean, doesn&amp;#39;t a bachelors in statistics, especially coupled with a few graduate level stacked courses in statistics, basically qualify you, as &amp;quot;pretty much as knowledgeable as a masters in statistics with no undergrad statistics related coursework, in terms of theoretical knowledge of probability, regression, inference&amp;quot;, I&amp;#39;m not asking any questions. It&amp;#39;s just that a person with connections and a bachelors in english, can get a job in analytics, and I am having trouble. I call it how I see it, my knowledge of statistics is not nearly as important, as having something tangible, that says &amp;quot;I&amp;#39;m of value&amp;quot; and &amp;quot;people can rely on me&amp;quot;, and knowing people. Especially when employers aren&amp;#39;t going to ask my professors how I was like,  I imagine I&amp;#39;ll have a better chance getting into a PhD program. Even if you get a PhD, you still need to fight for job, learn new skills, deal with layoffs, and probably, continue the wage-slave life like most people in America (which is a good life I admit for most people). No question here, I&amp;#39;m just saying how I feel at the moment, making no implicit claims. I can get good rec letters from my professors, I&amp;#39;m pretty sure, lol, ya think I can get it at places like texas tech, iowa state, or kansas state?  Open to conversation about anything. So plan is, get a PhD, because I can&amp;#39;t get a job now, maybe yeet out with a masters, but honestly, I like teaching, I like learning, I don&amp;#39;t mind taking tests, and I know how to live with a low overhead (I don&amp;#39;t buy stuff I don&amp;#39;t need). I know what really matters, your basic needs, family, a few good friends. Ok, that&amp;#39;s not all that matters, there have been many people who have excelled in their fields, sacrificing their time with their family and friends, in order to do things that everybody would agree matters. Some I know regret it, others don&amp;#39;t.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17aba8a", "is_robot_indexable": true, "report_reasons": null, "author": "Sea-Bodybuilder-1277", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17aba8a/im_giving_up_finding_a_job_i_feel_like_ill_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17aba8a/im_giving_up_finding_a_job_i_feel_like_ill_have/", "subreddit_subscribers": 1088794, "created_utc": 1697583329.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am dealing with a specific problem: predicting the maximum number of cars that can stop in a parking lot on a daily basis. We have multiple parking lots in a region, each with a fixed number of parking slots. These slots are used multiple times throughout the day. I have access to historical data, including information on the time cars spent in the slots, the number of cars in any given period, the number of empty slots during specific time periods, and statistics for nearby areas.\n\nThe goal is to predict, for each parking lot, the maximum number of cars it can accommodate on each day during the pre-Christmas period. It's important to note that historically, none of the parking lots have probably reached their maximum capacity.\n\nAdditionally, we are faced with a challenge related to new parking lots. These lots lack extensive historical data, and many people may not be aware of their existence.\n\nHow would you recommend approaching this task?", "author_fullname": "t2_lku7zyn1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predict maximum capacity of parking lots", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179ub5l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697534713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am dealing with a specific problem: predicting the maximum number of cars that can stop in a parking lot on a daily basis. We have multiple parking lots in a region, each with a fixed number of parking slots. These slots are used multiple times throughout the day. I have access to historical data, including information on the time cars spent in the slots, the number of cars in any given period, the number of empty slots during specific time periods, and statistics for nearby areas.&lt;/p&gt;\n\n&lt;p&gt;The goal is to predict, for each parking lot, the maximum number of cars it can accommodate on each day during the pre-Christmas period. It&amp;#39;s important to note that historically, none of the parking lots have probably reached their maximum capacity.&lt;/p&gt;\n\n&lt;p&gt;Additionally, we are faced with a challenge related to new parking lots. These lots lack extensive historical data, and many people may not be aware of their existence.&lt;/p&gt;\n\n&lt;p&gt;How would you recommend approaching this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179ub5l", "is_robot_indexable": true, "report_reasons": null, "author": "VGFenohmen", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179ub5l/predict_maximum_capacity_of_parking_lots/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179ub5l/predict_maximum_capacity_of_parking_lots/", "subreddit_subscribers": 1088794, "created_utc": 1697534713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am aware of a few tools that aid in converting XGBoost decision trees to \"if-then\" statements. I'm curious if anyone has experience with this approach, and how feasible/successful was the outcome?", "author_fullname": "t2_8qqebrm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting XGBoost decision models to \"if-then\" statements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a6ken", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697571198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am aware of a few tools that aid in converting XGBoost decision trees to &amp;quot;if-then&amp;quot; statements. I&amp;#39;m curious if anyone has experience with this approach, and how feasible/successful was the outcome?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a6ken", "is_robot_indexable": true, "report_reasons": null, "author": "MultiPass10", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a6ken/converting_xgboost_decision_models_to_ifthen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a6ken/converting_xgboost_decision_models_to_ifthen/", "subreddit_subscribers": 1088794, "created_utc": 1697571198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi r/datascience,\n\nFrom my experience working with data orchestration tools (Airflow primarily), I tend to deal with a lot of repetitive fixes with flaky pipelines such as resource exhaustion issues, single malformed entries or other edge cases, figuring out why a task isn't running, and so on. I was wondering whether any of you had the same experience in your day-to-day work. How much of the job is actually just dealing with repetitive issues and maintenance of pipelines, and do any of you know of any tools or tips to make the experience of working with these pipelines less time-consuming?\n\nThanks!", "author_fullname": "t2_mk6o8gs2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Repetitive airflow pipeline problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179r5li", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697520996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;From my experience working with data orchestration tools (Airflow primarily), I tend to deal with a lot of repetitive fixes with flaky pipelines such as resource exhaustion issues, single malformed entries or other edge cases, figuring out why a task isn&amp;#39;t running, and so on. I was wondering whether any of you had the same experience in your day-to-day work. How much of the job is actually just dealing with repetitive issues and maintenance of pipelines, and do any of you know of any tools or tips to make the experience of working with these pipelines less time-consuming?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179r5li", "is_robot_indexable": true, "report_reasons": null, "author": "dec_dev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179r5li/repetitive_airflow_pipeline_problems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179r5li/repetitive_airflow_pipeline_problems/", "subreddit_subscribers": 1088794, "created_utc": 1697520996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This has happened to me twice now, dunno if it\u2019s a new trend in recruitment processes. I\u2019m fine with it, to be honest, because it lets me show that I have the skills necessary for the job. I\u2019m not currently working in DS but in finance with some data analysis, but not much modeling work to show for (even though I have my master\u2019s in a computational quantitative field and so know the stats/theory behind most models).\n\n I didn\u2019t get the first job that required a live-coding exercise because they could only schedule it while I was on vacation and I didn\u2019t feel like I could really prepare, but now with this second position I passed the assignment and I have a 45 minute behavioral interview tomorrow. \n\nJust wondering for anyone who has had a similar recruitment process, does this mean that this recruitment process should be relatively quick? Just this interview and maybe one more technical one? (Am a bit desperate to switch jobs as my current job has a crazy high tempo and it\u2019s hard to find time to interview, which is the main reason I don\u2019t want a super dragged out process)", "author_fullname": "t2_ls91v62g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does it mean to get a take-home assignment before screening call?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179yve1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697551029.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697550765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This has happened to me twice now, dunno if it\u2019s a new trend in recruitment processes. I\u2019m fine with it, to be honest, because it lets me show that I have the skills necessary for the job. I\u2019m not currently working in DS but in finance with some data analysis, but not much modeling work to show for (even though I have my master\u2019s in a computational quantitative field and so know the stats/theory behind most models).&lt;/p&gt;\n\n&lt;p&gt;I didn\u2019t get the first job that required a live-coding exercise because they could only schedule it while I was on vacation and I didn\u2019t feel like I could really prepare, but now with this second position I passed the assignment and I have a 45 minute behavioral interview tomorrow. &lt;/p&gt;\n\n&lt;p&gt;Just wondering for anyone who has had a similar recruitment process, does this mean that this recruitment process should be relatively quick? Just this interview and maybe one more technical one? (Am a bit desperate to switch jobs as my current job has a crazy high tempo and it\u2019s hard to find time to interview, which is the main reason I don\u2019t want a super dragged out process)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179yve1", "is_robot_indexable": true, "report_reasons": null, "author": "Whole-Ad-8370", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179yve1/what_does_it_mean_to_get_a_takehome_assignment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179yve1/what_does_it_mean_to_get_a_takehome_assignment/", "subreddit_subscribers": 1088794, "created_utc": 1697550765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,   \n\n\nWe just hired a data analyst to analyse a time series representing a certain commodity value over time, we offered them the possibility to take the price data from a source of their choice, but they insisted that we provide it ourselves. Is this good or bad practice? Could someone give pros and cons of letting the analyst find their own publicly available data vs. the company providing them the data set?   \n\n\nThank you", "author_fullname": "t2_97o8m0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should data-scientists look for their own datasets or be provided by the hiring company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179ztob", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697553394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   &lt;/p&gt;\n\n&lt;p&gt;We just hired a data analyst to analyse a time series representing a certain commodity value over time, we offered them the possibility to take the price data from a source of their choice, but they insisted that we provide it ourselves. Is this good or bad practice? Could someone give pros and cons of letting the analyst find their own publicly available data vs. the company providing them the data set?   &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179ztob", "is_robot_indexable": true, "report_reasons": null, "author": "Cryptocheets", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179ztob/should_datascientists_look_for_their_own_datasets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179ztob/should_datascientists_look_for_their_own_datasets/", "subreddit_subscribers": 1088794, "created_utc": 1697553394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I cant make heads and tails of theory papers that have  mathematical notations and equation. Where do you start? Is there an ebook/primer that can help? I dont have an economics background but I did study advanced math in high school. I am in accounting if it matters", "author_fullname": "t2_8xfzxrs0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you start if you are new to mathematical models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ad4og", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697588385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I cant make heads and tails of theory papers that have  mathematical notations and equation. Where do you start? Is there an ebook/primer that can help? I dont have an economics background but I did study advanced math in high school. I am in accounting if it matters&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17ad4og", "is_robot_indexable": true, "report_reasons": null, "author": "relativefluffy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ad4og/where_do_you_start_if_you_are_new_to_mathematical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ad4og/where_do_you_start_if_you_are_new_to_mathematical/", "subreddit_subscribers": 1088794, "created_utc": 1697588385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_l2sqy1npn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the 30th percentile of a standard normal distribution closer to one standard deviation below the mean or two standard deviations below the mean?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aatez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697582110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17aatez", "is_robot_indexable": true, "report_reasons": null, "author": "Medical-Author-8166", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17aatez/is_the_30th_percentile_of_a_standard_normal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17aatez/is_the_30th_percentile_of_a_standard_normal/", "subreddit_subscribers": 1088794, "created_utc": 1697582110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking for a detailed glossary of terms in data scientist that an experienced data scientist should know. It's mostly for myself to test my knowledge. Anything from regression types to p values and much more.", "author_fullname": "t2_k79vgi4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good glossary for data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a5thj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697569244.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a detailed glossary of terms in data scientist that an experienced data scientist should know. It&amp;#39;s mostly for myself to test my knowledge. Anything from regression types to p values and much more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a5thj", "is_robot_indexable": true, "report_reasons": null, "author": "TheOmerAngi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a5thj/does_anyone_have_a_good_glossary_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a5thj/does_anyone_have_a_good_glossary_for_data_science/", "subreddit_subscribers": 1088794, "created_utc": 1697569244.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "**TL;DR:**\n\nBelow, I describe the info I'm tracking, and an algorithm I want to follow to produce a model that shows which factors matter and which don't. **My question is, does this algorithm already exist in some code library?** Or do I have to code it myself?\n\n**Background:**\n\nI've been keeping a spreadsheet of my sleep habits and energy levels for the last 60 days. I have looked a bit at simple correlations -- the highest correlation so far is (no surprise) the correlation between the number of hours a night I have been sleeping recently, and the energy level I feel in the morning. Other correlations, like drinks of alcohol or caffeine, are lower, but I wonder if they would show a stronger effect if I controlled for other factors.\n\n**Regression algorithm:**\n\nI used to work at a data science company where we would run studies we called \"regression hill climbs\", where we would iterate like this:\n\n1. identify the output factor (AKA \"dependent variable\"); in this case, it would be energy level on a given day\n2. for every input factor (AKA \"independent variable\", e.g. whether I taped my mouth shut the night before), calculate the correlations between it and each other input factor\n3. start with an empty \"model\", a set of independent variables\n4. start with a correlation between model and dependent variable of 0\n5. repeat until no more variables are selected to add to the model:\n   1. filter all candidate independent variables, omitting any with too high a correlation to any of the already selected variables in the model (e.g., must be under a threshold of 0.3; this avoids over-fitting) \n   2. of all remaining candidate independent variables, try adding each to the model, and running a new regression on the model's variables (to best predict the dependent variable)\n   3. select the candidate independent variable that most increased the resulting correlation between model and dependent variable, if and only if the increase is above some threshold (e.g., .02 improvement in correlation)\n\nThis results in a model whose total number of independent variables is small, where each is not influenced too much by the others, and where you can see how significant it is (and whether it is positive or negative!). \n\n**Why it matters:**\n\nFor instance, if I have nights where I'm more disciplined overall -- say, when I don't drink, I go to bed early, I set up my CPAP machine and use it all night, etc. -- it might turn out that there's a high (negative) correlation between drinking and sleep quality, but the model may omit alcohol as a variable because its value is really just captured entirely in hours of sleep and in CPAP compliance.\n\nOr, maybe, even taking these things into account, drinking alcohol does consistently disturb my sleep quality, and I should stop. Or maybe it has a slight positive effect! The point is, it's very hard to isolate it as a factor; this algorithm helps.\n\n**What I'm looking for:**\n\nA code library -- presumably in python -- that is built to perform such a \"regression hill climb\", and allow for the various thresholds and other settings to be specified.\n\nDoes anyone know of such a library? Or, is there something different I should do, or some way I'm misunderstanding the problem?\n\nThanks!", "author_fullname": "t2_1why", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Q: How to extract learnings from my spreadsheets, beyond simple correlations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2wb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697561640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Below, I describe the info I&amp;#39;m tracking, and an algorithm I want to follow to produce a model that shows which factors matter and which don&amp;#39;t. &lt;strong&gt;My question is, does this algorithm already exist in some code library?&lt;/strong&gt; Or do I have to code it myself?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been keeping a spreadsheet of my sleep habits and energy levels for the last 60 days. I have looked a bit at simple correlations -- the highest correlation so far is (no surprise) the correlation between the number of hours a night I have been sleeping recently, and the energy level I feel in the morning. Other correlations, like drinks of alcohol or caffeine, are lower, but I wonder if they would show a stronger effect if I controlled for other factors.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Regression algorithm:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I used to work at a data science company where we would run studies we called &amp;quot;regression hill climbs&amp;quot;, where we would iterate like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;identify the output factor (AKA &amp;quot;dependent variable&amp;quot;); in this case, it would be energy level on a given day&lt;/li&gt;\n&lt;li&gt;for every input factor (AKA &amp;quot;independent variable&amp;quot;, e.g. whether I taped my mouth shut the night before), calculate the correlations between it and each other input factor&lt;/li&gt;\n&lt;li&gt;start with an empty &amp;quot;model&amp;quot;, a set of independent variables&lt;/li&gt;\n&lt;li&gt;start with a correlation between model and dependent variable of 0&lt;/li&gt;\n&lt;li&gt;repeat until no more variables are selected to add to the model:\n\n&lt;ol&gt;\n&lt;li&gt;filter all candidate independent variables, omitting any with too high a correlation to any of the already selected variables in the model (e.g., must be under a threshold of 0.3; this avoids over-fitting) &lt;/li&gt;\n&lt;li&gt;of all remaining candidate independent variables, try adding each to the model, and running a new regression on the model&amp;#39;s variables (to best predict the dependent variable)&lt;/li&gt;\n&lt;li&gt;select the candidate independent variable that most increased the resulting correlation between model and dependent variable, if and only if the increase is above some threshold (e.g., .02 improvement in correlation)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This results in a model whose total number of independent variables is small, where each is not influenced too much by the others, and where you can see how significant it is (and whether it is positive or negative!). &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For instance, if I have nights where I&amp;#39;m more disciplined overall -- say, when I don&amp;#39;t drink, I go to bed early, I set up my CPAP machine and use it all night, etc. -- it might turn out that there&amp;#39;s a high (negative) correlation between drinking and sleep quality, but the model may omit alcohol as a variable because its value is really just captured entirely in hours of sleep and in CPAP compliance.&lt;/p&gt;\n\n&lt;p&gt;Or, maybe, even taking these things into account, drinking alcohol does consistently disturb my sleep quality, and I should stop. Or maybe it has a slight positive effect! The point is, it&amp;#39;s very hard to isolate it as a factor; this algorithm helps.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m looking for:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A code library -- presumably in python -- that is built to perform such a &amp;quot;regression hill climb&amp;quot;, and allow for the various thresholds and other settings to be specified.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of such a library? Or, is there something different I should do, or some way I&amp;#39;m misunderstanding the problem?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a2wb4", "is_robot_indexable": true, "report_reasons": null, "author": "brw12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a2wb4/q_how_to_extract_learnings_from_my_spreadsheets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a2wb4/q_how_to_extract_learnings_from_my_spreadsheets/", "subreddit_subscribers": 1088794, "created_utc": 1697561640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey Guys,  \nIt seems RAG is really taking off as an increasingly popular use case for LLMs to leverage contextual data. However, everybody is building their own contextual data sets and embedding them in their own silo'd vector dbs.   \n\n\nDo you guys think there's any utility in having a shared public vector db that anyone can tap into their API, without having to self-host, worry about the embedding pipelines and filling the vector db with enough data in the first place for their use cases? Would this save devs alot of time in quickly testing testing product ideas? (albeit it does seem that propriety data is what everyone's raving about today)  \n\n\n\\-  \n\n\nFor context, I'm building a social media product we're users can upload a few pieces (approx 10) of content (social media posts, websites, videos to start with), which becomes the verified human-curated list/Niche. We then classify and embed this into a vector db. From this, we have set up a data pipeline to scrape the web and find new content that is most similar which we suggest to users to add to the Niche (upvote, downvote style). When a piece of content is upvoted on its added to the verified list updating the Niche's classification string. Essentially we're aiming to construct an ever-growing, user-curated, contextually classified vector database from a relatively small set of sample data. ", "author_fullname": "t2_13im86", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shared Public Contextual Database for RAG", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179z4yq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697551500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,&lt;br/&gt;\nIt seems RAG is really taking off as an increasingly popular use case for LLMs to leverage contextual data. However, everybody is building their own contextual data sets and embedding them in their own silo&amp;#39;d vector dbs.   &lt;/p&gt;\n\n&lt;p&gt;Do you guys think there&amp;#39;s any utility in having a shared public vector db that anyone can tap into their API, without having to self-host, worry about the embedding pipelines and filling the vector db with enough data in the first place for their use cases? Would this save devs alot of time in quickly testing testing product ideas? (albeit it does seem that propriety data is what everyone&amp;#39;s raving about today)  &lt;/p&gt;\n\n&lt;p&gt;-  &lt;/p&gt;\n\n&lt;p&gt;For context, I&amp;#39;m building a social media product we&amp;#39;re users can upload a few pieces (approx 10) of content (social media posts, websites, videos to start with), which becomes the verified human-curated list/Niche. We then classify and embed this into a vector db. From this, we have set up a data pipeline to scrape the web and find new content that is most similar which we suggest to users to add to the Niche (upvote, downvote style). When a piece of content is upvoted on its added to the verified list updating the Niche&amp;#39;s classification string. Essentially we&amp;#39;re aiming to construct an ever-growing, user-curated, contextually classified vector database from a relatively small set of sample data. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179z4yq", "is_robot_indexable": true, "report_reasons": null, "author": "niksteel123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179z4yq/shared_public_contextual_database_for_rag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179z4yq/shared_public_contextual_database_for_rag/", "subreddit_subscribers": 1088794, "created_utc": 1697551500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products? Deploy: Part 3/4 - Doubling down on the power of Unified Experiences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_179ykfd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fIAPxsCawPBu-N75Vvoyg_keOmRvMjS4W4feJpzH2AE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697549881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?auto=webp&amp;s=ea619a7c524f8c5536cc06f8b13ba792caddd5b5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66add8fac528d26de7feccdd0c5022d39fd89850", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=85446cd271d715b3865f735be872ce747366b093", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=929b190abb183f4784a7d725a150dbb2083ba894", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e4c400d8251d52d7421fc6640499a17ea524db3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e716ed51284478d9d2a2569fdf859fe895fdf91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5219b40b806ce0099e4650da1d92ec438c0e19d8", "width": 1080, "height": 540}], "variants": {}, "id": "_WJ_OVW7dv_Q8Y2muH3UxiO88vjiOO6xTcwW1zV8-tA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "179ykfd", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179ykfd/how_to_build_data_products_deploy_part_34/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "subreddit_subscribers": 1088794, "created_utc": 1697549881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a musical director on a large cruise ship and am responsible for scheduling sets for 10 different bands around the ships 10 different music venues. I have to work around trivias, parties and shows in the aqua theatre and Theatre and other various venues on the ship. I want to analyze the data I have from Day 1 of the cruise a few weeks ago in a chart. How would you guys go about doing this? Thanks!", "author_fullname": "t2_a3amxwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cruise Ship Musicians Scheduling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179q91p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697517644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a musical director on a large cruise ship and am responsible for scheduling sets for 10 different bands around the ships 10 different music venues. I have to work around trivias, parties and shows in the aqua theatre and Theatre and other various venues on the ship. I want to analyze the data I have from Day 1 of the cruise a few weeks ago in a chart. How would you guys go about doing this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179q91p", "is_robot_indexable": true, "report_reasons": null, "author": "SaxTeacher1988", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179q91p/cruise_ship_musicians_scheduling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179q91p/cruise_ship_musicians_scheduling/", "subreddit_subscribers": 1088794, "created_utc": 1697517644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking to clustering on a dimensionally reduced dataset of 3D vectors. I\u2019ve tried using kmeans mini batches but the problem is that the visualization of the labelled data is not what I\u2019m looking for. I also tried using dbscan but I\u2019ve ran into performance issues where I run out of memory. For reference the dataset is over 100k rows and in the future I\u2019d like to use a similar clustering approach for gigabytes worth of data. Any alternatives or advice will be greatly appreciated.", "author_fullname": "t2_2pxniisp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance issues with dbscan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17adx4s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697590581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to clustering on a dimensionally reduced dataset of 3D vectors. I\u2019ve tried using kmeans mini batches but the problem is that the visualization of the labelled data is not what I\u2019m looking for. I also tried using dbscan but I\u2019ve ran into performance issues where I run out of memory. For reference the dataset is over 100k rows and in the future I\u2019d like to use a similar clustering approach for gigabytes worth of data. Any alternatives or advice will be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17adx4s", "is_robot_indexable": true, "report_reasons": null, "author": "spx416", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17adx4s/performance_issues_with_dbscan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17adx4s/performance_issues_with_dbscan/", "subreddit_subscribers": 1088794, "created_utc": 1697590581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHey!\n\n**Startup:**\n\n\\- Apply Script dot com \"Connect business and data professionals via pre-recorded standardized video interviews.\"\n\n**More details:**\n\n**Problems with Traditional Hiring**\n\n\\- Outdated: The current method of conducting interviews has become overly complex and outdated.\n\n\\- Time-Wasting: The process involves too many appointments, meetings, and stages, leading to communication errors.\n\n\\- Expensive: The man-hours invested by HR and engineering teams are costly.\n\n\\- Constraining: Interviews are fixed to specific times and locations.\n\n\\- Cumbersome: The experience is challenging for both businesses and professionals.\n\n**Our Solution**\n\n\\+ Talent Identification: We find top talent that matches your job post.\n\n\\+ Standardized Interviews: Professionals standardized pre-record their interviews (apples to apples comparison), covering areas such as CV, personality questions, project presentations, theory questions, coding tests, and hobbies.\n\n\\+ Efficiency: Businesses receive a pre-filtered batch of top applicants with their interviews ready for viewing.\n\n\\+ Time-Saving: Professionals can apply and businesses can employ candidates more quickly than with traditional methods.\n\n\\+ Reduced Workload: Minimize time spent reviewing applications; all interviews are pre-recorded.\n\n\\+ Flexibility: Managers can watch, speed up, or rewind interviews at their convenience.\n\n\\+ Transparency: Applicants receive immediate feedback on their applications to avoid being \"ghosted.\"\n\n**Life cycle stage:**\n\n\\- Validation: Currently looking to run our\u00a0***#1st pilot B2B***\u00a0with our first client.\n\n**My role:**\n\n\\- Founder\n\n**Goals for this month:**\n\n\\- Secure my first client for the pilot.\n\n\\- Obtain feedback from both the employee and business sides.\n\n\\- Optimize the product based on the feedback received.\n\n**How can I** **help?**\n\n\\- I am searching for a business, that wants to streamline and accelerate the hiring of top data professionals (ex.: Data Scientist, Machine Learning Engineer, Data Engineer, Data Analyst) in the USA.\n\n\\- in the USA.\n\nThx for the feedback ;)", "author_fullname": "t2_qrm5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feedback on my MVP project - Pre-Recorded Standardized Video Interviews Job Site for Data Professionals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aba8j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697583330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Startup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Apply Script dot com &amp;quot;Connect business and data professionals via pre-recorded standardized video interviews.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problems with Traditional Hiring&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Outdated: The current method of conducting interviews has become overly complex and outdated.&lt;/p&gt;\n\n&lt;p&gt;- Time-Wasting: The process involves too many appointments, meetings, and stages, leading to communication errors.&lt;/p&gt;\n\n&lt;p&gt;- Expensive: The man-hours invested by HR and engineering teams are costly.&lt;/p&gt;\n\n&lt;p&gt;- Constraining: Interviews are fixed to specific times and locations.&lt;/p&gt;\n\n&lt;p&gt;- Cumbersome: The experience is challenging for both businesses and professionals.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Our Solution&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;+ Talent Identification: We find top talent that matches your job post.&lt;/p&gt;\n\n&lt;p&gt;+ Standardized Interviews: Professionals standardized pre-record their interviews (apples to apples comparison), covering areas such as CV, personality questions, project presentations, theory questions, coding tests, and hobbies.&lt;/p&gt;\n\n&lt;p&gt;+ Efficiency: Businesses receive a pre-filtered batch of top applicants with their interviews ready for viewing.&lt;/p&gt;\n\n&lt;p&gt;+ Time-Saving: Professionals can apply and businesses can employ candidates more quickly than with traditional methods.&lt;/p&gt;\n\n&lt;p&gt;+ Reduced Workload: Minimize time spent reviewing applications; all interviews are pre-recorded.&lt;/p&gt;\n\n&lt;p&gt;+ Flexibility: Managers can watch, speed up, or rewind interviews at their convenience.&lt;/p&gt;\n\n&lt;p&gt;+ Transparency: Applicants receive immediate feedback on their applications to avoid being &amp;quot;ghosted.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Life cycle stage:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Validation: Currently looking to run our\u00a0&lt;strong&gt;&lt;em&gt;#1st pilot B2B&lt;/em&gt;&lt;/strong&gt;\u00a0with our first client.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My role:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Founder&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Goals for this month:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Secure my first client for the pilot.&lt;/p&gt;\n\n&lt;p&gt;- Obtain feedback from both the employee and business sides.&lt;/p&gt;\n\n&lt;p&gt;- Optimize the product based on the feedback received.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How can I&lt;/strong&gt; &lt;strong&gt;help?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- I am searching for a business, that wants to streamline and accelerate the hiring of top data professionals (ex.: Data Scientist, Machine Learning Engineer, Data Engineer, Data Analyst) in the USA.&lt;/p&gt;\n\n&lt;p&gt;- in the USA.&lt;/p&gt;\n\n&lt;p&gt;Thx for the feedback ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17aba8j", "is_robot_indexable": true, "report_reasons": null, "author": "glassAlloy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17aba8j/feedback_on_my_mvp_project_prerecorded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17aba8j/feedback_on_my_mvp_project_prerecorded/", "subreddit_subscribers": 1088794, "created_utc": 1697583330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6dgjvkmye", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We built An Open-Source platform to process relational and Graph Query simultaneously", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17aaayg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DhqpOvMyl_Iv_YtjtsxBAh9z9GgMdAorxuuioN0Q2Gs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697580766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/apache/age", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?auto=webp&amp;s=58a7d69efd2e23844b4e7f1535954b1621778b3f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f316a1202f5fa74b0e5f2fbf498b856c9f62215f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5c3f7954f6f80e9189469fca4c02f9e1d64bd86", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e99420e05d9a57934fcc0d938a675bbddc2fad0d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=674177fcbce104b5850c187b74bafb6bfa3a59a6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e5e2c41106d7223ffd0a647c8dcd9e9be77fef0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TRZymqGBMrpaHYPwm_ZcmXsUZbnGZyQguPSYhSihziU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4f518d28fcadfd0903c06acb87db698283bda6c", "width": 1080, "height": 540}], "variants": {}, "id": "OogeLufs8cl2yF7lU6HHhIEI9KAKQD2BU74UX-T7Kig"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "17aaayg", "is_robot_indexable": true, "report_reasons": null, "author": "SpecialEngineer7951", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17aaayg/we_built_an_opensource_platform_to_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/apache/age", "subreddit_subscribers": 1088794, "created_utc": 1697580766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What factors should I consider when deciding between enrolling in a traditional course or attending a boot camp for data science? I want to make a career switch and was wondering the difference between them. Any suggestions on where to begin would be greatly appreciated.", "author_fullname": "t2_m7ciapyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a8iv1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697576512.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697576302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What factors should I consider when deciding between enrolling in a traditional course or attending a boot camp for data science? I want to make a career switch and was wondering the difference between them. Any suggestions on where to begin would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a8iv1", "is_robot_indexable": true, "report_reasons": null, "author": "CompetitiveCollar991", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a8iv1/where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a8iv1/where_to_start/", "subreddit_subscribers": 1088794, "created_utc": 1697576302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHello, fellow data science enthusiasts! I'm participating in some machine learning competitions and I'm looking for insights on the strengths and weaknesses of various ML algorithms, along with their ideal use cases. This will help me choose the most suitable model for my competition tasks. Your expert opinions would be highly valuable!\n\n1. Could you please share your thoughts on the strengths and weaknesses of different ML algorithms, considering factors like accuracy, interpretability, computational requirements, etc.?\n2. What are some specific use cases or scenarios where certain ML algorithms excel? For example, which algorithms are best for image classification, natural language processing, or time series forecasting?\n3. Are there any resources, articles, or books that you would recommend for a deeper understanding of ML algorithm selection?\n\nYour insights will be greatly appreciated and will aid me in making more informed decisions for my competition endeavors. Thanks in advance!\"", "author_fullname": "t2_hhycptpk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Machine Learning Algorithm Selection for Competitions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a6t46", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697571824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, fellow data science enthusiasts! I&amp;#39;m participating in some machine learning competitions and I&amp;#39;m looking for insights on the strengths and weaknesses of various ML algorithms, along with their ideal use cases. This will help me choose the most suitable model for my competition tasks. Your expert opinions would be highly valuable!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Could you please share your thoughts on the strengths and weaknesses of different ML algorithms, considering factors like accuracy, interpretability, computational requirements, etc.?&lt;/li&gt;\n&lt;li&gt;What are some specific use cases or scenarios where certain ML algorithms excel? For example, which algorithms are best for image classification, natural language processing, or time series forecasting?&lt;/li&gt;\n&lt;li&gt;Are there any resources, articles, or books that you would recommend for a deeper understanding of ML algorithm selection?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Your insights will be greatly appreciated and will aid me in making more informed decisions for my competition endeavors. Thanks in advance!&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a6t46", "is_robot_indexable": true, "report_reasons": null, "author": "After_Reception1696", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a6t46/seeking_advice_on_machine_learning_algorithm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a6t46/seeking_advice_on_machine_learning_algorithm/", "subreddit_subscribers": 1088794, "created_utc": 1697571824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm building price tracker and want to plot prices over time for few dozens products. Seaborn relplot alike functions are pretty slow and I want to limit script run time to minimum.\n\nI thought about 2 solutions:\n\n1. sample data for each product in a way that keeps 'outliers' in dataset (i.e. spikes for visibility and dips to get notified that maybe it's time to buy it). Not sure if it's easy \n2. get rid of data points for which data trends flat based on moving average\n\n&amp;#x200B;\n\nAny better idea that is easy to implement?\n\n&amp;#x200B;", "author_fullname": "t2_5iv2njiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series data filtering - keep outliers and remove only flat trend", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a5u8t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697569289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building price tracker and want to plot prices over time for few dozens products. Seaborn relplot alike functions are pretty slow and I want to limit script run time to minimum.&lt;/p&gt;\n\n&lt;p&gt;I thought about 2 solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;sample data for each product in a way that keeps &amp;#39;outliers&amp;#39; in dataset (i.e. spikes for visibility and dips to get notified that maybe it&amp;#39;s time to buy it). Not sure if it&amp;#39;s easy &lt;/li&gt;\n&lt;li&gt;get rid of data points for which data trends flat based on moving average&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any better idea that is easy to implement?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a5u8t", "is_robot_indexable": true, "report_reasons": null, "author": "ratatsnow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a5u8t/time_series_data_filtering_keep_outliers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a5u8t/time_series_data_filtering_keep_outliers_and/", "subreddit_subscribers": 1088794, "created_utc": 1697569289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\n&amp;#x200B;\n\nI have access through my school to LinkedIn learning. I saw that they have different paths for python and data science, business intelligence, and data analyst.   \nHas anyone tried them or what do you guys think of them?  \nI saw these paths: Advance Your Python Skills for Data Science, Become a Business Intelligence Specialist, Getting Started as Business Analyst, and Become a Data Analyst.\n\n&amp;#x200B;\n\nIs it worth giving a try to any of these? I would be interested either in Business Intelligence or the Data Analyst one. I do have some time so could use the input before I just jump into one. My interest is to gain data analysis knowledge and make a transition over time. My background is in higher education and currently teach at the uni but do not see myself doing that for a long time.\n\n&amp;#x200B;\n\nAppreciate the input or help.", "author_fullname": "t2_3gvpw9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How good are the Linkedin Learning Paths?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a25ns", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697559655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have access through my school to LinkedIn learning. I saw that they have different paths for python and data science, business intelligence, and data analyst.&lt;br/&gt;\nHas anyone tried them or what do you guys think of them?&lt;br/&gt;\nI saw these paths: Advance Your Python Skills for Data Science, Become a Business Intelligence Specialist, Getting Started as Business Analyst, and Become a Data Analyst.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is it worth giving a try to any of these? I would be interested either in Business Intelligence or the Data Analyst one. I do have some time so could use the input before I just jump into one. My interest is to gain data analysis knowledge and make a transition over time. My background is in higher education and currently teach at the uni but do not see myself doing that for a long time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Appreciate the input or help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a25ns", "is_robot_indexable": true, "report_reasons": null, "author": "forgotendream", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a25ns/how_good_are_the_linkedin_learning_paths/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a25ns/how_good_are_the_linkedin_learning_paths/", "subreddit_subscribers": 1088794, "created_utc": 1697559655.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey fellow Redditors!\n\nEver wondered how technology can tackle a range of challenges like signature authentication, detecting cheaters in games, assessing neurological conditions, or dealing with pesky bots? The answer lies in the fascinating world of human motion analysis!\n\nIn this discussion, we delve into the concept of \"features\" in the context of human motion. Features are scalar values obtained from motion segments, offering insights into movement patterns. We explore how these features are essential in addressing diverse challenges and share insights into the basic features of movements.\n\n# \ud83d\udca1 What's a Feature?  \n\nIn this context, a feature refers to a scalar value obtained from a motion segment. For example, the average acceleration of a cursor. As depicted in the diagram below, users cannot be distinguished solely based on their average acceleration, but there are discernible individual tendencies.\n\n The next step involves constructing our feature space by identifying features that contain relevant information about movement patterns. \n\n# \ud83d\udcca Analyzing the appropriate time series\n\nThe majority of the data we work with consists of x and y coordinates that change over time, such as the position of a cursor or a pen on a screen. Therefore, we already have two time series. Additionally, we calculate directional speeds, accelerations, jerks, as well as direction-independent speed, acceleration, and jerk. \n\n**Unmasking forgery through speed analysis**\n\nLet\u2019s explain the use of derivatives through an example of **signature forgery**. Suppose someone attempts to replicate a signature they have seen before, familiar with its form. How would you approach this situation? Initially, one might **meticulously trace the line to be replicated, proceeding slowly and accurately**, inch by inch. The result would be a slow, nearly constant-speed movement. **The speed time series would exhibit an approximately constant value.**\n\nNow, imagine **someone writing their own signature**. **The speed can vary significantly**, but it won\u2019t remain constant. They would draw longer, straight lines more quickly and slow down at tight turns. When moving right and upwards, the arcs would be faster and more dynamic than when turning left. Even if the forged signature\u2019s image is an exact copy of the genuine one in terms of x-y coordinates, **the speed profiles would look entirely different.**\n\nOf course, this was a rather clumsy attempt at forgery; there are far more skilled individuals in this field. The speed of a signature can be estimated based on the signature image alone, either by assuming faster movement on straight lines and slower movement on curves or by considering the line quality.\n\nDelving into the details is beyond the scope of this discussion, but the key point is that estimating and replicating the speed of motion requires practice and talent. It is more challenging than simply replicating the x-y coordinates of the signature image. Moreover, forging the acceleration and other factors becomes even more difficult.\n\nIn theory, we could **take derivatives of our time series as many times as desired**. However, there is a practical limit as, **after a certain point, the derivative becomes more noise than meaningful information.**\n\nFrom this example, it becomes apparent why we thought utilizing derivatives (speed, acceleration, jerk) was a valuable approach for motion analysis. **When we began using this method, the results demonstrated exceptional accuracy**.\n\n# \ud83d\udd0d Describing Time Series with Scalar Values \n\n We have extracted various time series from our motion sample. To condense the valuable information of a lengthy time series into scalar values, **we employ a straightforward approach: calculating a few statistical characteristics**. Our selection criteria ensure that these characteristics effectively represent the distribution of the time series.\n\nSome of these characteristics are expected, such as the minimum, maximum, mean, and standard deviation.\n\nTo understand how the values progress from the minimum to the maximum, we utilize percentiles, including the 10th, 25th, 50th, 75th, and 90th percentiles. The minimum and maximum values are also considered as percentiles, specifically the 0th and 100th percentiles.\n\nTwo lesser-known statistical values are skewness and kurtosis. **Skewness measures the asymmetry of a distribution**. For instance, if the speed values below and above the average speed are evenly spaced, the skewness will be around zero. However, if there are numerous values below the mean but close to it, with only a few exceptionally high values above, the skewness will be positive.\n\nIn the context of cursor movement, this suggests that an individual typically uses the cursor at a relatively constant speed, but occasionally makes sudden moves. This could be a personal habit or characteristic.\n\nOn the other hand, **kurtosis indicates whether the values are concentrated around the mean or spread out across a broader range**.\n\nThese are the basic features we utilize for analysis.\n\nFeel free to join the discussion and share your thoughts on this fascinating intersection of technology and human motion analysis! \ud83d\udcac\ud83d\udd7a\ud83c\udffd\ud83d\udcbb", "author_fullname": "t2_gbypvyyx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cracking the Code of Human Motion: Describing Movement Patterns with Scalar Values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179u12d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697533471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Redditors!&lt;/p&gt;\n\n&lt;p&gt;Ever wondered how technology can tackle a range of challenges like signature authentication, detecting cheaters in games, assessing neurological conditions, or dealing with pesky bots? The answer lies in the fascinating world of human motion analysis!&lt;/p&gt;\n\n&lt;p&gt;In this discussion, we delve into the concept of &amp;quot;features&amp;quot; in the context of human motion. Features are scalar values obtained from motion segments, offering insights into movement patterns. We explore how these features are essential in addressing diverse challenges and share insights into the basic features of movements.&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udca1 What&amp;#39;s a Feature?&lt;/h1&gt;\n\n&lt;p&gt;In this context, a feature refers to a scalar value obtained from a motion segment. For example, the average acceleration of a cursor. As depicted in the diagram below, users cannot be distinguished solely based on their average acceleration, but there are discernible individual tendencies.&lt;/p&gt;\n\n&lt;p&gt;The next step involves constructing our feature space by identifying features that contain relevant information about movement patterns. &lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udcca Analyzing the appropriate time series&lt;/h1&gt;\n\n&lt;p&gt;The majority of the data we work with consists of x and y coordinates that change over time, such as the position of a cursor or a pen on a screen. Therefore, we already have two time series. Additionally, we calculate directional speeds, accelerations, jerks, as well as direction-independent speed, acceleration, and jerk. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Unmasking forgery through speed analysis&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s explain the use of derivatives through an example of &lt;strong&gt;signature forgery&lt;/strong&gt;. Suppose someone attempts to replicate a signature they have seen before, familiar with its form. How would you approach this situation? Initially, one might &lt;strong&gt;meticulously trace the line to be replicated, proceeding slowly and accurately&lt;/strong&gt;, inch by inch. The result would be a slow, nearly constant-speed movement. &lt;strong&gt;The speed time series would exhibit an approximately constant value.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now, imagine &lt;strong&gt;someone writing their own signature&lt;/strong&gt;. &lt;strong&gt;The speed can vary significantly&lt;/strong&gt;, but it won\u2019t remain constant. They would draw longer, straight lines more quickly and slow down at tight turns. When moving right and upwards, the arcs would be faster and more dynamic than when turning left. Even if the forged signature\u2019s image is an exact copy of the genuine one in terms of x-y coordinates, &lt;strong&gt;the speed profiles would look entirely different.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Of course, this was a rather clumsy attempt at forgery; there are far more skilled individuals in this field. The speed of a signature can be estimated based on the signature image alone, either by assuming faster movement on straight lines and slower movement on curves or by considering the line quality.&lt;/p&gt;\n\n&lt;p&gt;Delving into the details is beyond the scope of this discussion, but the key point is that estimating and replicating the speed of motion requires practice and talent. It is more challenging than simply replicating the x-y coordinates of the signature image. Moreover, forging the acceleration and other factors becomes even more difficult.&lt;/p&gt;\n\n&lt;p&gt;In theory, we could &lt;strong&gt;take derivatives of our time series as many times as desired&lt;/strong&gt;. However, there is a practical limit as, &lt;strong&gt;after a certain point, the derivative becomes more noise than meaningful information.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;From this example, it becomes apparent why we thought utilizing derivatives (speed, acceleration, jerk) was a valuable approach for motion analysis. &lt;strong&gt;When we began using this method, the results demonstrated exceptional accuracy&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udd0d Describing Time Series with Scalar Values&lt;/h1&gt;\n\n&lt;p&gt;We have extracted various time series from our motion sample. To condense the valuable information of a lengthy time series into scalar values, &lt;strong&gt;we employ a straightforward approach: calculating a few statistical characteristics&lt;/strong&gt;. Our selection criteria ensure that these characteristics effectively represent the distribution of the time series.&lt;/p&gt;\n\n&lt;p&gt;Some of these characteristics are expected, such as the minimum, maximum, mean, and standard deviation.&lt;/p&gt;\n\n&lt;p&gt;To understand how the values progress from the minimum to the maximum, we utilize percentiles, including the 10th, 25th, 50th, 75th, and 90th percentiles. The minimum and maximum values are also considered as percentiles, specifically the 0th and 100th percentiles.&lt;/p&gt;\n\n&lt;p&gt;Two lesser-known statistical values are skewness and kurtosis. &lt;strong&gt;Skewness measures the asymmetry of a distribution&lt;/strong&gt;. For instance, if the speed values below and above the average speed are evenly spaced, the skewness will be around zero. However, if there are numerous values below the mean but close to it, with only a few exceptionally high values above, the skewness will be positive.&lt;/p&gt;\n\n&lt;p&gt;In the context of cursor movement, this suggests that an individual typically uses the cursor at a relatively constant speed, but occasionally makes sudden moves. This could be a personal habit or characteristic.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, &lt;strong&gt;kurtosis indicates whether the values are concentrated around the mean or spread out across a broader range&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;These are the basic features we utilize for analysis.&lt;/p&gt;\n\n&lt;p&gt;Feel free to join the discussion and share your thoughts on this fascinating intersection of technology and human motion analysis! \ud83d\udcac\ud83d\udd7a\ud83c\udffd\ud83d\udcbb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "179u12d", "is_robot_indexable": true, "report_reasons": null, "author": "CursorInsight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/179u12d/cracking_the_code_of_human_motion_describing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/179u12d/cracking_the_code_of_human_motion_describing/", "subreddit_subscribers": 1088794, "created_utc": 1697533471.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey y'all, I have a 15 minute interview coming up with Kodiak.ai (Kodiak Robotics). Wondering what it's about. Any help is greatly appreciated!", "author_fullname": "t2_fz837yyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone got interview call from Kodiak Robotics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aahht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697581240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, I have a 15 minute interview coming up with Kodiak.ai (Kodiak Robotics). Wondering what it&amp;#39;s about. Any help is greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17aahht", "is_robot_indexable": true, "report_reasons": null, "author": "Chadsmithbass", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17aahht/anyone_got_interview_call_from_kodiak_robotics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17aahht/anyone_got_interview_call_from_kodiak_robotics/", "subreddit_subscribers": 1088794, "created_utc": 1697581240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\n I work at one of the big 4 consulting companies as a data scientist on their public sector accounts( with security clearance). I was a campus hire who started this year but I had a solid year of experience as a data science intern at a small tech company.  My bachelor degree was in statistics.\n\nI want to move to a data science or data analyst positions at big tech companies. I mainly want to work in analytics and data engineering. How many years of experience would I need to have a decent change?what would you recommend to increase my odds? Should I get a master degree? Where can I go to network other than cold email? Do certifications help?", "author_fullname": "t2_6cbsdgdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving to Big Tech from big government contractor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a9bma", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697578283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I work at one of the big 4 consulting companies as a data scientist on their public sector accounts( with security clearance). I was a campus hire who started this year but I had a solid year of experience as a data science intern at a small tech company.  My bachelor degree was in statistics.&lt;/p&gt;\n\n&lt;p&gt;I want to move to a data science or data analyst positions at big tech companies. I mainly want to work in analytics and data engineering. How many years of experience would I need to have a decent change?what would you recommend to increase my odds? Should I get a master degree? Where can I go to network other than cold email? Do certifications help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17a9bma", "is_robot_indexable": true, "report_reasons": null, "author": "Administrative_Bar46", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17a9bma/moving_to_big_tech_from_big_government_contractor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17a9bma/moving_to_big_tech_from_big_government_contractor/", "subreddit_subscribers": 1088794, "created_utc": 1697578283.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}