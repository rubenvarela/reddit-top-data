{"kind": "Listing", "data": {"after": "t3_179yvgb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does everyone interview prep, since it seems that all interview prep resources are more for software engineers than data engineers?\n\n\n\nIn general, software engineering interviews will ask more difficult data structure and algorithm questions.  But for data engineering interviews, I've noticed that the questions ask easier or moderately difficult Python questions involving string manipulations or dictionaries, followed by more difficult SQL questions.\n\n\n\n\nAlso, system design for software engineers, it seems they are asked to design a larger variety of services.  For data engineering, I've only been asked about data pipeline design.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Leetcode and System Design for data engineers vs. software engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179x2d7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697545205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does everyone interview prep, since it seems that all interview prep resources are more for software engineers than data engineers?&lt;/p&gt;\n\n&lt;p&gt;In general, software engineering interviews will ask more difficult data structure and algorithm questions.  But for data engineering interviews, I&amp;#39;ve noticed that the questions ask easier or moderately difficult Python questions involving string manipulations or dictionaries, followed by more difficult SQL questions.&lt;/p&gt;\n\n&lt;p&gt;Also, system design for software engineers, it seems they are asked to design a larger variety of services.  For data engineering, I&amp;#39;ve only been asked about data pipeline design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179x2d7", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/179x2d7/leetcode_and_system_design_for_data_engineers_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179x2d7/leetcode_and_system_design_for_data_engineers_vs/", "subreddit_subscribers": 134538, "created_utc": 1697545205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a fairly new data engineer that transferred from another career, I'd like to formally go through a curriculum from beginner Python knowledge all the way up to algorithms and architectures just to see what I'm missing. Currently, the wiki is set up in a way where its just a bunch of individual skills. How am I even supposed to know where to begin? Thoughts?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else feel like the wiki learning resources are too disorganized?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179lnd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697503395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a fairly new data engineer that transferred from another career, I&amp;#39;d like to formally go through a curriculum from beginner Python knowledge all the way up to algorithms and architectures just to see what I&amp;#39;m missing. Currently, the wiki is set up in a way where its just a bunch of individual skills. How am I even supposed to know where to begin? Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179lnd0", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179lnd0/does_anyone_else_feel_like_the_wiki_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179lnd0/does_anyone_else_feel_like_the_wiki_learning/", "subreddit_subscribers": 134538, "created_utc": 1697503395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI am trying to understand the differences between CDC and incremental append. I also get confused CDC with SCD. Can any one help me understand these concepts? When do we choose one over the other? \n\nThank you!!", "author_fullname": "t2_vgwleot0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC vs incremental append", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179m5rv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697506696.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697504857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am trying to understand the differences between CDC and incremental append. I also get confused CDC with SCD. Can any one help me understand these concepts? When do we choose one over the other? &lt;/p&gt;\n\n&lt;p&gt;Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179m5rv", "is_robot_indexable": true, "report_reasons": null, "author": "continuous_dataeng", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179m5rv/cdc_vs_incremental_append/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179m5rv/cdc_vs_incremental_append/", "subreddit_subscribers": 134538, "created_utc": 1697504857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a data scientist working on building a data platform for AI/ML and reporting use cases. Currently evaluating the most simple yet effective end to end tool for the task. \nI recently came across Dataiku and would like some feedback on how it is?", "author_fullname": "t2_6y1og8oq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do fellow data engineers use Dataiku?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179ofj0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697511527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a data scientist working on building a data platform for AI/ML and reporting use cases. Currently evaluating the most simple yet effective end to end tool for the task. \nI recently came across Dataiku and would like some feedback on how it is?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "179ofj0", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Ninja70", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179ofj0/how_do_fellow_data_engineers_use_dataiku/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179ofj0/how_do_fellow_data_engineers_use_dataiku/", "subreddit_subscribers": 134538, "created_utc": 1697511527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI could really use some guidance and advice on breaking into the field at an entry-level position. \n\nI don\u2019t have any peers or mentors so this subreddit is really my only connection to other data engineers.\n\n**TLDR**\n\n* I've learned programming and data engineering principles but struggle with confidence in my own ability to complete a project that would impress employers.\n* Burned out from tutorials and overwhelmed by the scope of some projects, I need guidance on narrowing down technologies and finding a realistic project.\n* I have a narrow time frame - I really need a job.\n* Advice on building a straightforward yet practical and comprehensive ETL/ELT pipeline (*asking for the impossible, I know..*) or recommendations for a reproducible project that would be appealing to employers would be greatly appreciated!\n\n**Full context**\n\nAfter spending almost a year learning programming and data engineering principles, I'm struggling to take the next step and become job-ready (I\u2019m applying to a few jobs here and there, but don\u2019t feel confident in my abilities to do more than that). While I have a good grasp of Python, SQL, and Relational Databases, I'm finding it difficult to complete a full project that would impress potential employers.\n\nI'm experiencing burnout from tutorials and feeling overwhelmed when trying to combine different technologies into a finished project. I understand the conceptual aspects, but the scope of work required to create a satisfactory end product is daunting.\n\nNarrowing down which technologies to use from the many available options is also proving challenging. I understand how to build rudimentary ETL scripts in Bash and Python, schedule using cron, connect to a relational database and insert data using SQL, but this doesn\u2019t feel like enough when I try to construct a practical or comprehensive data pipeline.\n\nWhen I try to learn and implement additional components like orchestration frameworks, transformation workflows, APIs, cloud computing, CI/CD workflows, monitoring procedures, UI interfaces, and deployment, it becomes too much to handle all at once. I\u2019m sure with more time, as well as renewed energy and motivation I will get to a point where I will be able to work with all of these technologies simultaneously but right now I am stressed out trying to just build *something* that resembles best practices, that would be appealing to a potential employer.\n\nVideo explanations and reverse engineering example repositories have been frustrating and often either don't provide the depth of knowledge I need, will lack clear explanations, or are out of date. (If I hear one more person describe something as a *simple* process while mumbling a half-baked explanation across a 3-hour unedited \"tutorial\" I might scream).\n\nAdditionally, having ADHD makes it difficult to stay focused and finish tasks without getting easily distracted by the feeling that I need to learn something else in order to move forward.\n\nI guess I\u2019m looking for guidance from someone who has successfully built a project that realistically covers the requirements of an entry-level position. I would really appreciate any advice or recommendations on how to produce a project that I can showcase and discuss during job interviews. Ideally, this would be a reproducible ETL/ELT pipeline following best practices, with a dashboard and data visualization component.\n\n*Sorry for the long post, thanks for reading and please don't hesitate to share any thoughts and/or advice!*", "author_fullname": "t2_dih4xe7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on breaking into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a1htx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697557861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I could really use some guidance and advice on breaking into the field at an entry-level position. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t have any peers or mentors so this subreddit is really my only connection to other data engineers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve learned programming and data engineering principles but struggle with confidence in my own ability to complete a project that would impress employers.&lt;/li&gt;\n&lt;li&gt;Burned out from tutorials and overwhelmed by the scope of some projects, I need guidance on narrowing down technologies and finding a realistic project.&lt;/li&gt;\n&lt;li&gt;I have a narrow time frame - I really need a job.&lt;/li&gt;\n&lt;li&gt;Advice on building a straightforward yet practical and comprehensive ETL/ELT pipeline (&lt;em&gt;asking for the impossible, I know..&lt;/em&gt;) or recommendations for a reproducible project that would be appealing to employers would be greatly appreciated!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Full context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;After spending almost a year learning programming and data engineering principles, I&amp;#39;m struggling to take the next step and become job-ready (I\u2019m applying to a few jobs here and there, but don\u2019t feel confident in my abilities to do more than that). While I have a good grasp of Python, SQL, and Relational Databases, I&amp;#39;m finding it difficult to complete a full project that would impress potential employers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m experiencing burnout from tutorials and feeling overwhelmed when trying to combine different technologies into a finished project. I understand the conceptual aspects, but the scope of work required to create a satisfactory end product is daunting.&lt;/p&gt;\n\n&lt;p&gt;Narrowing down which technologies to use from the many available options is also proving challenging. I understand how to build rudimentary ETL scripts in Bash and Python, schedule using cron, connect to a relational database and insert data using SQL, but this doesn\u2019t feel like enough when I try to construct a practical or comprehensive data pipeline.&lt;/p&gt;\n\n&lt;p&gt;When I try to learn and implement additional components like orchestration frameworks, transformation workflows, APIs, cloud computing, CI/CD workflows, monitoring procedures, UI interfaces, and deployment, it becomes too much to handle all at once. I\u2019m sure with more time, as well as renewed energy and motivation I will get to a point where I will be able to work with all of these technologies simultaneously but right now I am stressed out trying to just build &lt;em&gt;something&lt;/em&gt; that resembles best practices, that would be appealing to a potential employer.&lt;/p&gt;\n\n&lt;p&gt;Video explanations and reverse engineering example repositories have been frustrating and often either don&amp;#39;t provide the depth of knowledge I need, will lack clear explanations, or are out of date. (If I hear one more person describe something as a &lt;em&gt;simple&lt;/em&gt; process while mumbling a half-baked explanation across a 3-hour unedited &amp;quot;tutorial&amp;quot; I might scream).&lt;/p&gt;\n\n&lt;p&gt;Additionally, having ADHD makes it difficult to stay focused and finish tasks without getting easily distracted by the feeling that I need to learn something else in order to move forward.&lt;/p&gt;\n\n&lt;p&gt;I guess I\u2019m looking for guidance from someone who has successfully built a project that realistically covers the requirements of an entry-level position. I would really appreciate any advice or recommendations on how to produce a project that I can showcase and discuss during job interviews. Ideally, this would be a reproducible ETL/ELT pipeline following best practices, with a dashboard and data visualization component.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Sorry for the long post, thanks for reading and please don&amp;#39;t hesitate to share any thoughts and/or advice!&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a1htx", "is_robot_indexable": true, "report_reasons": null, "author": "Weekly_Yellow1256", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a1htx/seeking_advice_on_breaking_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a1htx/seeking_advice_on_breaking_into_data_engineering/", "subreddit_subscribers": 134538, "created_utc": 1697557861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi , I have 2 years of experience in Data Engineering and preparing for AWS SOLUTIONS ARCHITECT- Associate exam .Does it help in hiring and how much ?", "author_fullname": "t2_acpflxyf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Cloud Certification help in hiring?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179v26l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697537871.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi , I have 2 years of experience in Data Engineering and preparing for AWS SOLUTIONS ARCHITECT- Associate exam .Does it help in hiring and how much ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179v26l", "is_robot_indexable": true, "report_reasons": null, "author": "Hungry_Bee5131", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179v26l/do_cloud_certification_help_in_hiring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179v26l/do_cloud_certification_help_in_hiring/", "subreddit_subscribers": 134538, "created_utc": 1697537871.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "While [Apache Kafka](https://kafka.apache.org/?ref=blog.min.io) is somewhat of an industry standard for streaming data, there are other options emerging in the ecosystem. Given the importance of streaming in the modern data lakehouse, we thought we would take a look at one of the new cool kids on the block \u2013 [WarpStream](https://www.warpstream.com/?ref=blog.min.io). It should be noted that WarpStream is still \u201cunder development\u201d in many ways - it is really cool, very simple and exceptionally cost-effective, but taking it to production for mission-critical workloads should be a carefully considered decision.\u00a0\n\n[https://blog.min.io/streamlining-data-streaming-a-guide-to-warpstream-and-minio/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=data\\_streaming\\_warpstream](https://blog.min.io/streamlining-data-streaming-a-guide-to-warpstream-and-minio/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=data_streaming_warpstream)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streamlining Data Streaming: A Guide to WarpStream and MinIO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179jr4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697498023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While &lt;a href=\"https://kafka.apache.org/?ref=blog.min.io\"&gt;Apache Kafka&lt;/a&gt; is somewhat of an industry standard for streaming data, there are other options emerging in the ecosystem. Given the importance of streaming in the modern data lakehouse, we thought we would take a look at one of the new cool kids on the block \u2013 &lt;a href=\"https://www.warpstream.com/?ref=blog.min.io\"&gt;WarpStream&lt;/a&gt;. It should be noted that WarpStream is still \u201cunder development\u201d in many ways - it is really cool, very simple and exceptionally cost-effective, but taking it to production for mission-critical workloads should be a carefully considered decision.\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/streamlining-data-streaming-a-guide-to-warpstream-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=data_streaming_warpstream\"&gt;https://blog.min.io/streamlining-data-streaming-a-guide-to-warpstream-and-minio/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=data_streaming_warpstream&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?auto=webp&amp;s=5fff46eb69a9035720cd9378873669afc7103872", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cbd2dc08d739764d781fcc75978e48aa301a22de", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c00fdccd01eeea1a7d3bded5d9c647b1998254a0", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=362c6d2da7d5d35b9ea24b7156b9ee33a7fca71f", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b109a3bad947f8b1c51b198aabdb8cb871f8325c", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d14d9a78fd0f5a289bf9550efb57798e6a2c765c", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/STRBWQkjJZYib_KmKAAU799Ms02x-ykF76gR5rSBP2w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aafe28153986ac7be3f90cedab747b7a7a6204ae", "width": 1080, "height": 1080}], "variants": {}, "id": "7GM9EFqrq7tekYVR6UgHPYPAjwpoFHXC6zoogNui4Mg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "179jr4r", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179jr4r/streamlining_data_streaming_a_guide_to_warpstream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179jr4r/streamlining_data_streaming_a_guide_to_warpstream/", "subreddit_subscribers": 134538, "created_utc": 1697498023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI work for a fintech startup which is a small business where I'm the sole individual responsible for building data applications to inform the business. I'm looking for the best data infrastructure for a small business using a modern data stack.\n\nTo date, I've addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I'm pondering about the next professional step, especially as we anticipate our system to grow.\n\nOur primary data sources are:\n\n* **RDS (MySQL)**: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there's a hefty table with several JSONs weighing 14GB.\n* **DynamoDB**: A table for financial decisions with 2 million rows, taking up 3GB.\n* **Google Sheets**: Tables with fewer than a thousand rows.\n* **Bank API for fetching rates**: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.\n\nThe solution I'm considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I'd connect Power BI with Amazon Athena to these transformed databases in S3.\n\nDoes this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I'm leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company's size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.\n\nWould you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?\n\nI'm grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.\n\nThank you in advance for your advice and expertise!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Infrastructure on AWS without Redshift for a small business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a6g6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697570897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I work for a fintech startup which is a small business where I&amp;#39;m the sole individual responsible for building data applications to inform the business. I&amp;#39;m looking for the best data infrastructure for a small business using a modern data stack.&lt;/p&gt;\n\n&lt;p&gt;To date, I&amp;#39;ve addressed all the business queries by using SQL to craft views in MySQL, connecting Power BI to these views, using DAX to shape the information, and showcasing the visualizations in Power BI. This setup has been effective so far, but I&amp;#39;m pondering about the next professional step, especially as we anticipate our system to grow.&lt;/p&gt;\n\n&lt;p&gt;Our primary data sources are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;RDS (MySQL)&lt;/strong&gt;: Tables for transactions, users, and logs. The largest tables have about a million rows, growing by 10 to 30 thousand rows monthly. Most are under 1GB, though there&amp;#39;s a hefty table with several JSONs weighing 14GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt;: A table for financial decisions with 2 million rows, taking up 3GB.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Sheets&lt;/strong&gt;: Tables with fewer than a thousand rows.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Bank API for fetching rates&lt;/strong&gt;: Tables also with fewer than a thousand rows. I currently transfer this to Google Sheets and then connect.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The solution I&amp;#39;m considering is moving these data sources to an S3 bucket (acting as my Data Lake), and from there, to another S3 bucket added to the data catalog, which would essentially be my Data Warehouse. Then, I&amp;#39;d connect Power BI with Amazon Athena to these transformed databases in S3.&lt;/p&gt;\n\n&lt;p&gt;Does this approach sound sensible to you, or would you suggest a different route? Would you store the data in S3 or opt for RDS or another solution? I&amp;#39;m leaning towards S3 + data catalog connected with Athena instead of Redshift for the Data Warehouse because, given our company&amp;#39;s size, Redshift seems overpriced. We are a small team of 30, and our total AWS cost is about $6,000 a month. I turned on Redshift for three days, and it cost $600; it seems unjustifiable for our scale.&lt;/p&gt;\n\n&lt;p&gt;Would you carry out the ETL with Lambda, Glue, or another tool? Would you consider implementing Airflow, DBT, Git, CI/CD, or other tools at any stage of the process?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m grappling with these considerations and would truly appreciate any insights, opinions, or resource recommendations before undertaking these significant shifts in our organization.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your advice and expertise!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a6g6i", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a6g6i/data_infrastructure_on_aws_without_redshift_for_a/", "subreddit_subscribers": 134538, "created_utc": 1697570897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog Post from OpenMetadata team [https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364](https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364) on how to build/deploy , capture results and set alerts all via OpenMetadata. ", "author_fullname": "t2_4h8ymgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplifying Data Quality using OpenMetadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3so0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog Post from OpenMetadata team &lt;a href=\"https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364\"&gt;https://blog.open-metadata.org/simple-easy-and-efficient-data-quality-with-openmetadata-1c4e7d329364&lt;/a&gt; on how to build/deploy , capture results and set alerts all via OpenMetadata. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?auto=webp&amp;s=11b603c7839cb0bcfb1836e0d02264077be4df18", "width": 1200, "height": 483}, "resolutions": [{"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d9a2f96a33e3f9f02be6785888e32ab79cf764b", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2870e974d4c22835d1ce3c0cf7dbda2104e5d1bb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46355b53cdbb9c4e84d01e385b56b914cf89f122", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65ef23face4242dad2d4237e7ce5853283b426da", "width": 640, "height": 257}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b9d85a129f9b34ef5cb4608e8fba45664753524", "width": 960, "height": 386}, {"url": "https://external-preview.redd.it/ocu6FqeQ54mEuHpE0hCnN_86kkU2SuuL44A9h7pRq34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9159457240321cfbc20d4b8e633a605abdc1b31a", "width": 1080, "height": 434}], "variants": {}, "id": "jibYnJc1eg8fLQihM9kKlMVQV76i3ag7MfEojzuA7Ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17a3so0", "is_robot_indexable": true, "report_reasons": null, "author": "d3fmacro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3so0/simplifying_data_quality_using_openmetadata/", "subreddit_subscribers": 134538, "created_utc": 1697563984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am very new to aws so sorry for the dumb question but I was hoping for some advice on the best way to do this. I am pretty sure that snowball is not an option here too. I was considering the following two options but I am unsure which would be faster:\n\n1. use either `aws s3 cp --recursive` or `aws s3 sync` and move all the files to a s3 bucket and then once all the files are in the bucket, I will do the same and move them from the bucket to the volume.\n\n2. again use either `aws s3 cp --recursive` or `aws s3 sync` to move the files to the bucket but then have a trigger set so that all incoming files to the s3 bucket automatically get copied into the EBS volume.\n\nWhich of these ways would be faster and are there better ways that I should consider to do all of this? Thanks!\n\nSome edits:\n* I should be more clear on the data. I have not exactly seen that data yet but I do know that it is 8TiB of images but I am unsure of the file type.", "author_fullname": "t2_3h5b6a2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some ways I can move 8TiB of data from my local machine to an EBS volume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2zbn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697580680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697561856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very new to aws so sorry for the dumb question but I was hoping for some advice on the best way to do this. I am pretty sure that snowball is not an option here too. I was considering the following two options but I am unsure which would be faster:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;use either &lt;code&gt;aws s3 cp --recursive&lt;/code&gt; or &lt;code&gt;aws s3 sync&lt;/code&gt; and move all the files to a s3 bucket and then once all the files are in the bucket, I will do the same and move them from the bucket to the volume.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;again use either &lt;code&gt;aws s3 cp --recursive&lt;/code&gt; or &lt;code&gt;aws s3 sync&lt;/code&gt; to move the files to the bucket but then have a trigger set so that all incoming files to the s3 bucket automatically get copied into the EBS volume.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Which of these ways would be faster and are there better ways that I should consider to do all of this? Thanks!&lt;/p&gt;\n\n&lt;p&gt;Some edits:\n* I should be more clear on the data. I have not exactly seen that data yet but I do know that it is 8TiB of images but I am unsure of the file type.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a2zbn", "is_robot_indexable": true, "report_reasons": null, "author": "psssat", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2zbn/what_are_some_ways_i_can_move_8tib_of_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2zbn/what_are_some_ways_i_can_move_8tib_of_data_from/", "subreddit_subscribers": 134538, "created_utc": 1697561856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nCurrently, I am working on a project that when you input a bunch of SQL statements (SELECT, CREATE are the most useful), it will produce a visualization of the column-level lineage among the files. It can work both with a database connection (for higher accuracy) or without, and tries to provide the column-lineage with the base tables too. It has a similar function as the DataHub's lineage exploration, but it comes in a Python library and can work with DBT when you just give the path to the folder containing the manifest file. I know this tool can be used for data exploration, debugging or quality purposes, so I was wondering have you ever encountered a real-life scenario that this kind of tool would be greatly beneficial?\n\nThank you a lot in advance!", "author_fullname": "t2_14ztj9sa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Motivation for a column-level SQL lineage tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179vg4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697539380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;Currently, I am working on a project that when you input a bunch of SQL statements (SELECT, CREATE are the most useful), it will produce a visualization of the column-level lineage among the files. It can work both with a database connection (for higher accuracy) or without, and tries to provide the column-lineage with the base tables too. It has a similar function as the DataHub&amp;#39;s lineage exploration, but it comes in a Python library and can work with DBT when you just give the path to the folder containing the manifest file. I know this tool can be used for data exploration, debugging or quality purposes, so I was wondering have you ever encountered a real-life scenario that this kind of tool would be greatly beneficial?&lt;/p&gt;\n\n&lt;p&gt;Thank you a lot in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "179vg4n", "is_robot_indexable": true, "report_reasons": null, "author": "zshandy1994", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179vg4n/motivation_for_a_columnlevel_sql_lineage_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179vg4n/motivation_for_a_columnlevel_sql_lineage_tool/", "subreddit_subscribers": 134538, "created_utc": 1697539380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do \"big data\" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). \n\nI'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?\n\nMaybe I'm reading too much into this, and their description was just a weird way of saying \"expect standard data structures leetcode\"?", "author_fullname": "t2_mz663ln7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've got a DE Interview, but they're not letting me use libraries or SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17aadnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697580962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they&amp;#39;ll have me do &amp;quot;big data&amp;quot; code challenges, and explicitly states that SQL, Pandas, etc. won&amp;#39;t be allowed (only default Python). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m honestly confused as most interviews I&amp;#39;ve had expected me to use data-related technologies, I find it odd that they&amp;#39;d explicitly exclude them. Has anyone encountered a similar situation?&lt;/p&gt;\n\n&lt;p&gt;Maybe I&amp;#39;m reading too much into this, and their description was just a weird way of saying &amp;quot;expect standard data structures leetcode&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17aadnb", "is_robot_indexable": true, "report_reasons": null, "author": "arkoftheconvenient", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/", "subreddit_subscribers": 134538, "created_utc": 1697580962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "## Introduction\n\nHey guys, I am a solo data engineer at a small-medium sized manufacturing company. Something that I run into pretty often at my current company is the need for a solution for data capture of some manual process in order to create a KPI. These scenarios don't typically merit the construction of a full application around the process, whose data can then be tapped to report on the KPI of \\_\\_\\_\\_. Most stakeholder's simply want a place to enter their hand collected weekly data, and so the idea that I have come up with is using a cloud Excel workbook with a table where the stakeholder can manually enter their weekly data. I'm not satisfied with this solution, but I'm having trouble thinking of alternatives that don't require more work on my end than the KPI seems to even be worth.\n\n## Question\n\nI was wondering if there is a best-practice approach in the industry to this sort of situation.", "author_fullname": "t2_fjnfv117", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom solutions for data capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179zkaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697552670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Introduction&lt;/h2&gt;\n\n&lt;p&gt;Hey guys, I am a solo data engineer at a small-medium sized manufacturing company. Something that I run into pretty often at my current company is the need for a solution for data capture of some manual process in order to create a KPI. These scenarios don&amp;#39;t typically merit the construction of a full application around the process, whose data can then be tapped to report on the KPI of ____. Most stakeholder&amp;#39;s simply want a place to enter their hand collected weekly data, and so the idea that I have come up with is using a cloud Excel workbook with a table where the stakeholder can manually enter their weekly data. I&amp;#39;m not satisfied with this solution, but I&amp;#39;m having trouble thinking of alternatives that don&amp;#39;t require more work on my end than the KPI seems to even be worth.&lt;/p&gt;\n\n&lt;h2&gt;Question&lt;/h2&gt;\n\n&lt;p&gt;I was wondering if there is a best-practice approach in the industry to this sort of situation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "179zkaa", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Wear3951", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179zkaa/custom_solutions_for_data_capture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179zkaa/custom_solutions_for_data_capture/", "subreddit_subscribers": 134538, "created_utc": 1697552670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE's) into a false sense of reliability, but will ultimately be flawed and become a liability.\n\nI am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I've been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.\n\nIn some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd\n\nI want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting\n\nI've googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):\n\n1. Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?\n2. Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022\n3. Data Pipelines Pocket Reference -  James Densmore, 2021, again isn't it too old?\n4. Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?\n5. Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?\n\nI would like the books to be as close to the real world scenarios, production data engineering.\n\nHow would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Close the gap and consolidate fundamentals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a900f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697577679.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697577490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,I am coming here with a popular problem. In one of thread, Redditor said that BI developer/analyst can in a pinch, deliver a best effort on the DE field that will deceive everyone (including the DE&amp;#39;s) into a false sense of reliability, but will ultimately be flawed and become a liability.&lt;/p&gt;\n\n&lt;p&gt;I am consultant in data, means I work with data in various ways - most likely it was BI. Developing dashboards, some back-end in BI, some analysis. I&amp;#39;ve been exposed to various tools like PowerBI, SQL, Python (pyspark, pandas), R, Qlik, Azure (Data factory, Synapse, Databricks, Logic, Functions, also storages, key vaults etc), Snowflake, AWS. Some of them I know for years, some are pretty new, some are obsolete and have been used in the past.&lt;/p&gt;\n\n&lt;p&gt;In some projects the work is pretty basic, in other it gets more complex. I think that I know basics, fundamentals maybe a bit above in some areas of it but my knowledge is not organized. There are things which I had to figured out on my own and I have not fully confidence if I understood them properly, but worked etc. I am thinking about data modeling concepts, warehousing, pipelines, some ci/cd&lt;/p&gt;\n\n&lt;p&gt;I want to finish my work as consultant and go to company which DE will be internal, with good engineering practices. I will downgrade my salary for sure, but eventually will catch up and the work seems completly more interesting&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve googled a fair bit and found some books on the fundamentals, which to my opinion would be the best (not in particular order):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Engineering with Python -  Paul Crickard  although its 2020, so isnt it a bit old?&lt;/li&gt;\n&lt;li&gt;Fundamentals of Data Engineering -  Joe Reis, Matt Housley, June 2022&lt;/li&gt;\n&lt;li&gt;Data Pipelines Pocket Reference -  James Densmore, 2021, again isn&amp;#39;t it too old?&lt;/li&gt;\n&lt;li&gt;Designing Data-Intensive Applications -  Martin Kleppmann - i know its older but I guess more theory, less approach?&lt;/li&gt;\n&lt;li&gt;Looking also  for a great book that goes through the popular data architecture patterns end-to-end. With code samples ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would like the books to be as close to the real world scenarios, production data engineering.&lt;/p&gt;\n\n&lt;p&gt;How would you rate these books? Obviously I dont have time to pick up all of them right now, but lets say three most important out there, closes to production data engineering, which gives me best fundamentals on DE, connect the dots and let me be build proper solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a900f", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a900f/close_the_gap_and_consolidate_fundamentals/", "subreddit_subscribers": 134538, "created_utc": 1697577490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products? Deploy: Part 3/4 - Doubling down on the power of Unified Experiences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_179ycsl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fIAPxsCawPBu-N75Vvoyg_keOmRvMjS4W4feJpzH2AE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697549245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?auto=webp&amp;s=ea619a7c524f8c5536cc06f8b13ba792caddd5b5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66add8fac528d26de7feccdd0c5022d39fd89850", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=85446cd271d715b3865f735be872ce747366b093", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=929b190abb183f4784a7d725a150dbb2083ba894", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e4c400d8251d52d7421fc6640499a17ea524db3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e716ed51284478d9d2a2569fdf859fe895fdf91", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XZbtq-aQh02nKdrMzpZYd3-viI17NIEpazgIpUJ0liI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5219b40b806ce0099e4650da1d92ec438c0e19d8", "width": 1080, "height": 540}], "variants": {}, "id": "_WJ_OVW7dv_Q8Y2muH3UxiO88vjiOO6xTcwW1zV8-tA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "179ycsl", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179ycsl/how_to_build_data_products_deploy_part_34/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/how-to-build-data-products-deploy", "subreddit_subscribers": 134538, "created_utc": 1697549245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey! I've decided to ask this here as I've been lurking for a while and the community feels very approachable. I've googled this question and searched the Kimball's forum but to no avail. I also don't remember this from Kimball's book.\n\n&amp;#x200B;\n\nI'm creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these 'sub-dimensions'. The thing is... It's getting too wide? \n\nThat's the question I can't really answer. When is a dimension table too wide? Right now I'm around 100 columns, which feels excessive and might be hard to use for reporting. \n\nI'm thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_l38l5aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is a dimension table too denormalized? | Kimball", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17aaznc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697582566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I&amp;#39;ve decided to ask this here as I&amp;#39;ve been lurking for a while and the community feels very approachable. I&amp;#39;ve googled this question and searched the Kimball&amp;#39;s forum but to no avail. I also don&amp;#39;t remember this from Kimball&amp;#39;s book.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m creating a data model for bank transactions. Each transaction has an account associated with it. Each account has one client which is the owner and a beneficiary in case something happens to the owner. This three things are very correlated, so it felt natural to create a dimension called ClientAccount which has all attributes from these &amp;#39;sub-dimensions&amp;#39;. The thing is... It&amp;#39;s getting too wide? &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s the question I can&amp;#39;t really answer. When is a dimension table too wide? Right now I&amp;#39;m around 100 columns, which feels excessive and might be hard to use for reporting. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thorn between two options: for each transaction have 3 dimensions associated: client, account, beneficiary. Or have just the ClientAccount. This is ignoring other dimensions such as date dim, transactionChannel dim, and some others.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aaznc", "is_robot_indexable": true, "report_reasons": null, "author": "ArgenEgo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aaznc/when_is_a_dimension_table_too_denormalized_kimball/", "subreddit_subscribers": 134538, "created_utc": 1697582566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm seriously confused about this.\n\nfrom what i gather. it's like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.\n\ni'm trying to use pyiceberg to implement this.\n\ni created an iceberg table in s3. but now how do i actually point it to parquet files?\n\ni tried putting them in the same directory, that didn't do it.\n\ni tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.\n\nseriously confused.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "can anyone explain apache iceberg better than chatgpt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a8dfa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697575896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m seriously confused about this.&lt;/p&gt;\n\n&lt;p&gt;from what i gather. it&amp;#39;s like a pointer to some files in s3, and that pointer has some metadata that is supposed to help you sift through those files faster/better.&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m trying to use pyiceberg to implement this.&lt;/p&gt;\n\n&lt;p&gt;i created an iceberg table in s3. but now how do i actually point it to parquet files?&lt;/p&gt;\n\n&lt;p&gt;i tried putting them in the same directory, that didn&amp;#39;t do it.&lt;/p&gt;\n\n&lt;p&gt;i tried looking for any kind of methods using polars, pyarrow, pandas, whatever - i see guides that let you read from iceberg tables, but nothing really just explaining the basics of how to implement this without java or emr or whatever.&lt;/p&gt;\n\n&lt;p&gt;seriously confused.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a8dfa", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a8dfa/can_anyone_explain_apache_iceberg_better_than/", "subreddit_subscribers": 134538, "created_utc": 1697575896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have a nice tool or good practice for managing and monitoring the usage of UDFs in Snowflake or other RDS? ", "author_fullname": "t2_4b4oubcv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Tool or Practice to Monitor and Manage UDFs in Snowflake or other RDS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a7ute", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697574547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have a nice tool or good practice for managing and monitoring the usage of UDFs in Snowflake or other RDS? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a7ute", "is_robot_indexable": true, "report_reasons": null, "author": "trajan_augustus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a7ute/best_tool_or_practice_to_monitor_and_manage_udfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a7ute/best_tool_or_practice_to_monitor_and_manage_udfs/", "subreddit_subscribers": 134538, "created_utc": 1697574547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?\n\nTrying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise\n\nThanks in advance!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a3k4m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697563339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company is reorganizing and I\u2019m the owner of the Data Warehouse team. My question is\u2026 how are your company\u2019s structured. Does your dba team have access to your DW from a governance/oversight perspective or do they own it?? or is your team apart from tech entirely and roll up through analytics?&lt;/p&gt;\n\n&lt;p&gt;Trying to understand where the path for me is. We have moved my department \u201cdata and analytics\u201d around a lot this last year as we grow and I\u2019m just curious as to what others are dealing with structure wise&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17a3k4m", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a3k4m/question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a3k4m/question/", "subreddit_subscribers": 134538, "created_utc": 1697563339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nWe are looking to modernize our ETL infrastructure for one of our business processes that leverages informatica. I don't have any experience with the backend of informatica nor a modern comparator like darabrica. But I know we should push towards it. Can anyone give a strong technical rationale why this is the way to go?", "author_fullname": "t2_4oldr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modernizing from informatica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2tlf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697561442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;We are looking to modernize our ETL infrastructure for one of our business processes that leverages informatica. I don&amp;#39;t have any experience with the backend of informatica nor a modern comparator like darabrica. But I know we should push towards it. Can anyone give a strong technical rationale why this is the way to go?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17a2tlf", "is_robot_indexable": true, "report_reasons": null, "author": "lysis_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2tlf/modernizing_from_informatica/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2tlf/modernizing_from_informatica/", "subreddit_subscribers": 134538, "created_utc": 1697561442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "background :  started my current data role about a year ago. I had minimal azure skills but was able to pickup quite a bit as I previously worked on a similar stack. I\u2019ve mainly been doing Data factory pipeline and SQL for data loads. As an org we are still maturing in data practices and i introduced new process like automated deployment release and Incremental data loads (and received great feedback from manager)\n\norg issues :\n 1 - my team lead is seems overbearing and wants to rehash things a few times. So basically whenever I work on something new, he wants me go over things atleast 4-5 time. I\u2019ve noticed that he does that with 2 other ETL developers in the team. He does minimal hands on work but still wants everything to be explained to him until he gets it. I find that quite draining and many times I feel like I could use my time better working on stuff. \n\n2- second issue I am dealing with is the devs who\u2019ve been around there 2-3 year longer. They usually don\u2019t support my ideas even though it could be better for the team. I have resolved code issues in the past that they weren\u2019t able to and have generally noticed a slower pace of development on their side (these folks are contractors , unfortunately my org is slightly old school , contractors are paid quite high and expected to implement a lot of stuff quickly) \n\nAnyone navigated similar issues ? How do you boost morale and continue with growth in your organization? And perhaps gain more autonomy to implement new ideas in midst of pushy/adamant co workers ?", "author_fullname": "t2_d90jcqqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to gain more autonomy as a new-ish employee?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2njr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697560995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;background :  started my current data role about a year ago. I had minimal azure skills but was able to pickup quite a bit as I previously worked on a similar stack. I\u2019ve mainly been doing Data factory pipeline and SQL for data loads. As an org we are still maturing in data practices and i introduced new process like automated deployment release and Incremental data loads (and received great feedback from manager)&lt;/p&gt;\n\n&lt;p&gt;org issues :\n 1 - my team lead is seems overbearing and wants to rehash things a few times. So basically whenever I work on something new, he wants me go over things atleast 4-5 time. I\u2019ve noticed that he does that with 2 other ETL developers in the team. He does minimal hands on work but still wants everything to be explained to him until he gets it. I find that quite draining and many times I feel like I could use my time better working on stuff. &lt;/p&gt;\n\n&lt;p&gt;2- second issue I am dealing with is the devs who\u2019ve been around there 2-3 year longer. They usually don\u2019t support my ideas even though it could be better for the team. I have resolved code issues in the past that they weren\u2019t able to and have generally noticed a slower pace of development on their side (these folks are contractors , unfortunately my org is slightly old school , contractors are paid quite high and expected to implement a lot of stuff quickly) &lt;/p&gt;\n\n&lt;p&gt;Anyone navigated similar issues ? How do you boost morale and continue with growth in your organization? And perhaps gain more autonomy to implement new ideas in midst of pushy/adamant co workers ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17a2njr", "is_robot_indexable": true, "report_reasons": null, "author": "Proof-Yellow6362", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2njr/how_to_gain_more_autonomy_as_a_newish_employee/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2njr/how_to_gain_more_autonomy_as_a_newish_employee/", "subreddit_subscribers": 134538, "created_utc": 1697560995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm appearing for the first time for an adf, pyspark interview. If anyone has attended an interview for this skill set, can you please share the list of interview questions, the most asked questions, anything that would help me crack the interview. I did a couple of courses, certifications and hands on. I'm switching technology from etl to azure cloud and Pyspark. Thanks in advance.", "author_fullname": "t2_iilsa8bv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a2l2a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697560814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m appearing for the first time for an adf, pyspark interview. If anyone has attended an interview for this skill set, can you please share the list of interview questions, the most asked questions, anything that would help me crack the interview. I did a couple of courses, certifications and hands on. I&amp;#39;m switching technology from etl to azure cloud and Pyspark. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17a2l2a", "is_robot_indexable": true, "report_reasons": null, "author": "gonegura", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a2l2a/interview_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a2l2a/interview_questions/", "subreddit_subscribers": 134538, "created_utc": 1697560814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on a staff that makes pdf fillable forms. for each part of the process there are three sharepoint databases the done part of the form goes into. each sharepoint databases I need to fill out forms and attach the new form.\n\nCould I use a tool like Access or SQL to automate this process?", "author_fullname": "t2_hprxt4fhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best way to move multiple files to multiple shairpoint forms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a107c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697556569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a staff that makes pdf fillable forms. for each part of the process there are three sharepoint databases the done part of the form goes into. each sharepoint databases I need to fill out forms and attach the new form.&lt;/p&gt;\n\n&lt;p&gt;Could I use a tool like Access or SQL to automate this process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17a107c", "is_robot_indexable": true, "report_reasons": null, "author": "whimsicalwhisper39", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a107c/best_way_to_move_multiple_files_to_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a107c/best_way_to_move_multiple_files_to_multiple/", "subreddit_subscribers": 134538, "created_utc": 1697556569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering community,\n\nI'm excited to introduce **Cookiecutter-Azure-DBX**, a Python project template designed to streamline data engineering/science projects with a focus on seamless integration with Azure and Databricks. This project template provides an efficient and structured way to kick-start your data engineering projects, especially if you're working with Azure services and Databricks.\n\n**Key Features:**\n\n* **Continuous Integration in Azure:** Set up automated CI/CD pipelines for your projects.\n* **Databricks Workflow Deployment:** Easily deploy workflows using DBX, the CLI tool for advanced Databricks workflow management.\n* **Python Project Structure:** Get started with a well-organized Python project structure.\n* **Code Formatting and Linting:** Includes tools like Black, Pylint, Bandit, Flake8, and more for code quality.\n* **Environment Variable Management:** Load environment variables using python-dotenv.\n* **Logging with Structlog:** Configure structured logging for easy tracking of function execution.\n* **Pydantic Integration:** Utilize Pydantic for settings management.\n* **Azure Pipelines:** Simple CI using Docker Tasks.\n\nYou can find the project on [GitHub](https://github.com/stgoa/cookiecutter-azure-dbx/).\n\nPlease feel free to check it out, provide feedback, and contribute. Let's collaborate and make data engineering more efficient!\n\nLooking forward to your thoughts and discussions.", "author_fullname": "t2_6atg5eip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Cookiecutter-Azure-DBX: A Python Project Template for Data Engineering with Databricks Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17a0e2l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697554928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to introduce &lt;strong&gt;Cookiecutter-Azure-DBX&lt;/strong&gt;, a Python project template designed to streamline data engineering/science projects with a focus on seamless integration with Azure and Databricks. This project template provides an efficient and structured way to kick-start your data engineering projects, especially if you&amp;#39;re working with Azure services and Databricks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Continuous Integration in Azure:&lt;/strong&gt; Set up automated CI/CD pipelines for your projects.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Databricks Workflow Deployment:&lt;/strong&gt; Easily deploy workflows using DBX, the CLI tool for advanced Databricks workflow management.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Python Project Structure:&lt;/strong&gt; Get started with a well-organized Python project structure.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Formatting and Linting:&lt;/strong&gt; Includes tools like Black, Pylint, Bandit, Flake8, and more for code quality.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Environment Variable Management:&lt;/strong&gt; Load environment variables using python-dotenv.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Logging with Structlog:&lt;/strong&gt; Configure structured logging for easy tracking of function execution.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pydantic Integration:&lt;/strong&gt; Utilize Pydantic for settings management.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Azure Pipelines:&lt;/strong&gt; Simple CI using Docker Tasks.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can find the project on &lt;a href=\"https://github.com/stgoa/cookiecutter-azure-dbx/\"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Please feel free to check it out, provide feedback, and contribute. Let&amp;#39;s collaborate and make data engineering more efficient!&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your thoughts and discussions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?auto=webp&amp;s=0b80591440047dab22d246cd0ab2d1a367bd3ea5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=81ff64c7ff2313a7d81338f87a454ebee3163e59", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbf5ef2bfe49e7cad7598563c59bf683016d9f88", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=510c8c8301a8c303fcc24fb74e2eb75ea99d0e4e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0283601ce16ed468079d44326a2c619df57df1ad", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b94883318a7181b37ca6f7dedbcd50c21839ea1c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8pp1RxThVc30_afUDLhnCNGqVoIKjG9QN0hzhpAveOY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e808748ef881195bf19e471b0f07a3466693683", "width": 1080, "height": 540}], "variants": {}, "id": "dG5DasDKnvTB5n9J59awl1XSYqhQU8u02i7sSKKSwAM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17a0e2l", "is_robot_indexable": true, "report_reasons": null, "author": "lezapete", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17a0e2l/introducing_cookiecutterazuredbx_a_python_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17a0e2l/introducing_cookiecutterazuredbx_a_python_project/", "subreddit_subscribers": 134538, "created_utc": 1697554928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I(M23) have just started an internship in an organization under the Data Engineering section. I have a background in Statistics. I am currently looking for resources, Big data, Apache Nifi and Hadoop, both on premise and cloud(AWS) to facilitate an easy transition for me in the team. Any and all learning materials you may have are welcome, Thank you.", "author_fullname": "t2_cxzdcift", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do I start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_179yvgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697550770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I(M23) have just started an internship in an organization under the Data Engineering section. I have a background in Statistics. I am currently looking for resources, Big data, Apache Nifi and Hadoop, both on premise and cloud(AWS) to facilitate an easy transition for me in the team. Any and all learning materials you may have are welcome, Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "179yvgb", "is_robot_indexable": true, "report_reasons": null, "author": "Lazy_Secret8626", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/179yvgb/where_do_i_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/179yvgb/where_do_i_start/", "subreddit_subscribers": 134538, "created_utc": 1697550770.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}