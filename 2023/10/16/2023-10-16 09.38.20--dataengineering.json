{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Maybe when you meet on a vacation, in a bus. \n(Spare the Tech Stack Question please)", "author_fullname": "t2_t9iw9tl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the first few questions you ask when you come across a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178jc4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697387992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe when you meet on a vacation, in a bus. \n(Spare the Tech Stack Question please)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178jc4k", "is_robot_indexable": true, "report_reasons": null, "author": "Straight-End4310", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178jc4k/whats_the_first_few_questions_you_ask_when_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178jc4k/whats_the_first_few_questions_you_ask_when_you/", "subreddit_subscribers": 134280, "created_utc": 1697387992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nIm currently working in a consulting company as a data engineer and loving everything about it. The pace, the exposure to different tech stacks and projects so far. What I\u2019m seeing is there is a ton of need for intermediate work and projects, so consulting makes sense for our clients. \n\nAll my clients have been BIG companies across pharmacy, entertainment and energy industries.\n\nNow question for the community, what\u2019s in house role like? \nSeems like mid size and smaller businesses are far behind the curve, since our massive clients are just now optimizing their pipelines. \nWould that imply that in house DE roles are mainly present for big companies or mid size with a team of a few DEs?\n\nMainly thinking through future moves, how long to stay, when to switch (if at all)?\n\nIs there enough tech exposure for in-house DE roles and are projects lengthy and boring? This is mainly in gaining experience in-house vs consulting. \n\nAny input is greatly appreciated.", "author_fullname": "t2_l2qjkxgyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In House Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178kxkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697394536.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697392578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Im currently working in a consulting company as a data engineer and loving everything about it. The pace, the exposure to different tech stacks and projects so far. What I\u2019m seeing is there is a ton of need for intermediate work and projects, so consulting makes sense for our clients. &lt;/p&gt;\n\n&lt;p&gt;All my clients have been BIG companies across pharmacy, entertainment and energy industries.&lt;/p&gt;\n\n&lt;p&gt;Now question for the community, what\u2019s in house role like? \nSeems like mid size and smaller businesses are far behind the curve, since our massive clients are just now optimizing their pipelines. \nWould that imply that in house DE roles are mainly present for big companies or mid size with a team of a few DEs?&lt;/p&gt;\n\n&lt;p&gt;Mainly thinking through future moves, how long to stay, when to switch (if at all)?&lt;/p&gt;\n\n&lt;p&gt;Is there enough tech exposure for in-house DE roles and are projects lengthy and boring? This is mainly in gaining experience in-house vs consulting. &lt;/p&gt;\n\n&lt;p&gt;Any input is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178kxkd", "is_robot_indexable": true, "report_reasons": null, "author": "George_mate_", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178kxkd/in_house_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178kxkd/in_house_data_engineering/", "subreddit_subscribers": 134280, "created_utc": 1697392578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building an application that accepts exercise data from devices then kicks off a ETL pipeline. This data is then displayed to users. Right now I get really slowed down when I have to test changes and as I add more devices this will only become a bigger problem. Does anyone have experience creating automated tests for something like this.", "author_fullname": "t2_5ntpnb49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automated testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178j1j1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697387181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building an application that accepts exercise data from devices then kicks off a ETL pipeline. This data is then displayed to users. Right now I get really slowed down when I have to test changes and as I add more devices this will only become a bigger problem. Does anyone have experience creating automated tests for something like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178j1j1", "is_robot_indexable": true, "report_reasons": null, "author": "ExtensionResearcher2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178j1j1/automated_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178j1j1/automated_testing/", "subreddit_subscribers": 134280, "created_utc": 1697387181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking for a solution I can integrate into my app that can convert data stored in human readable formats from one schema to another. Ideally it would convert both ways from CSV, JSON, etc, but I need it to also be able to handle relational and nested data. \nSo it would need to be able to nest and un-nest data, flatten and un- flatten data, deal with arrays and hierarchy, etc. convert data from multiple tables into one and back. You get the picture.", "author_fullname": "t2_gjrwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an open source solution that handles complex conversion of data structures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178hvif", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697383892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for a solution I can integrate into my app that can convert data stored in human readable formats from one schema to another. Ideally it would convert both ways from CSV, JSON, etc, but I need it to also be able to handle relational and nested data. \nSo it would need to be able to nest and un-nest data, flatten and un- flatten data, deal with arrays and hierarchy, etc. convert data from multiple tables into one and back. You get the picture.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178hvif", "is_robot_indexable": true, "report_reasons": null, "author": "prutwo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178hvif/is_there_an_open_source_solution_that_handles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178hvif/is_there_an_open_source_solution_that_handles/", "subreddit_subscribers": 134280, "created_utc": 1697383892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There's a senior manager who's keen to modernize their approach to data, but doesn't know what they want. What are you asking for / putting in place?", "author_fullname": "t2_2s6myxsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You've just joined a new company who do everything in Excel, but....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1790jby", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697442029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a senior manager who&amp;#39;s keen to modernize their approach to data, but doesn&amp;#39;t know what they want. What are you asking for / putting in place?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1790jby", "is_robot_indexable": true, "report_reasons": null, "author": "Dog_In_A_Human_Suit", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1790jby/youve_just_joined_a_new_company_who_do_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1790jby/youve_just_joined_a_new_company_who_do_everything/", "subreddit_subscribers": 134280, "created_utc": 1697442029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m surprised I never hear about this tool despite it seeming like it could fit into the modern data stack. Or do I not understand the tool correctly ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your team use workato if at all, why / why not ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178ythu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697434633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m surprised I never hear about this tool despite it seeming like it could fit into the modern data stack. Or do I not understand the tool correctly ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178ythu", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178ythu/how_does_your_team_use_workato_if_at_all_why_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178ythu/how_does_your_team_use_workato_if_at_all_why_why/", "subreddit_subscribers": 134280, "created_utc": 1697434633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context: Our application is single tenant such that each customer is provisioned their own deployed instance and database. As a result, we have 100s of separate MongoDB databases across multiple clusters. This also means that each database has a copy of the same table, so any final table like FooItems would require aggregating the FooItems from each customer database.\n\n&amp;#x200B;\n\nWe're looking to build out an automated data pipeline for the first time, and I'm looking for suggestions on moving our data into BigQuery. In particular:\n\n\\- Are there any tools that seamlessly handle this number of MongoDB databases? Looking at UI based tools, it seems untenable to have to manually setup the connection for each individual database. I experimented with Meltano as well, and the existing tap-mongodb extractor doesn't handle this case too well. Is my best option going to be writing a custom extraction layer?\n\n\\- Our MongoDB schemas are quite complicated, and my sense is that we're best off doing at least a light transform before pulling the data into BigQuery. Does this sound like the right move? The alternative seems to be loading the MongoDB data in as-is and extracting the JSON fields into a new table using SQL queries. This seems like it could be pretty cumbersome depending on how nested and convoluted the schema is, but I'm curious what everyone's experiences have been with this approach and if you would recommend it.\n\n&amp;#x200B;\n\nThanks all!", "author_fullname": "t2_112bde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for moving data from hundreds of MongoDB databases to BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178wdat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697425607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: Our application is single tenant such that each customer is provisioned their own deployed instance and database. As a result, we have 100s of separate MongoDB databases across multiple clusters. This also means that each database has a copy of the same table, so any final table like FooItems would require aggregating the FooItems from each customer database.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re looking to build out an automated data pipeline for the first time, and I&amp;#39;m looking for suggestions on moving our data into BigQuery. In particular:&lt;/p&gt;\n\n&lt;p&gt;- Are there any tools that seamlessly handle this number of MongoDB databases? Looking at UI based tools, it seems untenable to have to manually setup the connection for each individual database. I experimented with Meltano as well, and the existing tap-mongodb extractor doesn&amp;#39;t handle this case too well. Is my best option going to be writing a custom extraction layer?&lt;/p&gt;\n\n&lt;p&gt;- Our MongoDB schemas are quite complicated, and my sense is that we&amp;#39;re best off doing at least a light transform before pulling the data into BigQuery. Does this sound like the right move? The alternative seems to be loading the MongoDB data in as-is and extracting the JSON fields into a new table using SQL queries. This seems like it could be pretty cumbersome depending on how nested and convoluted the schema is, but I&amp;#39;m curious what everyone&amp;#39;s experiences have been with this approach and if you would recommend it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178wdat", "is_robot_indexable": true, "report_reasons": null, "author": "aldtran", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178wdat/suggestions_for_moving_data_from_hundreds_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178wdat/suggestions_for_moving_data_from_hundreds_of/", "subreddit_subscribers": 134280, "created_utc": 1697425607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_27c3plee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Blog post: Handling physical deletes from the source and continue populating your analytical data store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_178uxpm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dC52RRrBJ_S6MKGGRlq4kE7z3rA8GMGiRyagAXac18M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697421045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datanrg.blogspot.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datanrg.blogspot.com/2023/10/handling-physical-deletes-from-source.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?auto=webp&amp;s=d6cfa0df8706b5bba3d0c7f31dcbfbc834637629", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1e97a6aaa7956f28d6af7ad778718b737b56312", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d50c0f2b8fccbb8d0177be48b57533c50d737b11", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a944ef0a5394f83abff96f5cf3a53f9328710e7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a712c9d1927af9f8a7bebe08a41b8375a3dd1722", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5c51044a06e719670399f0b4fcb617fcbe2d53fd", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/3u8Rf2b7HJJObmnBGZCubpT8DQlPT-ghiU9pf7NKEZk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31a10e58116da2161d7a6050c5b874f8a5f3ac8a", "width": 1080, "height": 567}], "variants": {}, "id": "lQWOHseGQ3wTp8bxdYa6Ra20gKdaE4YeaxA3VCunWy8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "178uxpm", "is_robot_indexable": true, "report_reasons": null, "author": "RayisImayev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178uxpm/blog_post_handling_physical_deletes_from_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datanrg.blogspot.com/2023/10/handling-physical-deletes-from-source.html", "subreddit_subscribers": 134280, "created_utc": 1697421045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I need help debugging or understanding why my Postgres instance hits throughput limit instead of IOPS limit. The instance is a single AZ m6g.2xlarge instance running on AWS RDS.\n\n  \nHere's the EBS configuration for the underlying EC2 instance (m6g.2xlarge):  \n- IOPS baseline: 12000, burst max: 20000  \n- Throughput baseline: 287.5 MBps, burst max: 593 MBps  \n\n\nHere's the Postgres RDS configuration:  \n- m6g.2xlarge  \n- IOPS: 12000  \n- Throughput: 500 MBps  \n- Storage: 4TB  \n\n\nHere's the details from the monitoring at 07:51:  \n- Read IOPS: 2335  \n- Write IOPS: 5310  \n- **Total IOPS: ~7700**\n- Read Throughput: 106 MBps  \n- Write Throughput: 399 MBps  \n- **Total Throughput: ~505 MBps**\n\n\n**QUESTION**: Postgres operates with an 8KB block size, therefore at 7700 IOPS, the total throughput should be (7700 * 8 = 61600 KBps) i.e. 61.7 MBps. How the hell is the throughput hitting 505 MBps, which is also the max configured throughput for the RDS instance? Does the underlying EBS also have a memory based cache?  \n- Even if the IOPS hits the max configured 12000 IOPS, the throughput shouldn't exceed (12000 * 8 = 96000 KBps) 96 MBps right?  \n- Even if the IOPS hits the max EC2 m6g.2x large max burst of 20000 IOPS, the throughput shouldn't exceed (20000 * 8 = 160000 KBps) 160 MBps right?", "author_fullname": "t2_l2e9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does my Postgres hit throughput limit instead of IOPS limit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17914ss", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697444715.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I need help debugging or understanding why my Postgres instance hits throughput limit instead of IOPS limit. The instance is a single AZ m6g.2xlarge instance running on AWS RDS.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the EBS configuration for the underlying EC2 instance (m6g.2xlarge):&lt;br/&gt;\n- IOPS baseline: 12000, burst max: 20000&lt;br/&gt;\n- Throughput baseline: 287.5 MBps, burst max: 593 MBps  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the Postgres RDS configuration:&lt;br/&gt;\n- m6g.2xlarge&lt;br/&gt;\n- IOPS: 12000&lt;br/&gt;\n- Throughput: 500 MBps&lt;br/&gt;\n- Storage: 4TB  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the details from the monitoring at 07:51:&lt;br/&gt;\n- Read IOPS: 2335&lt;br/&gt;\n- Write IOPS: 5310&lt;br/&gt;\n- &lt;strong&gt;Total IOPS: ~7700&lt;/strong&gt;\n- Read Throughput: 106 MBps&lt;br/&gt;\n- Write Throughput: 399 MBps&lt;br/&gt;\n- &lt;strong&gt;Total Throughput: ~505 MBps&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;QUESTION&lt;/strong&gt;: Postgres operates with an 8KB block size, therefore at 7700 IOPS, the total throughput should be (7700 * 8 = 61600 KBps) i.e. 61.7 MBps. How the hell is the throughput hitting 505 MBps, which is also the max configured throughput for the RDS instance? Does the underlying EBS also have a memory based cache?&lt;br/&gt;\n- Even if the IOPS hits the max configured 12000 IOPS, the throughput shouldn&amp;#39;t exceed (12000 * 8 = 96000 KBps) 96 MBps right?&lt;br/&gt;\n- Even if the IOPS hits the max EC2 m6g.2x large max burst of 20000 IOPS, the throughput shouldn&amp;#39;t exceed (20000 * 8 = 160000 KBps) 160 MBps right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17914ss", "is_robot_indexable": true, "report_reasons": null, "author": "nitred", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17914ss/why_does_my_postgres_hit_throughput_limit_instead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17914ss/why_does_my_postgres_hit_throughput_limit_instead/", "subreddit_subscribers": 134280, "created_utc": 1697444715.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently my team profiles yml look something like this:\n\n    profile:\n    #PRD ENV\n        staging_prod:\n          type: snowflake\n          database: database_intermediate\n          schema: schema_1\n        output_prod:\n          type: snowflake\n          database: database_output\n          schema: schema_1\n\nWhen we run our pipelines, we write most tables to database\\_intermediate.schema\\_1 and then we have 1 or 2 tables per data product writing some \"output\" tables to database\\_output.schema\\_1. The final \"product\" of the data product is just the tables in the output database and schema, every data consumer has access to that schema but not to the database\\_intermediate one, that one is just for my team for our intermediate tables. \n\nWhile I think this is a good practice to follow, I find it very \"hacky\" to get this to work elegantly in dbt. Models that are supposed to go to this output database use a combination of selectors, scripts in cicd yml and jinja if statement + specific config in the specific models to get to work. This is definitely not the way of doing this since dbt compilation will not be as seamless as it should be and it's a very \"manual\" process. \n\nI've been thinking of other ways of doing this, one is to remove all that I stated in the paragraph above and have a run-operations command in my dag that will create a zero clone copy of whatever table I want from the database\\_intermediate database to the database\\_output. Probably not the best practice but it should work well and very easy to do. Other way of doing it is maybe using a selector or a tag \"output\" in the model and in the dag I would select only that and just change the target to output\\_pord. \n\nwhat would be the best practice here? and what would be the easiest most efficient way of doing this? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to handle intermediate and output tables in different target profile in dbt snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17914ci", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697444656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my team profiles yml look something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profile:\n#PRD ENV\n    staging_prod:\n      type: snowflake\n      database: database_intermediate\n      schema: schema_1\n    output_prod:\n      type: snowflake\n      database: database_output\n      schema: schema_1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When we run our pipelines, we write most tables to database_intermediate.schema_1 and then we have 1 or 2 tables per data product writing some &amp;quot;output&amp;quot; tables to database_output.schema_1. The final &amp;quot;product&amp;quot; of the data product is just the tables in the output database and schema, every data consumer has access to that schema but not to the database_intermediate one, that one is just for my team for our intermediate tables. &lt;/p&gt;\n\n&lt;p&gt;While I think this is a good practice to follow, I find it very &amp;quot;hacky&amp;quot; to get this to work elegantly in dbt. Models that are supposed to go to this output database use a combination of selectors, scripts in cicd yml and jinja if statement + specific config in the specific models to get to work. This is definitely not the way of doing this since dbt compilation will not be as seamless as it should be and it&amp;#39;s a very &amp;quot;manual&amp;quot; process. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking of other ways of doing this, one is to remove all that I stated in the paragraph above and have a run-operations command in my dag that will create a zero clone copy of whatever table I want from the database_intermediate database to the database_output. Probably not the best practice but it should work well and very easy to do. Other way of doing it is maybe using a selector or a tag &amp;quot;output&amp;quot; in the model and in the dag I would select only that and just change the target to output_pord. &lt;/p&gt;\n\n&lt;p&gt;what would be the best practice here? and what would be the easiest most efficient way of doing this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17914ci", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17914ci/how_to_handle_intermediate_and_output_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17914ci/how_to_handle_intermediate_and_output_tables_in/", "subreddit_subscribers": 134280, "created_utc": 1697444656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently my team profiles yml look something like this:\n\n    profile:\n    #PRD ENV\n        staging_prod:\n          type: snowflake\n          database: database_intermediate\n          schema: schema_1\n        output_prod:\n          type: snowflake\n          database: database_output\n          schema: schema_1\n\nWhen we run our pipelines, we write most tables to database\\_intermediate.schema\\_1 and then we have 1 or 2 tables per data product writing some \"output\" tables to database\\_output.schema\\_1. The final \"product\" of the data product is just the tables in the output database and schema, every data consumer has access to that schema but not to the database\\_intermediate one, that one is just for my team for our intermediate tables. \n\nWhile I think this is a good practice to follow, I find it very \"hacky\" to get this to work elegantly in dbt. Models that are supposed to go to this output database use a combination of selectors, scripts in cicd yml and jinja if statement + specific config in the specific models to get to work. This is definitely not the way of doing this since dbt compilation will not be as seamless as it should be and it's a very \"manual\" process. \n\nI've been thinking of other ways of doing this, one is to remove all that I stated in the paragraph above and have a run-operations command in my dag that will create a zero clone copy of whatever table I want from the database\\_intermediate database to the database\\_output. Probably not the best practice but it should work well and very easy to do. Other way of doing it is maybe using a selector or a tag \"output\" in the model and in the dag I would select only that and just change the target to output\\_pord. \n\nwhat would be the best practice here? and what would be the easiest most efficient way of doing this? ", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to handle intermediate and output tables in different target profile in dbt snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17914bs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697444653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently my team profiles yml look something like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profile:\n#PRD ENV\n    staging_prod:\n      type: snowflake\n      database: database_intermediate\n      schema: schema_1\n    output_prod:\n      type: snowflake\n      database: database_output\n      schema: schema_1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When we run our pipelines, we write most tables to database_intermediate.schema_1 and then we have 1 or 2 tables per data product writing some &amp;quot;output&amp;quot; tables to database_output.schema_1. The final &amp;quot;product&amp;quot; of the data product is just the tables in the output database and schema, every data consumer has access to that schema but not to the database_intermediate one, that one is just for my team for our intermediate tables. &lt;/p&gt;\n\n&lt;p&gt;While I think this is a good practice to follow, I find it very &amp;quot;hacky&amp;quot; to get this to work elegantly in dbt. Models that are supposed to go to this output database use a combination of selectors, scripts in cicd yml and jinja if statement + specific config in the specific models to get to work. This is definitely not the way of doing this since dbt compilation will not be as seamless as it should be and it&amp;#39;s a very &amp;quot;manual&amp;quot; process. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking of other ways of doing this, one is to remove all that I stated in the paragraph above and have a run-operations command in my dag that will create a zero clone copy of whatever table I want from the database_intermediate database to the database_output. Probably not the best practice but it should work well and very easy to do. Other way of doing it is maybe using a selector or a tag &amp;quot;output&amp;quot; in the model and in the dag I would select only that and just change the target to output_pord. &lt;/p&gt;\n\n&lt;p&gt;what would be the best practice here? and what would be the easiest most efficient way of doing this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17914bs", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17914bs/how_to_handle_intermediate_and_output_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17914bs/how_to_handle_intermediate_and_output_tables_in/", "subreddit_subscribers": 134280, "created_utc": 1697444653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to setup dbt in my company for BigQuery and this is the approach I want to go for (as an analyst thrown into doing engineering). I tried Cloud Composer already and wow that's expensive plus we are looking for a serverless approach. The issue I am having now is every video and blog about this seems to think let's run everything at the same time and call it a day. I have some models to be run every hour and some daily. I understand that I can maybe have the multi repo approach but I would honestly prefer to maintain multiple python functions than multiple dbt repos. My idea is maybe have multiple functions on a schedule doing a dbt run with the project hosted somewhere but I am unsure of how to go about this or if anyone has done the same thing.", "author_fullname": "t2_jt4dxcgmi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt + Cloud Run but models run on different schedules?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178z24e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697435633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to setup dbt in my company for BigQuery and this is the approach I want to go for (as an analyst thrown into doing engineering). I tried Cloud Composer already and wow that&amp;#39;s expensive plus we are looking for a serverless approach. The issue I am having now is every video and blog about this seems to think let&amp;#39;s run everything at the same time and call it a day. I have some models to be run every hour and some daily. I understand that I can maybe have the multi repo approach but I would honestly prefer to maintain multiple python functions than multiple dbt repos. My idea is maybe have multiple functions on a schedule doing a dbt run with the project hosted somewhere but I am unsure of how to go about this or if anyone has done the same thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178z24e", "is_robot_indexable": true, "report_reasons": null, "author": "codeejen", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178z24e/dbt_cloud_run_but_models_run_on_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178z24e/dbt_cloud_run_but_models_run_on_different/", "subreddit_subscribers": 134280, "created_utc": 1697435633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do have a lack of experience, but I am curious. \n\nI never heard about the HCL Domino database, but I want to try it. Is it used for ETL automation?\n\nPlease let me know what your thought is. ", "author_fullname": "t2_l6g1m3u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your thought on HCL Domino?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178sv6a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697414613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do have a lack of experience, but I am curious. &lt;/p&gt;\n\n&lt;p&gt;I never heard about the HCL Domino database, but I want to try it. Is it used for ETL automation?&lt;/p&gt;\n\n&lt;p&gt;Please let me know what your thought is. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178sv6a", "is_robot_indexable": true, "report_reasons": null, "author": "JustRevolution9686", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178sv6a/what_is_your_thought_on_hcl_domino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178sv6a/what_is_your_thought_on_hcl_domino/", "subreddit_subscribers": 134280, "created_utc": 1697414613.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}