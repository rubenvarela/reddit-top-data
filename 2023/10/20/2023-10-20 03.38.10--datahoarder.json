{"kind": "Listing", "data": {"after": "t3_17bmb0s", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8tsg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft\u2019s futuristic \u2018Project Silica\u2019 stores data on glass plates for 10,000 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17biw3w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 189, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 189, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OTMu4DVYDzt7IOU_NTd_xPOgga0OVw0Td99FjkuYGho.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697721376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pcworld.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.pcworld.com/article/2108839/microsoft-project-small-glass-pane-stores-terabytes-of-data.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?auto=webp&amp;s=2d30144a695c77b4ce4b679c90e63ac9e2726168", "width": 1024, "height": 575}, "resolutions": [{"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e5c31ec8cbd304fab589f130f0207770bb92001", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cff8cecf4d7b324cb385da84782e4903626cc80", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac7aa8aa10743aa42a7264d6fb04b4df880e9160", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4e36b88e57979e429695bc6aac29e60cceeaf4e", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b4b1c8f211deb5b9c41e597cdfce64617cc6645", "width": 960, "height": 539}], "variants": {}, "id": "0JwP-8C3waN7Mn9jdKH4cpA3HlvlZb28KHYw6Y0TpVA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17biw3w", "is_robot_indexable": true, "report_reasons": null, "author": "Agathocles_of_Sicily", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17biw3w/microsofts_futuristic_project_silica_stores_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.pcworld.com/article/2108839/microsoft-project-small-glass-pane-stores-terabytes-of-data.html", "subreddit_subscribers": 707591, "created_utc": 1697721376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The X24 is being released", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate unleases Hard Drives with 24TB Storage Capacity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17bl6ux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 134, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 134, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DMJgkcAcw7f0iFGlNk5XI0VnNlE27dv1zZ6P2pVna4Y.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697727604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "guru3d.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The X24 is being released&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.guru3d.com/story/seagate-unleases-hard-drives-with-24tb-storage-capacity/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?auto=webp&amp;s=21d1e93c5461462e4126153f98136c6196bc4358", "width": 640, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aea5988028851f23fba764d364c2b27e95a83eb3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4219bede6df7277cc5b0523b95df08a0a6711622", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fbc49559406bf3d3a0d288b8ff603ba92970cd6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c74cd02526ccef47a23a48ee95a39dc22854f933", "width": 640, "height": 640}], "variants": {}, "id": "toqLiPggpSPq6boCyjgLxiAWP7ECNO785V__2oSwtkg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bl6ux", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17bl6ux/seagate_unleases_hard_drives_with_24tb_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.guru3d.com/story/seagate-unleases-hard-drives-with-24tb-storage-capacity/", "subreddit_subscribers": 707591, "created_utc": 1697727604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4aynv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "British Museum sets out plans to digitise fully the collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bpro3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697739630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "britishmuseum.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.britishmuseum.org/sites/default/files/2023-10/British_Museum_sets_out_plans_to_digitise_fully_the_collection.pdf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bpro3", "is_robot_indexable": true, "report_reasons": null, "author": "retrac1324", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bpro3/british_museum_sets_out_plans_to_digitise_fully/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.britishmuseum.org/sites/default/files/2023-10/British_Museum_sets_out_plans_to_digitise_fully_the_collection.pdf", "subreddit_subscribers": 707591, "created_utc": 1697739630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys, I always used Taiyo Yuden discs for DVD, if I remember correctly they were mostly released by JVC and maybe Verbatim.\n\nIn DVD people always said that is the best quality for the price you can buy. Is there an alternative like that for BDs?", "author_fullname": "t2_2xftem1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there Taiyo Yuden alternative for BD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bdkcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697701054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I always used Taiyo Yuden discs for DVD, if I remember correctly they were mostly released by JVC and maybe Verbatim.&lt;/p&gt;\n\n&lt;p&gt;In DVD people always said that is the best quality for the price you can buy. Is there an alternative like that for BDs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bdkcl", "is_robot_indexable": true, "report_reasons": null, "author": "Leonhardt90", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bdkcl/is_there_taiyo_yuden_alternative_for_bd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bdkcl/is_there_taiyo_yuden_alternative_for_bd/", "subreddit_subscribers": 707591, "created_utc": 1697701054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi People \ud83d\udc4b \n\nI'm setting up a unraid server. To save some money I purchased 2x6tb SAS drives and 2x 4tb SATA drives.\n\nUnfortunately my SAS drives are extremely slow. Read speed is around 7MB/s.\n\nAnyone seen this? Are the disks just bad?\n\nMy HBA is https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;mkevt=1&amp;mkrid=705-154756-20017-0&amp;ssspo=OwHu05S9RzG&amp;sssrc=4429486&amp;ssuid=4SgCEM6tSOK&amp;var=&amp;widget_ver=artemis&amp;media=COPY\n\nHere is the HDD pro results https://ibb.co/GPLtYR9\n\nCrystal Disk https://ibb.co/7WRYywq\n\nAnd some basic info https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt", "author_fullname": "t2_ezbuay12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extremely slow SAS drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bd6vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697699522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi People \ud83d\udc4b &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m setting up a unraid server. To save some money I purchased 2x6tb SAS drives and 2x 4tb SATA drives.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately my SAS drives are extremely slow. Read speed is around 7MB/s.&lt;/p&gt;\n\n&lt;p&gt;Anyone seen this? Are the disks just bad?&lt;/p&gt;\n\n&lt;p&gt;My HBA is &lt;a href=\"https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=705-154756-20017-0&amp;amp;ssspo=OwHu05S9RzG&amp;amp;sssrc=4429486&amp;amp;ssuid=4SgCEM6tSOK&amp;amp;var=&amp;amp;widget_ver=artemis&amp;amp;media=COPY\"&gt;https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=705-154756-20017-0&amp;amp;ssspo=OwHu05S9RzG&amp;amp;sssrc=4429486&amp;amp;ssuid=4SgCEM6tSOK&amp;amp;var=&amp;amp;widget_ver=artemis&amp;amp;media=COPY&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is the HDD pro results &lt;a href=\"https://ibb.co/GPLtYR9\"&gt;https://ibb.co/GPLtYR9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Crystal Disk &lt;a href=\"https://ibb.co/7WRYywq\"&gt;https://ibb.co/7WRYywq&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And some basic info &lt;a href=\"https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt\"&gt;https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?auto=webp&amp;s=885e761d5e665b5195a9ed59b18d7d53847d6798", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c15328787186a4041f321b27f72ffc66916bbe13", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a6a12ec3c380e81a0c4bf78f9ec6aee48194830", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c04cb666c0f2ef6bfdc2c35e4501b09be02e1eee", "width": 320, "height": 240}], "variants": {}, "id": "wwPpmkcxUy-NVu2lDnzIkiOl8Q94uUFgYC2en_zVmZs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bd6vh", "is_robot_indexable": true, "report_reasons": null, "author": "ClownWorld11", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bd6vh/extremely_slow_sas_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bd6vh/extremely_slow_sas_drives/", "subreddit_subscribers": 707591, "created_utc": 1697699522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just recently 2 HDDs I had(1 4TB WD Elements and 1 3TB Seagate) stopped working suddenly, and since I can't find a way to mount them on my PCs anymore, I had to consider their data fully lost. This made me realize the importance of having data backed up in at least one additional HDD, since years of my life were stolen from this event, and now I'm trying to back up a set of HDDs I have. My problem is that Copy/Pasting from Windows seems like an slow process, and also one that requires having my PC on, which limits the amount of data I can backup per day(I only keep my PC on 5hr per day due high energy consumption).\n\nMy question is: What's the most efficient(Or fastest) way to backup, TBs in size, HDDs? As a side note, while I can't have my PC on that long, I've a RPi 4 that is on 24/7, so if there is any software for it or a device that I can connect to it and do the trick, would be better.", "author_fullname": "t2_13y5jf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most efficient/fastest way to clone/backup data from HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bpzww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697740215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently 2 HDDs I had(1 4TB WD Elements and 1 3TB Seagate) stopped working suddenly, and since I can&amp;#39;t find a way to mount them on my PCs anymore, I had to consider their data fully lost. This made me realize the importance of having data backed up in at least one additional HDD, since years of my life were stolen from this event, and now I&amp;#39;m trying to back up a set of HDDs I have. My problem is that Copy/Pasting from Windows seems like an slow process, and also one that requires having my PC on, which limits the amount of data I can backup per day(I only keep my PC on 5hr per day due high energy consumption).&lt;/p&gt;\n\n&lt;p&gt;My question is: What&amp;#39;s the most efficient(Or fastest) way to backup, TBs in size, HDDs? As a side note, while I can&amp;#39;t have my PC on that long, I&amp;#39;ve a RPi 4 that is on 24/7, so if there is any software for it or a device that I can connect to it and do the trick, would be better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bpzww", "is_robot_indexable": true, "report_reasons": null, "author": "NoirSkell", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bpzww/most_efficientfastest_way_to_clonebackup_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bpzww/most_efficientfastest_way_to_clonebackup_data/", "subreddit_subscribers": 707591, "created_utc": 1697740215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw plenty of people suggesting serverpartdeal, goharddrive, or jb-computer or geizhals or, for new stuff, alternate, mindfactory or of course Amazon.\nThe problem is the first two are American, this mean that beside shipping, there is customs to factor in. Most often than not you'd have to add something like 20% more of VAT, depending on the country, making it useless. At least to me, would be just better off spending a bit more and having it new and with easier warranty to claim off Amazon or something.\nThe others are mainly German websites, that either have new stuff that cost just the same as in other countries, making it pointless, or they don't ship to other countries. In my case is Italy.\n\nWhere can someone find good deal in Europe then?", "author_fullname": "t2_k99xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to buy certified refurbished drives in Europe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bltai", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697729216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw plenty of people suggesting serverpartdeal, goharddrive, or jb-computer or geizhals or, for new stuff, alternate, mindfactory or of course Amazon.\nThe problem is the first two are American, this mean that beside shipping, there is customs to factor in. Most often than not you&amp;#39;d have to add something like 20% more of VAT, depending on the country, making it useless. At least to me, would be just better off spending a bit more and having it new and with easier warranty to claim off Amazon or something.\nThe others are mainly German websites, that either have new stuff that cost just the same as in other countries, making it pointless, or they don&amp;#39;t ship to other countries. In my case is Italy.&lt;/p&gt;\n\n&lt;p&gt;Where can someone find good deal in Europe then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bltai", "is_robot_indexable": true, "report_reasons": null, "author": "NaXter24R", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bltai/where_to_buy_certified_refurbished_drives_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bltai/where_to_buy_certified_refurbished_drives_in/", "subreddit_subscribers": 707591, "created_utc": 1697729216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As the title says, bdfr keeps skipping reddit hosted videos. Is there a parameter I'm missing to allow this, or at least a parameter that can log whatever I miss so I can at least do it myself?", "author_fullname": "t2_npr63", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BDFR skipping Reddit hosted videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bdjef", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697700947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, bdfr keeps skipping reddit hosted videos. Is there a parameter I&amp;#39;m missing to allow this, or at least a parameter that can log whatever I miss so I can at least do it myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bdjef", "is_robot_indexable": true, "report_reasons": null, "author": "XionXionHolix", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bdjef/bdfr_skipping_reddit_hosted_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bdjef/bdfr_skipping_reddit_hosted_videos/", "subreddit_subscribers": 707591, "created_utc": 1697700947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to build my library of audiobooks.  I found a free site that lets me play them on the site but I can't figure out how to download the files so I can play in a player (on my phone the site stops playing when I close browser and other times).  The site is tokybook.com\n\nCan anyone recommend a program or something that will get these audio files?  In bulk would be great but even one by one at least I'd have something to keep me busy.  I looked at the page source on the player but can't seem to find the full path to the media (I did find the file names with .mp3 extension) its in java so it's a bit foreign to me.", "author_fullname": "t2_fbbrfkqd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to scrape this site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bulnk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697751934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to build my library of audiobooks.  I found a free site that lets me play them on the site but I can&amp;#39;t figure out how to download the files so I can play in a player (on my phone the site stops playing when I close browser and other times).  The site is tokybook.com&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a program or something that will get these audio files?  In bulk would be great but even one by one at least I&amp;#39;d have something to keep me busy.  I looked at the page source on the player but can&amp;#39;t seem to find the full path to the media (I did find the file names with .mp3 extension) its in java so it&amp;#39;s a bit foreign to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bulnk", "is_robot_indexable": true, "report_reasons": null, "author": "CryGeneral9999", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bulnk/best_way_to_scrape_this_site/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bulnk/best_way_to_scrape_this_site/", "subreddit_subscribers": 707591, "created_utc": 1697751934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "They say it costs $.006 per GB, so 200GB would be $1.20/month?   Is there a minimum?\n\nDo they have restrictions on what gets uploaded?\n\nAre my uploads private?  Can they see what is on their servers or just me?\n\nThanks.", "author_fullname": "t2_t44bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BackBlaze B2 questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17brqrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697744717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They say it costs $.006 per GB, so 200GB would be $1.20/month?   Is there a minimum?&lt;/p&gt;\n\n&lt;p&gt;Do they have restrictions on what gets uploaded?&lt;/p&gt;\n\n&lt;p&gt;Are my uploads private?  Can they see what is on their servers or just me?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17brqrp", "is_robot_indexable": true, "report_reasons": null, "author": "818sfv", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17brqrp/backblaze_b2_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17brqrp/backblaze_b2_questions/", "subreddit_subscribers": 707591, "created_utc": 1697744717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For cold storage, I got a pack of 50 Blu-rays, all rewritable... but when I tried again to find more, all sellers only had BD-R. One even said it's getting harder to find any BD-RE (rewritable discs). Is this true? How hard it is to find these everywhere?\n\nAlso, can I expect some issues, even if I store all of them correctly, compared to BD-R? I don't like the idea of having that ammount of data in a way that can't be modified a single bit.\n\nEDIT: Thoughts on BD-RE: [https://www.iljitsch.com/2022/01-09-longevity-of-recordable-blu-ray-discs-bd-r-bd-re.html](https://www.iljitsch.com/2022/01-09-longevity-of-recordable-blu-ray-discs-bd-r-bd-re.html)\n\nThis link says they last more:\n\n[https://www.canada.ca/en/conservation-institute/services/conservation-preservation-publications/canadian-conservation-institute-notes/longevity-recordable-cds-dvds.html](https://www.canada.ca/en/conservation-institute/services/conservation-preservation-publications/canadian-conservation-institute-notes/longevity-recordable-cds-dvds.html)", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't find BD-RE, and even if I do, should I opt for this instead of BD-R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmsfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697772707.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697731816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For cold storage, I got a pack of 50 Blu-rays, all rewritable... but when I tried again to find more, all sellers only had BD-R. One even said it&amp;#39;s getting harder to find any BD-RE (rewritable discs). Is this true? How hard it is to find these everywhere?&lt;/p&gt;\n\n&lt;p&gt;Also, can I expect some issues, even if I store all of them correctly, compared to BD-R? I don&amp;#39;t like the idea of having that ammount of data in a way that can&amp;#39;t be modified a single bit.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thoughts on BD-RE: &lt;a href=\"https://www.iljitsch.com/2022/01-09-longevity-of-recordable-blu-ray-discs-bd-r-bd-re.html\"&gt;https://www.iljitsch.com/2022/01-09-longevity-of-recordable-blu-ray-discs-bd-r-bd-re.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This link says they last more:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.canada.ca/en/conservation-institute/services/conservation-preservation-publications/canadian-conservation-institute-notes/longevity-recordable-cds-dvds.html\"&gt;https://www.canada.ca/en/conservation-institute/services/conservation-preservation-publications/canadian-conservation-institute-notes/longevity-recordable-cds-dvds.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bmsfx", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bmsfx/cant_find_bdre_and_even_if_i_do_should_i_opt_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bmsfx/cant_find_bdre_and_even_if_i_do_should_i_opt_for/", "subreddit_subscribers": 707591, "created_utc": 1697731816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As I am approaching my first 1TB of data hoarding, I thought I would look for some input from the professionals. I've considered doing it just to make moving a large number of files faster.\n\nDo you compress your folders at all?\n\nWhen do you find it is necessary to start zipping up folders?\n\nIf so, what compression format do you use? As I have been using Zip, but I have been reading 7z is a bit smaller.\n\nDo you find it even worth doing?\n\nScrew it buy another hard drive?\n\nThanks!", "author_fullname": "t2_lkpsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compressed Folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bhvk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697718203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As I am approaching my first 1TB of data hoarding, I thought I would look for some input from the professionals. I&amp;#39;ve considered doing it just to make moving a large number of files faster.&lt;/p&gt;\n\n&lt;p&gt;Do you compress your folders at all?&lt;/p&gt;\n\n&lt;p&gt;When do you find it is necessary to start zipping up folders?&lt;/p&gt;\n\n&lt;p&gt;If so, what compression format do you use? As I have been using Zip, but I have been reading 7z is a bit smaller.&lt;/p&gt;\n\n&lt;p&gt;Do you find it even worth doing?&lt;/p&gt;\n\n&lt;p&gt;Screw it buy another hard drive?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17bhvk8", "is_robot_indexable": true, "report_reasons": null, "author": "Skulleddino", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bhvk8/compressed_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bhvk8/compressed_folders/", "subreddit_subscribers": 707591, "created_utc": 1697718203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, I'm in a bit of a pickle.  I currently have two 16TB HDD mirrored in TrueNAS and am almost at capacity.  I recently picked up two more 16TB but I'm not sure what RAID configuration I should use and how to set it up without losing data from my existing mirror.  \n\nI do also have a separate machine with a 16TB I use for backup, so i can copy everything over again if I had to lose my data on the mirror, just would prefer not to have to copy 16TB of data over my network.  \n\nAnybody have any good suggestions, can I expand my current pool with the new drives?  I assume I can't really since a mirror is usually two drives, it seems like I will have to create a new RAID and copy everything over, but I would love some opinions.  Thanks!", "author_fullname": "t2_b3tdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TrueNAS raid configuration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bu2ao", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697750592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I&amp;#39;m in a bit of a pickle.  I currently have two 16TB HDD mirrored in TrueNAS and am almost at capacity.  I recently picked up two more 16TB but I&amp;#39;m not sure what RAID configuration I should use and how to set it up without losing data from my existing mirror.  &lt;/p&gt;\n\n&lt;p&gt;I do also have a separate machine with a 16TB I use for backup, so i can copy everything over again if I had to lose my data on the mirror, just would prefer not to have to copy 16TB of data over my network.  &lt;/p&gt;\n\n&lt;p&gt;Anybody have any good suggestions, can I expand my current pool with the new drives?  I assume I can&amp;#39;t really since a mirror is usually two drives, it seems like I will have to create a new RAID and copy everything over, but I would love some opinions.  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bu2ao", "is_robot_indexable": true, "report_reasons": null, "author": "slash65", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bu2ao/truenas_raid_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bu2ao/truenas_raid_configuration/", "subreddit_subscribers": 707591, "created_utc": 1697750592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI am currently hosting a bunch of things on my Gigabyte brix  (jellyfin, vaultwarden, pihole, openHAB, paperless, Hyperion, \u2026)\n\nAdditionally in am storing all my pictures there.\n\nEverything is stored on an attached 4TB external HDD.\n\nNow as I start to get worried about data loss and I am missing a bit of power (to host Immich for example) I am considering updating my system to an Intel NUC or similar,\n\nHowever as HDD are pretty expensive I am unsure how to proceed with the storage. My first thought was to run everything as raidz2 but I quickly noticed that storage space is super expensive and 4x18TB HDD would set me back easily 1000\u20ac. As far as I understand I can\u2019t have a raidz and use different HDD sizes and can also not really add HDD on the go (as long as they have different sizes)\n\nNow I was thinking to have a solution that is scalable and buy additional storage when needed, so currently my idea is to run a raidz1 with 3x8tb (or 3x6tb) for the very important data like the paperless data and the pictures and additionally just start with one 18TB standalone HDD for the media for jellyfin and add more when needed.\n\nDoes anyone have any other smart ideas how to organize this? It would be great to have some kind of data protection for the media too, but I feel in doubt this is the stuff I could afford to loose cause I can reobtain it\n\nHow are you guys organizing your drives?\n\nEdit: to add and be clear, I am fully aware that a NAS is not a backup solution. I already have a 3-2-1 backup solution in place for the most important stuff, I just want even more reliability (and capacity) on the internal ones withouth breaking the bank", "author_fullname": "t2_61dr7f6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD setup for NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17brtch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697746566.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697744901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am currently hosting a bunch of things on my Gigabyte brix  (jellyfin, vaultwarden, pihole, openHAB, paperless, Hyperion, \u2026)&lt;/p&gt;\n\n&lt;p&gt;Additionally in am storing all my pictures there.&lt;/p&gt;\n\n&lt;p&gt;Everything is stored on an attached 4TB external HDD.&lt;/p&gt;\n\n&lt;p&gt;Now as I start to get worried about data loss and I am missing a bit of power (to host Immich for example) I am considering updating my system to an Intel NUC or similar,&lt;/p&gt;\n\n&lt;p&gt;However as HDD are pretty expensive I am unsure how to proceed with the storage. My first thought was to run everything as raidz2 but I quickly noticed that storage space is super expensive and 4x18TB HDD would set me back easily 1000\u20ac. As far as I understand I can\u2019t have a raidz and use different HDD sizes and can also not really add HDD on the go (as long as they have different sizes)&lt;/p&gt;\n\n&lt;p&gt;Now I was thinking to have a solution that is scalable and buy additional storage when needed, so currently my idea is to run a raidz1 with 3x8tb (or 3x6tb) for the very important data like the paperless data and the pictures and additionally just start with one 18TB standalone HDD for the media for jellyfin and add more when needed.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other smart ideas how to organize this? It would be great to have some kind of data protection for the media too, but I feel in doubt this is the stuff I could afford to loose cause I can reobtain it&lt;/p&gt;\n\n&lt;p&gt;How are you guys organizing your drives?&lt;/p&gt;\n\n&lt;p&gt;Edit: to add and be clear, I am fully aware that a NAS is not a backup solution. I already have a 3-2-1 backup solution in place for the most important stuff, I just want even more reliability (and capacity) on the internal ones withouth breaking the bank&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17brtch", "is_robot_indexable": true, "report_reasons": null, "author": "Narrow_Smoke", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17brtch/hdd_setup_for_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17brtch/hdd_setup_for_nas/", "subreddit_subscribers": 707591, "created_utc": 1697744901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nThis is my own program, I released this update to provide a couple of fixes.\n\n* Resolved an issue where files hashed directly  inside a volume root were stored with the drive letter, so the .lfhash file contained their full file path. This means that the files could not be verified if moved; or if copied, the old, still-existing version would be verified rather than the new one.\n* Disk type detection is now functional for disks containing multiple data volumes.\n* Resolved NaN progress reported when hashing empty files.\n\nAnyone who is currently using the program will see the update notification when they next run it.\n\nThe purpose of this program is to store checksums next to your files. This is most useful with backups. You can right-click files, folders, or volumes, and generate checksums for them. You can double-click the resulting checksum file to verify.\n\nOfficial website and Download: [https://www.liamfoot.com/lfh](https://www.liamfoot.com/lfh)\n\nDirect download link: [https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0](https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0)", "author_fullname": "t2_h62v4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LiamFootHash v1.3 - A functional and very fast checksumming program for Windows.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bor8y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697737033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;This is my own program, I released this update to provide a couple of fixes.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Resolved an issue where files hashed directly  inside a volume root were stored with the drive letter, so the .lfhash file contained their full file path. This means that the files could not be verified if moved; or if copied, the old, still-existing version would be verified rather than the new one.&lt;/li&gt;\n&lt;li&gt;Disk type detection is now functional for disks containing multiple data volumes.&lt;/li&gt;\n&lt;li&gt;Resolved NaN progress reported when hashing empty files.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone who is currently using the program will see the update notification when they next run it.&lt;/p&gt;\n\n&lt;p&gt;The purpose of this program is to store checksums next to your files. This is most useful with backups. You can right-click files, folders, or volumes, and generate checksums for them. You can double-click the resulting checksum file to verify.&lt;/p&gt;\n\n&lt;p&gt;Official website and Download: &lt;a href=\"https://www.liamfoot.com/lfh\"&gt;https://www.liamfoot.com/lfh&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Direct download link: &lt;a href=\"https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0\"&gt;https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bor8y", "is_robot_indexable": true, "report_reasons": null, "author": "Liam2349", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bor8y/liamfoothash_v13_a_functional_and_very_fast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bor8y/liamfoothash_v13_a_functional_and_very_fast/", "subreddit_subscribers": 707591, "created_utc": 1697737033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have around 200 to 300 GB of photos with multiple users. Most of data i have on 2 different pc and a decent copy on gdrive.\n\nOther problems are that users are not that tech savvy and they upload some data on gdrive some on pc, some on 15year old external drive and it became a mess. Plus i almost run out of space on gdrive.\n\nGoal is to have 2 full copies of data.\n\nBudget 2bay nas would be ideal for my need but i dont need it running 24/7, i would need it maybe once a month to drop photos and leave it there.\n\nIs this a good solution?\nBuy 2 external hard drives same size. Let users upload data to one drive and have a second drive as a copy. End of each month, run a file sync between two drives.\n\nBetter solutions are welcomed aswell.", "author_fullname": "t2_4yatpgfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup solution for my needs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bm57l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697730104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have around 200 to 300 GB of photos with multiple users. Most of data i have on 2 different pc and a decent copy on gdrive.&lt;/p&gt;\n\n&lt;p&gt;Other problems are that users are not that tech savvy and they upload some data on gdrive some on pc, some on 15year old external drive and it became a mess. Plus i almost run out of space on gdrive.&lt;/p&gt;\n\n&lt;p&gt;Goal is to have 2 full copies of data.&lt;/p&gt;\n\n&lt;p&gt;Budget 2bay nas would be ideal for my need but i dont need it running 24/7, i would need it maybe once a month to drop photos and leave it there.&lt;/p&gt;\n\n&lt;p&gt;Is this a good solution?\nBuy 2 external hard drives same size. Let users upload data to one drive and have a second drive as a copy. End of each month, run a file sync between two drives.&lt;/p&gt;\n\n&lt;p&gt;Better solutions are welcomed aswell.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bm57l", "is_robot_indexable": true, "report_reasons": null, "author": "lagerixx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bm57l/backup_solution_for_my_needs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bm57l/backup_solution_for_my_needs/", "subreddit_subscribers": 707591, "created_utc": 1697730104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Includes google maps and matterport.", "author_fullname": "t2_dh4hjpj4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you download and view 3d Virtual tours offline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf8fi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697708303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Includes google maps and matterport.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bf8fi", "is_robot_indexable": true, "report_reasons": null, "author": "__Acedia_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bf8fi/how_do_you_download_and_view_3d_virtual_tours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bf8fi/how_do_you_download_and_view_3d_virtual_tours/", "subreddit_subscribers": 707591, "created_utc": 1697708303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been thinking of keeping an archive of tweets or at least the images/media from my account but don\u2019t know what to use? Had an old Chrome extension that did it for media but apparently that is defunct now. What would be the recommended programs or extension for that now?", "author_fullname": "t2_s7xsyapd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to downloading posts on Twitter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b96b5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697684988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been thinking of keeping an archive of tweets or at least the images/media from my account but don\u2019t know what to use? Had an old Chrome extension that did it for media but apparently that is defunct now. What would be the recommended programs or extension for that now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b96b5", "is_robot_indexable": true, "report_reasons": null, "author": "ElysiumRailgun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b96b5/alternatives_to_downloading_posts_on_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b96b5/alternatives_to_downloading_posts_on_twitter/", "subreddit_subscribers": 707591, "created_utc": 1697684988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just installed a 9211-8i (Dell H310) in IT mode.\n\n It has tested fine however when shutting down Windows the one drive I had hooked up to it made the loud spin down whine my Exos drives make when performing a dreaded hard shutdown.\n\nIs there a spindown before shutdown setting I am missing for for drives attached via LSI HBA?", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSI HBA hard shutting down drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17btp07", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697756956.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697749685.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed a 9211-8i (Dell H310) in IT mode.&lt;/p&gt;\n\n&lt;p&gt;It has tested fine however when shutting down Windows the one drive I had hooked up to it made the loud spin down whine my Exos drives make when performing a dreaded hard shutdown.&lt;/p&gt;\n\n&lt;p&gt;Is there a spindown before shutdown setting I am missing for for drives attached via LSI HBA?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17btp07", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17btp07/lsi_hba_hard_shutting_down_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17btp07/lsi_hba_hard_shutting_down_drives/", "subreddit_subscribers": 707591, "created_utc": 1697749685.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "doesnt wanna load profiles and so laggy ", "author_fullname": "t2_86tkmqsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "anyone knows what happened to TT sver ? its not working for me anymore .. and apparently they lowered the max download from 1000 to 100", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bsdsq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697746335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;doesnt wanna load profiles and so laggy &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bsdsq", "is_robot_indexable": true, "report_reasons": null, "author": "Acceptable42", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bsdsq/anyone_knows_what_happened_to_tt_sver_its_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bsdsq/anyone_knows_what_happened_to_tt_sver_its_not/", "subreddit_subscribers": 707591, "created_utc": 1697746335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey,\n\nForgive me if it's been answered already, didn't manage to find such topics unfortunately.\n\nWhat do you think would be the best backup strategy to backup let's say 1.1TB of data with 2TB hard drive (so less than 2x space)? I'm looking to minimize time spent for backup creation and space usage.\n\nI was using incremental backups before, the problem is that during the consolidation, there's a moment when you need 2x the space for the backup copy and the drive is becoming too small for that.\n\nShould I just delete old backup and do only full backups all the time?\n\nOr continue with incremental backups and when drive is full delete everything and start a new chain?\n\nMaybe some other ideas? Ideally I'd like to automate it, instead of manually watching how much space is left on the drive.\n\nThanks :)", "author_fullname": "t2_1qkh42fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup policy with small backup drive space (Veeam)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bjgpl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697722985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;Forgive me if it&amp;#39;s been answered already, didn&amp;#39;t manage to find such topics unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What do you think would be the best backup strategy to backup let&amp;#39;s say 1.1TB of data with 2TB hard drive (so less than 2x space)? I&amp;#39;m looking to minimize time spent for backup creation and space usage.&lt;/p&gt;\n\n&lt;p&gt;I was using incremental backups before, the problem is that during the consolidation, there&amp;#39;s a moment when you need 2x the space for the backup copy and the drive is becoming too small for that.&lt;/p&gt;\n\n&lt;p&gt;Should I just delete old backup and do only full backups all the time?&lt;/p&gt;\n\n&lt;p&gt;Or continue with incremental backups and when drive is full delete everything and start a new chain?&lt;/p&gt;\n\n&lt;p&gt;Maybe some other ideas? Ideally I&amp;#39;d like to automate it, instead of manually watching how much space is left on the drive.&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bjgpl", "is_robot_indexable": true, "report_reasons": null, "author": "Pegietix", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bjgpl/backup_policy_with_small_backup_drive_space_veeam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bjgpl/backup_policy_with_small_backup_drive_space_veeam/", "subreddit_subscribers": 707591, "created_utc": 1697722985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, i\u2019ve got a smaller collection of around 16tb of data across 2x8tb drives &amp; a 3tb drive. \n\nAll external drives connected via USB to an old macbook. it\u2019s a bit jallopy but i\u2019ve been using it as a setup for some of my work &amp; my partners photography &amp; videography for about 4 years. \n\nI\u2019ve got backblaze as my \u201coh cr*p\u201d i\u2019ve had a drive failure recovery solution. \n\nI was wondering is there a compression method to save some space for files which are truly not in need of retrieval often?", "author_fullname": "t2_n0af3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Part time data hoarder questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bsg3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697746507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, i\u2019ve got a smaller collection of around 16tb of data across 2x8tb drives &amp;amp; a 3tb drive. &lt;/p&gt;\n\n&lt;p&gt;All external drives connected via USB to an old macbook. it\u2019s a bit jallopy but i\u2019ve been using it as a setup for some of my work &amp;amp; my partners photography &amp;amp; videography for about 4 years. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got backblaze as my \u201coh cr*p\u201d i\u2019ve had a drive failure recovery solution. &lt;/p&gt;\n\n&lt;p&gt;I was wondering is there a compression method to save some space for files which are truly not in need of retrieval often?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bsg3q", "is_robot_indexable": true, "report_reasons": null, "author": "madragonn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bsg3q/part_time_data_hoarder_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bsg3q/part_time_data_hoarder_questions/", "subreddit_subscribers": 707591, "created_utc": 1697746507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "doesnt wanna load profiles and so laggy ", "author_fullname": "t2_86tkmqsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "anyone knows what happened to TT sver ? its not working for me anymore .. and apparently they lowered the max download from 1000 to 100", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bsdsx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697746336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;doesnt wanna load profiles and so laggy &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bsdsx", "is_robot_indexable": true, "report_reasons": null, "author": "Acceptable42", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bsdsx/anyone_knows_what_happened_to_tt_sver_its_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bsdsx/anyone_knows_what_happened_to_tt_sver_its_not/", "subreddit_subscribers": 707591, "created_utc": 1697746336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, as I said I just bought 4x20TB HDD to make a new RAID 5 array and I am wondering if I should write data unevenly on 3 of them before doing the RAID 5 to use a little bit of their lifespan so they will not die at the same time or while rebuilding the RAID 5.The data I would write isn't important, it's only to reduce their lifespan.\n\nOn the first I write nothing.On the second I write 40TB.On the third I write 80TB.And on the forth I write 120TB.And only then I erase the data I just wrote and I make the RAID 5.\n\nIt's not very sensitive data, but if I can save me from a complete data loss of this RAID 5 it's cool.btw it's 4 Seagate Exos X20 of 20TB. May not be the best but it's cheap.\n\nIs it meaningless to do so or can it increases my chances of a clean rebuild if one of them fail?", "author_fullname": "t2_c94mw03d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I bought 4x20TB HDD for my RAID 5. Will they die at the same time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bss87", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697768894.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697747380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, as I said I just bought 4x20TB HDD to make a new RAID 5 array and I am wondering if I should write data unevenly on 3 of them before doing the RAID 5 to use a little bit of their lifespan so they will not die at the same time or while rebuilding the RAID 5.The data I would write isn&amp;#39;t important, it&amp;#39;s only to reduce their lifespan.&lt;/p&gt;\n\n&lt;p&gt;On the first I write nothing.On the second I write 40TB.On the third I write 80TB.And on the forth I write 120TB.And only then I erase the data I just wrote and I make the RAID 5.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not very sensitive data, but if I can save me from a complete data loss of this RAID 5 it&amp;#39;s cool.btw it&amp;#39;s 4 Seagate Exos X20 of 20TB. May not be the best but it&amp;#39;s cheap.&lt;/p&gt;\n\n&lt;p&gt;Is it meaningless to do so or can it increases my chances of a clean rebuild if one of them fail?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bss87", "is_robot_indexable": true, "report_reasons": null, "author": "RAYTEKSO", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bss87/i_bought_4x20tb_hdd_for_my_raid_5_will_they_die/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bss87/i_bought_4x20tb_hdd_for_my_raid_5_will_they_die/", "subreddit_subscribers": 707591, "created_utc": 1697747380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI run a couple of unraid servers and am running low on space on one of them so I've started having a look for another 12TB drive. I usually buy external a shuck them. I'm shocked at the prices of them\n\n3 years ago I bought 2 12TB WD Elements for \u00a3166 pounds each on amazon. Now the same is \u00a3214 used. I know there was a spike in prices during covid but it's crazy that a drive could be purchased 3 years ago much cheaper. Why have prices risen so much again? What have I missed?", "author_fullname": "t2_ecjd325v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard Drives still so expensive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmb0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697730546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I run a couple of unraid servers and am running low on space on one of them so I&amp;#39;ve started having a look for another 12TB drive. I usually buy external a shuck them. I&amp;#39;m shocked at the prices of them&lt;/p&gt;\n\n&lt;p&gt;3 years ago I bought 2 12TB WD Elements for \u00a3166 pounds each on amazon. Now the same is \u00a3214 used. I know there was a spike in prices during covid but it&amp;#39;s crazy that a drive could be purchased 3 years ago much cheaper. Why have prices risen so much again? What have I missed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17bmb0s", "is_robot_indexable": true, "report_reasons": null, "author": "Upbeat_Platypus1833", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bmb0s/hard_drives_still_so_expensive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bmb0s/hard_drives_still_so_expensive/", "subreddit_subscribers": 707591, "created_utc": 1697730546.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}