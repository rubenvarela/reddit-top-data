{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI am an experienced ETL developer with over 10 years and I am skilled in using tools like SSIS, Informatica and Python for ETL development.\n\nI am currently trying to Advance my career in the field and I was wondering which path to take. My major consideration is one that will still be in-demand for the 10 years or more and also one that has good pay grade as well as WFH opportunities.\n\nI am considering going into full blown career in data engineering (pipeline design, a good programming and server experience as well as  Containerization. **OR** one that focuses more on BIG DATA development.\n\nI know big data has been the buzz word since some years now but I am a bit concerned that its gradually not as loud as it used to be.\n\nI will appreciate your advise so much.", "author_fullname": "t2_dcnhwe2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A career in Data Engineering or Big Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17blv3d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697729350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am an experienced ETL developer with over 10 years and I am skilled in using tools like SSIS, Informatica and Python for ETL development.&lt;/p&gt;\n\n&lt;p&gt;I am currently trying to Advance my career in the field and I was wondering which path to take. My major consideration is one that will still be in-demand for the 10 years or more and also one that has good pay grade as well as WFH opportunities.&lt;/p&gt;\n\n&lt;p&gt;I am considering going into full blown career in data engineering (pipeline design, a good programming and server experience as well as  Containerization. &lt;strong&gt;OR&lt;/strong&gt; one that focuses more on BIG DATA development.&lt;/p&gt;\n\n&lt;p&gt;I know big data has been the buzz word since some years now but I am a bit concerned that its gradually not as loud as it used to be.&lt;/p&gt;\n\n&lt;p&gt;I will appreciate your advise so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17blv3d", "is_robot_indexable": true, "report_reasons": null, "author": "MediumCat4064", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17blv3d/a_career_in_data_engineering_or_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17blv3d/a_career_in_data_engineering_or_big_data/", "subreddit_subscribers": 134925, "created_utc": 1697729350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone obtained this? How was it for difficulty? I've never used GCP before but have just completed the *\"Preparing for your Professional Data Engineer Journey\"* course so not sure how much this will help.\n\nI'm fairly proficient is Python and SQL, I have some AWS experience. Looking to get certified as I'm trying to transition to the Data Engineering space rather than python/sql all rounder and it's better to have some GCP cert than nothing.\n\nAny advice is appreciated.", "author_fullname": "t2_5iincanh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Platform - Professional Data Engineer Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17baklu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697689485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone obtained this? How was it for difficulty? I&amp;#39;ve never used GCP before but have just completed the &lt;em&gt;&amp;quot;Preparing for your Professional Data Engineer Journey&amp;quot;&lt;/em&gt; course so not sure how much this will help.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly proficient is Python and SQL, I have some AWS experience. Looking to get certified as I&amp;#39;m trying to transition to the Data Engineering space rather than python/sql all rounder and it&amp;#39;s better to have some GCP cert than nothing.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17baklu", "is_robot_indexable": true, "report_reasons": null, "author": "Willing_Excuse1652", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17baklu/google_cloud_platform_professional_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17baklu/google_cloud_platform_professional_data_engineer/", "subreddit_subscribers": 134925, "created_utc": 1697689485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nWe at [dlt](https://dlthub.com/) (the loading library before dbt) created 2 **dbt** **runners** to enable kicking off dbt jobs after loading. They are lightweight, and you can use them anywhere.\n\nThe **dbt core runner** features an optional venv creation for resolving library conflicts and accepts credentials from dlt (easier to pass, can pass in code too)\n\nThe **dbt cloud runner** supports starting and polling a job so you can run the transform after the load on a tight schedule for example.\n\nI wrote a blog post to describe the use cases why you would use them too.\n\nI hope they are useful to you, and that they might solve some of the issues with running dbt.\n\n**Feedback welcome!**\n\n**Article Link:** [dbt-runners-usage](https://dlthub.com/docs/blog/dbt-runners-usage)\n\n**And the docs&amp;links:** [**Cloud runner**](https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/dbt_cloud)**,** [**Core runner**](https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/), [Join dlt slack community for questions](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)\u00a0\n\n**Examples:**\n\n**dbt Cloud** runner:\n\n    from dlt.helpers.dbt_cloud import run_dbt_cloud_job\n    \n    # Trigger a job run with additional data\n    additional_data = {\n        \"git_sha\": \"abcd1234\",\n        \"schema_override\": \"custom_schema\",\n        # ... other parameters\n    }\n    status = run_dbt_cloud_job(job_id=1234, data=additional_data, wait_for_outcome=True)\n    print(f\"Job run status: {status['status_humanized']}\")\n\n**dbt Core** runner:\n\n    pipeline = dlt.pipeline(\n        pipeline_name='pipedrive',\n        destination='bigquery',\n        dataset_name='pipedrive_dbt'\n    )\n    \n    # make or restore venv for dbt, using latest dbt version\n    venv = dlt.dbt.get_venv(pipeline)\n    \n    # get runner, optionally pass the venv\n    dbt = dlt.dbt.package(\n        pipeline,\n        \"pipedrive/dbt_pipedrive/pipedrive\",\n        venv=venv\n    )\n    \n    # run the models and collect any info\n    # If running fails, the error will be raised with full stack trace\n    models = dbt.run_all()", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT core and cloud runners and their use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bb0fe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697691097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;We at &lt;a href=\"https://dlthub.com/\"&gt;dlt&lt;/a&gt; (the loading library before dbt) created 2 &lt;strong&gt;dbt&lt;/strong&gt; &lt;strong&gt;runners&lt;/strong&gt; to enable kicking off dbt jobs after loading. They are lightweight, and you can use them anywhere.&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;dbt core runner&lt;/strong&gt; features an optional venv creation for resolving library conflicts and accepts credentials from dlt (easier to pass, can pass in code too)&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;dbt cloud runner&lt;/strong&gt; supports starting and polling a job so you can run the transform after the load on a tight schedule for example.&lt;/p&gt;\n\n&lt;p&gt;I wrote a blog post to describe the use cases why you would use them too.&lt;/p&gt;\n\n&lt;p&gt;I hope they are useful to you, and that they might solve some of the issues with running dbt.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Feedback welcome!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Article Link:&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/blog/dbt-runners-usage\"&gt;dbt-runners-usage&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;And the docs&amp;amp;links:&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/dbt_cloud\"&gt;&lt;strong&gt;Cloud runner&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/\"&gt;&lt;strong&gt;Core runner&lt;/strong&gt;&lt;/a&gt;, &lt;a href=\"https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g\"&gt;Join dlt slack community for questions&lt;/a&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;dbt Cloud&lt;/strong&gt; runner:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from dlt.helpers.dbt_cloud import run_dbt_cloud_job\n\n# Trigger a job run with additional data\nadditional_data = {\n    &amp;quot;git_sha&amp;quot;: &amp;quot;abcd1234&amp;quot;,\n    &amp;quot;schema_override&amp;quot;: &amp;quot;custom_schema&amp;quot;,\n    # ... other parameters\n}\nstatus = run_dbt_cloud_job(job_id=1234, data=additional_data, wait_for_outcome=True)\nprint(f&amp;quot;Job run status: {status[&amp;#39;status_humanized&amp;#39;]}&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;dbt Core&lt;/strong&gt; runner:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline = dlt.pipeline(\n    pipeline_name=&amp;#39;pipedrive&amp;#39;,\n    destination=&amp;#39;bigquery&amp;#39;,\n    dataset_name=&amp;#39;pipedrive_dbt&amp;#39;\n)\n\n# make or restore venv for dbt, using latest dbt version\nvenv = dlt.dbt.get_venv(pipeline)\n\n# get runner, optionally pass the venv\ndbt = dlt.dbt.package(\n    pipeline,\n    &amp;quot;pipedrive/dbt_pipedrive/pipedrive&amp;quot;,\n    venv=venv\n)\n\n# run the models and collect any info\n# If running fails, the error will be raised with full stack trace\nmodels = dbt.run_all()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?auto=webp&amp;s=1bd62d75d0936333c30fb69034c8c176429e93cf", "width": 1200, "height": 898}, "resolutions": [{"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b7dfc5649e6aa2d1b7c128781fdc7fc6556ae07", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f17115b9faa677c01bdf076bf97909b0f2194fd2", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7957af9a90b452cb31fe0d69578baed68c7526fb", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2009c85ad047390080ff6acbc319d96819eb885", "width": 640, "height": 478}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=394371b1ece2fdf65d23d108139c0f09980e3f42", "width": 960, "height": 718}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02df7d26ef0ba37069425c8cd0e0d38e4aecc1f7", "width": 1080, "height": 808}], "variants": {}, "id": "cUuEsBHL5TMhrkQNJ9leYxFMOF6VgvTJ_EKHwExrn8Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17bb0fe", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bb0fe/dbt_core_and_cloud_runners_and_their_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bb0fe/dbt_core_and_cloud_runners_and_their_use_cases/", "subreddit_subscribers": 134925, "created_utc": 1697691097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At the moment, when we \"run\" a dbt project, we are more or less running every model in the project. Our largest project has ~450 models and takes under an hour to complete. We have a cron schedule that invokes the run and a step function that defines the flow between states (projects and their various dbt operations).\n\nDo folks who use airflow run smaller batches at a time? This seems hard to manage, also seems like you would end up duplicating steps as you build a shared dependency multiple times.", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How exactly are you using airflow with dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmbov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697730598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At the moment, when we &amp;quot;run&amp;quot; a dbt project, we are more or less running every model in the project. Our largest project has ~450 models and takes under an hour to complete. We have a cron schedule that invokes the run and a step function that defines the flow between states (projects and their various dbt operations).&lt;/p&gt;\n\n&lt;p&gt;Do folks who use airflow run smaller batches at a time? This seems hard to manage, also seems like you would end up duplicating steps as you build a shared dependency multiple times.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17bmbov", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bmbov/how_exactly_are_you_using_airflow_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bmbov/how_exactly_are_you_using_airflow_with_dbt/", "subreddit_subscribers": 134925, "created_utc": 1697730598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The biggest new dbt Cloud features from Coalesce 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17bsage", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Pt2ZfLn9svd8KwEp2PFvdSkL16Ws3FifX7uUkLCUc9o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697746098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdbt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.getdbt.com/blog/new-dbt-cloud-features-announced-at-coalesce-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?auto=webp&amp;s=6347fcb3daf9a842bf820f8ccec577c12487f347", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77d0d353f5c6d77bfa60e43bbececf4b270a794e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=391a03e2803f86acb44234cf70a6db41d9a7fc63", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=991dad38adac8bb4c5ed0ffbc180ead7be80e687", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a5e9f4869a32ac351f89fc52effc05e53b3e0188", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f864e440c37ff7a996711d0a0e09e386defe2bf6", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/nT3InHJvC8X2psQSmTIchUUyztBUfSYy0lynLmMS0b8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28f27d147ddf282fc9123b71c3e113168a518fcd", "width": 1080, "height": 607}], "variants": {}, "id": "jPyehZjVjFnnPXeUvtW24q2zxUIQShEm8Z8ELBIJQyw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17bsage", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bsage/the_biggest_new_dbt_cloud_features_from_coalesce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.getdbt.com/blog/new-dbt-cloud-features-announced-at-coalesce-2023", "subreddit_subscribers": 134925, "created_utc": 1697746098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking into starting some certs to help my chances of breaking into a DE r\u00f4le. I have a year of experience as a SWE and I hope to switch to DE next year. Will Azure/GCP certifications actually add any value to my applications or do companies not really care?", "author_fullname": "t2_i30nbi2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certifications make or break?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17be4im", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697703427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into starting some certs to help my chances of breaking into a DE r\u00f4le. I have a year of experience as a SWE and I hope to switch to DE next year. Will Azure/GCP certifications actually add any value to my applications or do companies not really care?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17be4im", "is_robot_indexable": true, "report_reasons": null, "author": "iishadowsii_", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17be4im/certifications_make_or_break/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17be4im/certifications_make_or_break/", "subreddit_subscribers": 134925, "created_utc": 1697703427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uue07cnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bebep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697704285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing//", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17bebep", "is_robot_indexable": true, "report_reasons": null, "author": "mowerr708", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bebep/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing//", "subreddit_subscribers": 134925, "created_utc": 1697704285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here use the change tracking feature of sql server? How do you usually stream the data to a datalake or dw if streaming will be coming from 30 sql servers and need atleast 10 tables per servers.\n\nIm seeing some hints for kafka but its mostly about debezium which is only capable for CDC.....spark streaming would be possible but changes will not be so many. It is like 10-30 changes every 5 minutes.\n\nMaybe there's more efficient way, hoping to hear some", "author_fullname": "t2_5g5u53hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple sql server stream to datalake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bficq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697709507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here use the change tracking feature of sql server? How do you usually stream the data to a datalake or dw if streaming will be coming from 30 sql servers and need atleast 10 tables per servers.&lt;/p&gt;\n\n&lt;p&gt;Im seeing some hints for kafka but its mostly about debezium which is only capable for CDC.....spark streaming would be possible but changes will not be so many. It is like 10-30 changes every 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Maybe there&amp;#39;s more efficient way, hoping to hear some&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bficq", "is_robot_indexable": true, "report_reasons": null, "author": "chanchan_delier", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bficq/multiple_sql_server_stream_to_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bficq/multiple_sql_server_stream_to_datalake/", "subreddit_subscribers": 134925, "created_utc": 1697709507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to Airflow and Docker and have a best practice question regarding a production deployment.\n\nI have Airflow with Docker installed on a remote linux host. I also currently have a pyenv virtual environment with python 3.11 on the same remote host and installed Airflow's modules into it via pip.  This is a dev environment. No issues in this setup, I can create and run simple DAG's in VS Code directing python to use the interpreter in the virtual environment.\n\nI have a nagging feeling this setup is not ideal for production as the virtual environment is maintained and dependent on the local host.\n\nQuestion:  For production, should I create another Docker container just for the Airflow modules so that everything (Airflow and the python interpreter) is running through Docker?\n\nOr should I keep the current setup for production?\n\nThere are many resources available on setting up Airflow on Docker but little in way of production guidance. (re where to put your python environments and use them) Your thoughts and ideas are appreciated, thank you.", "author_fullname": "t2_3amuxhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow w/Python Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf4bh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697708457.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697707817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to Airflow and Docker and have a best practice question regarding a production deployment.&lt;/p&gt;\n\n&lt;p&gt;I have Airflow with Docker installed on a remote linux host. I also currently have a pyenv virtual environment with python 3.11 on the same remote host and installed Airflow&amp;#39;s modules into it via pip.  This is a dev environment. No issues in this setup, I can create and run simple DAG&amp;#39;s in VS Code directing python to use the interpreter in the virtual environment.&lt;/p&gt;\n\n&lt;p&gt;I have a nagging feeling this setup is not ideal for production as the virtual environment is maintained and dependent on the local host.&lt;/p&gt;\n\n&lt;p&gt;Question:  For production, should I create another Docker container just for the Airflow modules so that everything (Airflow and the python interpreter) is running through Docker?&lt;/p&gt;\n\n&lt;p&gt;Or should I keep the current setup for production?&lt;/p&gt;\n\n&lt;p&gt;There are many resources available on setting up Airflow on Docker but little in way of production guidance. (re where to put your python environments and use them) Your thoughts and ideas are appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bf4bh", "is_robot_indexable": true, "report_reasons": null, "author": "maxafrass", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bf4bh/airflow_wpython_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bf4bh/airflow_wpython_best_practice/", "subreddit_subscribers": 134925, "created_utc": 1697707817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI've been assigned to support our Finance team and help them become a data-driven function.\n\nKeen to hear your stories if you've worked with a similar function before.\n\nMy observations so far:\n1- They are often left behind in terms of data changes upstream (eg: new revenue data)\n2- They constantly need data exports but won't/can't self-serve.\n3- Their needs seem quite simple though (bank reports in csv, accounting data from Xero, product and subscription data from our OLTP) - I might be oversimplifying/just see the tip of the iceberg here.\n\nI've also been trying to get them to use new BI tools (trialling Metabase atm) but so far not been very successfull in getting them to self-serve (it seems they mostly want to export to Excel/Sheets).", "author_fullname": "t2_ilw3j5ci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finance Team DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17beb0r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697704242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been assigned to support our Finance team and help them become a data-driven function.&lt;/p&gt;\n\n&lt;p&gt;Keen to hear your stories if you&amp;#39;ve worked with a similar function before.&lt;/p&gt;\n\n&lt;p&gt;My observations so far:\n1- They are often left behind in terms of data changes upstream (eg: new revenue data)\n2- They constantly need data exports but won&amp;#39;t/can&amp;#39;t self-serve.\n3- Their needs seem quite simple though (bank reports in csv, accounting data from Xero, product and subscription data from our OLTP) - I might be oversimplifying/just see the tip of the iceberg here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also been trying to get them to use new BI tools (trialling Metabase atm) but so far not been very successfull in getting them to self-serve (it seems they mostly want to export to Excel/Sheets).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17beb0r", "is_robot_indexable": true, "report_reasons": null, "author": "GiacomoLeopardi6", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17beb0r/finance_team_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17beb0r/finance_team_de/", "subreddit_subscribers": 134925, "created_utc": 1697704242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I mount the volumes from a CSI (Rook with Ceph). InitContainers with permissions for the volumes for both Zeppelin and Spark.\n\nIf I also deploy a second Spark (spark2 from now) and submit an app from spark2 using spark1 cluster i can write on the volume. Do the same in Zeppelin and get permissions denied. \n\nI know for a fact that it is because of the user.  But cannot figure out why it works for an app with same permissions that the other has but is not working for the other. \n\nWilling to share conf files if needed. \n\nThanks a lot \ud83d\ude4f\ud83c\udffc", "author_fullname": "t2_d0ifg2cb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know about Zeppelin and Spark with Kubernetes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17btopz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697749666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mount the volumes from a CSI (Rook with Ceph). InitContainers with permissions for the volumes for both Zeppelin and Spark.&lt;/p&gt;\n\n&lt;p&gt;If I also deploy a second Spark (spark2 from now) and submit an app from spark2 using spark1 cluster i can write on the volume. Do the same in Zeppelin and get permissions denied. &lt;/p&gt;\n\n&lt;p&gt;I know for a fact that it is because of the user.  But cannot figure out why it works for an app with same permissions that the other has but is not working for the other. &lt;/p&gt;\n\n&lt;p&gt;Willing to share conf files if needed. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot \ud83d\ude4f\ud83c\udffc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17btopz", "is_robot_indexable": true, "report_reasons": null, "author": "Awkward-Cupcake6219", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17btopz/does_anyone_know_about_zeppelin_and_spark_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17btopz/does_anyone_know_about_zeppelin_and_spark_with/", "subreddit_subscribers": 134925, "created_utc": 1697749666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, someone of you know how to do that?\nI'm looking in internet about bulk copy but if i have a dagster/mage orchestration how can i do that?", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can copy data bethween csv in S3/GCS to SQL server DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bnfsc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697733545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, someone of you know how to do that?\nI&amp;#39;m looking in internet about bulk copy but if i have a dagster/mage orchestration how can i do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bnfsc", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bnfsc/how_can_copy_data_bethween_csv_in_s3gcs_to_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bnfsc/how_can_copy_data_bethween_csv_in_s3gcs_to_sql/", "subreddit_subscribers": 134925, "created_utc": 1697733545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you all serve up product data ( usage , enriched backend) to engineers to help them improve products at the road map design level or even just experience/ experience ?", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serving product data to product engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bx6ix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697758906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you all serve up product data ( usage , enriched backend) to engineers to help them improve products at the road map design level or even just experience/ experience ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17bx6ix", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bx6ix/serving_product_data_to_product_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bx6ix/serving_product_data_to_product_engineers/", "subreddit_subscribers": 134925, "created_utc": 1697758906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nI've been using SQL Server to BULK import .csv files stored in Azure Blob Storage, using scoped credentials, external data source, master key, etc.\n\nThis works fine. However, I'd like to upsert an existing table, say Invoices, with a new version of the .csv file corresponding to the Invoices table.\n\nI know that I can use MERGE, USING, ON, WHEN MATCHED, WHEN NOT MATCHED, etc, if I load the new source .csv file in a temp_Invoices table and then do the merge between the two.\n\nHowever, this defeats the whole purpose, since... I'd be loading the full table anyways, and I just want to load the new and modified data.\n\nIs this possible to do?\n\nI know CDC exists but the source databases doesn't allow this. If this is not possible, is full load the only practical solution?", "author_fullname": "t2_h920ytdnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upserting SQL Server table with .csv file, directly (incremental loading?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bv9ou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697753655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using SQL Server to BULK import .csv files stored in Azure Blob Storage, using scoped credentials, external data source, master key, etc.&lt;/p&gt;\n\n&lt;p&gt;This works fine. However, I&amp;#39;d like to upsert an existing table, say Invoices, with a new version of the .csv file corresponding to the Invoices table.&lt;/p&gt;\n\n&lt;p&gt;I know that I can use MERGE, USING, ON, WHEN MATCHED, WHEN NOT MATCHED, etc, if I load the new source .csv file in a temp_Invoices table and then do the merge between the two.&lt;/p&gt;\n\n&lt;p&gt;However, this defeats the whole purpose, since... I&amp;#39;d be loading the full table anyways, and I just want to load the new and modified data.&lt;/p&gt;\n\n&lt;p&gt;Is this possible to do?&lt;/p&gt;\n\n&lt;p&gt;I know CDC exists but the source databases doesn&amp;#39;t allow this. If this is not possible, is full load the only practical solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bv9ou", "is_robot_indexable": true, "report_reasons": null, "author": "ineedanswersiswear", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bv9ou/upserting_sql_server_table_with_csv_file_directly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bv9ou/upserting_sql_server_table_with_csv_file_directly/", "subreddit_subscribers": 134925, "created_utc": 1697753655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have a use case where scientists generate large volumes of datasets on an output S3 bucket. They annotate metadata for each dataset on an XLSX hosted in SharePoint (feature-wise, it's very important to them that they can \"drag-and-drop\" rows down to quickly create like rows because they can work with up to 50 replicates at a time, all with very similar columns). This SharePoint site is polled and any update to the XLSX triggers ingestion of both XLSX and associated datasets (rows).\n\nThis solution works, and it's simple (i.e. we don't need to build a custom application that needs to be maintained). The challenge here was always that we didn't have any source system to inject metadata from, so XLSX was an easy solution when we first started.\n\nHowever, now we're in production, and management doesn't like having XLSX involved in our automation. We need to find a new solution, but without building an entirely custom LIMS application here, I'm not sure how to solve this in a simple way! I'm looking for an open source solution for users to enter data through a UI, preferably as a table. It would be great if the solution had API integration and some sort of quality checks built in!\n\nDoes something like this exist? I'm curious how others deal with these problems?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for open source tools for users to manually enter data in tables (but not Excel)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bqz6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697742727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have a use case where scientists generate large volumes of datasets on an output S3 bucket. They annotate metadata for each dataset on an XLSX hosted in SharePoint (feature-wise, it&amp;#39;s very important to them that they can &amp;quot;drag-and-drop&amp;quot; rows down to quickly create like rows because they can work with up to 50 replicates at a time, all with very similar columns). This SharePoint site is polled and any update to the XLSX triggers ingestion of both XLSX and associated datasets (rows).&lt;/p&gt;\n\n&lt;p&gt;This solution works, and it&amp;#39;s simple (i.e. we don&amp;#39;t need to build a custom application that needs to be maintained). The challenge here was always that we didn&amp;#39;t have any source system to inject metadata from, so XLSX was an easy solution when we first started.&lt;/p&gt;\n\n&lt;p&gt;However, now we&amp;#39;re in production, and management doesn&amp;#39;t like having XLSX involved in our automation. We need to find a new solution, but without building an entirely custom LIMS application here, I&amp;#39;m not sure how to solve this in a simple way! I&amp;#39;m looking for an open source solution for users to enter data through a UI, preferably as a table. It would be great if the solution had API integration and some sort of quality checks built in!&lt;/p&gt;\n\n&lt;p&gt;Does something like this exist? I&amp;#39;m curious how others deal with these problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bqz6r", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bqz6r/looking_for_open_source_tools_for_users_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bqz6r/looking_for_open_source_tools_for_users_to/", "subreddit_subscribers": 134925, "created_utc": 1697742727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " The  data is being made available as SQL Server Analysis Services Cubes. I  know I can use a program like Microsoft Power BI to connect directly to  these, but if I have a PHP site, how can I either connect to these to  query them (even if I have to do MDX query or likewise), or, how can I  convert the output from these Cubes back into a regular database so I  can query it using regular SQL?\n\nI  just need some way to get the data I need out of these cubes to use on a  website which uses php. whether its directly or by snapshotting the  data into a DB periodically to run queries against.\n\nThanks", "author_fullname": "t2_9lf6bs4dn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Query SSAS OLAP/MOLAP Cube with PHP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bngc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697733588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The  data is being made available as SQL Server Analysis Services Cubes. I  know I can use a program like Microsoft Power BI to connect directly to  these, but if I have a PHP site, how can I either connect to these to  query them (even if I have to do MDX query or likewise), or, how can I  convert the output from these Cubes back into a regular database so I  can query it using regular SQL?&lt;/p&gt;\n\n&lt;p&gt;I  just need some way to get the data I need out of these cubes to use on a  website which uses php. whether its directly or by snapshotting the  data into a DB periodically to run queries against.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bngc2", "is_robot_indexable": true, "report_reasons": null, "author": "Intense_011", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bngc2/query_ssas_olapmolap_cube_with_php/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bngc2/query_ssas_olapmolap_cube_with_php/", "subreddit_subscribers": 134925, "created_utc": 1697733588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_llltp0a4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Cloud Migration: Harnessing the Power of Multiple Cloud Providers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17bhig4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9hw6gXTcDzOBkFv0ZVRbiUBStJeOOKwAgGzJNI5KOVM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697716956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "nallas.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://nallas.com/multi-cloud-migration-harnessing-the-power-of-multiple-cloud-providers/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?auto=webp&amp;s=c11933f1af26e3add60f428492e18039805b02de", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=69b9c355e840a8fab7746dd10b021e305905e9ac", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=266004acbd6b8130c8d13f0b3d00abe02e2d3c2b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb1b9de0984719a89290040fa9e9aabab7aeadc7", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b47b38d3f6518feb3ba5373567ccca18b7a37317", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8415a7d5013da02d9d30ec2c16ab83cd8be0ddfb", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/nvG0dGBXBpSJsYZzvsByXm3J_FjQSFdf6CbL45IRFQw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c63c7d2fa4d4e09fdb48cccb65bd7c85a302296", "width": 1080, "height": 607}], "variants": {}, "id": "vYFXNiuP1JBmetpGcn3zJMkxjHtw1nlqh_Ii70RdfB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17bhig4", "is_robot_indexable": true, "report_reasons": null, "author": "Nallas_Corporation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bhig4/multicloud_migration_harnessing_the_power_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://nallas.com/multi-cloud-migration-harnessing-the-power-of-multiple-cloud-providers/", "subreddit_subscribers": 134925, "created_utc": 1697716956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_yeda6sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "7 Best Cloud Database Platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17bhb8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RKkiKGHlrDcGPgc6uBjugTttjPdOP4hUHNBG_wgw3Ro.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697716305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kdnuggets.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.kdnuggets.com/7-best-cloud-database-platforms", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?auto=webp&amp;s=e89ef80e6b1ee59abe3fffecdbb5a3ed9ef209ef", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9245add326de740c1d4cfed9dbcf6a7e17f6b0fe", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0adb90fb3a109acc723c3c46d43fdd6825fe9d2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bee64c8a735946fc95bce485fc79581bd1e7ef4f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9ebd204412e0b4228b98397f69808e7c95d48d05", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=14eaf5bb59ae810a21304cc588d43e8356ba8e17", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/L24YKD58BvR1otbal94VG5TKf-CmbaNCyNf9JhyRv8I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a76190808291c2a0295aa215b69808888a3ace24", "width": 1080, "height": 607}], "variants": {}, "id": "3MF_0s7NsLIV2fyjw7881H55ThBHV301l6WbehLLuTM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17bhb8s", "is_robot_indexable": true, "report_reasons": null, "author": "kingabzpro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bhb8s/7_best_cloud_database_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.kdnuggets.com/7-best-cloud-database-platforms", "subreddit_subscribers": 134925, "created_utc": 1697716305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI\u2019m currently a Senior Data Engineer working with  AWS and I would like to know your opinion if the AWS SAA cert is worth it?\n\nI already have the knowledge, just thinking if having the certification will help somehow for new roles in the future.\n\nAnd also, if going for any cert, should just skip SAA and go for the Data Analytics Specialty?", "author_fullname": "t2_3ng50ktz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is AWS Solutions Architect useful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17be0d0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697702912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a Senior Data Engineer working with  AWS and I would like to know your opinion if the AWS SAA cert is worth it?&lt;/p&gt;\n\n&lt;p&gt;I already have the knowledge, just thinking if having the certification will help somehow for new roles in the future.&lt;/p&gt;\n\n&lt;p&gt;And also, if going for any cert, should just skip SAA and go for the Data Analytics Specialty?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17be0d0", "is_robot_indexable": true, "report_reasons": null, "author": "BramosR", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17be0d0/is_aws_solutions_architect_useful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17be0d0/is_aws_solutions_architect_useful/", "subreddit_subscribers": 134925, "created_utc": 1697702912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! i've been asked to connect/pull data from SAP successfactors but i'm a bit confused, wondering if there is some help here.\n\nThe docs they gave us basically says \"get a saml assertion\" and that's about it... so i guess my question is has anyone had success with getting connected/might have some resources?\n\nThe SAML bit is new to me, so i'm looking to understand a bit better on that piece, specifically generating the assertions. We are an azure house with AD sign in for it, so i feel like there is something there?\n\nWe are looking to pull the data out for storing in the warehouse for Power BI reporting internally. Looking to target the odata feed endpoints (which i am comfortable with!)\n\nalternatively, is there any paid platforms that have this like fivetran...\n\nThanks!\n\nhttps://help.sap.com/docs/SAP_SUCCESSFACTORS_PLATFORM/d599f15995d348a1b45ba5603e2aba9b/d9a9545305004187986c866de2b66987.html?locale=en-US", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP Successfactors HXM - SAML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17brk05", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697744225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! i&amp;#39;ve been asked to connect/pull data from SAP successfactors but i&amp;#39;m a bit confused, wondering if there is some help here.&lt;/p&gt;\n\n&lt;p&gt;The docs they gave us basically says &amp;quot;get a saml assertion&amp;quot; and that&amp;#39;s about it... so i guess my question is has anyone had success with getting connected/might have some resources?&lt;/p&gt;\n\n&lt;p&gt;The SAML bit is new to me, so i&amp;#39;m looking to understand a bit better on that piece, specifically generating the assertions. We are an azure house with AD sign in for it, so i feel like there is something there?&lt;/p&gt;\n\n&lt;p&gt;We are looking to pull the data out for storing in the warehouse for Power BI reporting internally. Looking to target the odata feed endpoints (which i am comfortable with!)&lt;/p&gt;\n\n&lt;p&gt;alternatively, is there any paid platforms that have this like fivetran...&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://help.sap.com/docs/SAP_SUCCESSFACTORS_PLATFORM/d599f15995d348a1b45ba5603e2aba9b/d9a9545305004187986c866de2b66987.html?locale=en-US\"&gt;https://help.sap.com/docs/SAP_SUCCESSFACTORS_PLATFORM/d599f15995d348a1b45ba5603e2aba9b/d9a9545305004187986c866de2b66987.html?locale=en-US&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17brk05", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17brk05/sap_successfactors_hxm_saml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17brk05/sap_successfactors_hxm_saml/", "subreddit_subscribers": 134925, "created_utc": 1697744225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\nI have a spark application which requires a token to access a certain sysyem. The lib I am using to connect the said sysyem requires token in application.conf file. How can I make sure that my spark scala job have the conf file at runtime. As of now I am manually copying the file to hdfs directory. But if I put it in src/main/resources directory workers are not able to find it. As its a access token I cannot commit the application.conf to git repo. It will fail security check.  What options I have to make a resource available to all workers at runtime?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cannot find application.conf issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmk9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697731230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nI have a spark application which requires a token to access a certain sysyem. The lib I am using to connect the said sysyem requires token in application.conf file. How can I make sure that my spark scala job have the conf file at runtime. As of now I am manually copying the file to hdfs directory. But if I put it in src/main/resources directory workers are not able to find it. As its a access token I cannot commit the application.conf to git repo. It will fail security check.  What options I have to make a resource available to all workers at runtime?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17bmk9x", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bmk9x/cannot_find_applicationconf_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bmk9x/cannot_find_applicationconf_issue/", "subreddit_subscribers": 134925, "created_utc": 1697731230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an good friend  (bachelors in politics and and another in CS) who is a senior data engineer, and I thought it would be valuable to share his knowledge with others who are interested in getting into the same field. This is kind of a \u2018if you don\u2019t know, you don\u2019t know\u2019 what to ask situation for me. So, I would like to know what questions would be of value to you. I can post the questions and his answers here if you want as well.", "author_fullname": "t2_102ywc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What questions would you like to ask a Senior Data engineer in the Health Management Industry (medicare plans)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bx78h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697758969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an good friend  (bachelors in politics and and another in CS) who is a senior data engineer, and I thought it would be valuable to share his knowledge with others who are interested in getting into the same field. This is kind of a \u2018if you don\u2019t know, you don\u2019t know\u2019 what to ask situation for me. So, I would like to know what questions would be of value to you. I can post the questions and his answers here if you want as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17bx78h", "is_robot_indexable": true, "report_reasons": null, "author": "escis", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bx78h/what_questions_would_you_like_to_ask_a_senior/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bx78h/what_questions_would_you_like_to_ask_a_senior/", "subreddit_subscribers": 134925, "created_utc": 1697758969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey hey\n\nWe (Canonical) launched a solution for Apache Spark on Kubernetes this week. Read all about it [right over here](https://medium.com/p/4cfe17872eb6)\n\nIf you're a Spark user, feel free to check it out\n\n/over", "author_fullname": "t2_bbfcogp4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why we built a Spark solution for Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bb7m8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697691805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey hey&lt;/p&gt;\n\n&lt;p&gt;We (Canonical) launched a solution for Apache Spark on Kubernetes this week. Read all about it &lt;a href=\"https://medium.com/p/4cfe17872eb6\"&gt;right over here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re a Spark user, feel free to check it out&lt;/p&gt;\n\n&lt;p&gt;/over&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?auto=webp&amp;s=fe8b0c2eb61e1378e2b621b2ee621fac1b522bb7", "width": 1200, "height": 1500}, "resolutions": [{"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd76f66771d8947b55a0a698924e63b534308cc8", "width": 108, "height": 135}, {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd3a709b69cb5540c0c04b34580e680a50d95277", "width": 216, "height": 270}, {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0271de2ea935cde1da29eebc93f27e21a756e4e2", "width": 320, "height": 400}, {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cdd0616b6032b054f346ab22b16385df0f8e5704", "width": 640, "height": 800}, {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dac53f6f4c402290a2631d8a5818bf79be8c28a5", "width": 960, "height": 1200}, {"url": "https://external-preview.redd.it/qnyw4o9Zist8-5JhUTrWBGRturTPJT4Q-wGF9Agvmos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87de647738fdada1953ba62fe8bbcc7d7c0b4a52", "width": 1080, "height": 1350}], "variants": {}, "id": "RMl5_1XEtlDbzMnC7a74bFXN3BoohiJJsxM2ItR9m0o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17bb7m8", "is_robot_indexable": true, "report_reasons": null, "author": "SomethingSuperFluffy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bb7m8/why_we_built_a_spark_solution_for_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bb7m8/why_we_built_a_spark_solution_for_kubernetes/", "subreddit_subscribers": 134925, "created_utc": 1697691805.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}