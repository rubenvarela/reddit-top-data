{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do you still perform technical duties or is it nonstop meetings and people management? If it's the latter, do you miss the hands-on technical aspects or is it better on the leadership side?", "author_fullname": "t2_131bi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Those that have moved from a technical position to a leadership/supervisory position, do you regret it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b32vd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697667537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you still perform technical duties or is it nonstop meetings and people management? If it&amp;#39;s the latter, do you miss the hands-on technical aspects or is it better on the leadership side?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17b32vd", "is_robot_indexable": true, "report_reasons": null, "author": "WadeEffingWilson", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17b32vd/those_that_have_moved_from_a_technical_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17b32vd/those_that_have_moved_from_a_technical_position/", "subreddit_subscribers": 1091166, "created_utc": 1697667537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a data scientist with 5 yoe now, and I've never needed to implement a tree, a linked list, a graph, a stack, or a queue. If I need a decision tree, I use a package like sklearn. If I'm doing graph analysis, typically I treat it like a matrix. I don't even have any idea what models might need a queue, but maybe that's really important for data processing or training somewhere?\n\n&amp;#x200B;\n\nHave any of you really needed to implement these data structures, or do you just use packages that are using them under the hood? Would I actually be meaningfully better at my day to day job if I knew when and how to use a linked list or a stack?", "author_fullname": "t2_50gkezqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will Understanding Advanced Data Structures Make Me a Better Data Scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17az6v2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697657516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist with 5 yoe now, and I&amp;#39;ve never needed to implement a tree, a linked list, a graph, a stack, or a queue. If I need a decision tree, I use a package like sklearn. If I&amp;#39;m doing graph analysis, typically I treat it like a matrix. I don&amp;#39;t even have any idea what models might need a queue, but maybe that&amp;#39;s really important for data processing or training somewhere?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have any of you really needed to implement these data structures, or do you just use packages that are using them under the hood? Would I actually be meaningfully better at my day to day job if I knew when and how to use a linked list or a stack?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17az6v2", "is_robot_indexable": true, "report_reasons": null, "author": "EagerMonkey", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17az6v2/will_understanding_advanced_data_structures_make/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17az6v2/will_understanding_advanced_data_structures_make/", "subreddit_subscribers": 1091166, "created_utc": 1697657516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Mods, where are you? There are countless posts every week with questions that were answered already.  \n\nShould I learn Python? \nMasters degree worth it?\nJob market sucks, what projects should I do?\n\nAll of these are valid questions, and my heart goes out to those who are struggling to land their first job. BUT, a quick lookup will yield answers for most of the questions online. It also frustrating to find the same question to which you have answered a day ago.  Let alone the fact that many of these posts are low effort ones and their questions aren\u2019t even phrased correctly.\n\nAll of this spam drives seniors away, and instead of making a discussion about ds content, hopefully more advanced stuff, we keep answering questions about which is better, a project of a master.", "author_fullname": "t2_4udseb4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[rant] Required - A designated tread for transitioning to DS and repeating questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmc70", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697730631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mods, where are you? There are countless posts every week with questions that were answered already.  &lt;/p&gt;\n\n&lt;p&gt;Should I learn Python? \nMasters degree worth it?\nJob market sucks, what projects should I do?&lt;/p&gt;\n\n&lt;p&gt;All of these are valid questions, and my heart goes out to those who are struggling to land their first job. BUT, a quick lookup will yield answers for most of the questions online. It also frustrating to find the same question to which you have answered a day ago.  Let alone the fact that many of these posts are low effort ones and their questions aren\u2019t even phrased correctly.&lt;/p&gt;\n\n&lt;p&gt;All of this spam drives seniors away, and instead of making a discussion about ds content, hopefully more advanced stuff, we keep answering questions about which is better, a project of a master.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bmc70", "is_robot_indexable": true, "report_reasons": null, "author": "David202023", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bmc70/rant_required_a_designated_tread_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bmc70/rant_required_a_designated_tread_for/", "subreddit_subscribers": 1091166, "created_utc": 1697730631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I come from a Math background, and im fascinated by Math topics like Functional Analysis, Differential Geometry, Topology, Manifold Learning etc, Im actually looking for ways where i can apply these Math topics in my day to day Data Science/ML work. Is there any DS or ML roles which can allow me to delve deep into these Math topics? This has been my dream job. Where can i find roles that will allow me to such expertise, my main issue being that im unable to find such roles.", "author_fullname": "t2_k17nck5e2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use cases of Advanced Math in Data Science and Machine Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bg00y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697711887.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697711491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a Math background, and im fascinated by Math topics like Functional Analysis, Differential Geometry, Topology, Manifold Learning etc, Im actually looking for ways where i can apply these Math topics in my day to day Data Science/ML work. Is there any DS or ML roles which can allow me to delve deep into these Math topics? This has been my dream job. Where can i find roles that will allow me to such expertise, my main issue being that im unable to find such roles.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bg00y", "is_robot_indexable": true, "report_reasons": null, "author": "honghuiying", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bg00y/use_cases_of_advanced_math_in_data_science_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bg00y/use_cases_of_advanced_math_in_data_science_and/", "subreddit_subscribers": 1091166, "created_utc": 1697711491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my past work I've become familiar with various techniques for *predictive* modeling--NNs, of course, but also more \"classical\" methods like random forests or LASSO regression (along with their implementations in Python, which was probably one of the best decisions I ever made was picking up Python--I've loved using sklearn and nltk, and I haven't even gotten to using pytorch yet).\n\nAll that said, I haven't worked as much so far with *explanatory* modeling and I'm looking to get more into it. I understand the conceptual differences: in predictive tasks, we care more about signal than significance--we might, for example, include variables that are predictive but not statistically significant, or exclude variables that are significant but not predictive. What's more, in the explanatory environment, there's a much greater emphasis on *model interpretability*--that is to say, models like NNs or even random forests that can get kind of \"black boxy\" are disfavored compared to simpler models with much more straightforward interpretability.\n\nSo what's the state-of-the-art, go-to model(s) for explanatory tasks? Stepwise regression??", "author_fullname": "t2_jzcyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predictive vs Explanatory modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17bqhb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697741452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my past work I&amp;#39;ve become familiar with various techniques for &lt;em&gt;predictive&lt;/em&gt; modeling--NNs, of course, but also more &amp;quot;classical&amp;quot; methods like random forests or LASSO regression (along with their implementations in Python, which was probably one of the best decisions I ever made was picking up Python--I&amp;#39;ve loved using sklearn and nltk, and I haven&amp;#39;t even gotten to using pytorch yet).&lt;/p&gt;\n\n&lt;p&gt;All that said, I haven&amp;#39;t worked as much so far with &lt;em&gt;explanatory&lt;/em&gt; modeling and I&amp;#39;m looking to get more into it. I understand the conceptual differences: in predictive tasks, we care more about signal than significance--we might, for example, include variables that are predictive but not statistically significant, or exclude variables that are significant but not predictive. What&amp;#39;s more, in the explanatory environment, there&amp;#39;s a much greater emphasis on &lt;em&gt;model interpretability&lt;/em&gt;--that is to say, models like NNs or even random forests that can get kind of &amp;quot;black boxy&amp;quot; are disfavored compared to simpler models with much more straightforward interpretability.&lt;/p&gt;\n\n&lt;p&gt;So what&amp;#39;s the state-of-the-art, go-to model(s) for explanatory tasks? Stepwise regression??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bqhb0", "is_robot_indexable": true, "report_reasons": null, "author": "19andoverlol", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bqhb0/predictive_vs_explanatory_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bqhb0/predictive_vs_explanatory_modeling/", "subreddit_subscribers": 1091166, "created_utc": 1697741452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! I am currently working on a binary classification model for a highly imbalanced dataset with lots of missing values in there. I tried using multiple techniques for resampling (Random, SMOTE, and SMOTETomak) and imputation (MICE), as well as a bit of tweaking of class weights and loss function, but still I am not able to get higher than this.\n\nCatBoost Accuracy:  0.843492894540015\n              precision    recall  f1-score   support\n\n         0.0       0.95      0.87      0.91      4775\n         1.0       0.36      0.62      0.46       573\n\n    accuracy                           0.84      5348\n   macro avg       0.66      0.74      0.68      5348\nweighted avg       0.89      0.84      0.86      5348\n\nAny ideas on what can I also try considering such results and above mentioned trials? Any feature engineering techniques that I might not know? \n\nAlso one of the interesting things about dataset is relatively large amount of categorical features - 30 out of 140 (two of them have 2k different options, others are in the range from 3 to 30). I used multiple different methods for encoding here depending on the amount of categories - One Hot, Binary and Target.\n\nOne of the main issues of the dataset is lack of context so I'm mostly trying to improve both precision and recall (and f1 as a result) at least to somewhat degree.\n\nThanks in advance for any possible ideas?", "author_fullname": "t2_uitvt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on imbalanced datasets with many missing values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b3lup", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697668940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am currently working on a binary classification model for a highly imbalanced dataset with lots of missing values in there. I tried using multiple techniques for resampling (Random, SMOTE, and SMOTETomak) and imputation (MICE), as well as a bit of tweaking of class weights and loss function, but still I am not able to get higher than this.&lt;/p&gt;\n\n&lt;p&gt;CatBoost Accuracy:  0.843492894540015\n              precision    recall  f1-score   support&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;     0.0       0.95      0.87      0.91      4775\n     1.0       0.36      0.62      0.46       573\n\naccuracy                           0.84      5348\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;macro avg       0.66      0.74      0.68      5348\nweighted avg       0.89      0.84      0.86      5348&lt;/p&gt;\n\n&lt;p&gt;Any ideas on what can I also try considering such results and above mentioned trials? Any feature engineering techniques that I might not know? &lt;/p&gt;\n\n&lt;p&gt;Also one of the interesting things about dataset is relatively large amount of categorical features - 30 out of 140 (two of them have 2k different options, others are in the range from 3 to 30). I used multiple different methods for encoding here depending on the amount of categories - One Hot, Binary and Target.&lt;/p&gt;\n\n&lt;p&gt;One of the main issues of the dataset is lack of context so I&amp;#39;m mostly trying to improve both precision and recall (and f1 as a result) at least to somewhat degree.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any possible ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17b3lup", "is_robot_indexable": true, "report_reasons": null, "author": "_degamus_", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17b3lup/advice_on_imbalanced_datasets_with_many_missing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17b3lup/advice_on_imbalanced_datasets_with_many_missing/", "subreddit_subscribers": 1091166, "created_utc": 1697668940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been studying Data Science for quite some time now and would love to apply but don\u2019t feel ready yet. What tech position would you recommend for someone that has 0 tech experience and has only studied data science for about 5 months? A position with a decent source of income would be a plus \n\nI was told a mailroom position is the best way to get into a tech company hands down, which doesn\u2019t seem right to me.", "author_fullname": "t2_9xivnomo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tech job to get before becoming Data Analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17bq7wz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697740791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been studying Data Science for quite some time now and would love to apply but don\u2019t feel ready yet. What tech position would you recommend for someone that has 0 tech experience and has only studied data science for about 5 months? A position with a decent source of income would be a plus &lt;/p&gt;\n\n&lt;p&gt;I was told a mailroom position is the best way to get into a tech company hands down, which doesn\u2019t seem right to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bq7wz", "is_robot_indexable": true, "report_reasons": null, "author": "Basic_Set3926", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bq7wz/what_tech_job_to_get_before_becoming_data_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bq7wz/what_tech_job_to_get_before_becoming_data_analyst/", "subreddit_subscribers": 1091166, "created_utc": 1697740791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a very broad question about building a model using xgboost and feature selection. \n\nAs an example, let\u2019s say I have tabular dataset and build a binary classification model using xgboost to predict a purchase. I run the model with a 10 fold cv and get an auc score of x. I then remove some columns and rerun the model, everything else the same, and get an auc score &gt; x. \n\nIn this case, would the columns that were removed be random / wrong and is this common? I believe I can use a t-test to compare the scores and see if it\u2019s due to random chance. \n\nMy assumption was that xgboost would automatically find the best split in the data but wanted to know other peoples thoughts.", "author_fullname": "t2_hs2uczj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wrong data in dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bobx9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697735878.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very broad question about building a model using xgboost and feature selection. &lt;/p&gt;\n\n&lt;p&gt;As an example, let\u2019s say I have tabular dataset and build a binary classification model using xgboost to predict a purchase. I run the model with a 10 fold cv and get an auc score of x. I then remove some columns and rerun the model, everything else the same, and get an auc score &amp;gt; x. &lt;/p&gt;\n\n&lt;p&gt;In this case, would the columns that were removed be random / wrong and is this common? I believe I can use a t-test to compare the scores and see if it\u2019s due to random chance. &lt;/p&gt;\n\n&lt;p&gt;My assumption was that xgboost would automatically find the best split in the data but wanted to know other peoples thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bobx9", "is_robot_indexable": true, "report_reasons": null, "author": "Kilroy_was_here__", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bobx9/wrong_data_in_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bobx9/wrong_data_in_dataset/", "subreddit_subscribers": 1091166, "created_utc": 1697735878.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI am currently writing my master thesis about gaze estimation and i am using a combination of a cnn for finding feature map and transformer for the regression task. The gaze angular error is a common metric in this field, because it calculates the angle and ignores the depth of two gaze predictions. But in most of the papers about this topic the L1 or L2 loss is used and gaze angular error is only the evaluation metric.\n\nDo you know why gaze angular error is not used as loss as well?", "author_fullname": "t2_8hi7gpok", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Different loss function than evaluation metric", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17becn2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697704437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am currently writing my master thesis about gaze estimation and i am using a combination of a cnn for finding feature map and transformer for the regression task. The gaze angular error is a common metric in this field, because it calculates the angle and ignores the depth of two gaze predictions. But in most of the papers about this topic the L1 or L2 loss is used and gaze angular error is only the evaluation metric.&lt;/p&gt;\n\n&lt;p&gt;Do you know why gaze angular error is not used as loss as well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17becn2", "is_robot_indexable": true, "report_reasons": null, "author": "mesheaa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17becn2/different_loss_function_than_evaluation_metric/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17becn2/different_loss_function_than_evaluation_metric/", "subreddit_subscribers": 1091166, "created_utc": 1697704437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone!  \nIs it possible to create a market timing strategy using unsupervised learning? \n\nLet's find out.\n\nRelevant Topics:\n\n* Used S&amp;P500 data\n* Segmenting Time series using Online Change Detection Point\n* Clustering segments with KMeans\n* Risk Allocation\n* Value at Risk\n\nHere's the notebook:  \n[https://www.kaggle.com/code/mandmdatascience/mt-rm-portfolio-allocation/notebook](https://www.kaggle.com/code/mandmdatascience/mt-rm-portfolio-allocation/notebook)\n\nEvery and each comment / feedback is greatly appreciated!\n\nThank you!  \nM&amp;M", "author_fullname": "t2_81hr0pq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Market Timing &amp; Risk Management - Portfolio allocation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bdzis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697702807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;br/&gt;\nIs it possible to create a market timing strategy using unsupervised learning? &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s find out.&lt;/p&gt;\n\n&lt;p&gt;Relevant Topics:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Used S&amp;amp;P500 data&lt;/li&gt;\n&lt;li&gt;Segmenting Time series using Online Change Detection Point&lt;/li&gt;\n&lt;li&gt;Clustering segments with KMeans&lt;/li&gt;\n&lt;li&gt;Risk Allocation&lt;/li&gt;\n&lt;li&gt;Value at Risk&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s the notebook:&lt;br/&gt;\n&lt;a href=\"https://www.kaggle.com/code/mandmdatascience/mt-rm-portfolio-allocation/notebook\"&gt;https://www.kaggle.com/code/mandmdatascience/mt-rm-portfolio-allocation/notebook&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Every and each comment / feedback is greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;br/&gt;\nM&amp;amp;M&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pGkUiqnzs5jbCTZHIr5iqbS-02twocrs_Rcv-L6QkD8.jpg?auto=webp&amp;s=22521ebf2516bed4d65862aff258ed16364f9bc0", "width": 100, "height": 100}, "resolutions": [], "variants": {}, "id": "1jf__EqbE3dYF9fUHKWvv0as9QmxITBOHBbPBchNPHQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bdzis", "is_robot_indexable": true, "report_reasons": null, "author": "MandM-DataScience", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bdzis/market_timing_risk_management_portfolio_allocation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bdzis/market_timing_risk_management_portfolio_allocation/", "subreddit_subscribers": 1091166, "created_utc": 1697702807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I\u2019m trying to build a model to find some inappropriate payments. So I have the data of all payments made, however, some of them were audited while the vast majority aren\u2019t. \n\nThe ones that aren\u2019t audited are just automatically approved while the ones that are audited are approved or rejected based on auditors judgment. So my plan is to just use all the payments that has been audited as the population and ignore the payments that have never been audited since they don\u2019t really tell us much.  \n\nSo probably about 1% of payments are audited and if that about 7% are rejected. \n\nNow the issue is that most of the payments that were rejected were for minor issues. Maybe the person who\u2019s entered the payment made a slight typo for the invoice number, so that was rejected and they had to resubmit it, or something minor. \n\nThose payments aren\u2019t abiding by some minor rules and they need to be rejected and resubmitted after being corrected. They\u2019re wrong but not really worth the time because we aren\u2019t saving any money. Unfortunately, that\u2019s about 70% of rejected payments. \n\nNow the other 30% is where the real savings is happening, potential fraud, the accountant mistyped the amount to be paid or whatever. And that\u2019s what I\u2019m really trying to find, if I find the others that\u2019s cool but doesn\u2019t really do much. \n\nHow would I go about selecting my data for that. Would I just ignore the 60% of rejected payments that aren\u2019t that big of a deal and proceed without. If so, would I also reduce the number the number of payments that were accepted as well by 60%. \n\nOr any alternative suggestions?", "author_fullname": "t2_8cjp70ft", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Binary classification question?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b2d08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019m trying to build a model to find some inappropriate payments. So I have the data of all payments made, however, some of them were audited while the vast majority aren\u2019t. &lt;/p&gt;\n\n&lt;p&gt;The ones that aren\u2019t audited are just automatically approved while the ones that are audited are approved or rejected based on auditors judgment. So my plan is to just use all the payments that has been audited as the population and ignore the payments that have never been audited since they don\u2019t really tell us much.  &lt;/p&gt;\n\n&lt;p&gt;So probably about 1% of payments are audited and if that about 7% are rejected. &lt;/p&gt;\n\n&lt;p&gt;Now the issue is that most of the payments that were rejected were for minor issues. Maybe the person who\u2019s entered the payment made a slight typo for the invoice number, so that was rejected and they had to resubmit it, or something minor. &lt;/p&gt;\n\n&lt;p&gt;Those payments aren\u2019t abiding by some minor rules and they need to be rejected and resubmitted after being corrected. They\u2019re wrong but not really worth the time because we aren\u2019t saving any money. Unfortunately, that\u2019s about 70% of rejected payments. &lt;/p&gt;\n\n&lt;p&gt;Now the other 30% is where the real savings is happening, potential fraud, the accountant mistyped the amount to be paid or whatever. And that\u2019s what I\u2019m really trying to find, if I find the others that\u2019s cool but doesn\u2019t really do much. &lt;/p&gt;\n\n&lt;p&gt;How would I go about selecting my data for that. Would I just ignore the 60% of rejected payments that aren\u2019t that big of a deal and proceed without. If so, would I also reduce the number the number of payments that were accepted as well by 60%. &lt;/p&gt;\n\n&lt;p&gt;Or any alternative suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17b2d08", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainVJ", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17b2d08/binary_classification_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17b2d08/binary_classification_question/", "subreddit_subscribers": 1091166, "created_utc": 1697665699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\n## Python\n\nUndoubtedly,\u00a0**the uncrowned king**\u00a0of machine learning and data analysis, the ubiquitous language that data scientists turn to for a bit of number crunching,\u00a0**is Python**. This is down to several reasons; the three most important among them are its\u00a0**maturity**, the enormous\u00a0**community**, and, last but not least, a\u00a0**vast array of robust third-party libraries**. But even if Python is a magnanimous sovereign that many developers love, it doesn\u2019t mean that there can\u2019t be contenders occasionally.\n\n## Julia\n\nFourteen years ago, in a bold attempt to combine all the good properties of well-established programming languages while getting rid of the less favorable ones, four developers came up with the idea of a new programming language that has a\u00a0**friendly syntax**, offers\u00a0**efficient mathematical computations**\u00a0out of the box, at a\u00a0**performance on par with compiled languages**. And thus,\u00a0[**Julia**](https://julialang.org/)\u00a0was born (here\u2019s a manifesto explaining\u00a0[**why**](https://julialang.org/blog/2012/02/why-we-created-julia/)\u00a0in more detail). Its first version was launched a bit more than eleven years ago.\n\n## Our choice\n\nMany in-depth comparisons of Python and Julia on the web (such as\u00a0[**this one**](https://richardpelgrim.medium.com/julia-vs-python-for-data-science-in-2022-1f5f6f38f3ac)\u00a0or\u00a0[**this**](https://www.turing.com/kb/julia-vs-python)) cover both the objective and subjective benefits and drawbacks of choosing one over the other. And given Julia\u2019s growing popularity, we are sure more will follow. In the rest of this blog post, however, let\u2019s explore why we picked Julia for our purposes. And that\u2019s not to say that we don\u2019t use Python for data science. On the contrary, we\u00a0**often run analyses in both ecosystems simultaneously**\u00a0to help each other out where one is lacking or to reduce the chances of mistakes by comparing their results.\n\n## The advantages of Julia\n\nSo what makes Julia so compelling to us?\n\n## Language features\n\nJulia has:\n\n* a friendly, easy-to-read (and write) syntax;\n* \u00a0a flexible and expressive (part static, part dynamic) type system;\n* \u00a0powerful mathematical notations, such as built-in vector and matrix operations;\n* \u00a0efficient\u00a0[**multiple dispatches**](https://en.wikipedia.org/wiki/Multiple_dispatch), a form of function polymorphism working with runtime types;\n* \u00a0convenient and reliable parallel computing facilities;\n* \u00a0meta-programming with macros and generated functions.\n\n## Fast code execution\n\nJulia compiles the source code to\u00a0**native binary at runtime**\u00a0via LLVM. This approach combines the flexibility of interpreters, such as Python, with the performance of compiled languages, like C++ or Rust. The drawback is that code loading and the first run takes longer;\u00a0**the benefits start to shine when a piece of code is run multiple times**. This unique feature makes it an excellent tool for number crunching but less than ideal for scripting.\n\n## Built-in package management\n\nJulia has a pretty good (albeit not perfect) built-in package management tool, implemented as a base library; and a general registry of open-source packages. The offering of\u00a0**stable and well-designed packages**\u00a0is growing steadily along with the Julia community, especially in data science. Unit testing utilities are also part of the standard library.\n\n## Interactive tools\n\nJulia offers an\u00a0**advanced**\u00a0[**REPL**](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop)\u00a0with all the goodies of an interpreted language environment. These include:\n\n* code and variable inspection,\n* \u00a0code completion,\n* \u00a0an interactive debugger,\n* \u00a0benchmarking and profiling tools,\n* \u00a0and a built-in help system.\n\n**With third-party libraries, it can also be extended**\u00a0with\u00a0[**syntax highlighting**](https://github.com/KristofferC/OhMyREPL.jl),\u00a0[**source code lookup**](https://github.com/tkf/InteractiveCodeSearch.jl)\u00a0(even for base libraries), automatic\u00a0[**code reload**](https://timholy.github.io/Revise.jl/stable/), and many more exciting, modern features.\n\nAll these together make Julia an\u00a0**ideal environment for rapid prototyping**.\n\n## From prototyping to production code\n\nBecause of the high-level interactive tools and fast code execution,\u00a0**the transition from a rapid prototype to production-ready code can be as continuous as you\u2019d like**. More often than not, we find that most of the code that implements our business logic in the research code can also be used in the final product.\n\nThanks to its friendly syntax and built-in package management,\u00a0**the road to maintainable code is well paved**. Nothing replaces good API design, coding discipline, and rigorous testing, but Julia helps you to focus on these topics.\n\nAs a consequence, a computation pieced together in the REPL can easily become a piece of prototyping code in a POC module; then later, after some refactoring and unit testing, turn into a chunk of core code in an internal library, and finally, following more cleanup, find itself in a production package.\n\n## The disadvantages of Julia\n\nThat said, every benefit comes at a cost, and Julia is not free from issues. Here are a few stumbling blocks worthy of mentioning:\n\n* the very powerful tool of broadcasting and vectorization can be intimidating at first;\n* [**time to first plot**](https://discourse.julialang.org/t/time-to-first-plot-clarification/58534)\u00a0can be surprising, sometimes inconveniently long, although considerable effort has been put into making it shorter;\n* \u00a0many packages never reach a stable state or just become unmaintained; others are poorly designed or written;\n* [**releasing a binary package**](https://github.com/JuliaLang/PackageCompiler.jl)\u00a0can be challenging, and compilation time can be unexpectedly long, not to mention obfuscation, which can also be tricky.\n\n## Summary\n\nIn conclusion, the choice between programming languages for data analysis is not always clear-cut. While Python has been the go-to language for many data scientists, Julia is rapidly gaining popularity for its unique set of features that make it an attractive option. In this blog post, we explored why we chose Julia over Python for our purposes, highlighting its language features, fast code execution, built-in package management, interactive tools, and ease of transitioning from prototyping to production code. However, we also acknowledged that Julia has challenges, including overcoming some learning curves and the occasional instability of packages. Ultimately, the choice between Julia and Python (or any other programming language) will depend on specific project requirements, personal preferences, and available resources.\n\nStill, in the past years,\u00a0**Julia has proved to be our reliable and faithful companion**. It has evolved, matured, and improved significantly, and we would be less happy and less successful without it. So cheers, Julia; we are excited to see what your future brings!", "author_fullname": "t2_gbypvyyx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Programming language for machine learning and data analysis \u2013 Our choice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf3cx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697707699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h2&gt;Python&lt;/h2&gt;\n\n&lt;p&gt;Undoubtedly,\u00a0&lt;strong&gt;the uncrowned king&lt;/strong&gt;\u00a0of machine learning and data analysis, the ubiquitous language that data scientists turn to for a bit of number crunching,\u00a0&lt;strong&gt;is Python&lt;/strong&gt;. This is down to several reasons; the three most important among them are its\u00a0&lt;strong&gt;maturity&lt;/strong&gt;, the enormous\u00a0&lt;strong&gt;community&lt;/strong&gt;, and, last but not least, a\u00a0&lt;strong&gt;vast array of robust third-party libraries&lt;/strong&gt;. But even if Python is a magnanimous sovereign that many developers love, it doesn\u2019t mean that there can\u2019t be contenders occasionally.&lt;/p&gt;\n\n&lt;h2&gt;Julia&lt;/h2&gt;\n\n&lt;p&gt;Fourteen years ago, in a bold attempt to combine all the good properties of well-established programming languages while getting rid of the less favorable ones, four developers came up with the idea of a new programming language that has a\u00a0&lt;strong&gt;friendly syntax&lt;/strong&gt;, offers\u00a0&lt;strong&gt;efficient mathematical computations&lt;/strong&gt;\u00a0out of the box, at a\u00a0&lt;strong&gt;performance on par with compiled languages&lt;/strong&gt;. And thus,\u00a0&lt;a href=\"https://julialang.org/\"&gt;&lt;strong&gt;Julia&lt;/strong&gt;&lt;/a&gt;\u00a0was born (here\u2019s a manifesto explaining\u00a0&lt;a href=\"https://julialang.org/blog/2012/02/why-we-created-julia/\"&gt;&lt;strong&gt;why&lt;/strong&gt;&lt;/a&gt;\u00a0in more detail). Its first version was launched a bit more than eleven years ago.&lt;/p&gt;\n\n&lt;h2&gt;Our choice&lt;/h2&gt;\n\n&lt;p&gt;Many in-depth comparisons of Python and Julia on the web (such as\u00a0&lt;a href=\"https://richardpelgrim.medium.com/julia-vs-python-for-data-science-in-2022-1f5f6f38f3ac\"&gt;&lt;strong&gt;this one&lt;/strong&gt;&lt;/a&gt;\u00a0or\u00a0&lt;a href=\"https://www.turing.com/kb/julia-vs-python\"&gt;&lt;strong&gt;this&lt;/strong&gt;&lt;/a&gt;) cover both the objective and subjective benefits and drawbacks of choosing one over the other. And given Julia\u2019s growing popularity, we are sure more will follow. In the rest of this blog post, however, let\u2019s explore why we picked Julia for our purposes. And that\u2019s not to say that we don\u2019t use Python for data science. On the contrary, we\u00a0&lt;strong&gt;often run analyses in both ecosystems simultaneously&lt;/strong&gt;\u00a0to help each other out where one is lacking or to reduce the chances of mistakes by comparing their results.&lt;/p&gt;\n\n&lt;h2&gt;The advantages of Julia&lt;/h2&gt;\n\n&lt;p&gt;So what makes Julia so compelling to us?&lt;/p&gt;\n\n&lt;h2&gt;Language features&lt;/h2&gt;\n\n&lt;p&gt;Julia has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a friendly, easy-to-read (and write) syntax;&lt;/li&gt;\n&lt;li&gt;\u00a0a flexible and expressive (part static, part dynamic) type system;&lt;/li&gt;\n&lt;li&gt;\u00a0powerful mathematical notations, such as built-in vector and matrix operations;&lt;/li&gt;\n&lt;li&gt;\u00a0efficient\u00a0&lt;a href=\"https://en.wikipedia.org/wiki/Multiple_dispatch\"&gt;&lt;strong&gt;multiple dispatches&lt;/strong&gt;&lt;/a&gt;, a form of function polymorphism working with runtime types;&lt;/li&gt;\n&lt;li&gt;\u00a0convenient and reliable parallel computing facilities;&lt;/li&gt;\n&lt;li&gt;\u00a0meta-programming with macros and generated functions.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Fast code execution&lt;/h2&gt;\n\n&lt;p&gt;Julia compiles the source code to\u00a0&lt;strong&gt;native binary at runtime&lt;/strong&gt;\u00a0via LLVM. This approach combines the flexibility of interpreters, such as Python, with the performance of compiled languages, like C++ or Rust. The drawback is that code loading and the first run takes longer;\u00a0&lt;strong&gt;the benefits start to shine when a piece of code is run multiple times&lt;/strong&gt;. This unique feature makes it an excellent tool for number crunching but less than ideal for scripting.&lt;/p&gt;\n\n&lt;h2&gt;Built-in package management&lt;/h2&gt;\n\n&lt;p&gt;Julia has a pretty good (albeit not perfect) built-in package management tool, implemented as a base library; and a general registry of open-source packages. The offering of\u00a0&lt;strong&gt;stable and well-designed packages&lt;/strong&gt;\u00a0is growing steadily along with the Julia community, especially in data science. Unit testing utilities are also part of the standard library.&lt;/p&gt;\n\n&lt;h2&gt;Interactive tools&lt;/h2&gt;\n\n&lt;p&gt;Julia offers an\u00a0&lt;strong&gt;advanced&lt;/strong&gt;\u00a0&lt;a href=\"https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop\"&gt;&lt;strong&gt;REPL&lt;/strong&gt;&lt;/a&gt;\u00a0with all the goodies of an interpreted language environment. These include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;code and variable inspection,&lt;/li&gt;\n&lt;li&gt;\u00a0code completion,&lt;/li&gt;\n&lt;li&gt;\u00a0an interactive debugger,&lt;/li&gt;\n&lt;li&gt;\u00a0benchmarking and profiling tools,&lt;/li&gt;\n&lt;li&gt;\u00a0and a built-in help system.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;With third-party libraries, it can also be extended&lt;/strong&gt;\u00a0with\u00a0&lt;a href=\"https://github.com/KristofferC/OhMyREPL.jl\"&gt;&lt;strong&gt;syntax highlighting&lt;/strong&gt;&lt;/a&gt;,\u00a0&lt;a href=\"https://github.com/tkf/InteractiveCodeSearch.jl\"&gt;&lt;strong&gt;source code lookup&lt;/strong&gt;&lt;/a&gt;\u00a0(even for base libraries), automatic\u00a0&lt;a href=\"https://timholy.github.io/Revise.jl/stable/\"&gt;&lt;strong&gt;code reload&lt;/strong&gt;&lt;/a&gt;, and many more exciting, modern features.&lt;/p&gt;\n\n&lt;p&gt;All these together make Julia an\u00a0&lt;strong&gt;ideal environment for rapid prototyping&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;h2&gt;From prototyping to production code&lt;/h2&gt;\n\n&lt;p&gt;Because of the high-level interactive tools and fast code execution,\u00a0&lt;strong&gt;the transition from a rapid prototype to production-ready code can be as continuous as you\u2019d like&lt;/strong&gt;. More often than not, we find that most of the code that implements our business logic in the research code can also be used in the final product.&lt;/p&gt;\n\n&lt;p&gt;Thanks to its friendly syntax and built-in package management,\u00a0&lt;strong&gt;the road to maintainable code is well paved&lt;/strong&gt;. Nothing replaces good API design, coding discipline, and rigorous testing, but Julia helps you to focus on these topics.&lt;/p&gt;\n\n&lt;p&gt;As a consequence, a computation pieced together in the REPL can easily become a piece of prototyping code in a POC module; then later, after some refactoring and unit testing, turn into a chunk of core code in an internal library, and finally, following more cleanup, find itself in a production package.&lt;/p&gt;\n\n&lt;h2&gt;The disadvantages of Julia&lt;/h2&gt;\n\n&lt;p&gt;That said, every benefit comes at a cost, and Julia is not free from issues. Here are a few stumbling blocks worthy of mentioning:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the very powerful tool of broadcasting and vectorization can be intimidating at first;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://discourse.julialang.org/t/time-to-first-plot-clarification/58534\"&gt;&lt;strong&gt;time to first plot&lt;/strong&gt;&lt;/a&gt;\u00a0can be surprising, sometimes inconveniently long, although considerable effort has been put into making it shorter;&lt;/li&gt;\n&lt;li&gt;\u00a0many packages never reach a stable state or just become unmaintained; others are poorly designed or written;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/JuliaLang/PackageCompiler.jl\"&gt;&lt;strong&gt;releasing a binary package&lt;/strong&gt;&lt;/a&gt;\u00a0can be challenging, and compilation time can be unexpectedly long, not to mention obfuscation, which can also be tricky.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h2&gt;Summary&lt;/h2&gt;\n\n&lt;p&gt;In conclusion, the choice between programming languages for data analysis is not always clear-cut. While Python has been the go-to language for many data scientists, Julia is rapidly gaining popularity for its unique set of features that make it an attractive option. In this blog post, we explored why we chose Julia over Python for our purposes, highlighting its language features, fast code execution, built-in package management, interactive tools, and ease of transitioning from prototyping to production code. However, we also acknowledged that Julia has challenges, including overcoming some learning curves and the occasional instability of packages. Ultimately, the choice between Julia and Python (or any other programming language) will depend on specific project requirements, personal preferences, and available resources.&lt;/p&gt;\n\n&lt;p&gt;Still, in the past years,\u00a0&lt;strong&gt;Julia has proved to be our reliable and faithful companion&lt;/strong&gt;. It has evolved, matured, and improved significantly, and we would be less happy and less successful without it. So cheers, Julia; we are excited to see what your future brings!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17bf3cx", "is_robot_indexable": true, "report_reasons": null, "author": "CursorInsight", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17bf3cx/programming_language_for_machine_learning_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17bf3cx/programming_language_for_machine_learning_and/", "subreddit_subscribers": 1091166, "created_utc": 1697707699.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}