{"kind": "Listing", "data": {"after": "t3_17bf4bh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?\n\nWould love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.\n\nTrying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you seen any examples of \u201cserious\u201d companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17asnwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from other DEs that serve data to &lt;strong&gt;pro-code&lt;/strong&gt; visualization tools like Shiny, Dash, or D3.js.&lt;/p&gt;\n\n&lt;p&gt;Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17asnwh", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 153, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "subreddit_subscribers": 134801, "created_utc": 1697640732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.\n\nIt can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.\n\nPyGWalker Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)\n\n[pygwalker in juypter lab](https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;format=png&amp;auto=webp&amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd)\n\nA simple example of how to use pygwalker\n\n    import pygwalker as pyg\n    import pandas as pd\n    \n    df = pd.read_csv(\"you_data\")\n    \n    # then pass it to pygwalker\n    pyg.walk(df)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker: a Python library for data engineer that turns your dataframe into tableau-like data app.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lor544wm82vb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/lor544wm82vb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce7e7c2e461b8c5f8a62fa4fc0ec9dfd6ace2c3e"}, {"y": 122, "x": 216, "u": "https://preview.redd.it/lor544wm82vb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee7e80217d9179cba847be4f8db9894125b634c"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/lor544wm82vb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=295a8a44847cbb4645e996e87ca86f66bf3cf360"}, {"y": 361, "x": 640, "u": "https://preview.redd.it/lor544wm82vb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c1a63ea82817d913b5141976ce8757419705c0c"}, {"y": 542, "x": 960, "u": "https://preview.redd.it/lor544wm82vb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=00a6472b88d633baf9f30cc48639ca51854b9463"}, {"y": 610, "x": 1080, "u": "https://preview.redd.it/lor544wm82vb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38271e7d4e710122f24a3fc973486ac483cc23cc"}], "s": {"y": 1696, "x": 3002, "u": "https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;format=png&amp;auto=webp&amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd"}, "id": "lor544wm82vb1"}}, "name": "t3_17b6wdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d1ctUbcbuJYjdb3IUb1F2qoxEKrfIsQL_tCBj6-pC_I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697678204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.&lt;/p&gt;\n\n&lt;p&gt;It can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.&lt;/p&gt;\n\n&lt;p&gt;PyGWalker Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd\"&gt;pygwalker in juypter lab&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A simple example of how to use pygwalker&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pygwalker as pyg\nimport pandas as pd\n\ndf = pd.read_csv(&amp;quot;you_data&amp;quot;)\n\n# then pass it to pygwalker\npyg.walk(df)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17b6wdl", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b6wdl/pygwalker_a_python_library_for_data_engineer_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b6wdl/pygwalker_a_python_library_for_data_engineer_that/", "subreddit_subscribers": 134801, "created_utc": 1697678204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.\n\nThey announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d\n\nNot sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).", "author_fullname": "t2_nexbbb26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company is implementing stack ranking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b1zaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697664710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.&lt;/p&gt;\n\n&lt;p&gt;They announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d&lt;/p&gt;\n\n&lt;p&gt;Not sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b1zaw", "is_robot_indexable": true, "report_reasons": null, "author": "keep_it_professional", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "subreddit_subscribers": 134801, "created_utc": 1697664710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.\n\n&amp;#x200B;\n\nI frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.\n\n&amp;#x200B;\n\nAfter conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.\n\n&amp;#x200B;\n\nYou can try it out at [http://sqlvisual.net](http://sqlvisual.net) without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_la0ljviui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make SQL easy to understand with sqlvisual", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17au1n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697644390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can try it out at &lt;a href=\"http://sqlvisual.net\"&gt;http://sqlvisual.net&lt;/a&gt; without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17au1n7", "is_robot_indexable": true, "report_reasons": null, "author": "glxrun", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "subreddit_subscribers": 134801, "created_utc": 1697644390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. \n\nWe are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. \n\nWe are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.   \nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.   \nOur goal is to work on anonymized data in the DEV &amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. \n\nI have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it's not even necessary (for at least the next 6 months).\n\nNow we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. \n\nI am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn't seem required to us). \n\nI understand that the Unity Catalog is the \"new hot thing\" that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? ", "author_fullname": "t2_5iao6str", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Unity Catalog worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17app3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697632428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. &lt;/p&gt;\n\n&lt;p&gt;We are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. &lt;/p&gt;\n\n&lt;p&gt;We are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.&lt;br/&gt;\nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.&lt;br/&gt;\nOur goal is to work on anonymized data in the DEV &amp;amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. &lt;/p&gt;\n\n&lt;p&gt;I have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it&amp;#39;s not even necessary (for at least the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;Now we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;I am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn&amp;#39;t seem required to us). &lt;/p&gt;\n\n&lt;p&gt;I understand that the Unity Catalog is the &amp;quot;new hot thing&amp;quot; that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17app3b", "is_robot_indexable": true, "report_reasons": null, "author": "Far-String829", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "subreddit_subscribers": 134801, "created_utc": 1697632428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn more about data engineering (architecture, pipelines, etc.), but I don't just wan to spin up Python/SQL and solve problems for the sake of solving them. I want to potentially build something that I can passively make income from in the long term. \n\nI know there is a skill gap between data engineering and web app development, but is there any better thing to work on to develop my skills and potentially a steady income stream than a web app?\n\nI have an idea for a web app that would likely be very API heavy or might require a great amount of data scraping which would then need to be processed and organized.", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is creating a web app the best way to expand data engineering knowledge while possibly building the foundation of a sellable product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b730n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697678757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn more about data engineering (architecture, pipelines, etc.), but I don&amp;#39;t just wan to spin up Python/SQL and solve problems for the sake of solving them. I want to potentially build something that I can passively make income from in the long term. &lt;/p&gt;\n\n&lt;p&gt;I know there is a skill gap between data engineering and web app development, but is there any better thing to work on to develop my skills and potentially a steady income stream than a web app?&lt;/p&gt;\n\n&lt;p&gt;I have an idea for a web app that would likely be very API heavy or might require a great amount of data scraping which would then need to be processed and organized.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b730n", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b730n/is_creating_a_web_app_the_best_way_to_expand_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b730n/is_creating_a_web_app_the_best_way_to_expand_data/", "subreddit_subscribers": 134801, "created_utc": 1697678757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nWe at [dlt](https://dlthub.com/) (the loading library before dbt) created 2 **dbt** **runners** to enable kicking off dbt jobs after loading. They are lightweight, and you can use them anywhere.\n\nThe **dbt core runner** features an optional venv creation for resolving library conflicts and accepts credentials from dlt (easier to pass, can pass in code too)\n\nThe **dbt cloud runner** supports starting and polling a job so you can run the transform after the load on a tight schedule for example.\n\nI wrote a blog post to describe the use cases why you would use them too.\n\nI hope they are useful to you, and that they might solve some of the issues with running dbt.\n\n**Feedback welcome!**\n\n**Article Link:** [dbt-runners-usage](https://dlthub.com/docs/blog/dbt-runners-usage)\n\n**And the docs&amp;links:** [**Cloud runner**](https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/dbt_cloud)**,** [**Core runner**](https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/), [Join dlt slack community for questions](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)\u00a0\n\n**Examples:**\n\n**dbt Cloud** runner:\n\n    from dlt.helpers.dbt_cloud import run_dbt_cloud_job\n    \n    # Trigger a job run with additional data\n    additional_data = {\n        \"git_sha\": \"abcd1234\",\n        \"schema_override\": \"custom_schema\",\n        # ... other parameters\n    }\n    status = run_dbt_cloud_job(job_id=1234, data=additional_data, wait_for_outcome=True)\n    print(f\"Job run status: {status['status_humanized']}\")\n\n**dbt Core** runner:\n\n    pipeline = dlt.pipeline(\n        pipeline_name='pipedrive',\n        destination='bigquery',\n        dataset_name='pipedrive_dbt'\n    )\n    \n    # make or restore venv for dbt, using latest dbt version\n    venv = dlt.dbt.get_venv(pipeline)\n    \n    # get runner, optionally pass the venv\n    dbt = dlt.dbt.package(\n        pipeline,\n        \"pipedrive/dbt_pipedrive/pipedrive\",\n        venv=venv\n    )\n    \n    # run the models and collect any info\n    # If running fails, the error will be raised with full stack trace\n    models = dbt.run_all()", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT core and cloud runners and their use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bb0fe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697691097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;We at &lt;a href=\"https://dlthub.com/\"&gt;dlt&lt;/a&gt; (the loading library before dbt) created 2 &lt;strong&gt;dbt&lt;/strong&gt; &lt;strong&gt;runners&lt;/strong&gt; to enable kicking off dbt jobs after loading. They are lightweight, and you can use them anywhere.&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;dbt core runner&lt;/strong&gt; features an optional venv creation for resolving library conflicts and accepts credentials from dlt (easier to pass, can pass in code too)&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;dbt cloud runner&lt;/strong&gt; supports starting and polling a job so you can run the transform after the load on a tight schedule for example.&lt;/p&gt;\n\n&lt;p&gt;I wrote a blog post to describe the use cases why you would use them too.&lt;/p&gt;\n\n&lt;p&gt;I hope they are useful to you, and that they might solve some of the issues with running dbt.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Feedback welcome!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Article Link:&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/blog/dbt-runners-usage\"&gt;dbt-runners-usage&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;And the docs&amp;amp;links:&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/dbt_cloud\"&gt;&lt;strong&gt;Cloud runner&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/\"&gt;&lt;strong&gt;Core runner&lt;/strong&gt;&lt;/a&gt;, &lt;a href=\"https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g\"&gt;Join dlt slack community for questions&lt;/a&gt;\u00a0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;dbt Cloud&lt;/strong&gt; runner:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from dlt.helpers.dbt_cloud import run_dbt_cloud_job\n\n# Trigger a job run with additional data\nadditional_data = {\n    &amp;quot;git_sha&amp;quot;: &amp;quot;abcd1234&amp;quot;,\n    &amp;quot;schema_override&amp;quot;: &amp;quot;custom_schema&amp;quot;,\n    # ... other parameters\n}\nstatus = run_dbt_cloud_job(job_id=1234, data=additional_data, wait_for_outcome=True)\nprint(f&amp;quot;Job run status: {status[&amp;#39;status_humanized&amp;#39;]}&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;dbt Core&lt;/strong&gt; runner:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;pipeline = dlt.pipeline(\n    pipeline_name=&amp;#39;pipedrive&amp;#39;,\n    destination=&amp;#39;bigquery&amp;#39;,\n    dataset_name=&amp;#39;pipedrive_dbt&amp;#39;\n)\n\n# make or restore venv for dbt, using latest dbt version\nvenv = dlt.dbt.get_venv(pipeline)\n\n# get runner, optionally pass the venv\ndbt = dlt.dbt.package(\n    pipeline,\n    &amp;quot;pipedrive/dbt_pipedrive/pipedrive&amp;quot;,\n    venv=venv\n)\n\n# run the models and collect any info\n# If running fails, the error will be raised with full stack trace\nmodels = dbt.run_all()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?auto=webp&amp;s=1bd62d75d0936333c30fb69034c8c176429e93cf", "width": 1200, "height": 898}, "resolutions": [{"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b7dfc5649e6aa2d1b7c128781fdc7fc6556ae07", "width": 108, "height": 80}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f17115b9faa677c01bdf076bf97909b0f2194fd2", "width": 216, "height": 161}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7957af9a90b452cb31fe0d69578baed68c7526fb", "width": 320, "height": 239}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2009c85ad047390080ff6acbc319d96819eb885", "width": 640, "height": 478}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=394371b1ece2fdf65d23d108139c0f09980e3f42", "width": 960, "height": 718}, {"url": "https://external-preview.redd.it/WXmHsjH_hc03ysnjRGgpU0BpqPuyK4yoE4aetJevFcM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02df7d26ef0ba37069425c8cd0e0d38e4aecc1f7", "width": 1080, "height": 808}], "variants": {}, "id": "cUuEsBHL5TMhrkQNJ9leYxFMOF6VgvTJ_EKHwExrn8Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17bb0fe", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bb0fe/dbt_core_and_cloud_runners_and_their_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bb0fe/dbt_core_and_cloud_runners_and_their_use_cases/", "subreddit_subscribers": 134801, "created_utc": 1697691097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ifpztnvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aotyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697629743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aotyk", "is_robot_indexable": true, "report_reasons": null, "author": "getriglad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aotyk/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing/", "subreddit_subscribers": 134801, "created_utc": 1697629743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone obtained this? How was it for difficulty? I've never used GCP before but have just completed the *\"Preparing for your Professional Data Engineer Journey\"* course so not sure how much this will help.\n\nI'm fairly proficient is Python and SQL, I have some AWS experience. Looking to get certified as I'm trying to transition to the Data Engineering space rather than python/sql all rounder and it's better to have some GCP cert than nothing.\n\nAny advice is appreciated.", "author_fullname": "t2_5iincanh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Platform - Professional Data Engineer Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17baklu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697689485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone obtained this? How was it for difficulty? I&amp;#39;ve never used GCP before but have just completed the &lt;em&gt;&amp;quot;Preparing for your Professional Data Engineer Journey&amp;quot;&lt;/em&gt; course so not sure how much this will help.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly proficient is Python and SQL, I have some AWS experience. Looking to get certified as I&amp;#39;m trying to transition to the Data Engineering space rather than python/sql all rounder and it&amp;#39;s better to have some GCP cert than nothing.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17baklu", "is_robot_indexable": true, "report_reasons": null, "author": "Willing_Excuse1652", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17baklu/google_cloud_platform_professional_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17baklu/google_cloud_platform_professional_data_engineer/", "subreddit_subscribers": 134801, "created_utc": 1697689485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a DE coding interview at Apple, can someone please share some insights about the coding round. What type of questions I can expect?", "author_fullname": "t2_udkygwua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Coding Round at Apple", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b3nbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697669053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a DE coding interview at Apple, can someone please share some insights about the coding round. What type of questions I can expect?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17b3nbo", "is_robot_indexable": true, "report_reasons": null, "author": "Anxious-Reality3258", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b3nbo/de_coding_round_at_apple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b3nbo/de_coding_round_at_apple/", "subreddit_subscribers": 134801, "created_utc": 1697669053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine you're in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.\n\nHow would you architect the analytics portion of this system? Where would you store the data?\n\nAssume we ideally want the data available in real-time, but some delay is acceptable\n\nHow would that architecture change depending on traffic needs? I'm curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that's scaling up to support tens or hundreds of millions of impressions in traffic per month.\n\n\\---\n\nThe most dead simple way would be to just store event data in a SQL database, but I assume that's bad practice as it's not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?\n\nSorry if these are dumb questions. I'm a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don't understand on a practical level how and when to actually implement this stuff.", "author_fullname": "t2_7h4yync7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to architect an analytics system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17atuwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine you&amp;#39;re in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.&lt;/p&gt;\n\n&lt;p&gt;How would you architect the analytics portion of this system? Where would you store the data?&lt;/p&gt;\n\n&lt;p&gt;Assume we ideally want the data available in real-time, but some delay is acceptable&lt;/p&gt;\n\n&lt;p&gt;How would that architecture change depending on traffic needs? I&amp;#39;m curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that&amp;#39;s scaling up to support tens or hundreds of millions of impressions in traffic per month.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The most dead simple way would be to just store event data in a SQL database, but I assume that&amp;#39;s bad practice as it&amp;#39;s not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?&lt;/p&gt;\n\n&lt;p&gt;Sorry if these are dumb questions. I&amp;#39;m a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don&amp;#39;t understand on a practical level how and when to actually implement this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17atuwj", "is_robot_indexable": true, "report_reasons": null, "author": "techworker716", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "subreddit_subscribers": 134801, "created_utc": 1697643908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking into starting some certs to help my chances of breaking into a DE r\u00f4le. I have a year of experience as a SWE and I hope to switch to DE next year. Will Azure/GCP certifications actually add any value to my applications or do companies not really care?", "author_fullname": "t2_i30nbi2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certifications make or break?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17be4im", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697703427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking into starting some certs to help my chances of breaking into a DE r\u00f4le. I have a year of experience as a SWE and I hope to switch to DE next year. Will Azure/GCP certifications actually add any value to my applications or do companies not really care?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17be4im", "is_robot_indexable": true, "report_reasons": null, "author": "iishadowsii_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17be4im/certifications_make_or_break/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17be4im/certifications_make_or_break/", "subreddit_subscribers": 134801, "created_utc": 1697703427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn't pricey, since it's not something I make money off of lol. ", "author_fullname": "t2_k4oyb5z58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Servers for Personal Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17azvdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697659289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn&amp;#39;t pricey, since it&amp;#39;s not something I make money off of lol. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17azvdl", "is_robot_indexable": true, "report_reasons": null, "author": "SoccerFilmNerd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "subreddit_subscribers": 134801, "created_utc": 1697659289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any data engineering project courses available? I come from a different background than data engineering and have completed a diploma program and Zoomcamp in DE. I'm still actively searching for data engineering jobs and could use a mentor's guidance for improving my LinkedIn profile and resume. Simultaneously, I'm eager to embark on new projects that involve the community and help me gain more experience.", "author_fullname": "t2_71nzg158", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b83bd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697681709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any data engineering project courses available? I come from a different background than data engineering and have completed a diploma program and Zoomcamp in DE. I&amp;#39;m still actively searching for data engineering jobs and could use a mentor&amp;#39;s guidance for improving my LinkedIn profile and resume. Simultaneously, I&amp;#39;m eager to embark on new projects that involve the community and help me gain more experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b83bd", "is_robot_indexable": true, "report_reasons": null, "author": "BeltUsual3130", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b83bd/data_engineering_project_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b83bd/data_engineering_project_course/", "subreddit_subscribers": 134801, "created_utc": 1697681709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've released dbt-factory which enable you to write re-usable sql code blocks and compile to a dbt project through a yaml configuration\n\n**Benefits**\n\n* Construct pipeline without knowing SQL\n* Construct pipelines without learning dbt\n* Avoid duplicative code\n* Easier automation of pipeline creation, so your code can interact with yaml instead of sql (advanced users)\n\n&amp;#x200B;\n\nLet me know if anyone has any thoughts \ud83d\ude04", "author_fullname": "t2_9jvpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt factory: re-usable sql, configured through YAML, compiled to dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b7nim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697680425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve released dbt-factory which enable you to write re-usable sql code blocks and compile to a dbt project through a yaml configuration&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Construct pipeline without knowing SQL&lt;/li&gt;\n&lt;li&gt;Construct pipelines without learning dbt&lt;/li&gt;\n&lt;li&gt;Avoid duplicative code&lt;/li&gt;\n&lt;li&gt;Easier automation of pipeline creation, so your code can interact with yaml instead of sql (advanced users)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let me know if anyone has any thoughts \ud83d\ude04&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b7nim", "is_robot_indexable": true, "report_reasons": null, "author": "conradbez", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b7nim/dbt_factory_reusable_sql_configured_through_yaml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b7nim/dbt_factory_reusable_sql_configured_through_yaml/", "subreddit_subscribers": 134801, "created_utc": 1697680425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some money I can use for a short outsourced project (~$180k)\n\nCompany profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now\n\nI started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it's time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of \"important spreadsheets\" scattered around the place. Once we have the core data stack in place, I hope we'll be able to manage better and deliver more as a very small team\n\nSAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...\n\nWhat I'm thinking is that we build a short 3-5 page request for proposal (RFP) to \"build our data stack\" without actually specifying the tech stack we want, with some high level requirements of\n\n* Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure\n* Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting \n* Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too\n* We have a preference for popular and open tools with minimal maintenance\n\nAnother direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack\n\nThe main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don't know what it is\n\nWe've just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it's not a full enough platform yet, so it's a risk for us to dive into that\n\nThoughts? Anything I can add that would be helpful?\n\nThanks very much", "author_fullname": "t2_4t8zz1xo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you spend $180k on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b0jhs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697661031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some money I can use for a short outsourced project (~$180k)&lt;/p&gt;\n\n&lt;p&gt;Company profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now&lt;/p&gt;\n\n&lt;p&gt;I started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&amp;gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it&amp;#39;s time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of &amp;quot;important spreadsheets&amp;quot; scattered around the place. Once we have the core data stack in place, I hope we&amp;#39;ll be able to manage better and deliver more as a very small team&lt;/p&gt;\n\n&lt;p&gt;SAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m thinking is that we build a short 3-5 page request for proposal (RFP) to &amp;quot;build our data stack&amp;quot; without actually specifying the tech stack we want, with some high level requirements of&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure&lt;/li&gt;\n&lt;li&gt;Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting &lt;/li&gt;\n&lt;li&gt;Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too&lt;/li&gt;\n&lt;li&gt;We have a preference for popular and open tools with minimal maintenance&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Another direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack&lt;/p&gt;\n\n&lt;p&gt;The main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don&amp;#39;t know what it is&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it&amp;#39;s not a full enough platform yet, so it&amp;#39;s a risk for us to dive into that&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Anything I can add that would be helpful?&lt;/p&gt;\n\n&lt;p&gt;Thanks very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b0jhs", "is_robot_indexable": true, "report_reasons": null, "author": "c3f7", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "subreddit_subscribers": 134801, "created_utc": 1697661031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.\n\nSo far I've been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.\n\nAfter assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn't be using Debezium as, according to him and another expert, it doesn't simply read the db's logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db's log without needing a trigger from it.\n\nI've been doing some research and it turns out that there aren't many options on the table, especially if we consider that everything has to be **on premise** and according [to this paper from Netflix](https://arxiv.org/pdf/2010.12597.pdf) Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they're quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.\n\nSo I'm wondering whether the project is actually viable or if I'm headed into a dead end.\n\nI case it hasn't already transpired I'm not really a data engineer so I'm learning as much as possible during the process.", "author_fullname": "t2_79y3zlzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Viability of a CDC project paired with Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ash74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.&lt;/p&gt;\n\n&lt;p&gt;After assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn&amp;#39;t be using Debezium as, according to him and another expert, it doesn&amp;#39;t simply read the db&amp;#39;s logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db&amp;#39;s log without needing a trigger from it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing some research and it turns out that there aren&amp;#39;t many options on the table, especially if we consider that everything has to be &lt;strong&gt;on premise&lt;/strong&gt; and according &lt;a href=\"https://arxiv.org/pdf/2010.12597.pdf\"&gt;to this paper from Netflix&lt;/a&gt; Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they&amp;#39;re quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering whether the project is actually viable or if I&amp;#39;m headed into a dead end.&lt;/p&gt;\n\n&lt;p&gt;I case it hasn&amp;#39;t already transpired I&amp;#39;m not really a data engineer so I&amp;#39;m learning as much as possible during the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ash74", "is_robot_indexable": true, "report_reasons": null, "author": "ExactTreat593", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "subreddit_subscribers": 134801, "created_utc": 1697640225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently architecting a Azure solution for a DWH  that will integrate data extracted from \\~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.\n\nFor the extraction part, I'm leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered 'externally' via a call from ADF, Synapse pipelines or Airflow.\n\nI would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!", "author_fullname": "t2_lvjbzi2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting an Azure Solution for DWH Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aqwt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697635964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently architecting a Azure solution for a DWH  that will integrate data extracted from ~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.&lt;/p&gt;\n\n&lt;p&gt;For the extraction part, I&amp;#39;m leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered &amp;#39;externally&amp;#39; via a call from ADF, Synapse pipelines or Airflow.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aqwt9", "is_robot_indexable": true, "report_reasons": null, "author": "cloclosh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "subreddit_subscribers": 134801, "created_utc": 1697635964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.\n\nMy primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? \n\nAppreciate any insight.", "author_fullname": "t2_3auynywj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance on how event data is stored in a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b2d47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.&lt;/p&gt;\n\n&lt;p&gt;My primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? &lt;/p&gt;\n\n&lt;p&gt;Appreciate any insight.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b2d47", "is_robot_indexable": true, "report_reasons": null, "author": "currentlyreadingit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "subreddit_subscribers": 134801, "created_utc": 1697665706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I'm looking for help on a seemingly simple task that has me confused.\n\nThe vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.\n\nWe would hit the APIs about once a day to get updates on these conversions. Here's my idea so far:\n\n1. Our data isn't massive, so I'm ok with sticking with python+postgres and cron.\n\n2. Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.\n\n3. I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.\n\n4. Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.\n\n5. Use the LAG function to compare the status on this run to the status from the last run.\n\n6. If the status change is relevant to us, capture that conversion and forward it.\n\nThis seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.\n\nIs there a better way to do this that is standard practice?", "author_fullname": "t2_d8wq0wkd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for capturing changes in column values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17avpdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697648615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I&amp;#39;m looking for help on a seemingly simple task that has me confused.&lt;/p&gt;\n\n&lt;p&gt;The vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.&lt;/p&gt;\n\n&lt;p&gt;We would hit the APIs about once a day to get updates on these conversions. Here&amp;#39;s my idea so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Our data isn&amp;#39;t massive, so I&amp;#39;m ok with sticking with python+postgres and cron.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use the LAG function to compare the status on this run to the status from the last run.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If the status change is relevant to us, capture that conversion and forward it.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.&lt;/p&gt;\n\n&lt;p&gt;Is there a better way to do this that is standard practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17avpdg", "is_robot_indexable": true, "report_reasons": null, "author": "fabricmoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "subreddit_subscribers": 134801, "created_utc": 1697648615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:\n\n*We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json\\_normalize to flatten the json into a tabular format) However we've had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)*\n\n**What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?**\n\nOur current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can't directly load data that doesn't match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.\n\nWe want to try and keep this as close to realtime as possible and preferably serverless", "author_fullname": "t2_48vnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Problem: help with inconsistent day to day parquet schemas (pandas, AWS lambda)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq76s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json_normalize to flatten the json into a tabular format) However we&amp;#39;ve had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can&amp;#39;t directly load data that doesn&amp;#39;t match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.&lt;/p&gt;\n\n&lt;p&gt;We want to try and keep this as close to realtime as possible and preferably serverless&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aq76s", "is_robot_indexable": true, "report_reasons": null, "author": "freerangetrousers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "subreddit_subscribers": 134801, "created_utc": 1697633939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.\n\nGiving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar\n\nI know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).\n\nAlso what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: what software should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ap7is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.&lt;/p&gt;\n\n&lt;p&gt;Giving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar&lt;/p&gt;\n\n&lt;p&gt;I know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).&lt;/p&gt;\n\n&lt;p&gt;Also what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ap7is", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "subreddit_subscribers": 134801, "created_utc": 1697630909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here use the change tracking feature of sql server? How do you usually stream the data to a datalake or dw if streaming will be coming from 30 sql servers and need atleast 10 tables per servers.\n\nIm seeing some hints for kafka but its mostly about debezium which is only capable for CDC.....spark streaming would be possible but changes will not be so many. It is like 10-30 changes every 5 minutes.\n\nMaybe there's more efficient way, hoping to hear some", "author_fullname": "t2_5g5u53hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple sql server stream to datalake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17bficq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697709507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here use the change tracking feature of sql server? How do you usually stream the data to a datalake or dw if streaming will be coming from 30 sql servers and need atleast 10 tables per servers.&lt;/p&gt;\n\n&lt;p&gt;Im seeing some hints for kafka but its mostly about debezium which is only capable for CDC.....spark streaming would be possible but changes will not be so many. It is like 10-30 changes every 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Maybe there&amp;#39;s more efficient way, hoping to hear some&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bficq", "is_robot_indexable": true, "report_reasons": null, "author": "chanchan_delier", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bficq/multiple_sql_server_stream_to_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bficq/multiple_sql_server_stream_to_datalake/", "subreddit_subscribers": 134801, "created_utc": 1697709507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need a project to apply for a junior data engineer/data analyst position. Actually, I'm not very enthusiastic about data analysis, and I'd like to start an internship that can eventually lead me more towards software engineering. Nevertheless, I need to create something in a maximum of one week that involves the use of Python, SQL, and optionally Excel and Tableau. Do you have any YouTube or GitHub links to get (more than) inspiration from?", "author_fullname": "t2_l21iqntpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PLEASE: data engineering project for portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf6gd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697708061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a project to apply for a junior data engineer/data analyst position. Actually, I&amp;#39;m not very enthusiastic about data analysis, and I&amp;#39;d like to start an internship that can eventually lead me more towards software engineering. Nevertheless, I need to create something in a maximum of one week that involves the use of Python, SQL, and optionally Excel and Tableau. Do you have any YouTube or GitHub links to get (more than) inspiration from?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bf6gd", "is_robot_indexable": true, "report_reasons": null, "author": "dullcreation", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bf6gd/please_data_engineering_project_for_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bf6gd/please_data_engineering_project_for_portfolio/", "subreddit_subscribers": 134801, "created_utc": 1697708061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to Airflow and Docker and have a best practice question regarding a production deployment.\n\nI have Airflow with Docker installed on a remote linux host. I also currently have a pyenv virtual environment with python 3.11 on the same remote host and installed Airflow's modules into it via pip.  This is a dev environment. No issues in this setup, I can create and run simple DAG's in VS Code directing python to use the interpreter in the virtual environment.\n\nI have a nagging feeling this setup is not ideal for production as the virtual environment is maintained and dependent on the local host.\n\nQuestion:  For production, should I create another Docker container just for the Airflow modules so that everything (Airflow and the python interpreter) is running through Docker?\n\nOr should I keep the current setup for production?\n\nThere are many resources available on setting up Airflow on Docker but little in way of production guidance. (re where to put your python environments and use them) Your thoughts and ideas are appreciated, thank you.", "author_fullname": "t2_3amuxhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow w/Python Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf4bh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697708457.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697707817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to Airflow and Docker and have a best practice question regarding a production deployment.&lt;/p&gt;\n\n&lt;p&gt;I have Airflow with Docker installed on a remote linux host. I also currently have a pyenv virtual environment with python 3.11 on the same remote host and installed Airflow&amp;#39;s modules into it via pip.  This is a dev environment. No issues in this setup, I can create and run simple DAG&amp;#39;s in VS Code directing python to use the interpreter in the virtual environment.&lt;/p&gt;\n\n&lt;p&gt;I have a nagging feeling this setup is not ideal for production as the virtual environment is maintained and dependent on the local host.&lt;/p&gt;\n\n&lt;p&gt;Question:  For production, should I create another Docker container just for the Airflow modules so that everything (Airflow and the python interpreter) is running through Docker?&lt;/p&gt;\n\n&lt;p&gt;Or should I keep the current setup for production?&lt;/p&gt;\n\n&lt;p&gt;There are many resources available on setting up Airflow on Docker but little in way of production guidance. (re where to put your python environments and use them) Your thoughts and ideas are appreciated, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17bf4bh", "is_robot_indexable": true, "report_reasons": null, "author": "maxafrass", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17bf4bh/airflow_wpython_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17bf4bh/airflow_wpython_best_practice/", "subreddit_subscribers": 134801, "created_utc": 1697707817.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}