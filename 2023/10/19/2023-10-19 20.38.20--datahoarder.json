{"kind": "Listing", "data": {"after": "t3_17b8szx", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8tsg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft\u2019s futuristic \u2018Project Silica\u2019 stores data on glass plates for 10,000 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17biw3w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OTMu4DVYDzt7IOU_NTd_xPOgga0OVw0Td99FjkuYGho.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697721376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pcworld.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.pcworld.com/article/2108839/microsoft-project-small-glass-pane-stores-terabytes-of-data.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?auto=webp&amp;s=2d30144a695c77b4ce4b679c90e63ac9e2726168", "width": 1024, "height": 575}, "resolutions": [{"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e5c31ec8cbd304fab589f130f0207770bb92001", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2cff8cecf4d7b324cb385da84782e4903626cc80", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac7aa8aa10743aa42a7264d6fb04b4df880e9160", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4e36b88e57979e429695bc6aac29e60cceeaf4e", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/LOhOEQD_FzssHEkpKpOsBUzLqreQRL9kfTYDRUmZ5hw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b4b1c8f211deb5b9c41e597cdfce64617cc6645", "width": 960, "height": 539}], "variants": {}, "id": "0JwP-8C3waN7Mn9jdKH4cpA3HlvlZb28KHYw6Y0TpVA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17biw3w", "is_robot_indexable": true, "report_reasons": null, "author": "Agathocles_of_Sicily", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17biw3w/microsofts_futuristic_project_silica_stores_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.pcworld.com/article/2108839/microsoft-project-small-glass-pane-stores-terabytes-of-data.html", "subreddit_subscribers": 707560, "created_utc": 1697721376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The X24 is being released", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate unleases Hard Drives with 24TB Storage Capacity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17bl6ux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DMJgkcAcw7f0iFGlNk5XI0VnNlE27dv1zZ6P2pVna4Y.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697727604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "guru3d.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The X24 is being released&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.guru3d.com/story/seagate-unleases-hard-drives-with-24tb-storage-capacity/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?auto=webp&amp;s=21d1e93c5461462e4126153f98136c6196bc4358", "width": 640, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aea5988028851f23fba764d364c2b27e95a83eb3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4219bede6df7277cc5b0523b95df08a0a6711622", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fbc49559406bf3d3a0d288b8ff603ba92970cd6", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/_AfyhiQlKYY4e7Q6hW_K-yT0fB1C6nnGQ-zaVrndChA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c74cd02526ccef47a23a48ee95a39dc22854f933", "width": 640, "height": 640}], "variants": {}, "id": "toqLiPggpSPq6boCyjgLxiAWP7ECNO785V__2oSwtkg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bl6ux", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17bl6ux/seagate_unleases_hard_drives_with_24tb_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.guru3d.com/story/seagate-unleases-hard-drives-with-24tb-storage-capacity/", "subreddit_subscribers": 707560, "created_utc": 1697727604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My common sense says no since this is not ssd but wanted to ask you? Paid 120\u20ac per disk seagate 5TB 2 5 inch drives new shipped by amazon. Arrived in that paper envelope but each of them wrapped in bubble wrap and anti static bag.", "author_fullname": "t2_1s47qqwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this how amazon ships every hdd and is this safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmzi0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/bOfVrrM_9-dPUQ6bO0hwCAcN-XMMVv0bRpaUkCJe9M8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697732333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My common sense says no since this is not ssd but wanted to ask you? Paid 120\u20ac per disk seagate 5TB 2 5 inch drives new shipped by amazon. Arrived in that paper envelope but each of them wrapped in bubble wrap and anti static bag.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9q602p6vp6vb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?auto=webp&amp;s=061fc4a240967d7376ce82d9665ee324793072cb", "width": 1125, "height": 795}, "resolutions": [{"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=09a87fdc5c246379f3436f6fcf769461eacd1c48", "width": 108, "height": 76}, {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d97b85edf77eea3137caebe1c213608ea7ffbe84", "width": 216, "height": 152}, {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=265ce1d4e2b24dd764852ed30f1fd34adfc0d5d4", "width": 320, "height": 226}, {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b21fe2dbae3e072797d12beade6efbcca589d46", "width": 640, "height": 452}, {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2b4c258653ed37412655cd42404b23e55f8c47fb", "width": 960, "height": 678}, {"url": "https://preview.redd.it/9q602p6vp6vb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=816f14acd9a3cc824615ac41065a5baf63c0c240", "width": 1080, "height": 763}], "variants": {}, "id": "AUylUixxfaT2tD0myhkHlRfXNfFEOW6Cb-phN-qAjqI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bmzi0", "is_robot_indexable": true, "report_reasons": null, "author": "nEYncI", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bmzi0/is_this_how_amazon_ships_every_hdd_and_is_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9q602p6vp6vb1.jpg", "subreddit_subscribers": 707560, "created_utc": 1697732333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4aynv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "British Museum sets out plans to digitise fully the collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bpro3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697739630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "britishmuseum.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.britishmuseum.org/sites/default/files/2023-10/British_Museum_sets_out_plans_to_digitise_fully_the_collection.pdf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bpro3", "is_robot_indexable": true, "report_reasons": null, "author": "retrac1324", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bpro3/british_museum_sets_out_plans_to_digitise_fully/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.britishmuseum.org/sites/default/files/2023-10/British_Museum_sets_out_plans_to_digitise_fully_the_collection.pdf", "subreddit_subscribers": 707560, "created_utc": 1697739630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I saw plenty of people suggesting serverpartdeal, goharddrive, or jb-computer or geizhals or, for new stuff, alternate, mindfactory or of course Amazon.\nThe problem is the first two are American, this mean that beside shipping, there is customs to factor in. Most often than not you'd have to add something like 20% more of VAT, depending on the country, making it useless. At least to me, would be just better off spending a bit more and having it new and with easier warranty to claim off Amazon or something.\nThe others are mainly German websites, that either have new stuff that cost just the same as in other countries, making it pointless, or they don't ship to other countries. In my case is Italy.\n\nWhere can someone find good deal in Europe then?", "author_fullname": "t2_k99xt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to buy certified refurbished drives in Europe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bltai", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697729216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw plenty of people suggesting serverpartdeal, goharddrive, or jb-computer or geizhals or, for new stuff, alternate, mindfactory or of course Amazon.\nThe problem is the first two are American, this mean that beside shipping, there is customs to factor in. Most often than not you&amp;#39;d have to add something like 20% more of VAT, depending on the country, making it useless. At least to me, would be just better off spending a bit more and having it new and with easier warranty to claim off Amazon or something.\nThe others are mainly German websites, that either have new stuff that cost just the same as in other countries, making it pointless, or they don&amp;#39;t ship to other countries. In my case is Italy.&lt;/p&gt;\n\n&lt;p&gt;Where can someone find good deal in Europe then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bltai", "is_robot_indexable": true, "report_reasons": null, "author": "NaXter24R", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bltai/where_to_buy_certified_refurbished_drives_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bltai/where_to_buy_certified_refurbished_drives_in/", "subreddit_subscribers": 707560, "created_utc": 1697729216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys, I always used Taiyo Yuden discs for DVD, if I remember correctly they were mostly released by JVC and maybe Verbatim.\n\nIn DVD people always said that is the best quality for the price you can buy. Is there an alternative like that for BDs?", "author_fullname": "t2_2xftem1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there Taiyo Yuden alternative for BD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bdkcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697701054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I always used Taiyo Yuden discs for DVD, if I remember correctly they were mostly released by JVC and maybe Verbatim.&lt;/p&gt;\n\n&lt;p&gt;In DVD people always said that is the best quality for the price you can buy. Is there an alternative like that for BDs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bdkcl", "is_robot_indexable": true, "report_reasons": null, "author": "Leonhardt90", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bdkcl/is_there_taiyo_yuden_alternative_for_bd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bdkcl/is_there_taiyo_yuden_alternative_for_bd/", "subreddit_subscribers": 707560, "created_utc": 1697701054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi People \ud83d\udc4b \n\nI'm setting up a unraid server. To save some money I purchased 2x6tb SAS drives and 2x 4tb SATA drives.\n\nUnfortunately my SAS drives are extremely slow. Read speed is around 7MB/s.\n\nAnyone seen this? Are the disks just bad?\n\nMy HBA is https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;mkevt=1&amp;mkrid=705-154756-20017-0&amp;ssspo=OwHu05S9RzG&amp;sssrc=4429486&amp;ssuid=4SgCEM6tSOK&amp;var=&amp;widget_ver=artemis&amp;media=COPY\n\nHere is the HDD pro results https://ibb.co/GPLtYR9\n\nCrystal Disk https://ibb.co/7WRYywq\n\nAnd some basic info https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt", "author_fullname": "t2_ezbuay12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extremely slow SAS drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bd6vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697699522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi People \ud83d\udc4b &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m setting up a unraid server. To save some money I purchased 2x6tb SAS drives and 2x 4tb SATA drives.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately my SAS drives are extremely slow. Read speed is around 7MB/s.&lt;/p&gt;\n\n&lt;p&gt;Anyone seen this? Are the disks just bad?&lt;/p&gt;\n\n&lt;p&gt;My HBA is &lt;a href=\"https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=705-154756-20017-0&amp;amp;ssspo=OwHu05S9RzG&amp;amp;sssrc=4429486&amp;amp;ssuid=4SgCEM6tSOK&amp;amp;var=&amp;amp;widget_ver=artemis&amp;amp;media=COPY\"&gt;https://www.ebay.com.au/itm/143007085943?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=705-154756-20017-0&amp;amp;ssspo=OwHu05S9RzG&amp;amp;sssrc=4429486&amp;amp;ssuid=4SgCEM6tSOK&amp;amp;var=&amp;amp;widget_ver=artemis&amp;amp;media=COPY&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is the HDD pro results &lt;a href=\"https://ibb.co/GPLtYR9\"&gt;https://ibb.co/GPLtYR9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Crystal Disk &lt;a href=\"https://ibb.co/7WRYywq\"&gt;https://ibb.co/7WRYywq&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And some basic info &lt;a href=\"https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt\"&gt;https://jmp.sh/s/wziRp1aXHSMUgiCiJAOt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?auto=webp&amp;s=885e761d5e665b5195a9ed59b18d7d53847d6798", "width": 500, "height": 375}, "resolutions": [{"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c15328787186a4041f321b27f72ffc66916bbe13", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a6a12ec3c380e81a0c4bf78f9ec6aee48194830", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ahvBxScor9u5CS8u3xy57R9o4EJHLIgVUR_QY2ABNAY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c04cb666c0f2ef6bfdc2c35e4501b09be02e1eee", "width": 320, "height": 240}], "variants": {}, "id": "wwPpmkcxUy-NVu2lDnzIkiOl8Q94uUFgYC2en_zVmZs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bd6vh", "is_robot_indexable": true, "report_reasons": null, "author": "ClownWorld11", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bd6vh/extremely_slow_sas_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bd6vh/extremely_slow_sas_drives/", "subreddit_subscribers": 707560, "created_utc": 1697699522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The site mysteriously shut down back in August/September (I forgot the actual date), and now redirects to the official GMOD Steam workshop. This renders mostly every addon on Garrysmods.org completely lost.\n\nI emailed Adam (who ran the website) if he can move everything from Garrysmods.org to a newer website, but I didn't get a reply from him. I just sent him another email as of this post.\n\nAnd by the way, I heard from someone in the FaceWAN Discord server that they have most of the Garrysmods.org archive saved onto a USB drive.", "author_fullname": "t2_haovy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Other than someone in the FaceWAN Discord server, did anyone archive every addon in Garrysmods.org? Just being curious.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bka9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697725181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The site mysteriously shut down back in August/September (I forgot the actual date), and now redirects to the official GMOD Steam workshop. This renders mostly every addon on Garrysmods.org completely lost.&lt;/p&gt;\n\n&lt;p&gt;I emailed Adam (who ran the website) if he can move everything from Garrysmods.org to a newer website, but I didn&amp;#39;t get a reply from him. I just sent him another email as of this post.&lt;/p&gt;\n\n&lt;p&gt;And by the way, I heard from someone in the FaceWAN Discord server that they have most of the Garrysmods.org archive saved onto a USB drive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bka9w", "is_robot_indexable": true, "report_reasons": null, "author": "DylanMc6", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17bka9w/other_than_someone_in_the_facewan_discord_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bka9w/other_than_someone_in_the_facewan_discord_server/", "subreddit_subscribers": 707560, "created_utc": 1697725181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As I am approaching my first 1TB of data hoarding, I thought I would look for some input from the professionals. I've considered doing it just to make moving a large number of files faster.\n\nDo you compress your folders at all?\n\nWhen do you find it is necessary to start zipping up folders?\n\nIf so, what compression format do you use? As I have been using Zip, but I have been reading 7z is a bit smaller.\n\nDo you find it even worth doing?\n\nScrew it buy another hard drive?\n\nThanks!", "author_fullname": "t2_lkpsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compressed Folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bhvk8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697718203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As I am approaching my first 1TB of data hoarding, I thought I would look for some input from the professionals. I&amp;#39;ve considered doing it just to make moving a large number of files faster.&lt;/p&gt;\n\n&lt;p&gt;Do you compress your folders at all?&lt;/p&gt;\n\n&lt;p&gt;When do you find it is necessary to start zipping up folders?&lt;/p&gt;\n\n&lt;p&gt;If so, what compression format do you use? As I have been using Zip, but I have been reading 7z is a bit smaller.&lt;/p&gt;\n\n&lt;p&gt;Do you find it even worth doing?&lt;/p&gt;\n\n&lt;p&gt;Screw it buy another hard drive?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17bhvk8", "is_robot_indexable": true, "report_reasons": null, "author": "Skulleddino", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bhvk8/compressed_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bhvk8/compressed_folders/", "subreddit_subscribers": 707560, "created_utc": 1697718203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As the title says, bdfr keeps skipping reddit hosted videos. Is there a parameter I'm missing to allow this, or at least a parameter that can log whatever I miss so I can at least do it myself?", "author_fullname": "t2_npr63", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BDFR skipping Reddit hosted videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bdjef", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697700947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, bdfr keeps skipping reddit hosted videos. Is there a parameter I&amp;#39;m missing to allow this, or at least a parameter that can log whatever I miss so I can at least do it myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bdjef", "is_robot_indexable": true, "report_reasons": null, "author": "XionXionHolix", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bdjef/bdfr_skipping_reddit_hosted_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bdjef/bdfr_skipping_reddit_hosted_videos/", "subreddit_subscribers": 707560, "created_utc": 1697700947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone!\n\nI recently ran into a spindle with about 20+ CDs that my family members had saved with family photos over the years. They all still ran great and I was able to retrieve 99% of the pictures (only lost 4 out of the entire spindle). But doing this gave me the urge to finally get a decent backup solution going for me and my close family.\n\nFor context, I'm very much a N00B lvl 1 hoarder - My current backup solution is a 1TB external HDD and it's half full. It contains all my academic work, professional/personal documents, all my music CDs, photos from 2006-2023 and gameplay recordings for when I used to do Youtube videos.\n\nI was hoping to actually follow the 3-2-1 rule, using a NAS for \"hot\" storage and Blu-ray M-DISCs for cold storage. But this is where I hit a bit of a snag and I'd like to get the community opinion on this:\n\n1) Am I making the right call with my backup solution?\n\n2) Which format should I go for off-site backups? Rip another set of Blu-rays or get a second NAS? Keep in mind that I need it to be a cost-effective solution that won't get a lot of use and will be saved in my grandparents house.\n\n3) Do you have any other suggestions, considering that I'll be dealing with my family members, none of which are tech savvy?\n\nI don't have a set budget for this, but I'd like to be as cheap as possible. For context on pricing and availability, I live in Portugal, so I can get things from Amazon ES rather easily.\n\nI was thinking about building my own \"NAS\" with an older PC and a couple of SSDs since I'd like it to be fast and quiet and I'd like to use it as a Jellyfin server on top of that, but I don't want to push my luck in case of the drives fail or I want to expand in the future.\n\nHoping to hear everyone's feedback and thanks for reading :D", "author_fullname": "t2_4j1a8efu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Noob hoarder looking for a simple, cheap backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b8by2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697682433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I recently ran into a spindle with about 20+ CDs that my family members had saved with family photos over the years. They all still ran great and I was able to retrieve 99% of the pictures (only lost 4 out of the entire spindle). But doing this gave me the urge to finally get a decent backup solution going for me and my close family.&lt;/p&gt;\n\n&lt;p&gt;For context, I&amp;#39;m very much a N00B lvl 1 hoarder - My current backup solution is a 1TB external HDD and it&amp;#39;s half full. It contains all my academic work, professional/personal documents, all my music CDs, photos from 2006-2023 and gameplay recordings for when I used to do Youtube videos.&lt;/p&gt;\n\n&lt;p&gt;I was hoping to actually follow the 3-2-1 rule, using a NAS for &amp;quot;hot&amp;quot; storage and Blu-ray M-DISCs for cold storage. But this is where I hit a bit of a snag and I&amp;#39;d like to get the community opinion on this:&lt;/p&gt;\n\n&lt;p&gt;1) Am I making the right call with my backup solution?&lt;/p&gt;\n\n&lt;p&gt;2) Which format should I go for off-site backups? Rip another set of Blu-rays or get a second NAS? Keep in mind that I need it to be a cost-effective solution that won&amp;#39;t get a lot of use and will be saved in my grandparents house.&lt;/p&gt;\n\n&lt;p&gt;3) Do you have any other suggestions, considering that I&amp;#39;ll be dealing with my family members, none of which are tech savvy?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have a set budget for this, but I&amp;#39;d like to be as cheap as possible. For context on pricing and availability, I live in Portugal, so I can get things from Amazon ES rather easily.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about building my own &amp;quot;NAS&amp;quot; with an older PC and a couple of SSDs since I&amp;#39;d like it to be fast and quiet and I&amp;#39;d like to use it as a Jellyfin server on top of that, but I don&amp;#39;t want to push my luck in case of the drives fail or I want to expand in the future.&lt;/p&gt;\n\n&lt;p&gt;Hoping to hear everyone&amp;#39;s feedback and thanks for reading :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b8by2", "is_robot_indexable": true, "report_reasons": null, "author": "Unlevshed", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b8by2/noob_hoarder_looking_for_a_simple_cheap_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b8by2/noob_hoarder_looking_for_a_simple_cheap_backup/", "subreddit_subscribers": 707560, "created_utc": 1697682433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've used dupeguru, which is the best one, but I'd like something I can use from the CLI that doesn't take up as much RAM. My main requirements are use of reference folder from which no duplicates will be flagged. I'd also like the option of NOT flagging dupes if the names are different. And of moving dupes to a destination folder rather than deleted.\nAny ideas?", "author_fullname": "t2_4tyix39s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CLI Dedupe with Reference folder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b2555", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used dupeguru, which is the best one, but I&amp;#39;d like something I can use from the CLI that doesn&amp;#39;t take up as much RAM. My main requirements are use of reference folder from which no duplicates will be flagged. I&amp;#39;d also like the option of NOT flagging dupes if the names are different. And of moving dupes to a destination folder rather than deleted.\nAny ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b2555", "is_robot_indexable": true, "report_reasons": null, "author": "ericlindellnyc", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b2555/cli_dedupe_with_reference_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b2555/cli_dedupe_with_reference_folder/", "subreddit_subscribers": 707560, "created_utc": 1697665139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nThis is my own program, I released this update to provide a couple of fixes.\n\n* Resolved an issue where files hashed directly  inside a volume root were stored with the drive letter, so the .lfhash file contained their full file path. This means that the files could not be verified if moved; or if copied, the old, still-existing version would be verified rather than the new one.\n* Disk type detection is now functional for disks containing multiple data volumes.\n* Resolved NaN progress reported when hashing empty files.\n\nAnyone who is currently using the program will see the update notification when they next run it.\n\nThe purpose of this program is to store checksums next to your files. This is most useful with backups. You can right-click files, folders, or volumes, and generate checksums for them. You can double-click the resulting checksum file to verify.\n\nOfficial website and Download: [https://www.liamfoot.com/lfh](https://www.liamfoot.com/lfh)\n\nDirect download link: [https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0](https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0)", "author_fullname": "t2_h62v4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LiamFootHash v1.3 - A functional and very fast checksumming program for Windows.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bor8y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697737033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;This is my own program, I released this update to provide a couple of fixes.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Resolved an issue where files hashed directly  inside a volume root were stored with the drive letter, so the .lfhash file contained their full file path. This means that the files could not be verified if moved; or if copied, the old, still-existing version would be verified rather than the new one.&lt;/li&gt;\n&lt;li&gt;Disk type detection is now functional for disks containing multiple data volumes.&lt;/li&gt;\n&lt;li&gt;Resolved NaN progress reported when hashing empty files.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone who is currently using the program will see the update notification when they next run it.&lt;/p&gt;\n\n&lt;p&gt;The purpose of this program is to store checksums next to your files. This is most useful with backups. You can right-click files, folders, or volumes, and generate checksums for them. You can double-click the resulting checksum file to verify.&lt;/p&gt;\n\n&lt;p&gt;Official website and Download: &lt;a href=\"https://www.liamfoot.com/lfh\"&gt;https://www.liamfoot.com/lfh&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Direct download link: &lt;a href=\"https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0\"&gt;https://www.liamfoot.com/GetDownload/LiamFootHash/1.3.0.0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bor8y", "is_robot_indexable": true, "report_reasons": null, "author": "Liam2349", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bor8y/liamfoothash_v13_a_functional_and_very_fast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bor8y/liamfoothash_v13_a_functional_and_very_fast/", "subreddit_subscribers": 707560, "created_utc": 1697737033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For cold storage, I got a pack of 50 Blu-rays, all rewritable... but when I tried again to find more, all sellers only had BD-R. One even said it's getting harder to find any BD-RE (rewritable discs). Is this true? How hard it is to find these everywhere?\n\nAlso, can I expect some issues, even if I store all of them correctly, compared to BD-R? I don't like the idea of having that ammount of data in a way that can't be modified a single bit.", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't find BD-RE, and even if I do, should I opt for this instead of BD-R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bmsfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697731816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For cold storage, I got a pack of 50 Blu-rays, all rewritable... but when I tried again to find more, all sellers only had BD-R. One even said it&amp;#39;s getting harder to find any BD-RE (rewritable discs). Is this true? How hard it is to find these everywhere?&lt;/p&gt;\n\n&lt;p&gt;Also, can I expect some issues, even if I store all of them correctly, compared to BD-R? I don&amp;#39;t like the idea of having that ammount of data in a way that can&amp;#39;t be modified a single bit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bmsfx", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bmsfx/cant_find_bdre_and_even_if_i_do_should_i_opt_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bmsfx/cant_find_bdre_and_even_if_i_do_should_i_opt_for/", "subreddit_subscribers": 707560, "created_utc": 1697731816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I've been reading for weeks now on Storage Spaces and the issues most are talking about regarding parity and sluggish speeds.  Inerleave, columns, etc.  Though I have managed to make sense of (most) of the info I have read, I'm still looking for feedback before formatting my new array.\n\nI have 12x 12TB drives (spinning HHDs) all connected to my Windows Server 2022 server via SATA.\n\nI want 1 storage space, 1 vdisk, 1 volume.  This is for a personal file sharing system at home for pictures, movies, music, etc. with very little bandwidth demand (1 or 2 simultaneous clients for plex and such).\n\nI'm looking for feedback as to what people would do in this situation: refs or ntfs?  # of columns?  interleave?  I was thinking 2 drives for parity.  Make sense for a 12 disk array?  Even though I don't really need max speed in read or write, it's obviously a plus if I can get some good throughput.\n\nAny feedback, good or bad, is very welcome!  ", "author_fullname": "t2_g5xnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Storage Spaces advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b50l1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697672789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been reading for weeks now on Storage Spaces and the issues most are talking about regarding parity and sluggish speeds.  Inerleave, columns, etc.  Though I have managed to make sense of (most) of the info I have read, I&amp;#39;m still looking for feedback before formatting my new array.&lt;/p&gt;\n\n&lt;p&gt;I have 12x 12TB drives (spinning HHDs) all connected to my Windows Server 2022 server via SATA.&lt;/p&gt;\n\n&lt;p&gt;I want 1 storage space, 1 vdisk, 1 volume.  This is for a personal file sharing system at home for pictures, movies, music, etc. with very little bandwidth demand (1 or 2 simultaneous clients for plex and such).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for feedback as to what people would do in this situation: refs or ntfs?  # of columns?  interleave?  I was thinking 2 drives for parity.  Make sense for a 12 disk array?  Even though I don&amp;#39;t really need max speed in read or write, it&amp;#39;s obviously a plus if I can get some good throughput.&lt;/p&gt;\n\n&lt;p&gt;Any feedback, good or bad, is very welcome!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b50l1", "is_robot_indexable": true, "report_reasons": null, "author": "unicorn-boner", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b50l1/windows_storage_spaces_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b50l1/windows_storage_spaces_advice/", "subreddit_subscribers": 707560, "created_utc": 1697672789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm a CG artist that has lots of large simulation caches and asset files I need to back up for medium term storage, what would you recommend? SanDisk Pro G-RAID Shuttle 4 80TB vs. OWC Thunderbay 8 160TB?\n\nI'm also open to other options, I'm currently using the Synology DiskStation 8 bay NAS with a RAID config of about 80TB but it's honestly not really suited for the job of storing large asset data that I need to re-pull every so often. \n\nI need at LEAST 80TB, but would prefer more, ideally something that's as little maintenance as possible. I spend enough time troubleshooting Maya I don't want to worry about dealing with a real server (at least for my home setup).\n\nwhat do y'all think?", "author_fullname": "t2_rey6i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best options for storing accessible large simulation cache data? SD G-RAID Shuttle 4 vs. OWC Thunderbay 8?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b3y0o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697669840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a CG artist that has lots of large simulation caches and asset files I need to back up for medium term storage, what would you recommend? SanDisk Pro G-RAID Shuttle 4 80TB vs. OWC Thunderbay 8 160TB?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also open to other options, I&amp;#39;m currently using the Synology DiskStation 8 bay NAS with a RAID config of about 80TB but it&amp;#39;s honestly not really suited for the job of storing large asset data that I need to re-pull every so often. &lt;/p&gt;\n\n&lt;p&gt;I need at LEAST 80TB, but would prefer more, ideally something that&amp;#39;s as little maintenance as possible. I spend enough time troubleshooting Maya I don&amp;#39;t want to worry about dealing with a real server (at least for my home setup).&lt;/p&gt;\n\n&lt;p&gt;what do y&amp;#39;all think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b3y0o", "is_robot_indexable": true, "report_reasons": null, "author": "nathan_sam", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b3y0o/best_options_for_storing_accessible_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b3y0o/best_options_for_storing_accessible_large/", "subreddit_subscribers": 707560, "created_utc": 1697669840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ue2y1zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1.5 tb micro sd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_17bs4h0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HpF7IjCC6cviYQ_NfgUoHq6JtXqBTqC0UMYmm3Oq75I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697745687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ppv6gw7kt7vb1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?auto=webp&amp;s=d8e7e2955449fadd11dad352017cf8ad5c215f9f", "width": 1080, "height": 2400}, "resolutions": [{"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3385996aecc984748fa07e995cff187f342b009", "width": 108, "height": 216}, {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=56575525a22031ff8fff1f98783dd2374a520591", "width": 216, "height": 432}, {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbd8e0253ddd63ec749f4b56028e2a75a69a69e6", "width": 320, "height": 640}, {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=18778a518afad3cd378156590bed552e2d9ea3e2", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a300daf5a61fd519e26a54d7aa0cf3796c7920d", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/ppv6gw7kt7vb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a1c35978946bfaea0cee65116763f6309501c41d", "width": 1080, "height": 2160}], "variants": {}, "id": "Nwhjev-AnL1LbxKVu__oWVZHE7rWFbrWW1mooxyF7kA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bs4h0", "is_robot_indexable": true, "report_reasons": null, "author": "teemo03", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bs4h0/15_tb_micro_sd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ppv6gw7kt7vb1.png", "subreddit_subscribers": 707560, "created_utc": 1697745687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI am currently hosting a bunch of things on my Gigabyte brix  (jellyfin, vaultwarden, pihole, openHAB, paperless, Hyperion, \u2026)\n\nAdditionally in am storing all my pictures there.\n\nEverything is stored on an attached 4TB external HDD.\n\nNow as I start to get worried about data loss and I am missing a bit of power (to host Immich for example) I am considering updating my system to an Intel NUC or similar,\n\nHowever as HDD are pretty expensive I am unsure how to proceed with the storage. My first thought was to run everything as raidz2 but I quickly noticed that storage space is super expensive and 4x18TB HDD would set me back easily 1000\u20ac. As far as I understand I can\u2019t have a raidz and use different HDD sizes and can also not really add HDD on the go (as long as they have different sizes)\n\nNow I was thinking to have a solution that is scalable and buy additional storage when needed, so currently my idea is to run a raidz1 with 3x8tb (or 3x6tb) for the very important data like the paperless data and the pictures and additionally just start with one 18TB standalone HDD for the media for jellyfin and add more when needed.\n\nDoes anyone have any other smart ideas how to organize this? It would be great to have some kind of data protection for the media too, but I feel in doubt this is the stuff I could afford to loose cause I can reobtain it\n\nHow are you guys organizing your drives?\n\nEdit: to add and be clear, I am fully aware that a NAS is not a backup solution. I already have a 3-2-1 backup solution in place for the most important stuff, I just want even more reliability (and capacity) on the internal ones withouth breaking the bank", "author_fullname": "t2_61dr7f6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD setup for NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17brtch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697746566.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697744901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am currently hosting a bunch of things on my Gigabyte brix  (jellyfin, vaultwarden, pihole, openHAB, paperless, Hyperion, \u2026)&lt;/p&gt;\n\n&lt;p&gt;Additionally in am storing all my pictures there.&lt;/p&gt;\n\n&lt;p&gt;Everything is stored on an attached 4TB external HDD.&lt;/p&gt;\n\n&lt;p&gt;Now as I start to get worried about data loss and I am missing a bit of power (to host Immich for example) I am considering updating my system to an Intel NUC or similar,&lt;/p&gt;\n\n&lt;p&gt;However as HDD are pretty expensive I am unsure how to proceed with the storage. My first thought was to run everything as raidz2 but I quickly noticed that storage space is super expensive and 4x18TB HDD would set me back easily 1000\u20ac. As far as I understand I can\u2019t have a raidz and use different HDD sizes and can also not really add HDD on the go (as long as they have different sizes)&lt;/p&gt;\n\n&lt;p&gt;Now I was thinking to have a solution that is scalable and buy additional storage when needed, so currently my idea is to run a raidz1 with 3x8tb (or 3x6tb) for the very important data like the paperless data and the pictures and additionally just start with one 18TB standalone HDD for the media for jellyfin and add more when needed.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other smart ideas how to organize this? It would be great to have some kind of data protection for the media too, but I feel in doubt this is the stuff I could afford to loose cause I can reobtain it&lt;/p&gt;\n\n&lt;p&gt;How are you guys organizing your drives?&lt;/p&gt;\n\n&lt;p&gt;Edit: to add and be clear, I am fully aware that a NAS is not a backup solution. I already have a 3-2-1 backup solution in place for the most important stuff, I just want even more reliability (and capacity) on the internal ones withouth breaking the bank&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17brtch", "is_robot_indexable": true, "report_reasons": null, "author": "Narrow_Smoke", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17brtch/hdd_setup_for_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17brtch/hdd_setup_for_nas/", "subreddit_subscribers": 707560, "created_utc": 1697744901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "They say it costs $.006 per GB, so 200GB would be $1.20/month?   Is there a minimum?\n\nDo they have restrictions on what gets uploaded?\n\nAre my uploads private?  Can they see what is on their servers or just me?\n\nThanks.", "author_fullname": "t2_t44bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BackBlaze B2 questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17brqrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697744717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They say it costs $.006 per GB, so 200GB would be $1.20/month?   Is there a minimum?&lt;/p&gt;\n\n&lt;p&gt;Do they have restrictions on what gets uploaded?&lt;/p&gt;\n\n&lt;p&gt;Are my uploads private?  Can they see what is on their servers or just me?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17brqrp", "is_robot_indexable": true, "report_reasons": null, "author": "818sfv", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17brqrp/backblaze_b2_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17brqrp/backblaze_b2_questions/", "subreddit_subscribers": 707560, "created_utc": 1697744717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just recently 2 HDDs I had(1 4TB WD Elements and 1 3TB Seagate) stopped working suddenly, and since I can't find a way to mount them on my PCs anymore, I had to consider their data fully lost. This made me realize the importance of having data backed up in at least one additional HDD, since years of my life were stolen from this event, and now I'm trying to back up a set of HDDs I have. My problem is that Copy/Pasting from Windows seems like an slow process, and also one that requires having my PC on, which limits the amount of data I can backup per day(I only keep my PC on 5hr per day due high energy consumption).\n\nMy question is: What's the most efficient(Or fastest) way to backup, TBs in size, HDDs? As a side note, while I can't have my PC on that long, I've a RPi 4 that is on 24/7, so if there is any software for it or a device that I can connect to it and do the trick, would be better.", "author_fullname": "t2_13y5jf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most efficient/fastest way to clone/backup data from HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bpzww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697740215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently 2 HDDs I had(1 4TB WD Elements and 1 3TB Seagate) stopped working suddenly, and since I can&amp;#39;t find a way to mount them on my PCs anymore, I had to consider their data fully lost. This made me realize the importance of having data backed up in at least one additional HDD, since years of my life were stolen from this event, and now I&amp;#39;m trying to back up a set of HDDs I have. My problem is that Copy/Pasting from Windows seems like an slow process, and also one that requires having my PC on, which limits the amount of data I can backup per day(I only keep my PC on 5hr per day due high energy consumption).&lt;/p&gt;\n\n&lt;p&gt;My question is: What&amp;#39;s the most efficient(Or fastest) way to backup, TBs in size, HDDs? As a side note, while I can&amp;#39;t have my PC on that long, I&amp;#39;ve a RPi 4 that is on 24/7, so if there is any software for it or a device that I can connect to it and do the trick, would be better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bpzww", "is_robot_indexable": true, "report_reasons": null, "author": "NoirSkell", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bpzww/most_efficientfastest_way_to_clonebackup_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bpzww/most_efficientfastest_way_to_clonebackup_data/", "subreddit_subscribers": 707560, "created_utc": 1697740215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Am in a unique situation where I want to be able to store a file indefitnely but also make it a pain for me to access. How would y\u2019all personally go about this ?", "author_fullname": "t2_b8frcjktn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store a file but make it hard for the user to access ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bk3mq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697724674.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am in a unique situation where I want to be able to store a file indefitnely but also make it a pain for me to access. How would y\u2019all personally go about this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bk3mq", "is_robot_indexable": true, "report_reasons": null, "author": "Olympus-RED", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bk3mq/store_a_file_but_make_it_hard_for_the_user_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bk3mq/store_a_file_but_make_it_hard_for_the_user_to/", "subreddit_subscribers": 707560, "created_utc": 1697724674.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey,\n\nForgive me if it's been answered already, didn't manage to find such topics unfortunately.\n\nWhat do you think would be the best backup strategy to backup let's say 1.1TB of data with 2TB hard drive (so less than 2x space)? I'm looking to minimize time spent for backup creation and space usage.\n\nI was using incremental backups before, the problem is that during the consolidation, there's a moment when you need 2x the space for the backup copy and the drive is becoming too small for that.\n\nShould I just delete old backup and do only full backups all the time?\n\nOr continue with incremental backups and when drive is full delete everything and start a new chain?\n\nMaybe some other ideas? Ideally I'd like to automate it, instead of manually watching how much space is left on the drive.\n\nThanks :)", "author_fullname": "t2_1qkh42fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup policy with small backup drive space (Veeam)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bjgpl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697722985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;Forgive me if it&amp;#39;s been answered already, didn&amp;#39;t manage to find such topics unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What do you think would be the best backup strategy to backup let&amp;#39;s say 1.1TB of data with 2TB hard drive (so less than 2x space)? I&amp;#39;m looking to minimize time spent for backup creation and space usage.&lt;/p&gt;\n\n&lt;p&gt;I was using incremental backups before, the problem is that during the consolidation, there&amp;#39;s a moment when you need 2x the space for the backup copy and the drive is becoming too small for that.&lt;/p&gt;\n\n&lt;p&gt;Should I just delete old backup and do only full backups all the time?&lt;/p&gt;\n\n&lt;p&gt;Or continue with incremental backups and when drive is full delete everything and start a new chain?&lt;/p&gt;\n\n&lt;p&gt;Maybe some other ideas? Ideally I&amp;#39;d like to automate it, instead of manually watching how much space is left on the drive.&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bjgpl", "is_robot_indexable": true, "report_reasons": null, "author": "Pegietix", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bjgpl/backup_policy_with_small_backup_drive_space_veeam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bjgpl/backup_policy_with_small_backup_drive_space_veeam/", "subreddit_subscribers": 707560, "created_utc": 1697722985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Includes google maps and matterport.", "author_fullname": "t2_dh4hjpj4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you download and view 3d Virtual tours offline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17bf8fi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697708303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Includes google maps and matterport.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17bf8fi", "is_robot_indexable": true, "report_reasons": null, "author": "__Acedia_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17bf8fi/how_do_you_download_and_view_3d_virtual_tours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17bf8fi/how_do_you_download_and_view_3d_virtual_tours/", "subreddit_subscribers": 707560, "created_utc": 1697708303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been thinking of keeping an archive of tweets or at least the images/media from my account but don\u2019t know what to use? Had an old Chrome extension that did it for media but apparently that is defunct now. What would be the recommended programs or extension for that now?", "author_fullname": "t2_s7xsyapd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to downloading posts on Twitter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b96b5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697684988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been thinking of keeping an archive of tweets or at least the images/media from my account but don\u2019t know what to use? Had an old Chrome extension that did it for media but apparently that is defunct now. What would be the recommended programs or extension for that now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b96b5", "is_robot_indexable": true, "report_reasons": null, "author": "ElysiumRailgun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b96b5/alternatives_to_downloading_posts_on_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b96b5/alternatives_to_downloading_posts_on_twitter/", "subreddit_subscribers": 707560, "created_utc": 1697684988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All, Sorry I am new to NAS and still learning.\n\nLong story short. I had two NAS devices. BOTH DIED after a power surge and the power strip didn't do it's job. How can i recover the data on the discs since the NAS devices are over 10 years old.  I think they are Linux based so could I connect each SATA drive to SATA to USB deal and use software to recover it? I'm on WIN 7 if that helps. Do I need to install Linux or something as a second OS? I'm completely lost on this. please help.\n\nThank you in advance for your assistance. \n\nAll the best.... The learner.  ", "author_fullname": "t2_d2kjjv46", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS HELP PLEASE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b8szx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697683851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, Sorry I am new to NAS and still learning.&lt;/p&gt;\n\n&lt;p&gt;Long story short. I had two NAS devices. BOTH DIED after a power surge and the power strip didn&amp;#39;t do it&amp;#39;s job. How can i recover the data on the discs since the NAS devices are over 10 years old.  I think they are Linux based so could I connect each SATA drive to SATA to USB deal and use software to recover it? I&amp;#39;m on WIN 7 if that helps. Do I need to install Linux or something as a second OS? I&amp;#39;m completely lost on this. please help.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your assistance. &lt;/p&gt;\n\n&lt;p&gt;All the best.... The learner.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17b8szx", "is_robot_indexable": true, "report_reasons": null, "author": "AnyJuan23", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17b8szx/nas_help_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17b8szx/nas_help_please/", "subreddit_subscribers": 707560, "created_utc": 1697683851.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}