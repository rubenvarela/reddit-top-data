{"kind": "Listing", "data": {"after": "t3_17b2445", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?\n\nWould love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.\n\nTrying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have you seen any examples of \u201cserious\u201d companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17asnwh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear from other DEs that serve data to &lt;strong&gt;pro-code&lt;/strong&gt; visualization tools like Shiny, Dash, or D3.js.&lt;/p&gt;\n\n&lt;p&gt;Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it\u2019s just hobbyists and companies that can\u2019t afford Tableau/PBI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17asnwh", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 147, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/", "subreddit_subscribers": 134759, "created_utc": 1697640732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.\n\n&amp;#x200B;\n\nI frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.\n\n&amp;#x200B;\n\nAfter conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.\n\n&amp;#x200B;\n\nYou can try it out at [http://sqlvisual.net](http://sqlvisual.net) without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_la0ljviui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Make SQL easy to understand with sqlvisual", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17au1n7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697644390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As an experienced data developer with 7 years of experience, I often find myself dealing with lengthy SQL queries. When I review my previous SQL code or take over projects from colleagues, understanding the SQL can be quite challenging. I believe you may have experienced the same frustrations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I frequently pondered how beneficial it would be to visualize the information within SQL queries. After all, a digram is worth a thousand words. With visual representations, we can easily identify joins, WHERE conditions, and GROUP BY clauses.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After conducting extensive research, I developed SqlVisual, a tool that generates visual representations from SQL queries. As you hover over nodes in the generated graphics, the corresponding SQL code will be highlighted.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can try it out at &lt;a href=\"http://sqlvisual.net\"&gt;http://sqlvisual.net&lt;/a&gt; without providing schema information. Simply copy and paste your SQL code. I would greatly appreciate any feedback you may have.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17au1n7", "is_robot_indexable": true, "report_reasons": null, "author": "glxrun", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17au1n7/make_sql_easy_to_understand_with_sqlvisual/", "subreddit_subscribers": 134759, "created_utc": 1697644390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.\n\nThey announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d\n\nNot sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).", "author_fullname": "t2_nexbbb26", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My company is implementing stack ranking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b1zaw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697664710.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a company with 200+ data engineers, and a new Head of HR was brought in a few months.&lt;/p&gt;\n\n&lt;p&gt;They announced last week that in the Q1, we will be measuring individual performance using stack ranking with the intention to \u201csupport equitable and diverse career growth.\u201d&lt;/p&gt;\n\n&lt;p&gt;Not sure if I\u2019m crazy, but this is gonna be rough, right?  Everything I\u2019ve read about it suggests that stack ranking creates strong incentives that outright harm collaboration (which is a current strength at my company).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b1zaw", "is_robot_indexable": true, "report_reasons": null, "author": "keep_it_professional", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b1zaw/my_company_is_implementing_stack_ranking/", "subreddit_subscribers": 134759, "created_utc": 1697664710.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.\n\nIt can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.\n\nPyGWalker Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)\n\n[pygwalker in juypter lab](https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;format=png&amp;auto=webp&amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd)\n\nA simple example of how to use pygwalker\n\n    import pygwalker as pyg\n    import pandas as pd\n    \n    df = pd.read_csv(\"you_data\")\n    \n    # then pass it to pygwalker\n    pyg.walk(df)", "author_fullname": "t2_dnzigfn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PyGWalker: a Python library for data engineer that turns your dataframe into tableau-like data app.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lor544wm82vb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/lor544wm82vb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce7e7c2e461b8c5f8a62fa4fc0ec9dfd6ace2c3e"}, {"y": 122, "x": 216, "u": "https://preview.redd.it/lor544wm82vb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee7e80217d9179cba847be4f8db9894125b634c"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/lor544wm82vb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=295a8a44847cbb4645e996e87ca86f66bf3cf360"}, {"y": 361, "x": 640, "u": "https://preview.redd.it/lor544wm82vb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6c1a63ea82817d913b5141976ce8757419705c0c"}, {"y": 542, "x": 960, "u": "https://preview.redd.it/lor544wm82vb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=00a6472b88d633baf9f30cc48639ca51854b9463"}, {"y": 610, "x": 1080, "u": "https://preview.redd.it/lor544wm82vb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=38271e7d4e710122f24a3fc973486ac483cc23cc"}], "s": {"y": 1696, "x": 3002, "u": "https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;format=png&amp;auto=webp&amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd"}, "id": "lor544wm82vb1"}}, "name": "t3_17b6wdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/d1ctUbcbuJYjdb3IUb1F2qoxEKrfIsQL_tCBj6-pC_I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697678204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.&lt;/p&gt;\n\n&lt;p&gt;It can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.&lt;/p&gt;\n\n&lt;p&gt;PyGWalker Github: &lt;a href=\"https://github.com/Kanaries/pygwalker\"&gt;https://github.com/Kanaries/pygwalker&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lor544wm82vb1.png?width=3002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd\"&gt;pygwalker in juypter lab&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A simple example of how to use pygwalker&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import pygwalker as pyg\nimport pandas as pd\n\ndf = pd.read_csv(&amp;quot;you_data&amp;quot;)\n\n# then pass it to pygwalker\npyg.walk(df)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17b6wdl", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden_Beginning_597", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b6wdl/pygwalker_a_python_library_for_data_engineer_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b6wdl/pygwalker_a_python_library_for_data_engineer_that/", "subreddit_subscribers": 134759, "created_utc": 1697678204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. \n\nWe are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. \n\nWe are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.   \nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.   \nOur goal is to work on anonymized data in the DEV &amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. \n\nI have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it's not even necessary (for at least the next 6 months).\n\nNow we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. \n\nI am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn't seem required to us). \n\nI understand that the Unity Catalog is the \"new hot thing\" that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? ", "author_fullname": "t2_5iao6str", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks - Unity Catalog worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17app3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697632428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a cloud data platform in Azure in a tightly secured IT environment (financial institution). Everything is locked up and you need to design/document/get approval/wait for a couple of weeks until you can get anything done. &lt;/p&gt;\n\n&lt;p&gt;We are primarily doing Data Warehousing on a SQL Server on-premise (also moving to a managed instance in Azure), but we now also want to discover the power of Databricks with our data lake. &lt;/p&gt;\n\n&lt;p&gt;We are a team of around 3-5 data engineers. All of us get the same access to the relevant part of the data lake.&lt;br/&gt;\nThe source data in our data lakes (one per environment) is actually the same, but the different environments have different anonymization applied.&lt;br/&gt;\nOur goal is to work on anonymized data in the DEV &amp;amp; TEST environments, work on creating tested pipelines and use CI/CD to push them to the next environments into production. Due the data lakes in the different environments having the same source data and structure, we believe this can work. &lt;/p&gt;\n\n&lt;p&gt;I have experience in the last years using Databricks without the Unity Catalog: the data lakes are mounted to the Databricks instance, we develop notebooks that are checked in and easily deployed to the next environments (+ we orchestrate using Azure Data Factory).  I loved this approach and found it very easy to set up and get someone to work in it. Note that this also allows the use of plain old pandas to be used for less demanding workloads (IMO the development experience outweighs replacing this for another service). The Hive metastore can be used to create SQL endpoints, however, as the data will mostly be pushed to the SQL MI as a serving layer, it&amp;#39;s not even necessary (for at least the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;Now we have been in calls with Service Providers and Databricks architects and (of course) they are pushing us to use the Unity Catalog. &lt;/p&gt;\n\n&lt;p&gt;I am very much in doubt whether the benefits of the metastore will outweight the extra work that will be required to set up both this technical architecture and the additional complexity of governance that the metastore brings (which doesn&amp;#39;t seem required to us). &lt;/p&gt;\n\n&lt;p&gt;I understand that the Unity Catalog is the &amp;quot;new hot thing&amp;quot; that service providers and Databricks might be pushing due to a conflict of interest. What is your take on this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17app3b", "is_robot_indexable": true, "report_reasons": null, "author": "Far-String829", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17app3b/databricks_unity_catalog_worth_it/", "subreddit_subscribers": 134759, "created_utc": 1697632428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ifpztnvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software engineering is about thinking, not typing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aotyk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1697629743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jordankaye.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jordankaye.dev/posts/thinking-not-typing/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17aotyk", "is_robot_indexable": true, "report_reasons": null, "author": "getriglad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aotyk/software_engineering_is_about_thinking_not_typing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jordankaye.dev/posts/thinking-not-typing/", "subreddit_subscribers": 134759, "created_utc": 1697629743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to learn more about data engineering (architecture, pipelines, etc.), but I don't just wan to spin up Python/SQL and solve problems for the sake of solving them. I want to potentially build something that I can passively make income from in the long term. \n\nI know there is a skill gap between data engineering and web app development, but is there any better thing to work on to develop my skills and potentially a steady income stream than a web app?\n\nI have an idea for a web app that would likely be very API heavy or might require a great amount of data scraping which would then need to be processed and organized.", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is creating a web app the best way to expand data engineering knowledge while possibly building the foundation of a sellable product?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b730n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697678757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to learn more about data engineering (architecture, pipelines, etc.), but I don&amp;#39;t just wan to spin up Python/SQL and solve problems for the sake of solving them. I want to potentially build something that I can passively make income from in the long term. &lt;/p&gt;\n\n&lt;p&gt;I know there is a skill gap between data engineering and web app development, but is there any better thing to work on to develop my skills and potentially a steady income stream than a web app?&lt;/p&gt;\n\n&lt;p&gt;I have an idea for a web app that would likely be very API heavy or might require a great amount of data scraping which would then need to be processed and organized.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b730n", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b730n/is_creating_a_web_app_the_best_way_to_expand_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b730n/is_creating_a_web_app_the_best_way_to_expand_data/", "subreddit_subscribers": 134759, "created_utc": 1697678757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Imagine you're in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.\n\nHow would you architect the analytics portion of this system? Where would you store the data?\n\nAssume we ideally want the data available in real-time, but some delay is acceptable\n\nHow would that architecture change depending on traffic needs? I'm curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that's scaling up to support tens or hundreds of millions of impressions in traffic per month.\n\n\\---\n\nThe most dead simple way would be to just store event data in a SQL database, but I assume that's bad practice as it's not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?\n\nSorry if these are dumb questions. I'm a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don't understand on a practical level how and when to actually implement this stuff.", "author_fullname": "t2_7h4yync7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to architect an analytics system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17atuwj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697643908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine you&amp;#39;re in charge of  a web application that enables users to generate their own content, share it, and view the analytics on that content. This means tracking metrics like number of impressions, and being able to filter by geolocation, device, referral source, etc. - typical analytics stuff. There should also be support for tracking custom events.&lt;/p&gt;\n\n&lt;p&gt;How would you architect the analytics portion of this system? Where would you store the data?&lt;/p&gt;\n\n&lt;p&gt;Assume we ideally want the data available in real-time, but some delay is acceptable&lt;/p&gt;\n\n&lt;p&gt;How would that architecture change depending on traffic needs? I&amp;#39;m curious what the architecture should like like for the early stage startup building an MVP, as well as the fast-growing startup that&amp;#39;s scaling up to support tens or hundreds of millions of impressions in traffic per month.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;The most dead simple way would be to just store event data in a SQL database, but I assume that&amp;#39;s bad practice as it&amp;#39;s not scalable and expensive. But at what point would this not scale, and what would be the better way to store that event data?&lt;/p&gt;\n\n&lt;p&gt;Sorry if these are dumb questions. I&amp;#39;m a noob trying to improve my understanding of this stuff. I have a cursory knowledge of data warehouse, data lakes, and lambda architecture from reading up on them, but I don&amp;#39;t understand on a practical level how and when to actually implement this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17atuwj", "is_robot_indexable": true, "report_reasons": null, "author": "techworker716", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17atuwj/how_to_architect_an_analytics_system/", "subreddit_subscribers": 134759, "created_utc": 1697643908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.\n\nhttps://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;utm_campaign=post&amp;utm_medium=reddit", "author_fullname": "t2_a6t9ksvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From pipelines to platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17amyoo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Blog post about the shift from traditional pipelines to robust data platforms as a path to building data flywheels and generate value from analytical data at scale. It covers the significance of data contracts for streamlined communication and validation but also that the future of data engineering lies in automation and a unified architecture. Backed with some anecdotal stats/numbers from a real implementation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit\"&gt;https://robertsahlin.substack.com/p/from-pipelines-to-platform?r=7bvua&amp;amp;utm_campaign=post&amp;amp;utm_medium=reddit&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?auto=webp&amp;s=9c8e6e3536f0f4c65db581de64e64e99bab1b192", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aba92d06a8e7cbfde84a8a2608f9bb6b49e0f5fc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e1cd58976fb4b92ebbdb1950cd2e99e543ec074", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a11481ef0a5ec5245fc53e0770ef8c3883f2936d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9abc0eafe4bf2c1277f5b457028ec55073f86f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1cef300cde287850f63f6d5a3c52b7bb2fe92f2e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9vDwzgxgRDaPgMwHwSaufwjJHps83YVLxYXYd0juDM0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62146cd257889560244beacc968e6ce8235fe121", "width": 1080, "height": 540}], "variants": {}, "id": "qBpJuY4j45_ONii6LIPwu5GEADbSiI3mqZIgn1TG_4g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17amyoo", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_End_979", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17amyoo/from_pipelines_to_platform/", "subreddit_subscribers": 134759, "created_utc": 1697623030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I have recently started working at a mid-company as a DE but apparently there's not a lot of work on DE involved around production for it. Since I just started, there's not a lot of work around the corner for me. I thought that since I'm not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I've thought of trying kaggle. Do you guys recommend of any other way to do so?", "author_fullname": "t2_8laf2pzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upskilling myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ajlft", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697608918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have recently started working at a mid-company as a DE but apparently there&amp;#39;s not a lot of work on DE involved around production for it. Since I just started, there&amp;#39;s not a lot of work around the corner for me. I thought that since I&amp;#39;m not working on production for now, I can spend my time upskilling myself.Is there any way that I can upskill myself? I&amp;#39;ve thought of trying kaggle. Do you guys recommend of any other way to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ajlft", "is_robot_indexable": true, "report_reasons": null, "author": "Key_Consideration385", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ajlft/advice_on_upskilling_myself/", "subreddit_subscribers": 134759, "created_utc": 1697608918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.  \nIn my experience, 90% of the code and effort seems to be data plumbing.\n\nHow do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?\n\nI\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.\n\n[https://github.com/DataSQRL/sqrl](https://github.com/DataSQRL/sqrl)\n\nWhat do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution *could* exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!", "author_fullname": "t2_ihfn9f9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you solve data plumbing? Can we compile it away?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ahqib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697602012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Implementing data products as streaming data pipelines requires a ton of data plumbing: integrating various technologies (stream processors, databases, API servers), mapping schemas, configuring data access, orchestrating data flows, optimizing physical data models, etc.&lt;br/&gt;\nIn my experience, 90% of the code and effort seems to be data plumbing.&lt;/p&gt;\n\n&lt;p&gt;How do you solve data plumbing so it doesn\u2019t become a drag on your data products? How do you rapidly build and iterate on data products without data plumbing slowing you down?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been playing around with the idea of a compiler that can generate integrated data pipelines (source to API) from a declarative definition of the data flow and queries in SQL. In other words: use existing technologies but let a compiler handle the data plumbing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/DataSQRL/sqrl\"&gt;https://github.com/DataSQRL/sqrl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? I\u2019m interested in solving the data plumbing problem and not attached to my idea (mostly wanted to prove to myself that a solution &lt;em&gt;could&lt;/em&gt; exist), so please tear it to shreds, and let\u2019s find something that works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?auto=webp&amp;s=fa781d1ddcfbca87a6a2b23d5a68793504c49497", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c436bb0beceacd0ad7cfa97946894b6a4b7c9f97", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f989c23a59c0c3fee7040000bab38ffb717b6b0", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93d69e2944ffed693f0ff4ed0d5f90dd756528b5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d23b6b8613519343928733fe218625dd049fda91", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e47dca88094c9601ef51964fd1d3b49b94d6343", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Quop2ct3JKeptjIH-foYHQ143cb_mqV6K2htKWnvJos.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90bf29af59fa7323cce2efdadd72bc6fb0d1ac9f", "width": 1080, "height": 540}], "variants": {}, "id": "45fS3FFg9knjZShinUTVGtiesnQZRpUhGztbG1rpTBs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ahqib", "is_robot_indexable": true, "report_reasons": null, "author": "matthiasBcom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ahqib/how_do_you_solve_data_plumbing_can_we_compile_it/", "subreddit_subscribers": 134759, "created_utc": 1697602012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've released dbt-factory which enable you to write re-usable sql code blocks and compile to a dbt project through a yaml configuration\n\n**Benefits**\n\n* Construct pipeline without knowing SQL\n* Construct pipelines without learning dbt\n* Avoid duplicative code\n* Easier automation of pipeline creation, so your code can interact with yaml instead of sql (advanced users)\n\n&amp;#x200B;\n\nLet me know if anyone has any thoughts \ud83d\ude04", "author_fullname": "t2_9jvpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt factory: re-usable sql, configured through YAML, compiled to dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b7nim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697680425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve released dbt-factory which enable you to write re-usable sql code blocks and compile to a dbt project through a yaml configuration&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Construct pipeline without knowing SQL&lt;/li&gt;\n&lt;li&gt;Construct pipelines without learning dbt&lt;/li&gt;\n&lt;li&gt;Avoid duplicative code&lt;/li&gt;\n&lt;li&gt;Easier automation of pipeline creation, so your code can interact with yaml instead of sql (advanced users)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let me know if anyone has any thoughts \ud83d\ude04&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b7nim", "is_robot_indexable": true, "report_reasons": null, "author": "conradbez", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b7nim/dbt_factory_reusable_sql_configured_through_yaml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b7nim/dbt_factory_reusable_sql_configured_through_yaml/", "subreddit_subscribers": 134759, "created_utc": 1697680425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a DE coding interview at Apple, can someone please share some insights about the coding round. What type of questions I can expect?", "author_fullname": "t2_udkygwua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Coding Round at Apple", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b3nbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697669053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a DE coding interview at Apple, can someone please share some insights about the coding round. What type of questions I can expect?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17b3nbo", "is_robot_indexable": true, "report_reasons": null, "author": "Anxious-Reality3258", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b3nbo/de_coding_round_at_apple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b3nbo/de_coding_round_at_apple/", "subreddit_subscribers": 134759, "created_utc": 1697669053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some money I can use for a short outsourced project (~$180k)\n\nCompany profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now\n\nI started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it's time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of \"important spreadsheets\" scattered around the place. Once we have the core data stack in place, I hope we'll be able to manage better and deliver more as a very small team\n\nSAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...\n\nWhat I'm thinking is that we build a short 3-5 page request for proposal (RFP) to \"build our data stack\" without actually specifying the tech stack we want, with some high level requirements of\n\n* Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure\n* Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting \n* Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too\n* We have a preference for popular and open tools with minimal maintenance\n\nAnother direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack\n\nThe main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don't know what it is\n\nWe've just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it's not a full enough platform yet, so it's a risk for us to dive into that\n\nThoughts? Anything I can add that would be helpful?\n\nThanks very much", "author_fullname": "t2_4t8zz1xo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you spend $180k on?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b0jhs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697661031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some money I can use for a short outsourced project (~$180k)&lt;/p&gt;\n\n&lt;p&gt;Company profile: pharma manufacturing; ~800 people across a few sites, most people rely on data of some sort, mainly SAP. ~200 people have office roles and as such drill into data lot more. Our team is 3 people; me (~ 10 years data science + little bit of engineering background), 2 very junior (but good) others. No scope for more headcount now&lt;/p&gt;\n\n&lt;p&gt;I started the team in the company 2 years ago. Since then, we have done a lot of quick win analyses/apps (source data -&amp;gt; target outcome), without tackling any of the data integration, data governance, reporting challenges. I now feel it&amp;#39;s time we begin building out our data pipelines, and in particular, pulling data from our most important platforms (CRM; ERP - SAP, time series sensor data), and the various set of &amp;quot;important spreadsheets&amp;quot; scattered around the place. Once we have the core data stack in place, I hope we&amp;#39;ll be able to manage better and deliver more as a very small team&lt;/p&gt;\n\n&lt;p&gt;SAP (onprem S/4HANA) is the most important data source. We have a bit of SAP experience (relevant data objects and tables), and quite a bit of business domain understanding (what reports and analysis people want). Given some of my experience with SAP, I am very averse to wanting to go down the path of building more in SAP: BW4/HANA, SAC, ...&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m thinking is that we build a short 3-5 page request for proposal (RFP) to &amp;quot;build our data stack&amp;quot; without actually specifying the tech stack we want, with some high level requirements of&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Enable iterative building of data integrations from System X, Y, Z into a cloud platform staging area (e.g. data lake). These data integrations will only very rarely be required to be faster than daily batch jobs. We have a preference for Azure&lt;/li&gt;\n&lt;li&gt;Enable data transformations from staging area to build views/marts suitable for business reports. We have a preference for SQL, Python, R for transformation and PowerBI for reporting &lt;/li&gt;\n&lt;li&gt;Enable data analysis suitable for detailed investigations. We have a preference for R, but Python is desirable too&lt;/li&gt;\n&lt;li&gt;We have a preference for popular and open tools with minimal maintenance&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Another direction to go down would be data governance, but I feel that we can only really get started on that once we have a data stack&lt;/p&gt;\n\n&lt;p&gt;The main challenges we have are (a) compliance - we need to prove data landing in reports is the same (conditional on transformation) as in the source system, (b) low experience/skill IT team - we ask for X and they don&amp;#39;t know what it is&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve just signed up for Microsoft Fabric - which looks like we might be able to get started quickly - but I feel it&amp;#39;s not a full enough platform yet, so it&amp;#39;s a risk for us to dive into that&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Anything I can add that would be helpful?&lt;/p&gt;\n\n&lt;p&gt;Thanks very much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b0jhs", "is_robot_indexable": true, "report_reasons": null, "author": "c3f7", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b0jhs/what_would_you_spend_180k_on/", "subreddit_subscribers": 134759, "created_utc": 1697661031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn't pricey, since it's not something I make money off of lol. ", "author_fullname": "t2_k4oyb5z58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Servers for Personal Projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17azvdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697659289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have good recommendations for sql (preferably postgres) servers that I can use for side projects I have? Want something that isn&amp;#39;t pricey, since it&amp;#39;s not something I make money off of lol. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17azvdl", "is_robot_indexable": true, "report_reasons": null, "author": "SoccerFilmNerd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17azvdl/sql_servers_for_personal_projects/", "subreddit_subscribers": 134759, "created_utc": 1697659289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.\n\nSo far I've been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.\n\nAfter assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn't be using Debezium as, according to him and another expert, it doesn't simply read the db's logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db's log without needing a trigger from it.\n\nI've been doing some research and it turns out that there aren't many options on the table, especially if we consider that everything has to be **on premise** and according [to this paper from Netflix](https://arxiv.org/pdf/2010.12597.pdf) Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they're quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.\n\nSo I'm wondering whether the project is actually viable or if I'm headed into a dead end.\n\nI case it hasn't already transpired I'm not really a data engineer so I'm learning as much as possible during the process.", "author_fullname": "t2_79y3zlzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Viability of a CDC project paired with Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ash74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697640225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing an academical internship for my uni thesis in a company that would like to get up to speed on Apache Kafka in order to maybe decouple the connections between the components of their infrastructure in the future.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been able to set up a test Kafka Cluster paired with a Debezium Connector that reads from a MySQL source whose changes are then fed to a MySql Sink with the usual JDBC Connector Sink by Confluent.&lt;/p&gt;\n\n&lt;p&gt;After assessing the progress  with my boss it turned out that, even if everything looked good, I shouldn&amp;#39;t be using Debezium as, according to him and another expert, it doesn&amp;#39;t simply read the db&amp;#39;s logs but it apparently also requires the db to send a trigger to Debezium after every change potentially adding strain onto it. So they asked me to find a piece of software to be installed on the same machine in which the db is installed that continuously reads the db&amp;#39;s log without needing a trigger from it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing some research and it turns out that there aren&amp;#39;t many options on the table, especially if we consider that everything has to be &lt;strong&gt;on premise&lt;/strong&gt; and according &lt;a href=\"https://arxiv.org/pdf/2010.12597.pdf\"&gt;to this paper from Netflix&lt;/a&gt; Debezium might also stall any write that is being performed to the DB during log processing. Furthermore, while they&amp;#39;re quite eager to go with a paid enterprise solution in case they decide to implement this method in production, they only want me to leverage open/free (free as in price) solutions at this stage.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering whether the project is actually viable or if I&amp;#39;m headed into a dead end.&lt;/p&gt;\n\n&lt;p&gt;I case it hasn&amp;#39;t already transpired I&amp;#39;m not really a data engineer so I&amp;#39;m learning as much as possible during the process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ash74", "is_robot_indexable": true, "report_reasons": null, "author": "ExactTreat593", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ash74/viability_of_a_cdc_project_paired_with_kafka/", "subreddit_subscribers": 134759, "created_utc": 1697640225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently architecting a Azure solution for a DWH  that will integrate data extracted from \\~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.\n\nFor the extraction part, I'm leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered 'externally' via a call from ADF, Synapse pipelines or Airflow.\n\nI would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!", "author_fullname": "t2_lvjbzi2i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting an Azure Solution for DWH Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aqwt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697635964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently architecting a Azure solution for a DWH  that will integrate data extracted from ~15 REST APIs. As  per requirement, data extraction should be performed in daily batches,  followed by the transformation and loading of flat files into a structured data model.&lt;/p&gt;\n\n&lt;p&gt;For the extraction part, I&amp;#39;m leaning towards a solution which utilizes Azure Functions, either running on a schedule via a time trigger, or triggered &amp;#39;externally&amp;#39; via a call from ADF, Synapse pipelines or Airflow.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate insights, best practices, or suggestions on  how to approach this architectural decision. Thanks a ton for your input and guidance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aqwt9", "is_robot_indexable": true, "report_reasons": null, "author": "cloclosh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aqwt9/architecting_an_azure_solution_for_dwh_integration/", "subreddit_subscribers": 134759, "created_utc": 1697635964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any data engineering project courses available? I come from a different background than data engineering and have completed a diploma program and Zoomcamp in DE. I'm still actively searching for data engineering jobs and could use a mentor's guidance for improving my LinkedIn profile and resume. Simultaneously, I'm eager to embark on new projects that involve the community and help me gain more experience.", "author_fullname": "t2_71nzg158", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Project Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b83bd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697681709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any data engineering project courses available? I come from a different background than data engineering and have completed a diploma program and Zoomcamp in DE. I&amp;#39;m still actively searching for data engineering jobs and could use a mentor&amp;#39;s guidance for improving my LinkedIn profile and resume. Simultaneously, I&amp;#39;m eager to embark on new projects that involve the community and help me gain more experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b83bd", "is_robot_indexable": true, "report_reasons": null, "author": "BeltUsual3130", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b83bd/data_engineering_project_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b83bd/data_engineering_project_course/", "subreddit_subscribers": 134759, "created_utc": 1697681709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've just received an offer for a data engineering internship, which I think is what I want to go into. I only have a limited time to accept or decline it though. \n\nThe salary is $25/hr which is more than I'm making now, but is that good? Idk what an average intern salary should be for data engineering and haven't found much recent info online to help. It's in Texas to be specific with location. \n\nI know that if I do this internship I'm likely to have a full time job offer there after I graduate assuming it goes well. I'm not sure I want to live in Texas though, it's really far from all my family and I don't know how I'll do in the heat. I'm fine to do the internship because it's only 10 weeks and I will live, but I might want a different job after I graduate closer to where I live in Utah or WFH. Would I have a decent chance of landing a job with a different company? Or am I likely going to be stuck with where I intern at until I have more experience? Basically just not sure how competitive the industry is. From what I can tell, I haven't found many data engineering internships to even apply to compared to software engineering or data analytics so that's why I'm worried. \n\nSorry if these questions are annoying", "author_fullname": "t2_4qppib05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's a good internship salary? How hard is it to get a job after just an internship in this field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b4jvc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697671484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just received an offer for a data engineering internship, which I think is what I want to go into. I only have a limited time to accept or decline it though. &lt;/p&gt;\n\n&lt;p&gt;The salary is $25/hr which is more than I&amp;#39;m making now, but is that good? Idk what an average intern salary should be for data engineering and haven&amp;#39;t found much recent info online to help. It&amp;#39;s in Texas to be specific with location. &lt;/p&gt;\n\n&lt;p&gt;I know that if I do this internship I&amp;#39;m likely to have a full time job offer there after I graduate assuming it goes well. I&amp;#39;m not sure I want to live in Texas though, it&amp;#39;s really far from all my family and I don&amp;#39;t know how I&amp;#39;ll do in the heat. I&amp;#39;m fine to do the internship because it&amp;#39;s only 10 weeks and I will live, but I might want a different job after I graduate closer to where I live in Utah or WFH. Would I have a decent chance of landing a job with a different company? Or am I likely going to be stuck with where I intern at until I have more experience? Basically just not sure how competitive the industry is. From what I can tell, I haven&amp;#39;t found many data engineering internships to even apply to compared to software engineering or data analytics so that&amp;#39;s why I&amp;#39;m worried. &lt;/p&gt;\n\n&lt;p&gt;Sorry if these questions are annoying&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17b4jvc", "is_robot_indexable": true, "report_reasons": null, "author": "silvermoon_182", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b4jvc/whats_a_good_internship_salary_how_hard_is_it_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b4jvc/whats_a_good_internship_salary_how_hard_is_it_to/", "subreddit_subscribers": 134759, "created_utc": 1697671484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.\n\nMy primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? \n\nAppreciate any insight.", "author_fullname": "t2_3auynywj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance on how event data is stored in a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b2d47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - I\u2019m an experienced DE but I have never gotten the opportunity to work with real-time/event data as it\u2019s generated and stored. An example could be Instagram. Millions of people are clicking, searching, liking stuff every minute. I understand the need for a streaming pipeline that gets this data from APIs via Kafka Connect or something and potentially loads into a NoSQL database like Cassandra or MongoDB. This is all OLTP until now.&lt;/p&gt;\n\n&lt;p&gt;My primary question is how is this data stored in a data warehouse like Snowflake or BigQuery. More specifically, the  number of metrics to store (like/comment/share/search) are pretty big. So do each of these metrics become a 1/0 metric in the same fact table. Example, is_like, is_comment etc. Columnar databases can handle a lottttt of columns, so is this how data is actually stored for massive companies? Or am I thinking about it completely wrong? &lt;/p&gt;\n\n&lt;p&gt;Appreciate any insight.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b2d47", "is_robot_indexable": true, "report_reasons": null, "author": "currentlyreadingit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b2d47/guidance_on_how_event_data_is_stored_in_a_dwh/", "subreddit_subscribers": 134759, "created_utc": 1697665706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I'm looking for help on a seemingly simple task that has me confused.\n\nThe vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.\n\nWe would hit the APIs about once a day to get updates on these conversions. Here's my idea so far:\n\n1. Our data isn't massive, so I'm ok with sticking with python+postgres and cron.\n\n2. Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.\n\n3. I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.\n\n4. Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.\n\n5. Use the LAG function to compare the status on this run to the status from the last run.\n\n6. If the status change is relevant to us, capture that conversion and forward it.\n\nThis seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.\n\nIs there a better way to do this that is standard practice?", "author_fullname": "t2_d8wq0wkd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for capturing changes in column values", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17avpdg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697648615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently had the chance to explore e-commerce APIs (helping a friend out with their startup), and I&amp;#39;m looking for help on a seemingly simple task that has me confused.&lt;/p&gt;\n\n&lt;p&gt;The vendor APIs will provide info on various conversions by users. Fields of interest to us would be the conversion_id, and the conversion_status. We want to capture changes in the status and forward those conversions to another database. I.e. if a conversions status during the last update was different to its status today, we want to capture it.&lt;/p&gt;\n\n&lt;p&gt;We would hit the APIs about once a day to get updates on these conversions. Here&amp;#39;s my idea so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Our data isn&amp;#39;t massive, so I&amp;#39;m ok with sticking with python+postgres and cron.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Python to hit the API, go through all pages in the response, concat those into one long JSON and dump it into a jsonb column.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would use a table specifically as a data dump, with three columns: run_date, api_name, json_data.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Create views for each API, extracting out the fields specific to that vendor. These would generally include the conversion Id, run date and the status.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use the LAG function to compare the status on this run to the status from the last run.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If the status change is relevant to us, capture that conversion and forward it.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This seems like it would work, but it does seem a little hacky to me. Also not sure if it would scale with multiple vendors.&lt;/p&gt;\n\n&lt;p&gt;Is there a better way to do this that is standard practice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17avpdg", "is_robot_indexable": true, "report_reasons": null, "author": "fabricmoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17avpdg/advice_for_capturing_changes_in_column_values/", "subreddit_subscribers": 134759, "created_utc": 1697648615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:\n\n*We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json\\_normalize to flatten the json into a tabular format) However we've had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)*\n\n**What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?**\n\nOur current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can't directly load data that doesn't match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.\n\nWe want to try and keep this as close to realtime as possible and preferably serverless", "author_fullname": "t2_48vnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake Problem: help with inconsistent day to day parquet schemas (pandas, AWS lambda)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17aq76s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697633939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to write a full description of our datalake setup but when it comes down to it the problem is fairly succint:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We trigger lambda functions and use pandas to write parquet versions of json files into our datalake (after using json_normalize to flatten the json into a tabular format) However we&amp;#39;ve had issue between days where the inferred types in the parquet files for certain columns is not consistent (mainly floats and ints, because our data is coming from a JS front end)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be the best way to take a list of flat json objects and write them to a parquet file where some of the fields may be optional, but enforce a schema which is maintained elsewhere?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our current idea is using duckdb to create an empty table from a schema, and then adding the json data in, but you can&amp;#39;t directly load data that doesn&amp;#39;t match the full schema into the table, so it seems like the only way is to make one table of the schema and one of the actual data and then join them (which feels hacky) then writing that to parquet.&lt;/p&gt;\n\n&lt;p&gt;We want to try and keep this as close to realtime as possible and preferably serverless&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17aq76s", "is_robot_indexable": true, "report_reasons": null, "author": "freerangetrousers", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17aq76s/datalake_problem_help_with_inconsistent_day_to/", "subreddit_subscribers": 134759, "created_utc": 1697633939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.\n\nGiving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar\n\nI know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).\n\nAlso what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal project: what software should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ap7is", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697630909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m willing to dedicate some of my free time to create a site where users can read about financial education and also be able to play with some tools to analyse their personal situations. The tools for instance could be forms to gather data about the user situation and some design options, then the tool would process those settings along with financial data for instance, and output some sort of aggregated output that might guide the users to their financial decisions. The output may as well be interactive graphical reports (to this end I used something plotly and matplotlib). Users may wish to save their settings/states.&lt;/p&gt;\n\n&lt;p&gt;Giving this idea/requirements, I thought that I just need a simple site to display information: here the focus should be on the presentation of the information so that the user experience would be great. Then the backend would focus on the efficient and expertise to machine financial data and users data, and a database to store all these informations, e.g. historical financial data and users inputs. For the presentation of outputs I also need to be able to generate nice plots of all kinds. Am I missing something? What software would you use, Django maybe? Which plotting librar&lt;/p&gt;\n\n&lt;p&gt;I know already Python/Pandas a bit, I am not willing to learn master/learn in details css/javascript and frontend libraries. Just what is needeed to present things nicely (I used bootstrap in the past for a small site).&lt;/p&gt;\n\n&lt;p&gt;Also what cloud would you use to host this site eventually? Both in development and/or in production assuming very few people will use it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ap7is", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ap7is/personal_project_what_software_should_i_use/", "subreddit_subscribers": 134759, "created_utc": 1697630909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Referring to taking data from the transaction database to a warehouse. What ETL process did you run?", "author_fullname": "t2_legyu6zou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone had any experience modelling data from a Netsuite database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17an2k7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697623431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Referring to taking data from the transaction database to a warehouse. What ETL process did you run?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17an2k7", "is_robot_indexable": true, "report_reasons": null, "author": "anotherwetsock", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17an2k7/anyone_had_any_experience_modelling_data_from_a/", "subreddit_subscribers": 134759, "created_utc": 1697623431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, \n\nI have several parquet files in a datalake on premise. I would like to describe those datas and shemas using for  instance a json file as bellow. Does a standard format already exists for data catalog ? \n\n    {\n      \"url\":  \"albums.parquet\",\n      \"description\" : \"list of albums\" ,\n      \"schema\" : { \n       {\"name\": {\"string\", \"name of album\"},\n       {\"author\": {string, \"name of author\n      }\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_gdtn2w15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do we have a standard format for describing a data catalog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17b2445", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697665062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I have several parquet files in a datalake on premise. I would like to describe those datas and shemas using for  instance a json file as bellow. Does a standard format already exists for data catalog ? &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;url&amp;quot;:  &amp;quot;albums.parquet&amp;quot;,\n  &amp;quot;description&amp;quot; : &amp;quot;list of albums&amp;quot; ,\n  &amp;quot;schema&amp;quot; : { \n   {&amp;quot;name&amp;quot;: {&amp;quot;string&amp;quot;, &amp;quot;name of album&amp;quot;},\n   {&amp;quot;author&amp;quot;: {string, &amp;quot;name of author\n  }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17b2445", "is_robot_indexable": true, "report_reasons": null, "author": "TargetDangerous2216", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17b2445/do_we_have_a_standard_format_for_describing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17b2445/do_we_have_a_standard_format_for_describing_a/", "subreddit_subscribers": 134759, "created_utc": 1697665062.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}