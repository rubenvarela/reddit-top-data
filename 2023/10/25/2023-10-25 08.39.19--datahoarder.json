{"kind": "Listing", "data": {"after": "t3_17fm9mj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm tired of streaming. I already purchase music through [Qobuz](https://www.qobuz.com/us-en/shop) and am going to start purchasing all of the movies/TV I've \"obtained\" over the years on BluRay (mix of 1080p and 4k). I just purchased a USB BluRay drive that's LibreDrive flashed and a MakeMKV license.\n\nHowever, doing some math, I'm going to need more storage. I've done a few tests and a 1hr BluRay (1080p) is like 15GB straight out of MakeMKV. This is going to easily eat through my storage, so I was considering re-encoding. Looking at Handbrake, I've played around with the following settings. However, re-encoding take A LOT of time and seems like a really deep rabbit hole.\n\n* MKV\n* x265 10-bit\n* CRF 20 slow\n* AAC audio \n\nIs it easier to just buy more/bigger hard drives and leave all of my media as MKVs straight from MakeMKV?\n\nFor reference, I already have a small homelab consisting of an [ASRock DeskMini H470](https://www.asrock.com/nettop/Intel/DeskMini%20H470%20Series/index.asp) w/ i5-10400 (running Proxmox) and a 2-bay Synology with 2x 14TB WD Red Plus (RAID 1).", "author_fullname": "t2_isjro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Going to start ripping BluRays to watch on Jellyfin (Plex alternative). Do you guys generally re-encode them with Handbrake, or leave them as MKVs straight from MakeMKV?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ffifn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698161370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tired of streaming. I already purchase music through &lt;a href=\"https://www.qobuz.com/us-en/shop\"&gt;Qobuz&lt;/a&gt; and am going to start purchasing all of the movies/TV I&amp;#39;ve &amp;quot;obtained&amp;quot; over the years on BluRay (mix of 1080p and 4k). I just purchased a USB BluRay drive that&amp;#39;s LibreDrive flashed and a MakeMKV license.&lt;/p&gt;\n\n&lt;p&gt;However, doing some math, I&amp;#39;m going to need more storage. I&amp;#39;ve done a few tests and a 1hr BluRay (1080p) is like 15GB straight out of MakeMKV. This is going to easily eat through my storage, so I was considering re-encoding. Looking at Handbrake, I&amp;#39;ve played around with the following settings. However, re-encoding take A LOT of time and seems like a really deep rabbit hole.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MKV&lt;/li&gt;\n&lt;li&gt;x265 10-bit&lt;/li&gt;\n&lt;li&gt;CRF 20 slow&lt;/li&gt;\n&lt;li&gt;AAC audio &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is it easier to just buy more/bigger hard drives and leave all of my media as MKVs straight from MakeMKV?&lt;/p&gt;\n\n&lt;p&gt;For reference, I already have a small homelab consisting of an &lt;a href=\"https://www.asrock.com/nettop/Intel/DeskMini%20H470%20Series/index.asp\"&gt;ASRock DeskMini H470&lt;/a&gt; w/ i5-10400 (running Proxmox) and a 2-bay Synology with 2x 14TB WD Red Plus (RAID 1).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ffifn", "is_robot_indexable": true, "report_reasons": null, "author": "lmm7425", "discussion_type": null, "num_comments": 130, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ffifn/going_to_start_ripping_blurays_to_watch_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ffifn/going_to_start_ripping_blurays_to_watch_on/", "subreddit_subscribers": 708519, "created_utc": 1698161370.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "PSA, if anyone here is hoarding software you might want to mirror https://flings.vmware.com before 26.10. Full announcement from a VMware employee https://twitter.com/lamw/status/1716909940345045276?s=20 and https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;utm_medium=web2x&amp;context=3\n\nContext: Broadcom is taking over VMware so the flings website would be down either permanently (speculation) or for an extended period of time. Many people rely on Flings to achieve some niche solutions using VMware software. If you hoard such data you might be interested. Unfortunately I don\u2019t have the resources currently to mirror the entire thing and given the short period I can\u2019t realistically buy enough storage on time. \n\nEnjoy and thanks to however preserves those invaluable resources!", "author_fullname": "t2_hhere4lkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: VMware Flings website to be taken offline indefinitely", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fvy4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698205663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PSA, if anyone here is hoarding software you might want to mirror &lt;a href=\"https://flings.vmware.com\"&gt;https://flings.vmware.com&lt;/a&gt; before 26.10. Full announcement from a VMware employee &lt;a href=\"https://twitter.com/lamw/status/1716909940345045276?s=20\"&gt;https://twitter.com/lamw/status/1716909940345045276?s=20&lt;/a&gt; and &lt;a href=\"https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context: Broadcom is taking over VMware so the flings website would be down either permanently (speculation) or for an extended period of time. Many people rely on Flings to achieve some niche solutions using VMware software. If you hoard such data you might be interested. Unfortunately I don\u2019t have the resources currently to mirror the entire thing and given the short period I can\u2019t realistically buy enough storage on time. &lt;/p&gt;\n\n&lt;p&gt;Enjoy and thanks to however preserves those invaluable resources!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?auto=webp&amp;s=d073d57db5cc8bf41fdb15957274b08142891bd0", "width": 743, "height": 743}, "resolutions": [{"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19eee4059fa03c7321a40b81b3212a9c6b48e6ee", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e725264f4f4a6044d97d7c67c1423120e63532b6", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b6bd3655d4ce931ab1927be3c55703319c389f8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84572871cf288f66a0b0b8f5e4e86416bd073c5c", "width": 640, "height": 640}], "variants": {}, "id": "zlmL2m712SPSuZaEjTxkb2xX_TxZkQ2wNjwtx7OhFE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fvy4p", "is_robot_indexable": true, "report_reasons": null, "author": "Is-Not-El", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fvy4p/psa_vmware_flings_website_to_be_taken_offline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fvy4p/psa_vmware_flings_website_to_be_taken_offline/", "subreddit_subscribers": 708519, "created_utc": 1698205663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to find a way to build an all ssd DAS with either nvme or sata ssd. I need at least 10gbps speeds. I'm thinking about something like a PCIE enclosure with 3 or 4 slots in which I would put nvme cards in. I see Sonnet have solutions but it's quite expensive. Is there a DIY way ? By buying individual boards of some sort ?", "author_fullname": "t2_xmstz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PCIE to USB/Thunderbolt or similar solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fcc32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698152706.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to find a way to build an all ssd DAS with either nvme or sata ssd. I need at least 10gbps speeds. I&amp;#39;m thinking about something like a PCIE enclosure with 3 or 4 slots in which I would put nvme cards in. I see Sonnet have solutions but it&amp;#39;s quite expensive. Is there a DIY way ? By buying individual boards of some sort ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fcc32", "is_robot_indexable": true, "report_reasons": null, "author": "PhilippePaquet", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fcc32/pcie_to_usbthunderbolt_or_similar_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fcc32/pcie_to_usbthunderbolt_or_similar_solutions/", "subreddit_subscribers": 708519, "created_utc": 1698152706.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\nSorry if this is a duplicate post or it\u2019s been asked before but I couldn\u2019t specifically find an answer when looking.\n\nI have bought 2 lots of *new* Seagate Exos X18 16TB HDD from Amazon (sold, and shipped by Amazon - no 3rd parties) for \u00a3202.00 /ea. \n\nThe first one that arrived although the label even listing new, was labelled as certified refurbished (assuming this drive is not new?), and upon searching up the Serial on Seagates website stated that I needed to contact the place of purchase.\n\nI contacted Amazon and said the drive was labelled as certified refurbished and Seagate website listed warranty as contact seller and asked them if I did have a 5 year warranty and was the drive new. They wrote back and said likely no and no, but they dispatched the wrong item and they would return that for a refund and I could rebuy a new one.\n\nFast forward to today, and the second order I placed they have sent exactly the same drive. And checking the SN on Seagate yields the same results.\n\nAre these legit (and it\u2019s just me having a brain fart moment) or are these drives non-warranty and refurbished? Should I just send it back?\n\nI specifically avoided the \u201cOEM\u201d listings because of the warranty problem. This drive was not listed as OEM or refurbished.\n\nThanks", "author_fullname": "t2_3j9dm7sj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos 16TB Drives from Amazon have no warranty?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fhngu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698167002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nSorry if this is a duplicate post or it\u2019s been asked before but I couldn\u2019t specifically find an answer when looking.&lt;/p&gt;\n\n&lt;p&gt;I have bought 2 lots of &lt;em&gt;new&lt;/em&gt; Seagate Exos X18 16TB HDD from Amazon (sold, and shipped by Amazon - no 3rd parties) for \u00a3202.00 /ea. &lt;/p&gt;\n\n&lt;p&gt;The first one that arrived although the label even listing new, was labelled as certified refurbished (assuming this drive is not new?), and upon searching up the Serial on Seagates website stated that I needed to contact the place of purchase.&lt;/p&gt;\n\n&lt;p&gt;I contacted Amazon and said the drive was labelled as certified refurbished and Seagate website listed warranty as contact seller and asked them if I did have a 5 year warranty and was the drive new. They wrote back and said likely no and no, but they dispatched the wrong item and they would return that for a refund and I could rebuy a new one.&lt;/p&gt;\n\n&lt;p&gt;Fast forward to today, and the second order I placed they have sent exactly the same drive. And checking the SN on Seagate yields the same results.&lt;/p&gt;\n\n&lt;p&gt;Are these legit (and it\u2019s just me having a brain fart moment) or are these drives non-warranty and refurbished? Should I just send it back?&lt;/p&gt;\n\n&lt;p&gt;I specifically avoided the \u201cOEM\u201d listings because of the warranty problem. This drive was not listed as OEM or refurbished.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fhngu", "is_robot_indexable": true, "report_reasons": null, "author": "NWSpitfire", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fhngu/seagate_exos_16tb_drives_from_amazon_have_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fhngu/seagate_exos_16tb_drives_from_amazon_have_no/", "subreddit_subscribers": 708519, "created_utc": 1698167002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Everyone, I have been doing some research for my next system build and I am now trying to find a decently priced case that holds 12, 3.5\" HDDs. If anyone could recommend some cases to me, I would be grateful. Thank you for your time. \n\nEdit: have to leave now. Will be back later.", "author_fullname": "t2_4isg4jj9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for cases with 12 HDD bays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fhn2e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698169321.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698166975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, I have been doing some research for my next system build and I am now trying to find a decently priced case that holds 12, 3.5&amp;quot; HDDs. If anyone could recommend some cases to me, I would be grateful. Thank you for your time. &lt;/p&gt;\n\n&lt;p&gt;Edit: have to leave now. Will be back later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fhn2e", "is_robot_indexable": true, "report_reasons": null, "author": "happypessoa", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fhn2e/looking_for_cases_with_12_hdd_bays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fhn2e/looking_for_cases_with_12_hdd_bays/", "subreddit_subscribers": 708519, "created_utc": 1698166975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey!\n\nBasically, I want to expand my storage and tidy it up (atm I have more or less like 5 disks, external and internal ones).\n\nWhile researching, I saw two things that may be appropiate: docking stations and disk enclosures\n\nHowever, I am having some trouble understanding the difference (at least for my use case)\n\nMy use case is basically I have a server running ubuntu with all of my servers dockerized and just want to expand my storage to store movies and tv shows and make it look nicer. No RAID, no anything.\n\nI came across two options:\n\n[https://sabrent.com/collections/docking-station/products/ds-sc4b](https://sabrent.com/collections/docking-station/products/ds-sc4b)\n\nand\n\n[https://www.terra-master.com/global/d4-300.html](https://www.terra-master.com/global/d4-300.html)\n\nQuestions:\n\nWhich one do you think is better for my use case?\n\nWhat's the difference between docking stations and disk enclosures?\n\nAre there any differences for my use case?\n\nDo you know another device that may be better or have you had any bad experience with the aforementioned devices?\n\nThanks!", "author_fullname": "t2_3f7jceck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docking station vs disk enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ffdh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698161010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Basically, I want to expand my storage and tidy it up (atm I have more or less like 5 disks, external and internal ones).&lt;/p&gt;\n\n&lt;p&gt;While researching, I saw two things that may be appropiate: docking stations and disk enclosures&lt;/p&gt;\n\n&lt;p&gt;However, I am having some trouble understanding the difference (at least for my use case)&lt;/p&gt;\n\n&lt;p&gt;My use case is basically I have a server running ubuntu with all of my servers dockerized and just want to expand my storage to store movies and tv shows and make it look nicer. No RAID, no anything.&lt;/p&gt;\n\n&lt;p&gt;I came across two options:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://sabrent.com/collections/docking-station/products/ds-sc4b\"&gt;https://sabrent.com/collections/docking-station/products/ds-sc4b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.terra-master.com/global/d4-300.html\"&gt;https://www.terra-master.com/global/d4-300.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;p&gt;Which one do you think is better for my use case?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the difference between docking stations and disk enclosures?&lt;/p&gt;\n\n&lt;p&gt;Are there any differences for my use case?&lt;/p&gt;\n\n&lt;p&gt;Do you know another device that may be better or have you had any bad experience with the aforementioned devices?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XJhoIKemyPFGEZZtud1_Tp7lvMpWdkPrtkA0KJrknbM.jpg?auto=webp&amp;s=242e72ba0e875ca898b9d424c36b8c7f84cc0428", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XJhoIKemyPFGEZZtud1_Tp7lvMpWdkPrtkA0KJrknbM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=734a688c5bed5f52526d1f6850016f06df403810", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/XJhoIKemyPFGEZZtud1_Tp7lvMpWdkPrtkA0KJrknbM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b23ccd084a98047d34ed2512bb7188c6b960965", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/XJhoIKemyPFGEZZtud1_Tp7lvMpWdkPrtkA0KJrknbM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b8a07675761cfdaa40e0f30a6304f5b46c1a7f8", "width": 320, "height": 320}], "variants": {}, "id": "Q30YM4gtHPbJq2cxDZJKbtqIndZyf4XzKFhKk2E0UJs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ffdh7", "is_robot_indexable": true, "report_reasons": null, "author": "Rafa130397", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ffdh7/docking_station_vs_disk_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ffdh7/docking_station_vs_disk_enclosure/", "subreddit_subscribers": 708519, "created_utc": 1698161010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nDoes anybody know if the reliability of Samsung 870 QVO is on par with other SSD like Micron 5210 ION Series (or possibly others). From the datasheets:\n\n* [https://download.semiconductor.samsung.com/resources/data-sheet/Samsung\\_SSD\\_870\\_QVO\\_Data\\_Sheet\\_Rev1.1.pdf](https://download.semiconductor.samsung.com/resources/data-sheet/Samsung_SSD_870_QVO_Data_Sheet_Rev1.1.pdf)\n* [https://www.micron.com/-/media/client/global/documents/products/product-flyer/5210\\_ion\\_ssd\\_product\\_brief.pdf](https://www.micron.com/-/media/client/global/documents/products/product-flyer/5210_ion_ssd_product_brief.pdf?la=en)\n\nI would choose the Micron over the Samsung, but given it is the same technology, I wonder if there is an actual difference?\n\nAny insight on this question?\n\nRegards,", "author_fullname": "t2_b3cz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "QLC Nand SSD reliability: Samsung 870 QVO vs others", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ffoaf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698161788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Does anybody know if the reliability of Samsung 870 QVO is on par with other SSD like Micron 5210 ION Series (or possibly others). From the datasheets:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://download.semiconductor.samsung.com/resources/data-sheet/Samsung_SSD_870_QVO_Data_Sheet_Rev1.1.pdf\"&gt;https://download.semiconductor.samsung.com/resources/data-sheet/Samsung_SSD_870_QVO_Data_Sheet_Rev1.1.pdf&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.micron.com/-/media/client/global/documents/products/product-flyer/5210_ion_ssd_product_brief.pdf?la=en\"&gt;https://www.micron.com/-/media/client/global/documents/products/product-flyer/5210_ion_ssd_product_brief.pdf&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would choose the Micron over the Samsung, but given it is the same technology, I wonder if there is an actual difference?&lt;/p&gt;\n\n&lt;p&gt;Any insight on this question?&lt;/p&gt;\n\n&lt;p&gt;Regards,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ffoaf", "is_robot_indexable": true, "report_reasons": null, "author": "fpopineau", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ffoaf/qlc_nand_ssd_reliability_samsung_870_qvo_vs_others/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ffoaf/qlc_nand_ssd_reliability_samsung_870_qvo_vs_others/", "subreddit_subscribers": 708519, "created_utc": 1698161788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a website \"mywebsite.com\" that requires login and complex verification method. I want to make a offline copy of it for personal use.\n\nHttrack, wget or any other method fails during login. I have logged on with chrome and attached webdriver. I want to clone it/ download all pages under the domain [www.mywebsite.com/\\\\](http://www.mywebsite.com/%5C)\\* for offline use. Using selenium I can only download one page. How to download the whole website that includes all the links under that domain in relative relation .\n\n**ps: any tool that could do that will also work**.\n\n    import time\n    from selenium import webdriver\n    from selenium.webdriver.chrome.options import Options \n    options = webdriver.ChromeOptions()\n    options.add_argument(\"user-data-dir=C:\\\\Users\\\\mrmin\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n    options.add_experimental_option(\"detach\", True)\n    driver = webdriver.Chrome(options=options)\n    driver.get(\"https://www.mywebsite.com\")```", "author_fullname": "t2_a3rs29dtt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download whole website through chrome using selenium or any tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fwbh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698206952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a website &amp;quot;mywebsite.com&amp;quot; that requires login and complex verification method. I want to make a offline copy of it for personal use.&lt;/p&gt;\n\n&lt;p&gt;Httrack, wget or any other method fails during login. I have logged on with chrome and attached webdriver. I want to clone it/ download all pages under the domain [&lt;a href=\"http://www.mywebsite.com/%5C%5C%5D(http://www.mywebsite.com/%5C)%5C*\"&gt;www.mywebsite.com/\\\\](http://www.mywebsite.com/%5C)\\*&lt;/a&gt; for offline use. Using selenium I can only download one page. How to download the whole website that includes all the links under that domain in relative relation .&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;ps: any tool that could do that will also work&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import time\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options \noptions = webdriver.ChromeOptions()\noptions.add_argument(&amp;quot;user-data-dir=C:\\\\Users\\\\mrmin\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data&amp;quot;)\noptions.add_experimental_option(&amp;quot;detach&amp;quot;, True)\ndriver = webdriver.Chrome(options=options)\ndriver.get(&amp;quot;https://www.mywebsite.com&amp;quot;)```\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fwbh7", "is_robot_indexable": true, "report_reasons": null, "author": "mrmin87", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fwbh7/download_whole_website_through_chrome_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fwbh7/download_whole_website_through_chrome_using/", "subreddit_subscribers": 708519, "created_utc": 1698206952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been on google, github, stack overflow, and reddit for the last few days and have not been able to figure out how to accomplish what I am trying to do.   \n\n\nI have an index of a site I am trying to download. There are hundreds of folders, each folder has at least one subfolder, and some of the subfolders have subfolders. Files, mostly pdf's in all levels. The issue is, no matter what I use, best case scenario I am getting the main folders and the files in those. The subfolder will download, but it will be empty, or have an index of that subfolder.   \n\n\nManually downloading this is not an option. It would take me days to go one by one. My goal is to download as is, with all the folders, subfolder, files, etc... all in their place as it is listed in the sites index page.   \n\n\nSo far I have tried a few gui's like visualwget, jdownloader and a few chrome extensions.   \n\n\nOn my linux VM I have used wget with about every combination of flags I can think ok. Nothing has been able to work so far.  \n\n\nIs there any advice I can get from you guys?", "author_fullname": "t2_pyc58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading from Index of", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ftd0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698197680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been on google, github, stack overflow, and reddit for the last few days and have not been able to figure out how to accomplish what I am trying to do.   &lt;/p&gt;\n\n&lt;p&gt;I have an index of a site I am trying to download. There are hundreds of folders, each folder has at least one subfolder, and some of the subfolders have subfolders. Files, mostly pdf&amp;#39;s in all levels. The issue is, no matter what I use, best case scenario I am getting the main folders and the files in those. The subfolder will download, but it will be empty, or have an index of that subfolder.   &lt;/p&gt;\n\n&lt;p&gt;Manually downloading this is not an option. It would take me days to go one by one. My goal is to download as is, with all the folders, subfolder, files, etc... all in their place as it is listed in the sites index page.   &lt;/p&gt;\n\n&lt;p&gt;So far I have tried a few gui&amp;#39;s like visualwget, jdownloader and a few chrome extensions.   &lt;/p&gt;\n\n&lt;p&gt;On my linux VM I have used wget with about every combination of flags I can think ok. Nothing has been able to work so far.  &lt;/p&gt;\n\n&lt;p&gt;Is there any advice I can get from you guys?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ftd0y", "is_robot_indexable": true, "report_reasons": null, "author": "Kazelob", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ftd0y/downloading_from_index_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ftd0y/downloading_from_index_of/", "subreddit_subscribers": 708519, "created_utc": 1698197680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems to me Exos X is superior on every front, price: capacity, AFR, power consumption.\n\nWhat is the use case for Exos E? Why does it cost more? I don't understand.\n\n&amp;#x200B;\n\nDrives in question are here [https://www.seagate.com/products/enterprise-drives/](https://www.seagate.com/products/enterprise-drives/)\n\nI know their site says \"low TCO\" (total cost of ownership) but I don't see how that makes sense given X series is superior in terms of AFR (annualized failure rate) and MTBF", "author_fullname": "t2_13eqfc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos E vs Exos X", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fspmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698195774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems to me Exos X is superior on every front, price: capacity, AFR, power consumption.&lt;/p&gt;\n\n&lt;p&gt;What is the use case for Exos E? Why does it cost more? I don&amp;#39;t understand.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Drives in question are here &lt;a href=\"https://www.seagate.com/products/enterprise-drives/\"&gt;https://www.seagate.com/products/enterprise-drives/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know their site says &amp;quot;low TCO&amp;quot; (total cost of ownership) but I don&amp;#39;t see how that makes sense given X series is superior in terms of AFR (annualized failure rate) and MTBF&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?auto=webp&amp;s=b98f58f088fdf8fbeb225a485466816520892a66", "width": 1440, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=800aeac928e4d6917cdf3db161d37c173da92af5", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9549b2c27b55813f6941439e0246cac408ff1d", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=708b52a8cc3161e776c554b1f4a488231d8bfce6", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93eb129bea186541cdd32a88a0cb2d619f9a954c", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad5e3bcc25ba7cdf8dd591fb8e247b839fe5aa7", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b52b8bf108499e8a6d3bd58f428c8f644af3085", "width": 1080, "height": 675}], "variants": {}, "id": "GJ9K1o7VNo6J4JXg3IrZBPizWfgWXRc6b6FmSaP5cNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17fspmn", "is_robot_indexable": true, "report_reasons": null, "author": "tronicdude6", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fspmn/seagate_exos_e_vs_exos_x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fspmn/seagate_exos_e_vs_exos_x/", "subreddit_subscribers": 708519, "created_utc": 1698195774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently running Synology and I love the OS and everything around it, but it is getting really expensive to actually increase the size.\n\nI have about 110 TB right now, and the prices of expanding are getting a bit out of hand.\n\nSeriously considering dumping Syno all together and moving to something cheaper and friendlier storage expansion wise.\n\nLet's say I am planning on 500TB or so. How would that work? Can I even add 15+ drives to one motherboard? Will I need multiple motherboards? What CPU/GPU will I need to make Plex transcode and work properly on multiple devices at once? I assume I will need an external rack as well for the drives + cooling.\n\nI was hoping someone could maybe make simple list on [https://pcpartpicker.com/list/](https://pcpartpicker.com/list/) of stuff I need to actually do it.\n\nI will do it in small steps when things maybe go on discount so pricing is not a deal breaker, I just want something that can be easily expandable and only limited by the amount of drives I have. So if I want 30 drives, being able to actually have 30 of them.\n\nI build my last few PCs so I can start working on this as I go, and then when it is complete I would just need to move all the drives from my Syno to the new build.\n\nI have these parts from my old PC - [https://pcpartpicker.com/list/KqzPTB](https://pcpartpicker.com/list/KqzPTB).\n\nIf any of those can be reused, even better.", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware list for a 500TB+ media server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fpm6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698187102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently running Synology and I love the OS and everything around it, but it is getting really expensive to actually increase the size.&lt;/p&gt;\n\n&lt;p&gt;I have about 110 TB right now, and the prices of expanding are getting a bit out of hand.&lt;/p&gt;\n\n&lt;p&gt;Seriously considering dumping Syno all together and moving to something cheaper and friendlier storage expansion wise.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I am planning on 500TB or so. How would that work? Can I even add 15+ drives to one motherboard? Will I need multiple motherboards? What CPU/GPU will I need to make Plex transcode and work properly on multiple devices at once? I assume I will need an external rack as well for the drives + cooling.&lt;/p&gt;\n\n&lt;p&gt;I was hoping someone could maybe make simple list on &lt;a href=\"https://pcpartpicker.com/list/\"&gt;https://pcpartpicker.com/list/&lt;/a&gt; of stuff I need to actually do it.&lt;/p&gt;\n\n&lt;p&gt;I will do it in small steps when things maybe go on discount so pricing is not a deal breaker, I just want something that can be easily expandable and only limited by the amount of drives I have. So if I want 30 drives, being able to actually have 30 of them.&lt;/p&gt;\n\n&lt;p&gt;I build my last few PCs so I can start working on this as I go, and then when it is complete I would just need to move all the drives from my Syno to the new build.&lt;/p&gt;\n\n&lt;p&gt;I have these parts from my old PC - &lt;a href=\"https://pcpartpicker.com/list/KqzPTB\"&gt;https://pcpartpicker.com/list/KqzPTB&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;If any of those can be reused, even better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fpm6c", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fpm6c/hardware_list_for_a_500tb_media_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fpm6c/hardware_list_for_a_500tb_media_server/", "subreddit_subscribers": 708519, "created_utc": 1698187102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've currently got four 16TB WD Elements USB drives and two 5TB USB drives hooked up to a RPi 4 with a USB expansion hat and it's a bit of a mess with all the PSUs and USB leads, so I want to tidy it up by shucking the 16TB drives into a single enclosure. I'll probably try to sell or repurpose the 5TB drives, partly because it's a lot easier to find a 4-bay enclosure than a 6-bay one but also it's not very cost/power efficient to run these relatively small drives.\n\nI'm running a Chia farm at the moment but I'll probably switch it to a Storj node soon. Either way, I don't think the drives will need to be constantly active, so whatever enclosure I use should allow them to sleep, and it should have isolation mounts to reduce vibration noise.\n\nI don't want to spend a lot of money. If there's a compact PC case with 4+ drive bays that I could use, that could be an option. I'm not wedded to the RPi, so I can buy a Rockpi if that's better for this, and I could mount either inside the PC case. I'd need something to convert the drives from SATA to USB either way. If it's better/cheaper I could use two adapters that convert two drives each, rather than one for all four, or even one per drive with the RPi as I've got plenty of USB ports with the hat.\n\nAnother benefit with a PC case is being able to use quieter fans and I've got a few decent 120mm and 80mm ones spare. I'm not sure if any compact PC cases can accommodate 120mm fans though.\n\nI've got a spare 800W Gold PC PSU, but it would be a bit wasted on this and I should probably keep that in reserve for if my main PC's one dies. I've also got a passive PC power board which is powered by a Dell laptop PSU, but I'm not sure that can supply enough power for 4 3.5\" HDDs, so I might need to buy a PSU.", "author_fullname": "t2_4g7q3nm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need PC case or enclosure for 4+ drives and RPi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fnasy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698181446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve currently got four 16TB WD Elements USB drives and two 5TB USB drives hooked up to a RPi 4 with a USB expansion hat and it&amp;#39;s a bit of a mess with all the PSUs and USB leads, so I want to tidy it up by shucking the 16TB drives into a single enclosure. I&amp;#39;ll probably try to sell or repurpose the 5TB drives, partly because it&amp;#39;s a lot easier to find a 4-bay enclosure than a 6-bay one but also it&amp;#39;s not very cost/power efficient to run these relatively small drives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a Chia farm at the moment but I&amp;#39;ll probably switch it to a Storj node soon. Either way, I don&amp;#39;t think the drives will need to be constantly active, so whatever enclosure I use should allow them to sleep, and it should have isolation mounts to reduce vibration noise.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to spend a lot of money. If there&amp;#39;s a compact PC case with 4+ drive bays that I could use, that could be an option. I&amp;#39;m not wedded to the RPi, so I can buy a Rockpi if that&amp;#39;s better for this, and I could mount either inside the PC case. I&amp;#39;d need something to convert the drives from SATA to USB either way. If it&amp;#39;s better/cheaper I could use two adapters that convert two drives each, rather than one for all four, or even one per drive with the RPi as I&amp;#39;ve got plenty of USB ports with the hat.&lt;/p&gt;\n\n&lt;p&gt;Another benefit with a PC case is being able to use quieter fans and I&amp;#39;ve got a few decent 120mm and 80mm ones spare. I&amp;#39;m not sure if any compact PC cases can accommodate 120mm fans though.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a spare 800W Gold PC PSU, but it would be a bit wasted on this and I should probably keep that in reserve for if my main PC&amp;#39;s one dies. I&amp;#39;ve also got a passive PC power board which is powered by a Dell laptop PSU, but I&amp;#39;m not sure that can supply enough power for 4 3.5&amp;quot; HDDs, so I might need to buy a PSU.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fnasy", "is_robot_indexable": true, "report_reasons": null, "author": "Big-Finding2976", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fnasy/need_pc_case_or_enclosure_for_4_drives_and_rpi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fnasy/need_pc_case_or_enclosure_for_4_drives_and_rpi/", "subreddit_subscribers": 708519, "created_utc": 1698181446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i'm building an ios app that needs to get the transcript and description of a tikotk video, given a link to the video. what should my approach be? is it possible to use a selenium script and not face any limit issues?\n\nthanks", "author_fullname": "t2_4awfqs53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scraping tiktok", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fmgss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698179277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m building an ios app that needs to get the transcript and description of a tikotk video, given a link to the video. what should my approach be? is it possible to use a selenium script and not face any limit issues?&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fmgss", "is_robot_indexable": true, "report_reasons": null, "author": "lavendercandy19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fmgss/scraping_tiktok/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fmgss/scraping_tiktok/", "subreddit_subscribers": 708519, "created_utc": 1698179277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm an image hoarder... while most have a porn collection I have tons &amp; tons of pretty or cool anime illustrations since I just absolutely love them...\n\nHowever, I'm at a loss! Twitter has amazing artists but the worst ever set up for hosting art! There's no tag system agreed upon &amp; following means nothing since my timeline is flooded with every post &amp; retweet.\n\nIs there an easier way to keep track of the media tab per artist?\n\nI would have asked anywhere else if I knew where to ask, but I'm not even sure where I would ask this outside of here...\n\nI've googled but got nothing.\nI've been to GitHub &amp; FMHY as well.\nDanbooru &amp; Gelbooru have forums but I doubt you can ask this there...\n\nIf anyone knows of any tricks please I'm at a complete loss.\n\nThank you so much for your time \u208d\u208d (\u0328\u0321,,&gt; &lt;,, )\u0327\u0322 \u208e\u208e", "author_fullname": "t2_hyxmzsxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best &amp; Easiest Way to Keep Track of Twitter Artists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17flnd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698177189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an image hoarder... while most have a porn collection I have tons &amp;amp; tons of pretty or cool anime illustrations since I just absolutely love them...&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m at a loss! Twitter has amazing artists but the worst ever set up for hosting art! There&amp;#39;s no tag system agreed upon &amp;amp; following means nothing since my timeline is flooded with every post &amp;amp; retweet.&lt;/p&gt;\n\n&lt;p&gt;Is there an easier way to keep track of the media tab per artist?&lt;/p&gt;\n\n&lt;p&gt;I would have asked anywhere else if I knew where to ask, but I&amp;#39;m not even sure where I would ask this outside of here...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve googled but got nothing.\nI&amp;#39;ve been to GitHub &amp;amp; FMHY as well.\nDanbooru &amp;amp; Gelbooru have forums but I doubt you can ask this there...&lt;/p&gt;\n\n&lt;p&gt;If anyone knows of any tricks please I&amp;#39;m at a complete loss.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for your time \u208d\u208d (\u0328\u0321,,&amp;gt; &amp;lt;,, )\u0327\u0322 \u208e\u208e&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17flnd9", "is_robot_indexable": true, "report_reasons": null, "author": "nyaahilism", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17flnd9/best_easiest_way_to_keep_track_of_twitter_artists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17flnd9/best_easiest_way_to_keep_track_of_twitter_artists/", "subreddit_subscribers": 708519, "created_utc": 1698177189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've tried versions of YouTube-dl but everything I've tried grabs the trailer, not the movie. \n\nCan anyone figure out how to grab the actual movie from this site?\n\n[https://moviesjoyhd.to/watch-movie/ruby-62392.5463016](https://moviesjoyhd.to/watch-movie/ruby-62392.5463016)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_rr4vx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for way to grab video from website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fhvtl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698167612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried versions of YouTube-dl but everything I&amp;#39;ve tried grabs the trailer, not the movie. &lt;/p&gt;\n\n&lt;p&gt;Can anyone figure out how to grab the actual movie from this site?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://moviesjoyhd.to/watch-movie/ruby-62392.5463016\"&gt;https://moviesjoyhd.to/watch-movie/ruby-62392.5463016&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?auto=webp&amp;s=72d667fcb43d7ca350405433370ab9981d4c2304", "width": 648, "height": 425}, "resolutions": [{"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=048a3d82c4cb13d203b46c3451cd0e8bb68775b0", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3cc81a74c95709658369e4919a6ea418ec810b9f", "width": 216, "height": 141}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8de5120ff70363441983db2e5688ebb8662ea370", "width": 320, "height": 209}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=abe2e9c03f2495d51373c0dc2bdb2bffe9d26ebb", "width": 640, "height": 419}], "variants": {}, "id": "s6SBce-yDMS7QNV49GiC-FcN3ZvbxXmSnyMPJ7Ncx1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fhvtl", "is_robot_indexable": true, "report_reasons": null, "author": "MarkPugnerIII", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fhvtl/looking_for_way_to_grab_video_from_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fhvtl/looking_for_way_to_grab_video_from_website/", "subreddit_subscribers": 708519, "created_utc": 1698167612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The first rule of this sub states\n\n&gt;1. Search the Internet, this subreddit and our wiki before posting\n\n&gt;Search the internet, search the sub and check the wiki for commonly asked and answered questions. We aren't google.\n\nYet we get a post every other day asking about how to rip DVDs and Blu-rays which is something that has been discussed to death across the internet.", "author_fullname": "t2_ayza6h03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The first rule of this sub doesn't seem to be enforced enough at all(in relation to posts asking how rip DVDs and Blu-rays)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fgtmy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698164794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first rule of this sub states&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;ol&gt;\n&lt;li&gt;Search the Internet, this subreddit and our wiki before posting&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Search the internet, search the sub and check the wiki for commonly asked and answered questions. We aren&amp;#39;t google.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yet we get a post every other day asking about how to rip DVDs and Blu-rays which is something that has been discussed to death across the internet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17fgtmy", "is_robot_indexable": true, "report_reasons": null, "author": "ufs2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fgtmy/the_first_rule_of_this_sub_doesnt_seem_to_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fgtmy/the_first_rule_of_this_sub_doesnt_seem_to_be/", "subreddit_subscribers": 708519, "created_utc": 1698164794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, I'm looking to copy a volume onto another disk, so far - so good.\n\nHowever the volume I intend to clone is mirrored (and since I am on Windows, on two dynamic disks, lets call them A and B). I am yet to find a free partitioning tool that lets me copy the volume from A and B to a new drive D without a license. \n\nTried:\n\n* GParted: Doesn't work with Dynamic Disks\n* Windows Disk Management: No copy function\n* Paragon: Copying a volume from a Dynamic Disk requires license\n* AOMEI: same as Paragon\n* Mini Tool Partition Wizard: same as Paragon\n\nIs there a free tool, that let's me copy such a volume? Is there maybe even a way to achieve this copy without using a tool at all? Though I want the new disk D to remain a basic disk.", "author_fullname": "t2_tpbe6syf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best disk partitioning tool? (again)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fg59v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698163043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;m looking to copy a volume onto another disk, so far - so good.&lt;/p&gt;\n\n&lt;p&gt;However the volume I intend to clone is mirrored (and since I am on Windows, on two dynamic disks, lets call them A and B). I am yet to find a free partitioning tool that lets me copy the volume from A and B to a new drive D without a license. &lt;/p&gt;\n\n&lt;p&gt;Tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GParted: Doesn&amp;#39;t work with Dynamic Disks&lt;/li&gt;\n&lt;li&gt;Windows Disk Management: No copy function&lt;/li&gt;\n&lt;li&gt;Paragon: Copying a volume from a Dynamic Disk requires license&lt;/li&gt;\n&lt;li&gt;AOMEI: same as Paragon&lt;/li&gt;\n&lt;li&gt;Mini Tool Partition Wizard: same as Paragon&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is there a free tool, that let&amp;#39;s me copy such a volume? Is there maybe even a way to achieve this copy without using a tool at all? Though I want the new disk D to remain a basic disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fg59v", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Read9922", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fg59v/best_disk_partitioning_tool_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fg59v/best_disk_partitioning_tool_again/", "subreddit_subscribers": 708519, "created_utc": 1698163043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\n&amp;#x200B;\n\nI recently bought a new T7 Shield from Samsung to store more data (yay!). Now, apparently, I can't just read the SMART status of it. Before putting anything on it, I would like to confirm that it's healthy though. I went on a small Google journey, but I wasn't able to convince \\`smartctl\\` to do its' job. Now I tried to understand what the issue is, but I don't really get it.\n\nCould someone explain to me how I am supposed to validate the health of the SSD, even though Samsung put in effort to stop me from using the SMART status? Thanks a lot.\n\nI found this: https://www.smartmontools.org/ticket/1403 related ticket. From the GitHub issue (https://github.com/smartmontools/smartmontools/pull/102) I would say that the T7 is supported correctly now and I should be able to use smartctl on it just fine.", "author_fullname": "t2_lfnr7dbu3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELI5: SMART &amp; Samsung T7", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fek8r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698159157.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698158879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I recently bought a new T7 Shield from Samsung to store more data (yay!). Now, apparently, I can&amp;#39;t just read the SMART status of it. Before putting anything on it, I would like to confirm that it&amp;#39;s healthy though. I went on a small Google journey, but I wasn&amp;#39;t able to convince `smartctl` to do its&amp;#39; job. Now I tried to understand what the issue is, but I don&amp;#39;t really get it.&lt;/p&gt;\n\n&lt;p&gt;Could someone explain to me how I am supposed to validate the health of the SSD, even though Samsung put in effort to stop me from using the SMART status? Thanks a lot.&lt;/p&gt;\n\n&lt;p&gt;I found this: &lt;a href=\"https://www.smartmontools.org/ticket/1403\"&gt;https://www.smartmontools.org/ticket/1403&lt;/a&gt; related ticket. From the GitHub issue (&lt;a href=\"https://github.com/smartmontools/smartmontools/pull/102\"&gt;https://github.com/smartmontools/smartmontools/pull/102&lt;/a&gt;) I would say that the T7 is supported correctly now and I should be able to use smartctl on it just fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fek8r", "is_robot_indexable": true, "report_reasons": null, "author": "LinkeSpurParker", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fek8r/eli5_smart_samsung_t7/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fek8r/eli5_smart_samsung_t7/", "subreddit_subscribers": 708519, "created_utc": 1698158879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S.M.A.R.T. and HDD longevity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fd0zv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_y1qvau", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "storage", "selftext": "Hi there.\n\nI have a bunch of disks at home. Mostly 2,5\" USB/external, but 3 or 4 are just regular 3,5\" SATA drives.\n\nNever worried too much to look at SMART data until last week one of them started making funny \"click-like\" noises when I connect it to the PC. Makes 4 or 5 clicks and then it goes silent and data can be accessed as usual. Weird. And most probably a bad sign. Very bad sign.\n\nSince the the disk appears to be imminent to fail, I immediately thought of transferring the data to another disk. But before that I decided to check the clicking unit with smartctl, Linux cli utility.  \nPlease find below SMART data for that particular disk which to be fair is already some good \\~10 years old.\n\nAnd to rescue the data I decided to go with the most recent disk I have laying around which is a WD Blue 1TB drive that I think I bought in 2016. New at time of buying. Now 7 years old drive.\n\nPlease note that both drives have very little usage. Both have been stored in a drawer for most of their lifetime.\n\nI was basically shocked when looking at the SMART data. There's \"pre fail\" written all over the place on both drives.\n\nSo I went out to a local major reseller and bought 2 brand new Seagate SkyHawk 2TB 5400RPM 256MB SATA III. Yes, I know these drives were designed with surveillance in mind but those were the best deal I could find around for two 2TB 3,5\" SATA drives. I just had to solve my problem. I would deal with the details later.\n\nSo, I got home, connected both drives to my Pi server, ran a smartctl on it and...yep, lots of \"pre fail\" warnings.  \nSo...what the heck am I doing wrong? Where can I learn to properly interpret SMART reports? Looking at both my disks' reports, am I in trouble? Or the disks are fine'ish? Is it normal that a new disk reports pre fails??\n\nThanks in advance.\n\nCheers  \n\n\n    SMART - clicking drive\n    \n    rcorreia@pi3srv:~ $ cat smart_toshiba.txt                                     \n    smartctl 7.3 2022-02-28 r5338 [aarch64-linux-6.1.0-rpi4-rpi-v8] (local build)                                                                               \n    Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org                                                                                 \n                                                                                                                                                                \n    === START OF INFORMATION SECTION ===                                                                                                                        \n    Model Family:     Toshiba 2.5\" HDD MK..59GSM (AF)                                                                                                           \n    Device Model:     TOSHIBA MK1059GSM                                                                                                                         \n    Serial Number:    Y1S7P2QQT                                                                                                                                 \n    LU WWN Device Id: 5 000039 3a248e756                                                                                                                        \n    Firmware Version: GU001U                                                                                                                                    \n    User Capacity:    1,000,204,886,016 bytes [1.00 TB]                                                                                                         \n    Sector Sizes:     512 bytes logical, 4096 bytes physical                                                                                                    \n    Rotation Rate:    5400 rpm                                                                                                                                  \n    Form Factor:      2.5 inches                                                                                                                                \n    Device is:        In smartctl database 7.3/5319                                                                                                             \n    ATA Version is:   ATA8-ACS (minor revision not indicated)                                                                                                   \n    SATA Version is:  SATA 2.6, 3.0 Gb/s (current: 3.0 Gb/s)                                                                                                    \n    Local Time is:    Mon Oct 23 23:29:39 2023 WEST                                                                                                             \n    SMART support is: Available - device has SMART capability.                                                                                                  \n    SMART support is: Enabled                                                                                                                                   \n                                                                                                                                                                \n    === START OF READ SMART DATA SECTION ===                                                                                                                    \n    SMART overall-health self-assessment test result: PASSED                                                                                                    \n                                                                                                                                                                \n    General SMART Values:                                                                                                                                       \n    Offline data collection status:  (0x00) Offline data collection activity                                                                                    \n                                            was never started.                                                                                                  \n                                            Auto Offline Data Collection: Disabled.                                                                             \n    Self-test execution status:      (  25) The self-test routine was aborted by                                                                                \n                                            the host.                                                                                                           \n    Total time to complete Offline                                                                                                                              \n    data collection:                (  120) seconds.                                                                                                            \n    Offline data collection                                                                                                                                     \n    capabilities:                    (0x5b) SMART execute Offline immediate.                                                                                    \n                                            Auto Offline data collection on/off support.                                                                        \n                                            Suspend Offline collection upon new                                                                                 \n                                            command.                                                                                                            \n                                            Offline surface scan supported.                                                                                     \n                                            Self-test supported.                                                                                                \n                                            No Conveyance Self-test supported.                                                                                  \n                                            Selective Self-test supported.                                                                                      \n    SMART capabilities:            (0x0003) Saves SMART data before entering                                                                                    \n                                            power-saving mode.                                                                                                  \n                                            Supports SMART auto save timer.                                                                                     \n    Error logging capability:        (0x01) Error logging supported.                                                                                            \n                                            General Purpose Logging supported.    \n    Short self-test routine                                                                                                                                     \n    recommended polling time:        (   2) minutes.                              \n    Extended self-test routine                                                                                                                                  \n    recommended polling time:        ( 299) minutes.                                                                                                            \n    SCT capabilities:              (0x003d) SCT Status supported.                                                                                               \n                                            SCT Error Recovery Control supported.                                                                               \n                                            SCT Feature Control supported.                                                                                      \n                                            SCT Data Table supported.                                                                                           \n                                                                                                                                                                \n    SMART Attributes Data Structure revision number: 16                                                                                                         \n    Vendor Specific SMART Attributes with Thresholds:                                                                                                           \n    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE                                                            \n      1 Raw_Read_Error_Rate     0x000b   100   100   050    Pre-fail  Always       -       0                                                                    \n      2 Throughput_Performance  0x0005   100   100   050    Pre-fail  Offline      -       0                                                                    \n      3 Spin_Up_Time            0x0027   100   100   001    Pre-fail  Always       -       3956                                                                 \n      4 Start_Stop_Count        0x0032   100   100   000    Old_age   Always       -       100177                                                               \n      5 Reallocated_Sector_Ct   0x0033   100   100   050    Pre-fail  Always       -       0                                                                    \n      7 Seek_Error_Rate         0x000b   100   100   050    Pre-fail  Always       -       0                                                                    \n      8 Seek_Time_Performance   0x0005   100   100   050    Pre-fail  Offline      -       0                                                                    \n      9 Power_On_Hours          0x0032   046   046   000    Old_age   Always       -       21858                                                                \n     10 Spin_Retry_Count        0x0033   253   100   030    Pre-fail  Always       -       0                                                                    \n     12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       145                                                                  \n    191 G-Sense_Error_Rate      0x0032   100   100   000    Old_age   Always       -       18                                                                   \n    192 Power-Off_Retract_Count 0x0032   100   100   000    Old_age   Always       -       24                                                                   \n    193 Load_Cycle_Count        0x0032   071   071   000    Old_age   Always       -       294403                                                               \n    194 Temperature_Celsius     0x0022   100   100   000    Old_age   Always       -       30 (Min/Max 17/51)                                                   \n    196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       0                                                                    \n    197 Current_Pending_Sector  0x0032   100   100   000    Old_age   Always       -       0                                                                    \n    198 Offline_Uncorrectable   0x0030   100   100   000    Old_age   Offline      -       0                                                                    \n    199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0                                                                    \n    220 Disk_Shift              0x0002   100   100   000    Old_age   Always       -       76                                                                   \n    222 Loaded_Hours            0x0032   095   095   000    Old_age   Always       -       2339                                                                 \n    223 Load_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0                                                                    \n    224 Load_Friction           0x0022   100   100   000    Old_age   Always       -       0                                                                    \n    226 Load-in_Time            0x0026   100   100   000    Old_age   Always       -       273                                                                  \n    240 Head_Flying_Hours       0x0001   100   100   001    Pre-fail  Offline      -       0                                                                    \n                                                                                  \n    SMART Error Log Version: 1                                                                                                                                  \n    No Errors Logged                                                              \n                                                                                                                                                                \n    SMART Self-test log structure revision number 1                                                                                                             \n    Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error                                                             \n    # 1  Extended offline    Aborted by host               90%     21838         -                                                                              \n                                                                                                                                                                \n    SMART Selective self-test log data structure revision number 1                                                                                              \n     SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS                                                                                                                \n        1        0        0  Not_testing                                                                                                                        \n        2        0        0  Not_testing                                                                                                                        \n        3        0        0  Not_testing                                                                                                                        \n        4        0        0  Not_testing                                                                                                                        \n        5        0        0  Not_testing                                                                                                                        \n    Selective self-test flags (0x0):                                                                                                                            \n      After scanning selected spans, do NOT read-scan remainder of disk.                                                                                        \n    If Selective self-test is pending on power-up, resume after 0 minute delay.                                                                                 \n    \n\n&amp;#x200B;\n\n    SMART - drive to rescue the data\n    \n    rcorreia@pi3srv:~ $ cat smart_wd.txt                                                                                                                [47/295]\n    smartctl 7.3 2022-02-28 r5338 [aarch64-linux-6.1.0-rpi4-rpi-v8] (local build)\n    Copyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n    \n    === START OF INFORMATION SECTION ===\n    Model Family:     Western Digital Blue\n    Device Model:     WDC WD10EZRZ-00HTKB0\n    Serial Number:    WD-WCC4J2SYEF97\n    LU WWN Device Id: 5 0014ee 264920e36\n    Firmware Version: 01.01A01\n    User Capacity:    1,000,204,886,016 bytes [1.00 TB]\n    Sector Sizes:     512 bytes logical, 4096 bytes physical\n    Rotation Rate:    5400 rpm\n    Device is:        In smartctl database 7.3/5319\n    ATA Version is:   ACS-2, ACS-3 T13/2161-D revision 3b\n    SATA Version is:  SATA 3.1, 6.0 Gb/s (current: 3.0 Gb/s)\n    Local Time is:    Mon Oct 23 23:29:57 2023 WEST\n    SMART support is: Available - device has SMART capability.\n    SMART support is: Enabled\n    \n    === START OF READ SMART DATA SECTION ===\n    SMART overall-health self-assessment test result: PASSED\n    \n    General SMART Values:\n    Offline data collection status:  (0x82) Offline data collection activity\n                                            was completed without error.\n                                            Auto Offline Data Collection: Enabled. \n    Self-test execution status:      (   0) The previous self-test routine completed\n                                            without error or no self-test has ever  \n                                            been run.\n    Total time to complete Offline \n    data collection:                (13140) seconds.\n    Offline data collection\n    capabilities:                    (0x7b) SMART execute Offline immediate.\n                                            Auto Offline data collection on/off support.\n                                            Suspend Offline collection upon new\n                                            command.\n                                            Offline surface scan supported.\n                                            Self-test supported.\n                                            Conveyance Self-test supported.\n                                            Selective Self-test supported.\n    SMART capabilities:            (0x0003) Saves SMART data before entering\n                                            power-saving mode.\n                                            Supports SMART auto save timer.\n    Error logging capability:        (0x01) Error logging supported.\n                                            General Purpose Logging supported.\n    Short self-test routine \n    recommended polling time:        (   2) minutes.\n    Extended self-test routine\n    recommended polling time:        ( 149) minutes.\n    Conveyance self-test routine\n    recommended polling time:        (   5) minutes.\n    SCT capabilities:              (0x3035) SCT Status supported.\n                                            SCT Feature Control supported.\n                                            SCT Data Table supported.\n    \n    SMART Attributes Data Structure revision number: 16\n    Vendor Specific SMART Attributes with Thresholds:\n    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n      1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n      3 Spin_Up_Time            0x0027   137   136   021    Pre-fail  Always       -       4116\n      4 Start_Stop_Count        0x0032   100   100   000    Old_age   Always       -       168\n      5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n      7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n      9 Power_On_Hours          0x0032   099   098   000    Old_age   Always       -       1086\n     10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n     11 Calibration_Retry_Count 0x0032   100   253   000    Old_age   Always       -       0\n     12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       20\n    192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       3\n    193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       3132\n    194 Temperature_Celsius     0x0022   114   102   000    Old_age   Always       -       29\n    196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n    197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n    198 Offline_Uncorrectable   0x0030   200   200   000    Old_age   Offline      -       0\n    199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0\n    200 Multi_Zone_Error_Rate   0x0008   200   200   000    Old_age   Offline      -       0\n    \n    SMART Error Log Version: 1\n    No Errors Logged\n    \n    SMART Self-test log structure revision number 1\n    Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n    # 1  Short offline       Completed without error       00%      1049         - \n    \n    SMART Selective self-test log data structure revision number 1\n     SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n        1        0        0  Not_testing\n        2        0        0  Not_testing\n        3        0        0  Not_testing\n        4        0        0  Not_testing\n        5        0        0  Not_testing\n    Selective self-test flags (0x0):\n      After scanning selected spans, do NOT read-scan remainder of disk.\n    If Selective self-test is pending on power-up, resume after 0 minute delay.\n    \n    \n\n&amp;#x200B;", "author_fullname": "t2_y1qvau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S.M.A.R.T. and HDD longevity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/storage", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17exrvk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698101911.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.storage", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there.&lt;/p&gt;\n\n&lt;p&gt;I have a bunch of disks at home. Mostly 2,5&amp;quot; USB/external, but 3 or 4 are just regular 3,5&amp;quot; SATA drives.&lt;/p&gt;\n\n&lt;p&gt;Never worried too much to look at SMART data until last week one of them started making funny &amp;quot;click-like&amp;quot; noises when I connect it to the PC. Makes 4 or 5 clicks and then it goes silent and data can be accessed as usual. Weird. And most probably a bad sign. Very bad sign.&lt;/p&gt;\n\n&lt;p&gt;Since the the disk appears to be imminent to fail, I immediately thought of transferring the data to another disk. But before that I decided to check the clicking unit with smartctl, Linux cli utility.&lt;br/&gt;\nPlease find below SMART data for that particular disk which to be fair is already some good ~10 years old.&lt;/p&gt;\n\n&lt;p&gt;And to rescue the data I decided to go with the most recent disk I have laying around which is a WD Blue 1TB drive that I think I bought in 2016. New at time of buying. Now 7 years old drive.&lt;/p&gt;\n\n&lt;p&gt;Please note that both drives have very little usage. Both have been stored in a drawer for most of their lifetime.&lt;/p&gt;\n\n&lt;p&gt;I was basically shocked when looking at the SMART data. There&amp;#39;s &amp;quot;pre fail&amp;quot; written all over the place on both drives.&lt;/p&gt;\n\n&lt;p&gt;So I went out to a local major reseller and bought 2 brand new Seagate SkyHawk 2TB 5400RPM 256MB SATA III. Yes, I know these drives were designed with surveillance in mind but those were the best deal I could find around for two 2TB 3,5&amp;quot; SATA drives. I just had to solve my problem. I would deal with the details later.&lt;/p&gt;\n\n&lt;p&gt;So, I got home, connected both drives to my Pi server, ran a smartctl on it and...yep, lots of &amp;quot;pre fail&amp;quot; warnings.&lt;br/&gt;\nSo...what the heck am I doing wrong? Where can I learn to properly interpret SMART reports? Looking at both my disks&amp;#39; reports, am I in trouble? Or the disks are fine&amp;#39;ish? Is it normal that a new disk reports pre fails??&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n\n&lt;p&gt;Cheers  &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SMART - clicking drive\n\nrcorreia@pi3srv:~ $ cat smart_toshiba.txt                                     \nsmartctl 7.3 2022-02-28 r5338 [aarch64-linux-6.1.0-rpi4-rpi-v8] (local build)                                                                               \nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org                                                                                 \n\n=== START OF INFORMATION SECTION ===                                                                                                                        \nModel Family:     Toshiba 2.5&amp;quot; HDD MK..59GSM (AF)                                                                                                           \nDevice Model:     TOSHIBA MK1059GSM                                                                                                                         \nSerial Number:    Y1S7P2QQT                                                                                                                                 \nLU WWN Device Id: 5 000039 3a248e756                                                                                                                        \nFirmware Version: GU001U                                                                                                                                    \nUser Capacity:    1,000,204,886,016 bytes [1.00 TB]                                                                                                         \nSector Sizes:     512 bytes logical, 4096 bytes physical                                                                                                    \nRotation Rate:    5400 rpm                                                                                                                                  \nForm Factor:      2.5 inches                                                                                                                                \nDevice is:        In smartctl database 7.3/5319                                                                                                             \nATA Version is:   ATA8-ACS (minor revision not indicated)                                                                                                   \nSATA Version is:  SATA 2.6, 3.0 Gb/s (current: 3.0 Gb/s)                                                                                                    \nLocal Time is:    Mon Oct 23 23:29:39 2023 WEST                                                                                                             \nSMART support is: Available - device has SMART capability.                                                                                                  \nSMART support is: Enabled                                                                                                                                   \n\n=== START OF READ SMART DATA SECTION ===                                                                                                                    \nSMART overall-health self-assessment test result: PASSED                                                                                                    \n\nGeneral SMART Values:                                                                                                                                       \nOffline data collection status:  (0x00) Offline data collection activity                                                                                    \n                                        was never started.                                                                                                  \n                                        Auto Offline Data Collection: Disabled.                                                                             \nSelf-test execution status:      (  25) The self-test routine was aborted by                                                                                \n                                        the host.                                                                                                           \nTotal time to complete Offline                                                                                                                              \ndata collection:                (  120) seconds.                                                                                                            \nOffline data collection                                                                                                                                     \ncapabilities:                    (0x5b) SMART execute Offline immediate.                                                                                    \n                                        Auto Offline data collection on/off support.                                                                        \n                                        Suspend Offline collection upon new                                                                                 \n                                        command.                                                                                                            \n                                        Offline surface scan supported.                                                                                     \n                                        Self-test supported.                                                                                                \n                                        No Conveyance Self-test supported.                                                                                  \n                                        Selective Self-test supported.                                                                                      \nSMART capabilities:            (0x0003) Saves SMART data before entering                                                                                    \n                                        power-saving mode.                                                                                                  \n                                        Supports SMART auto save timer.                                                                                     \nError logging capability:        (0x01) Error logging supported.                                                                                            \n                                        General Purpose Logging supported.    \nShort self-test routine                                                                                                                                     \nrecommended polling time:        (   2) minutes.                              \nExtended self-test routine                                                                                                                                  \nrecommended polling time:        ( 299) minutes.                                                                                                            \nSCT capabilities:              (0x003d) SCT Status supported.                                                                                               \n                                        SCT Error Recovery Control supported.                                                                               \n                                        SCT Feature Control supported.                                                                                      \n                                        SCT Data Table supported.                                                                                           \n\nSMART Attributes Data Structure revision number: 16                                                                                                         \nVendor Specific SMART Attributes with Thresholds:                                                                                                           \nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE                                                            \n  1 Raw_Read_Error_Rate     0x000b   100   100   050    Pre-fail  Always       -       0                                                                    \n  2 Throughput_Performance  0x0005   100   100   050    Pre-fail  Offline      -       0                                                                    \n  3 Spin_Up_Time            0x0027   100   100   001    Pre-fail  Always       -       3956                                                                 \n  4 Start_Stop_Count        0x0032   100   100   000    Old_age   Always       -       100177                                                               \n  5 Reallocated_Sector_Ct   0x0033   100   100   050    Pre-fail  Always       -       0                                                                    \n  7 Seek_Error_Rate         0x000b   100   100   050    Pre-fail  Always       -       0                                                                    \n  8 Seek_Time_Performance   0x0005   100   100   050    Pre-fail  Offline      -       0                                                                    \n  9 Power_On_Hours          0x0032   046   046   000    Old_age   Always       -       21858                                                                \n 10 Spin_Retry_Count        0x0033   253   100   030    Pre-fail  Always       -       0                                                                    \n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       145                                                                  \n191 G-Sense_Error_Rate      0x0032   100   100   000    Old_age   Always       -       18                                                                   \n192 Power-Off_Retract_Count 0x0032   100   100   000    Old_age   Always       -       24                                                                   \n193 Load_Cycle_Count        0x0032   071   071   000    Old_age   Always       -       294403                                                               \n194 Temperature_Celsius     0x0022   100   100   000    Old_age   Always       -       30 (Min/Max 17/51)                                                   \n196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       0                                                                    \n197 Current_Pending_Sector  0x0032   100   100   000    Old_age   Always       -       0                                                                    \n198 Offline_Uncorrectable   0x0030   100   100   000    Old_age   Offline      -       0                                                                    \n199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0                                                                    \n220 Disk_Shift              0x0002   100   100   000    Old_age   Always       -       76                                                                   \n222 Loaded_Hours            0x0032   095   095   000    Old_age   Always       -       2339                                                                 \n223 Load_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0                                                                    \n224 Load_Friction           0x0022   100   100   000    Old_age   Always       -       0                                                                    \n226 Load-in_Time            0x0026   100   100   000    Old_age   Always       -       273                                                                  \n240 Head_Flying_Hours       0x0001   100   100   001    Pre-fail  Offline      -       0                                                                    \n\nSMART Error Log Version: 1                                                                                                                                  \nNo Errors Logged                                                              \n\nSMART Self-test log structure revision number 1                                                                                                             \nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error                                                             \n# 1  Extended offline    Aborted by host               90%     21838         -                                                                              \n\nSMART Selective self-test log data structure revision number 1                                                                                              \n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS                                                                                                                \n    1        0        0  Not_testing                                                                                                                        \n    2        0        0  Not_testing                                                                                                                        \n    3        0        0  Not_testing                                                                                                                        \n    4        0        0  Not_testing                                                                                                                        \n    5        0        0  Not_testing                                                                                                                        \nSelective self-test flags (0x0):                                                                                                                            \n  After scanning selected spans, do NOT read-scan remainder of disk.                                                                                        \nIf Selective self-test is pending on power-up, resume after 0 minute delay.                                                                                 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SMART - drive to rescue the data\n\nrcorreia@pi3srv:~ $ cat smart_wd.txt                                                                                                                [47/295]\nsmartctl 7.3 2022-02-28 r5338 [aarch64-linux-6.1.0-rpi4-rpi-v8] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Western Digital Blue\nDevice Model:     WDC WD10EZRZ-00HTKB0\nSerial Number:    WD-WCC4J2SYEF97\nLU WWN Device Id: 5 0014ee 264920e36\nFirmware Version: 01.01A01\nUser Capacity:    1,000,204,886,016 bytes [1.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    5400 rpm\nDevice is:        In smartctl database 7.3/5319\nATA Version is:   ACS-2, ACS-3 T13/2161-D revision 3b\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 3.0 Gb/s)\nLocal Time is:    Mon Oct 23 23:29:57 2023 WEST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x82) Offline data collection activity\n                                        was completed without error.\n                                        Auto Offline Data Collection: Enabled. \nSelf-test execution status:      (   0) The previous self-test routine completed\n                                        without error or no self-test has ever  \n                                        been run.\nTotal time to complete Offline \ndata collection:                (13140) seconds.\nOffline data collection\ncapabilities:                    (0x7b) SMART execute Offline immediate.\n                                        Auto Offline data collection on/off support.\n                                        Suspend Offline collection upon new\n                                        command.\n                                        Offline surface scan supported.\n                                        Self-test supported.\n                                        Conveyance Self-test supported.\n                                        Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                                        power-saving mode.\n                                        Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                                        General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:        (   2) minutes.\nExtended self-test routine\nrecommended polling time:        ( 149) minutes.\nConveyance self-test routine\nrecommended polling time:        (   5) minutes.\nSCT capabilities:              (0x3035) SCT Status supported.\n                                        SCT Feature Control supported.\n                                        SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x002f   200   200   051    Pre-fail  Always       -       0\n  3 Spin_Up_Time            0x0027   137   136   021    Pre-fail  Always       -       4116\n  4 Start_Stop_Count        0x0032   100   100   000    Old_age   Always       -       168\n  5 Reallocated_Sector_Ct   0x0033   200   200   140    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x002e   200   200   000    Old_age   Always       -       0\n  9 Power_On_Hours          0x0032   099   098   000    Old_age   Always       -       1086\n 10 Spin_Retry_Count        0x0032   100   100   000    Old_age   Always       -       0\n 11 Calibration_Retry_Count 0x0032   100   253   000    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       20\n192 Power-Off_Retract_Count 0x0032   200   200   000    Old_age   Always       -       3\n193 Load_Cycle_Count        0x0032   199   199   000    Old_age   Always       -       3132\n194 Temperature_Celsius     0x0022   114   102   000    Old_age   Always       -       29\n196 Reallocated_Event_Count 0x0032   200   200   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0032   200   200   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0030   200   200   000    Old_age   Offline      -       0\n199 UDMA_CRC_Error_Count    0x0032   200   200   000    Old_age   Always       -       0\n200 Multi_Zone_Error_Rate   0x0008   200   200   000    Old_age   Offline      -       0\n\nSMART Error Log Version: 1\nNo Errors Logged\n\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%      1049         - \n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2rgxx", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17exrvk", "is_robot_indexable": true, "report_reasons": null, "author": "rdscorreia", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/storage/comments/17exrvk/smart_and_hdd_longevity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/storage/comments/17exrvk/smart_and_hdd_longevity/", "subreddit_subscribers": 26765, "created_utc": 1698101911.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1698154660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.storage", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/storage/comments/17exrvk/smart_and_hdd_longevity/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fd0zv", "is_robot_indexable": true, "report_reasons": null, "author": "rdscorreia", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_17exrvk", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fd0zv/smart_and_hdd_longevity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/storage/comments/17exrvk/smart_and_hdd_longevity/", "subreddit_subscribers": 708519, "created_utc": 1698154660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm in the process of uploading tons of family and work videos to the cloud, and I've run into an issue where uploads will get \"stuck\" on certain video files. Nearly all of them are raw videos from my Canon camera (eg files like **MVI\\_0950.mov**).\n\nUpon further inspection, not only are these files not upload-able, but when I try to copy them from one drive to another, the transfer fails midway through, and it says \"cannot read from the source file or disk.\"\n\nHowever, the files are fully playable in VLC. They will also load into my video editing software, but do  crap out as they're being played on the timeline.\n\nThe drive in question is a secondary (E:) on my machine, a **Samsung SSD 980 Pro 2TB**, and it's not that old - bought it 2 years ago with the pre-built machine itself.\n\nI have **no problems reading/writing large amounts of NEW data** to the drive, and anything that I've put there within the lifespan of the drive itself (2 years) has no problem being moved, copied or uploaded.\n\nThe backstory on the \"OLD\" data (created prior to this drive) is that it came off of another computer that failed a while ago, but it was copied from a secondary storage drive (not the C: that failed in the previous machine).\n\nAdditionally, I have the **exact same files** on an external SSD, and those versions ***can*** be moved and copied without issue.\n\nI have run CHECK DISK on the E: and there are no errors. Windows also doesn't show any errors or issues when I inspect the problem files themselves.\n\nGiven the above, I'm mainly trying to deduce:\n\n* Were these errors introduced into the old files when they moved over from the previous machine? ie are the problems specific to the files themselves, or;\n* Should I be worried about this drive?\n\nGrateful for any tools or tips beyond the usual CHKDSK stuff!", "author_fullname": "t2_45w7m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Specific older video files are playable but can't be copied or transferred (\"cannot read from the source file or disk\")", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fgmgl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698164277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in the process of uploading tons of family and work videos to the cloud, and I&amp;#39;ve run into an issue where uploads will get &amp;quot;stuck&amp;quot; on certain video files. Nearly all of them are raw videos from my Canon camera (eg files like &lt;strong&gt;MVI_0950.mov&lt;/strong&gt;).&lt;/p&gt;\n\n&lt;p&gt;Upon further inspection, not only are these files not upload-able, but when I try to copy them from one drive to another, the transfer fails midway through, and it says &amp;quot;cannot read from the source file or disk.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;However, the files are fully playable in VLC. They will also load into my video editing software, but do  crap out as they&amp;#39;re being played on the timeline.&lt;/p&gt;\n\n&lt;p&gt;The drive in question is a secondary (E:) on my machine, a &lt;strong&gt;Samsung SSD 980 Pro 2TB&lt;/strong&gt;, and it&amp;#39;s not that old - bought it 2 years ago with the pre-built machine itself.&lt;/p&gt;\n\n&lt;p&gt;I have &lt;strong&gt;no problems reading/writing large amounts of NEW data&lt;/strong&gt; to the drive, and anything that I&amp;#39;ve put there within the lifespan of the drive itself (2 years) has no problem being moved, copied or uploaded.&lt;/p&gt;\n\n&lt;p&gt;The backstory on the &amp;quot;OLD&amp;quot; data (created prior to this drive) is that it came off of another computer that failed a while ago, but it was copied from a secondary storage drive (not the C: that failed in the previous machine).&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have the &lt;strong&gt;exact same files&lt;/strong&gt; on an external SSD, and those versions &lt;strong&gt;&lt;em&gt;can&lt;/em&gt;&lt;/strong&gt; be moved and copied without issue.&lt;/p&gt;\n\n&lt;p&gt;I have run CHECK DISK on the E: and there are no errors. Windows also doesn&amp;#39;t show any errors or issues when I inspect the problem files themselves.&lt;/p&gt;\n\n&lt;p&gt;Given the above, I&amp;#39;m mainly trying to deduce:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Were these errors introduced into the old files when they moved over from the previous machine? ie are the problems specific to the files themselves, or;&lt;/li&gt;\n&lt;li&gt;Should I be worried about this drive?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Grateful for any tools or tips beyond the usual CHKDSK stuff!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fgmgl", "is_robot_indexable": true, "report_reasons": null, "author": "OnlyLivingBoyInNY", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fgmgl/specific_older_video_files_are_playable_but_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fgmgl/specific_older_video_files_are_playable_but_cant/", "subreddit_subscribers": 708519, "created_utc": 1698164277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to store some memories for a long time like 40-50years. What would be a good solution? Will a few normal hdd\u2019s work or will they loose data overtime? Im not really gonna access it at all and it will just stay put in place for years without moving. Any advice will be appreciated.", "author_fullname": "t2_3rxtl7pp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to store data for a long time.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fja8o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.42, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698171166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to store some memories for a long time like 40-50years. What would be a good solution? Will a few normal hdd\u2019s work or will they loose data overtime? Im not really gonna access it at all and it will just stay put in place for years without moving. Any advice will be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fja8o", "is_robot_indexable": true, "report_reasons": null, "author": "sheryboy77", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fja8o/i_want_to_store_data_for_a_long_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fja8o/i_want_to_store_data_for_a_long_time/", "subreddit_subscribers": 708519, "created_utc": 1698171166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen posters that will obfuscate the name of the rar set password protect it and encrypt the file name and once you enter the correct password it is another rar archive.  What is the point of that wouldn't just password protecting and encrypting the file names be enough to keep an indexer from seeing what is inside?", "author_fullname": "t2_e7a0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAR why do posters do this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fwm2q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698207988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen posters that will obfuscate the name of the rar set password protect it and encrypt the file name and once you enter the correct password it is another rar archive.  What is the point of that wouldn&amp;#39;t just password protecting and encrypting the file names be enough to keep an indexer from seeing what is inside?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fwm2q", "is_robot_indexable": true, "report_reasons": null, "author": "gutty976", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fwm2q/rar_why_do_posters_do_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fwm2q/rar_why_do_posters_do_this/", "subreddit_subscribers": 708519, "created_utc": 1698207988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently bought a 2TB T7 shield and its only giving me constant speeds between 150 MBps and 200MBps write speeds on my windows PC.\nAlso I did run it in the CrystalDiskMark software and i was able to get speeds advertised on them. It\u2019s only when i try to write something onto it for real this happens.\nWhat could be wrong ??", "author_fullname": "t2_58sjzl2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "T7 shield kinda slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fumb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698201439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently bought a 2TB T7 shield and its only giving me constant speeds between 150 MBps and 200MBps write speeds on my windows PC.\nAlso I did run it in the CrystalDiskMark software and i was able to get speeds advertised on them. It\u2019s only when i try to write something onto it for real this happens.\nWhat could be wrong ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fumb0", "is_robot_indexable": true, "report_reasons": null, "author": "nova_pyman", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fumb0/t7_shield_kinda_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fumb0/t7_shield_kinda_slow/", "subreddit_subscribers": 708519, "created_utc": 1698201439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently downloaded my entire iCloud library which was 4tb's and I have it spread out over 4 external 2tb SSD hard drives because that is all I had free at the time. I also have about 8 other external SSD's with random stuff on them. I'd like to transfer all of the iCloud data and also backup the other smaller externals onto 1 larger external HD and then duplicate that onto another larger external. \n\nI'd like to purchase two at least 8TB+ (maybe even 16tb's) hard drives and after checking the usual suspects I'm finding it tough to decide and can't find anything on sale. Any suggestions? ", "author_fullname": "t2_qhc3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best deal on 8TB + external HD's?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fsxlm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698196436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently downloaded my entire iCloud library which was 4tb&amp;#39;s and I have it spread out over 4 external 2tb SSD hard drives because that is all I had free at the time. I also have about 8 other external SSD&amp;#39;s with random stuff on them. I&amp;#39;d like to transfer all of the iCloud data and also backup the other smaller externals onto 1 larger external HD and then duplicate that onto another larger external. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to purchase two at least 8TB+ (maybe even 16tb&amp;#39;s) hard drives and after checking the usual suspects I&amp;#39;m finding it tough to decide and can&amp;#39;t find anything on sale. Any suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fsxlm", "is_robot_indexable": true, "report_reasons": null, "author": "fatguybike", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fsxlm/best_deal_on_8tb_external_hds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fsxlm/best_deal_on_8tb_external_hds/", "subreddit_subscribers": 708519, "created_utc": 1698196436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I just bought three 1TB 2,5\u201d drives from a guy that replaced them with SSDs, and was selling these really cheap and near me. I plan to use one of these on my ps3 and got the other two just because was really cheap. \nOne WD drive was from 2018 with 2000 hours with no bad blocks, other was Toshiba from 2015 with no bad blocks and 2500 hours, and the last one was Toshiba from 2015 too but with 8000 hours and 49 bad blocks. I used easeus partition master to check all hdds. \n\nSo should I stop using this hdd? Is there a way to fix the bad blocks? How can I monitor if bad blocks are increasing without the need to fully scan the disk? Because it took a really long time to scan it.", "author_fullname": "t2_astog32v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad sectors on used HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fm9mj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698178768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I just bought three 1TB 2,5\u201d drives from a guy that replaced them with SSDs, and was selling these really cheap and near me. I plan to use one of these on my ps3 and got the other two just because was really cheap. \nOne WD drive was from 2018 with 2000 hours with no bad blocks, other was Toshiba from 2015 with no bad blocks and 2500 hours, and the last one was Toshiba from 2015 too but with 8000 hours and 49 bad blocks. I used easeus partition master to check all hdds. &lt;/p&gt;\n\n&lt;p&gt;So should I stop using this hdd? Is there a way to fix the bad blocks? How can I monitor if bad blocks are increasing without the need to fully scan the disk? Because it took a really long time to scan it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fm9mj", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Pick8773", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fm9mj/bad_sectors_on_used_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fm9mj/bad_sectors_on_used_hdd/", "subreddit_subscribers": 708519, "created_utc": 1698178768.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}