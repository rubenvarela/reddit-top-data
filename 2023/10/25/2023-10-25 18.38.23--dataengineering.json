{"kind": "Listing", "data": {"after": "t3_17g6da9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What made you get into data engineering and what is keeping you as one? I recently started self learning to become one but i\u2019m sure learning about data engineering is much different than actually being an engineer. Thanks", "author_fullname": "t2_2r5n0ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To my data engineers: why do you like working as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fr8d5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 97, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 97, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698191519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What made you get into data engineering and what is keeping you as one? I recently started self learning to become one but i\u2019m sure learning about data engineering is much different than actually being an engineer. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fr8d5", "is_robot_indexable": true, "report_reasons": null, "author": "naq98", "discussion_type": null, "num_comments": 110, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fr8d5/to_my_data_engineers_why_do_you_like_working_as_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fr8d5/to_my_data_engineers_why_do_you_like_working_as_a/", "subreddit_subscribers": 135900, "created_utc": 1698191519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Senior DE role, I've got this assignment.     \n\nBeen told it would take a couple of hours.    \n\nAssignment says(not the exact one, but similar):\n\n    Using an API(has streaming functionality), \n    stream data\n    model a data lake  \n    store the results in the data lake.\n    apply transformation for different layers.\n    store in a private github\n    \n    to consider: data quality, readability, maintaiabality of the code\n\nFrom experience, in an environment that works and all is prepared for development, creating a solution would take easily a day or two, depending on all the unforeseen complications that could come up, even more.    \n\nSetting it all up locally or applying for free tiers from the cloud providers would introduce a lot more time.     \n\nNot sure what I want to get posting this, just wanted to share my frustration. ", "author_fullname": "t2_mghwsvvb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think of this take home assignment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17flj6n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698176891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Senior DE role, I&amp;#39;ve got this assignment.     &lt;/p&gt;\n\n&lt;p&gt;Been told it would take a couple of hours.    &lt;/p&gt;\n\n&lt;p&gt;Assignment says(not the exact one, but similar):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Using an API(has streaming functionality), \nstream data\nmodel a data lake  \nstore the results in the data lake.\napply transformation for different layers.\nstore in a private github\n\nto consider: data quality, readability, maintaiabality of the code\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;From experience, in an environment that works and all is prepared for development, creating a solution would take easily a day or two, depending on all the unforeseen complications that could come up, even more.    &lt;/p&gt;\n\n&lt;p&gt;Setting it all up locally or applying for free tiers from the cloud providers would introduce a lot more time.     &lt;/p&gt;\n\n&lt;p&gt;Not sure what I want to get posting this, just wanted to share my frustration. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17flj6n", "is_robot_indexable": true, "report_reasons": null, "author": "No_Cancel_3754", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17flj6n/what_do_you_think_of_this_take_home_assignment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17flj6n/what_do_you_think_of_this_take_home_assignment/", "subreddit_subscribers": 135900, "created_utc": 1698176891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What\u2019s a good intermediate to advanced source that discusses data platform architectures? What works can what doesn\u2019t. When to use what and when. How are different technologies different. Real-time vs batch. Airflow and lake house vs dbt. Lake house vs data warehouse etc. \n\nBut not just a simple comparison, more of a detailed discussion of compete platform setups and comparing different ones. \n\nDoes this even exist?", "author_fullname": "t2_g7w5zhnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fv5m1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698203097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s a good intermediate to advanced source that discusses data platform architectures? What works can what doesn\u2019t. When to use what and when. How are different technologies different. Real-time vs batch. Airflow and lake house vs dbt. Lake house vs data warehouse etc. &lt;/p&gt;\n\n&lt;p&gt;But not just a simple comparison, more of a detailed discussion of compete platform setups and comparing different ones. &lt;/p&gt;\n\n&lt;p&gt;Does this even exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fv5m1", "is_robot_indexable": true, "report_reasons": null, "author": "long_spy200", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fv5m1/data_platform_architectures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fv5m1/data_platform_architectures/", "subreddit_subscribers": 135900, "created_utc": 1698203097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, we are developing a managed Airflow service at DoubleCloud and have a few hypotheses that I want to validate. We are seeking additional feedback from users who are already using Airflow in various environments, including self-hosted, MWAA, Astronomer, and possibly Composer (who knows, there might be someone out there).   \n\n\nBelow is a list of hypotheses. Please respond in the comments regarding whether each point holds true for you or not, or feel free to spark a debate:\n\n1. Airflow is primarily used as an orchestrator, not as a worker itself; therefore, the most intensive tasks should be executed outside of the worker nodes. This implies that the worker nodes should be quite small, requiring only hundreds of megabytes of RAM and about 0.25% of CPU maybe capacity.\n2. Utilizing spot instances could significantly reduce costs (by 40-60%) due to the stateless nature of the workers.\n3. Some users still rely on Airflow to run heavy workloads. We hypothesize that these users would require a diverse range of worker sizes, including those with GPU capabilities.\n4. In most scenarios, Airflow orchestrates Docker jobs on EKS, ECS, or Lambda. And to make it beneficial to have a streamlined process for setting up such integrations.\n\np.s  \nAlso if somebody wants to preview our service already just drop me a message or request an access [on our console](https://app.double.cloud/create) we have some gift cards perks and credit grants for those users.", "author_fullname": "t2_ho9zq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow usage hypothesis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g5crx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698242012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, we are developing a managed Airflow service at DoubleCloud and have a few hypotheses that I want to validate. We are seeking additional feedback from users who are already using Airflow in various environments, including self-hosted, MWAA, Astronomer, and possibly Composer (who knows, there might be someone out there).   &lt;/p&gt;\n\n&lt;p&gt;Below is a list of hypotheses. Please respond in the comments regarding whether each point holds true for you or not, or feel free to spark a debate:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Airflow is primarily used as an orchestrator, not as a worker itself; therefore, the most intensive tasks should be executed outside of the worker nodes. This implies that the worker nodes should be quite small, requiring only hundreds of megabytes of RAM and about 0.25% of CPU maybe capacity.&lt;/li&gt;\n&lt;li&gt;Utilizing spot instances could significantly reduce costs (by 40-60%) due to the stateless nature of the workers.&lt;/li&gt;\n&lt;li&gt;Some users still rely on Airflow to run heavy workloads. We hypothesize that these users would require a diverse range of worker sizes, including those with GPU capabilities.&lt;/li&gt;\n&lt;li&gt;In most scenarios, Airflow orchestrates Docker jobs on EKS, ECS, or Lambda. And to make it beneficial to have a streamlined process for setting up such integrations.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;p.s&lt;br/&gt;\nAlso if somebody wants to preview our service already just drop me a message or request an access &lt;a href=\"https://app.double.cloud/create\"&gt;on our console&lt;/a&gt; we have some gift cards perks and credit grants for those users.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17g5crx", "is_robot_indexable": true, "report_reasons": null, "author": "Gaploid", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g5crx/airflow_usage_hypothesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g5crx/airflow_usage_hypothesis/", "subreddit_subscribers": 135900, "created_utc": 1698242012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Leetcode / hackerran don\u2019t have extensive problems related to the above and I would like more practice problems. I asked chatgpt to make up some but any idea on how to get more discrete practice questions like the above (ie a question that is ultimately reduced to - recursively loop through a python dictionary to get a result or transform it )", "author_fullname": "t2_32eyna18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On DE interviews I am seeing a fair bit of questions related to transforming / aggregating json (essentially dictionary) data in python via standard library - but no online resources to practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fpd4q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698186439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Leetcode / hackerran don\u2019t have extensive problems related to the above and I would like more practice problems. I asked chatgpt to make up some but any idea on how to get more discrete practice questions like the above (ie a question that is ultimately reduced to - recursively loop through a python dictionary to get a result or transform it )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fpd4q", "is_robot_indexable": true, "report_reasons": null, "author": "acceptedcitizen", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fpd4q/on_de_interviews_i_am_seeing_a_fair_bit_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fpd4q/on_de_interviews_i_am_seeing_a_fair_bit_of/", "subreddit_subscribers": 135900, "created_utc": 1698186439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey DE folks, I've been diving deep into data engineering in the fintech industry and found it challenging to manage the external data sources. Fintech relies on tons of vendor data *(KYC, KYB, Buearue, Risk &amp; Fraud, Lending, etc.)* for making important decisions such as risk assessment, fraud detection, onboarding, and transaction processing.\n\nThe current data ingestion platforms like Fivetran and Airbyte don't cover fintech-specific data sources. *(e.g. Socure, Sentilink, Equifax, Ekata, etc.)* On the other hand, fintech aggregators like Alloy primarily focus on data aggregation rather than data ingestion.\n\nI would love to know how are you currently tackling the challenge of integrating external fintech data sources into your systems. Are there any specialized tools or strategies you've found particularly effective?", "author_fullname": "t2_lk8c99fy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing External Data Sources in Fintech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fxwgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698212972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey DE folks, I&amp;#39;ve been diving deep into data engineering in the fintech industry and found it challenging to manage the external data sources. Fintech relies on tons of vendor data &lt;em&gt;(KYC, KYB, Buearue, Risk &amp;amp; Fraud, Lending, etc.)&lt;/em&gt; for making important decisions such as risk assessment, fraud detection, onboarding, and transaction processing.&lt;/p&gt;\n\n&lt;p&gt;The current data ingestion platforms like Fivetran and Airbyte don&amp;#39;t cover fintech-specific data sources. &lt;em&gt;(e.g. Socure, Sentilink, Equifax, Ekata, etc.)&lt;/em&gt; On the other hand, fintech aggregators like Alloy primarily focus on data aggregation rather than data ingestion.&lt;/p&gt;\n\n&lt;p&gt;I would love to know how are you currently tackling the challenge of integrating external fintech data sources into your systems. Are there any specialized tools or strategies you&amp;#39;ve found particularly effective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fxwgi", "is_robot_indexable": true, "report_reasons": null, "author": "utsavshah17", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fxwgi/managing_external_data_sources_in_fintech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fxwgi/managing_external_data_sources_in_fintech/", "subreddit_subscribers": 135900, "created_utc": 1698212972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team has been heavily using Databricks for two years for our engineering, ML, and analytics. Mostly use PySpark for data processing tasks and the SQL editor for querying and we like it. However, the contract costs are higher so we need to explore alternative platforms that could offer a similar level.\n\nMost of the data are stored in AWS S3 and ingested into Databricks for further processing. After some preliminary research, we are considering migrating to a self-managed solution on AWS, potentially leveraging EC2 instances for running PySpark jobs and possibly utilizing AWS EMR for a more managed experience. Our primary goal is to transition to a more cost-effective solution.\n\nGiven the volume of data we handle, we plan to commence this migration journey by testing with a smaller dataset (million rows) to evaluate the performance and compare the cost and cost-efficiency of EC2 and EMR in comparison to Databricks.\n\nWe're leaning towards an open-source approach as much as possible to keep costs in check and I have more questions about this.\n\n1. Have any of you undertaken a similar migration from Databricks to AWS (EC2/EMR)?\n2. Are there any significant challenges, in terms of setup, performance, or cost, that we should anticipate?\n\nWe are in the early stages of this exploration and your suggestions could greatly shape our migration strategy. We appreciate any advice or insights you can provide!\n\nThank you in advance for your time and advice.", "author_fullname": "t2_akunr53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migration from Databricks to EC2/EMR: Seeking Cost-Efficient Open-Source Alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g64c0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698244206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team has been heavily using Databricks for two years for our engineering, ML, and analytics. Mostly use PySpark for data processing tasks and the SQL editor for querying and we like it. However, the contract costs are higher so we need to explore alternative platforms that could offer a similar level.&lt;/p&gt;\n\n&lt;p&gt;Most of the data are stored in AWS S3 and ingested into Databricks for further processing. After some preliminary research, we are considering migrating to a self-managed solution on AWS, potentially leveraging EC2 instances for running PySpark jobs and possibly utilizing AWS EMR for a more managed experience. Our primary goal is to transition to a more cost-effective solution.&lt;/p&gt;\n\n&lt;p&gt;Given the volume of data we handle, we plan to commence this migration journey by testing with a smaller dataset (million rows) to evaluate the performance and compare the cost and cost-efficiency of EC2 and EMR in comparison to Databricks.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re leaning towards an open-source approach as much as possible to keep costs in check and I have more questions about this.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Have any of you undertaken a similar migration from Databricks to AWS (EC2/EMR)?&lt;/li&gt;\n&lt;li&gt;Are there any significant challenges, in terms of setup, performance, or cost, that we should anticipate?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We are in the early stages of this exploration and your suggestions could greatly shape our migration strategy. We appreciate any advice or insights you can provide!&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your time and advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17g64c0", "is_robot_indexable": true, "report_reasons": null, "author": "north_pr", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g64c0/migration_from_databricks_to_ec2emr_seeking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g64c0/migration_from_databricks_to_ec2emr_seeking/", "subreddit_subscribers": 135900, "created_utc": 1698244206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've been a Data Scientist for a little over 7 years and don't necessarily see myself switching roles, but have recently thought that it would be nice to have the ability to take on some of the data engineering roles. I think it would also make myself more marketable if I could be more of a \"full stack\" DS as well as easier to work with from the perspective of the Data Engineers. \n\nI'm just curious if there are specific languages or environments that I should focus my learning efforts on? At my current company I know we use Kafka for the automated tasks and the data is housed in Snowflake. I've thought of learning Kafka since it's being used at my current role, but want to make sure that isn't somewhat of a wasted effort if I were to switch to a different company? ", "author_fullname": "t2_110aywvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Languages to Learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6p2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698245823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been a Data Scientist for a little over 7 years and don&amp;#39;t necessarily see myself switching roles, but have recently thought that it would be nice to have the ability to take on some of the data engineering roles. I think it would also make myself more marketable if I could be more of a &amp;quot;full stack&amp;quot; DS as well as easier to work with from the perspective of the Data Engineers. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just curious if there are specific languages or environments that I should focus my learning efforts on? At my current company I know we use Kafka for the automated tasks and the data is housed in Snowflake. I&amp;#39;ve thought of learning Kafka since it&amp;#39;s being used at my current role, but want to make sure that isn&amp;#39;t somewhat of a wasted effort if I were to switch to a different company? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17g6p2m", "is_robot_indexable": true, "report_reasons": null, "author": "lambo630", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g6p2m/best_languages_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g6p2m/best_languages_to_learn/", "subreddit_subscribers": 135900, "created_utc": 1698245823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of real estate data from multiple counties. Of course, all of them use a different format, with different columns, etc...\n\nI'm applying some cleaning steps and reformatting so all counties have identical data formats. Then, I'm using the generated parquet files to create an OLAP database or maybe just use DuckDB.\n\nIt's mostly a one-time thing, so I started just going through each manually with a notebook. However, we may have to run it a few times.\n\nSo I am working on a local Polars data pipeline that I've written myself (basically pipe and filters) where you pass in info about the data and desired transformations.\n\nI'm not really familiar with tools like dagster and dbt- might these be useful instead? Unfortunately, I can't spend too long just learning the tool at the moment.\n\nAny advice is appreciated. Data size is a few GB. Want multiple people to be able to query the end database. I'm the only developer.", "author_fullname": "t2_4axj4apz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I Reinventing the Wheel (local-ish Polars data pipeline)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fusbl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698201947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of real estate data from multiple counties. Of course, all of them use a different format, with different columns, etc...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m applying some cleaning steps and reformatting so all counties have identical data formats. Then, I&amp;#39;m using the generated parquet files to create an OLAP database or maybe just use DuckDB.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s mostly a one-time thing, so I started just going through each manually with a notebook. However, we may have to run it a few times.&lt;/p&gt;\n\n&lt;p&gt;So I am working on a local Polars data pipeline that I&amp;#39;ve written myself (basically pipe and filters) where you pass in info about the data and desired transformations.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not really familiar with tools like dagster and dbt- might these be useful instead? Unfortunately, I can&amp;#39;t spend too long just learning the tool at the moment.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated. Data size is a few GB. Want multiple people to be able to query the end database. I&amp;#39;m the only developer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fusbl", "is_robot_indexable": true, "report_reasons": null, "author": "waytoopunkrock", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fusbl/am_i_reinventing_the_wheel_localish_polars_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fusbl/am_i_reinventing_the_wheel_localish_polars_data/", "subreddit_subscribers": 135900, "created_utc": 1698201947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nFor those of you with experience using meltano in a production ELT setting, how do you like it?\n\nHigh level process is: Extract from system -&gt; load to Data Lake -&gt; pipelines to either data products (applications) or to data warehouse \"raw\" tables. Meltano would just handle the Extract and Load parts, not the transformations in the warehouse.\n\nI am in a large global manufacturing organization with about 100 factories across the globe. Each factory has a local SAP \"manufacturing execution system\" (MES). Essentially these are just oracle databases that track data about parts in production. We also have an endless number of API's from SaaS applications, like salesforce. And also a pretty hefty SAP ERP system.\n\nI am not too concerned with SAP ERP, as I would just use Azure SAP CDC connector in ADF with a batch schedule. But I am considering Meltano for the MES's and API extraction processes.\n\nFor these manufacturing systems, the latency in a batch job is not acceptable. The factory management needs to be able to report on where parts are in production with minimal delay (generally less than 5 minutes). For API's, batches are typically fine, but I just need a standard way to manage API extractions. Right now, we just have separate docker containers running a python script for each API.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meltano Experiences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6y21", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698246476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;For those of you with experience using meltano in a production ELT setting, how do you like it?&lt;/p&gt;\n\n&lt;p&gt;High level process is: Extract from system -&amp;gt; load to Data Lake -&amp;gt; pipelines to either data products (applications) or to data warehouse &amp;quot;raw&amp;quot; tables. Meltano would just handle the Extract and Load parts, not the transformations in the warehouse.&lt;/p&gt;\n\n&lt;p&gt;I am in a large global manufacturing organization with about 100 factories across the globe. Each factory has a local SAP &amp;quot;manufacturing execution system&amp;quot; (MES). Essentially these are just oracle databases that track data about parts in production. We also have an endless number of API&amp;#39;s from SaaS applications, like salesforce. And also a pretty hefty SAP ERP system.&lt;/p&gt;\n\n&lt;p&gt;I am not too concerned with SAP ERP, as I would just use Azure SAP CDC connector in ADF with a batch schedule. But I am considering Meltano for the MES&amp;#39;s and API extraction processes.&lt;/p&gt;\n\n&lt;p&gt;For these manufacturing systems, the latency in a batch job is not acceptable. The factory management needs to be able to report on where parts are in production with minimal delay (generally less than 5 minutes). For API&amp;#39;s, batches are typically fine, but I just need a standard way to manage API extractions. Right now, we just have separate docker containers running a python script for each API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17g6y21", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g6y21/meltano_experiences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g6y21/meltano_experiences/", "subreddit_subscribers": 135900, "created_utc": 1698246476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to use the output of a query from big query in a query for an internal server. Is this at all possible with snaplogic?", "author_fullname": "t2_wr2c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snaplogic question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17flk2t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698176956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to use the output of a query from big query in a query for an internal server. Is this at all possible with snaplogic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17flk2t", "is_robot_indexable": true, "report_reasons": null, "author": "blud97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17flk2t/snaplogic_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17flk2t/snaplogic_question/", "subreddit_subscribers": 135900, "created_utc": 1698176956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially, I fumbled into a DE job at a large commodities company. The department always hired external consultants. But it wasn\u2019t successful. Their solution was converting civil/materials engineers to manage their Data. It was easier to have a Civil engineer learn how to data engineer than vice versa. \n\nI was an external hire. I don\u2019t have coding experience. There is no data team, so essentially, I and another guy with a similar background. From what we have, it is a vendor database and gets transcribed into an internal database, mainly using SQL views. Then, the data gets connected to powerbi using a gateway. \n\nMy coworker set up a Linux server to run automated scripts to update the internal database. \n\nMost of the work is building Power BI reports and ensuring the scripts run. \n\nI\u2019m getting the hunch that my work isn\u2019t natural DE, and I\u2019m worried I won\u2019t be able to move forward.  I also don\u2019t have a team to learn from. \n\nWhat\u2019s the best way to get the most out of this role? \n\nIs it realistic to teach myself and use that to move forward, or would I be teaching myself bad habits?", "author_fullname": "t2_pcaci", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stumbled into DE job. Not sure if the work I do is developing actually competencies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17gavez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698256731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially, I fumbled into a DE job at a large commodities company. The department always hired external consultants. But it wasn\u2019t successful. Their solution was converting civil/materials engineers to manage their Data. It was easier to have a Civil engineer learn how to data engineer than vice versa. &lt;/p&gt;\n\n&lt;p&gt;I was an external hire. I don\u2019t have coding experience. There is no data team, so essentially, I and another guy with a similar background. From what we have, it is a vendor database and gets transcribed into an internal database, mainly using SQL views. Then, the data gets connected to powerbi using a gateway. &lt;/p&gt;\n\n&lt;p&gt;My coworker set up a Linux server to run automated scripts to update the internal database. &lt;/p&gt;\n\n&lt;p&gt;Most of the work is building Power BI reports and ensuring the scripts run. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m getting the hunch that my work isn\u2019t natural DE, and I\u2019m worried I won\u2019t be able to move forward.  I also don\u2019t have a team to learn from. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to get the most out of this role? &lt;/p&gt;\n\n&lt;p&gt;Is it realistic to teach myself and use that to move forward, or would I be teaching myself bad habits?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17gavez", "is_robot_indexable": true, "report_reasons": null, "author": "I-was_thinking", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17gavez/stumbled_into_de_job_not_sure_if_the_work_i_do_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17gavez/stumbled_into_de_job_not_sure_if_the_work_i_do_is/", "subreddit_subscribers": 135900, "created_utc": 1698256731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lowly analytics engineer here, who manages the entire data infrastructure for my company, using only plug-n-play tools like Fivetran, Stitch, and Segment. Our pipeline looks basically like this: Source &gt; Stitch &gt; Snowflake &gt; dbt &gt; Snowflake &gt; Looker.\n\nI would love insights from more experienced engineers on how you manage/process data deletion and masking, especially when it comes to GDPR compliance. My question:\n\nWhat is the best way for a SQL-only engineer to run a monthly job to \u2018mask\u2019 PII - name, email, address, phone, anything else we may be storing for deleted users? User records are deleted from source table (postgres) but not from Snowflake given the way Stitch or Fivetran load the data (incrementally?). We likely don\u2019t want to delete user records to avoid breakage in our revenue model. We do have a table which records which USER\\_IDs are being deleted in postgres. I'm wondering how I can leverage that to do an overwrite on the PII fields for these user IDs, on a monthly cadence, automated if possible. Our primary CI/CD with dbt Cloud isn't really equipped to do this, I don't think", "author_fullname": "t2_12aazb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Way to Execute a PII Masking Job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g8ovw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698251058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lowly analytics engineer here, who manages the entire data infrastructure for my company, using only plug-n-play tools like Fivetran, Stitch, and Segment. Our pipeline looks basically like this: Source &amp;gt; Stitch &amp;gt; Snowflake &amp;gt; dbt &amp;gt; Snowflake &amp;gt; Looker.&lt;/p&gt;\n\n&lt;p&gt;I would love insights from more experienced engineers on how you manage/process data deletion and masking, especially when it comes to GDPR compliance. My question:&lt;/p&gt;\n\n&lt;p&gt;What is the best way for a SQL-only engineer to run a monthly job to \u2018mask\u2019 PII - name, email, address, phone, anything else we may be storing for deleted users? User records are deleted from source table (postgres) but not from Snowflake given the way Stitch or Fivetran load the data (incrementally?). We likely don\u2019t want to delete user records to avoid breakage in our revenue model. We do have a table which records which USER_IDs are being deleted in postgres. I&amp;#39;m wondering how I can leverage that to do an overwrite on the PII fields for these user IDs, on a monthly cadence, automated if possible. Our primary CI/CD with dbt Cloud isn&amp;#39;t really equipped to do this, I don&amp;#39;t think&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17g8ovw", "is_robot_indexable": true, "report_reasons": null, "author": "ntdoyfanboy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g8ovw/best_way_to_execute_a_pii_masking_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g8ovw/best_way_to_execute_a_pii_masking_job/", "subreddit_subscribers": 135900, "created_utc": 1698251058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Date and DateTime Manipulation in Polars", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6ldz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1698245527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/date-and-datetime-manipulation-in-polar/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17g6ldz", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g6ldz/date_and_datetime_manipulation_in_polars/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/date-and-datetime-manipulation-in-polar/", "subreddit_subscribers": 135900, "created_utc": 1698245527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi dataengineering community,\n\nI have created my first open-source project and am now seeking honest feedback. My goal is to create an open-source alerts platform for developers, and I am releasing a beta version with a minimum feature set: [https://github.com/emitbase/emitbase-core](https://github.com/emitbase/emitbase-core).\n\nIf any of you can find the time to try it (or at least read through the [docs](https://emitbase.github.io/emitbase-website/docs/introduction)) and let me know what's wrong, if it's a bad idea, or what the product should contain to be usable, it would be very helpful for me.\n\nThank you very much!\n\nPS: I have also posted it in other Reddit communities. If you find it there as well, I apologize for any potential spam!", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My first open-source project: Emitbase (alerts platform for developers)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g06xh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698223147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi dataengineering community,&lt;/p&gt;\n\n&lt;p&gt;I have created my first open-source project and am now seeking honest feedback. My goal is to create an open-source alerts platform for developers, and I am releasing a beta version with a minimum feature set: &lt;a href=\"https://github.com/emitbase/emitbase-core\"&gt;https://github.com/emitbase/emitbase-core&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;If any of you can find the time to try it (or at least read through the &lt;a href=\"https://emitbase.github.io/emitbase-website/docs/introduction\"&gt;docs&lt;/a&gt;) and let me know what&amp;#39;s wrong, if it&amp;#39;s a bad idea, or what the product should contain to be usable, it would be very helpful for me.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n\n&lt;p&gt;PS: I have also posted it in other Reddit communities. If you find it there as well, I apologize for any potential spam!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?auto=webp&amp;s=bbc576480078d9c535fa1983ca2bc20db6308635", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a584e7acd8438e02e320c06cd6852de02a1bb9e8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6d783640c5a6f9011adb9993a011d86bfa724bd", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ae1e23c97189eb96fcfd961061eb5045b48038c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3d2cbb96ac461f3f2e2be01e3639c9dbfd37ca3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=501b07c33814703d2f10da6c195fe6d0a28c33a6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/5wjfMEVIJxM8MDSk8u51lArIlOXcK6Di2zmcCqSATNc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f44586156c92755409e698e0a18b6abebd537c0", "width": 1080, "height": 540}], "variants": {}, "id": "ByOyl7ohbkpzKuLd9nuYMq1NHFV8XQLKY0MoLk5CQzg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17g06xh", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g06xh/my_first_opensource_project_emitbase_alerts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g06xh/my_first_opensource_project_emitbase_alerts/", "subreddit_subscribers": 135900, "created_utc": 1698223147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Check out my latest Airflow video about  DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator!", "author_fullname": "t2_h4j43yry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17fyyj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uZy2Lwioi3g?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uZy2Lwioi3g?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uZy2Lwioi3g/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uZy2Lwioi3g?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17fyyj4", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/weHe6o4ePXHX6yUyqr5Fblyf2HFIuBoFjMROdYvoppc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698217506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check out my latest Airflow video about  DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/uZy2Lwioi3g", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Cqk44cGcKpM6ONahoVlsOwjKH50G9fwL7UNl1pvmtQ4.jpg?auto=webp&amp;s=0104b45765142f14b834dd84a27cfc66f29d1c6c", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Cqk44cGcKpM6ONahoVlsOwjKH50G9fwL7UNl1pvmtQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0e284f8743e5a16fbeae39c2b2f545b7fdaa0c6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Cqk44cGcKpM6ONahoVlsOwjKH50G9fwL7UNl1pvmtQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08c8e1b16bb828a6ff582b08e476c8008514f5d9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Cqk44cGcKpM6ONahoVlsOwjKH50G9fwL7UNl1pvmtQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec6e6fd26dbd5e8091c2aeba0446896e22d95108", "width": 320, "height": 240}], "variants": {}, "id": "d6dxYUo_UD8-QH3xgWC2t-MJ_7oZM9piSnjGDJl4MRY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17fyyj4", "is_robot_indexable": true, "report_reasons": null, "author": "Coder2j", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fyyj4/airflow_dockeroperator_endtoend_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/uZy2Lwioi3g", "subreddit_subscribers": 135900, "created_utc": 1698217506.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/uZy2Lwioi3g?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow DockerOperator: End-to-End Machine Learning Pipeline with Docker Operator\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/uZy2Lwioi3g/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New to DBT, looking around the interwebs for some kind of DBT package that lets me define specs for ERDs and displays them in the doc website.  Seems like people having been calling for this for a while, but not seeing any solid package recommendations.  Anybody got one?", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any update to a stable DBT ERD package?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fyf3m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698215139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to DBT, looking around the interwebs for some kind of DBT package that lets me define specs for ERDs and displays them in the doc website.  Seems like people having been calling for this for a while, but not seeing any solid package recommendations.  Anybody got one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fyf3m", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fyf3m/any_update_to_a_stable_dbt_erd_package/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fyf3m/any_update_to_a_stable_dbt_erd_package/", "subreddit_subscribers": 135900, "created_utc": 1698215139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone, \n\nI have this particular use case where we\u2019ve prepared a ML Model python code which we need to deploy of GCP. What would be the best approach since we can\u2019t afford to have any timeout errors. Time being on temporary basis we\u2019re using cloud functions. But due to 60 mins limitation for cloud functions we\u2019re looking for other alternatives. \n\nAny suggestions? \n\nThanks.", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying ML Model to GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fx1n2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698209589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone, &lt;/p&gt;\n\n&lt;p&gt;I have this particular use case where we\u2019ve prepared a ML Model python code which we need to deploy of GCP. What would be the best approach since we can\u2019t afford to have any timeout errors. Time being on temporary basis we\u2019re using cloud functions. But due to 60 mins limitation for cloud functions we\u2019re looking for other alternatives. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fx1n2", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fx1n2/deploying_ml_model_to_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fx1n2/deploying_ml_model_to_gcp/", "subreddit_subscribers": 135900, "created_utc": 1698209589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I am not sure if this is the best place to put this or not. I've been using airflow/composer for several years but I am hitting something I have not seen before and wanted to get some ideas of what to look at.\n\nIt's fairly easy to get a sigkill in your dag run by running out of CPU or memory in your workers. Been there, done that.\n\nThat is not my current situation.\n\nI have some jobs that dynamically get some configuration files and kicks off a different dag (TriggerDagRunOpetrator) for each config. We may kick off 1 or more dags at a time. If I only kick off 2 or 3, no problem. Actually, I can usually kick off more than that with no issues.\n\nBut when I get up to 10+, I start getting sigkill.\n\nOur workers are configured as 4 CPU, 8GB ram, and plenty of storage (set at 10GB per worker and they are at about 90% free storage).\n\nThe dags don't use much CPU or memory. They make a call to a machine that does some processing, returns when that finishes and passes back a file location in gcs (xcom string) and then that is sent to a gcstobq operator. Looking at stats, the workers are never above about 50% on CPU or memory, and usually less than that.\n\nLooking at the composer monitoring, I do not see us maxing out on anything. No errors other than sigkill, no pod evictions, etc.\n\nThe scheduler sometimes hits about 80% CPU but nothing over that and memory usage is minimal (less than 1GB all day long).\n\nDoes anyone have an idea of what I should look at to see why I am getting sigkills? Which log might have something? I've looked at everything I can think of.\n\nThanks.\n\n&amp;#x200B;", "author_fullname": "t2_7zl0a3dk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow and sigkill", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17gauzb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698256697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am not sure if this is the best place to put this or not. I&amp;#39;ve been using airflow/composer for several years but I am hitting something I have not seen before and wanted to get some ideas of what to look at.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s fairly easy to get a sigkill in your dag run by running out of CPU or memory in your workers. Been there, done that.&lt;/p&gt;\n\n&lt;p&gt;That is not my current situation.&lt;/p&gt;\n\n&lt;p&gt;I have some jobs that dynamically get some configuration files and kicks off a different dag (TriggerDagRunOpetrator) for each config. We may kick off 1 or more dags at a time. If I only kick off 2 or 3, no problem. Actually, I can usually kick off more than that with no issues.&lt;/p&gt;\n\n&lt;p&gt;But when I get up to 10+, I start getting sigkill.&lt;/p&gt;\n\n&lt;p&gt;Our workers are configured as 4 CPU, 8GB ram, and plenty of storage (set at 10GB per worker and they are at about 90% free storage).&lt;/p&gt;\n\n&lt;p&gt;The dags don&amp;#39;t use much CPU or memory. They make a call to a machine that does some processing, returns when that finishes and passes back a file location in gcs (xcom string) and then that is sent to a gcstobq operator. Looking at stats, the workers are never above about 50% on CPU or memory, and usually less than that.&lt;/p&gt;\n\n&lt;p&gt;Looking at the composer monitoring, I do not see us maxing out on anything. No errors other than sigkill, no pod evictions, etc.&lt;/p&gt;\n\n&lt;p&gt;The scheduler sometimes hits about 80% CPU but nothing over that and memory usage is minimal (less than 1GB all day long).&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea of what I should look at to see why I am getting sigkills? Which log might have something? I&amp;#39;ve looked at everything I can think of.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17gauzb", "is_robot_indexable": true, "report_reasons": null, "author": "solgul", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17gauzb/airflow_and_sigkill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17gauzb/airflow_and_sigkill/", "subreddit_subscribers": 135900, "created_utc": 1698256697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, so i worked as a freelance data engineer. Most of the time, i gets project which are batch based data pipeline setup for sources such as flat files, Facebook ads, Salesforce, crm, etc.\nThe data volume is also not too huge, it's like 10-15k rows a day maximum. I use python along with gcp.\nI feel like , i am not learning anything new, not growing. Since I work from my home, i feel less active and motivated. \nI wanna learn new and advance stuff, such as real time data processing, data pipelines involving volume like 1-2 million a day or more than that. \nHow can I learn that, any tips suggestions would be really helpful.\nThanks", "author_fullname": "t2_8et5juc7e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to improve.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17gassh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698256530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, so i worked as a freelance data engineer. Most of the time, i gets project which are batch based data pipeline setup for sources such as flat files, Facebook ads, Salesforce, crm, etc.\nThe data volume is also not too huge, it&amp;#39;s like 10-15k rows a day maximum. I use python along with gcp.\nI feel like , i am not learning anything new, not growing. Since I work from my home, i feel less active and motivated. \nI wanna learn new and advance stuff, such as real time data processing, data pipelines involving volume like 1-2 million a day or more than that. \nHow can I learn that, any tips suggestions would be really helpful.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17gassh", "is_robot_indexable": true, "report_reasons": null, "author": "readme31", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17gassh/need_suggestions_to_improve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17gassh/need_suggestions_to_improve/", "subreddit_subscribers": 135900, "created_utc": 1698256530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I thought this project by Anvaka was pretty cool:  \n[https://anvaka.github.io/sayit/?query=dataengineering](https://anvaka.github.io/sayit/?query=dataengineering)\n\nhttps://preview.redd.it/hpl8c4wxzdwb1.png?width=926&amp;format=png&amp;auto=webp&amp;s=32eba2842d196c68915490492913e6df3b787cea", "author_fullname": "t2_pl4q8ng7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where data engineers hang out on Reddit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": true, "media_metadata": {"hpl8c4wxzdwb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 70, "x": 108, "u": "https://preview.redd.it/hpl8c4wxzdwb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f18cf4237cfdf2ed84e4fb0c632737fd5b61dd76"}, {"y": 141, "x": 216, "u": "https://preview.redd.it/hpl8c4wxzdwb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b64ef83417e1e45915551b92a974b1a6d0882789"}, {"y": 210, "x": 320, "u": "https://preview.redd.it/hpl8c4wxzdwb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=698a1acb8b64f14eecca00b49d0e562bd3506cda"}, {"y": 420, "x": 640, "u": "https://preview.redd.it/hpl8c4wxzdwb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0a9282f093b2efbe976feb00ad900cb603f267a"}], "s": {"y": 608, "x": 926, "u": "https://preview.redd.it/hpl8c4wxzdwb1.png?width=926&amp;format=png&amp;auto=webp&amp;s=32eba2842d196c68915490492913e6df3b787cea"}, "id": "hpl8c4wxzdwb1"}}, "name": "t3_17gaqpq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_84AkyxMEfq9dPQmbQAtgc-Wi8iv-dKGdGMIPyhmWWQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698256380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought this project by Anvaka was pretty cool:&lt;br/&gt;\n&lt;a href=\"https://anvaka.github.io/sayit/?query=dataengineering\"&gt;https://anvaka.github.io/sayit/?query=dataengineering&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hpl8c4wxzdwb1.png?width=926&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32eba2842d196c68915490492913e6df3b787cea\"&gt;https://preview.redd.it/hpl8c4wxzdwb1.png?width=926&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32eba2842d196c68915490492913e6df3b787cea&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?auto=webp&amp;s=a23b0412b77111bb6ea939abefb35a1def2e131e", "width": 1712, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f7eaf1d30026c5d4f194d43601de927a6f8e98b", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e00dc8eecdc4693de2804f9521e4cc9e8c3db16", "width": 216, "height": 182}, {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a8e20e84dcb6e502e1cbf6008531c0ba66ed47e", "width": 320, "height": 270}, {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=192cca37d0e6ae1e22d8f29c33dbf6b81cc20764", "width": 640, "height": 541}, {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae6040375adb17a76d29a88ac103a1029066b0c0", "width": 960, "height": 811}, {"url": "https://external-preview.redd.it/iU-gZF9Q6oLDAAgE2M6tYRKKbV2U92ci6JWGsXTkxv0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c72fc98a00190b007fc770739d3e205711561e77", "width": 1080, "height": 913}], "variants": {}, "id": "sKZVCz4_R-jSodtclLt-OBW_BuUObldDB1sSMcBLAF0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17gaqpq", "is_robot_indexable": true, "report_reasons": null, "author": "droppedorphan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17gaqpq/where_data_engineers_hang_out_on_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17gaqpq/where_data_engineers_hang_out_on_reddit/", "subreddit_subscribers": 135900, "created_utc": 1698256380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "for context:\n\nI was hired as a data engineering lead on  a US-based consultancy firm and based from a previous experience, my responsibility is to lead the team, design solutions architecture, and maintain a system.  So most of it is technical and nature with minimal technical documentation or requirements gathering from the stake holders or clients.\n\nBut recently, I was hired as a DE lead, but when I started the job, my role was changed to tech. lead and so far 90% of what is do is creating solutions design, technical documentation,  creating backlogs/user stories, and working directly with clients, and communicate extensively with clients to collect information.  I have mixed admin and technical responsibilities but worked lesser (around 10%) on technical task.  \n\nMy questions is, are DE lead and technical lead in the US are of the same level especially on seniority and compensation?   I was told by my US counterpart that they are the same in the US but I'm skeptical because  I have worked in the same role in different countries and even MNC, tech leads are of higher seniority and payscale.\n\ntyia\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_unhenl1b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "difference between data engineering lead and tech. lead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g945z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698252155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;for context:&lt;/p&gt;\n\n&lt;p&gt;I was hired as a data engineering lead on  a US-based consultancy firm and based from a previous experience, my responsibility is to lead the team, design solutions architecture, and maintain a system.  So most of it is technical and nature with minimal technical documentation or requirements gathering from the stake holders or clients.&lt;/p&gt;\n\n&lt;p&gt;But recently, I was hired as a DE lead, but when I started the job, my role was changed to tech. lead and so far 90% of what is do is creating solutions design, technical documentation,  creating backlogs/user stories, and working directly with clients, and communicate extensively with clients to collect information.  I have mixed admin and technical responsibilities but worked lesser (around 10%) on technical task.  &lt;/p&gt;\n\n&lt;p&gt;My questions is, are DE lead and technical lead in the US are of the same level especially on seniority and compensation?   I was told by my US counterpart that they are the same in the US but I&amp;#39;m skeptical because  I have worked in the same role in different countries and even MNC, tech leads are of higher seniority and payscale.&lt;/p&gt;\n\n&lt;p&gt;tyia&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17g945z", "is_robot_indexable": true, "report_reasons": null, "author": "ExpertClassroom1534", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g945z/difference_between_data_engineering_lead_and_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g945z/difference_between_data_engineering_lead_and_tech/", "subreddit_subscribers": 135900, "created_utc": 1698252155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI am trying to consider my options for creating a central hub for data, a lot of the current reporting from previous employees goes direct to source and makes it really difficult to manage and departments have freedom to connect straight to source which needs cutting out ASAP. \n\nI need an idea of which Microsoft cloud tools would be best to make use of for connecting to business apps and databases as well as provide end users with a mechanism to upload their own CSV files which we can then import into our own central hub. \n\nI\u2019ve messed around with Fabric and made a proof of concept but I feel it\u2019s still in its infancy and doesn\u2019t feel right to have all our data in there without any backup options currently available. \n\nMy main idea is to use Azure Files or Azure Storage Explorer to upload the files to, I\u2019ve not used this but I\u2019m sure we can sort security for each folder as this needs to be exposed to the end user without building a web app. \n\nPython function app to fetch data from an API or any logic that needs to be implemented. \n\nADF to connect to APIs, Function App and Databases \n\nAzure SQL Server to store data. \n\nPower BI as the reporting layer.\n\nI don\u2019t believe there is a vast amount of data, my previous organisation had a lot of transactional data  whereas the main issue here is just having a central place for getting data from that supports end users putting their csv files in.", "author_fullname": "t2_fd5v0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Azure data storage should we use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g8nds", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698250950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to consider my options for creating a central hub for data, a lot of the current reporting from previous employees goes direct to source and makes it really difficult to manage and departments have freedom to connect straight to source which needs cutting out ASAP. &lt;/p&gt;\n\n&lt;p&gt;I need an idea of which Microsoft cloud tools would be best to make use of for connecting to business apps and databases as well as provide end users with a mechanism to upload their own CSV files which we can then import into our own central hub. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve messed around with Fabric and made a proof of concept but I feel it\u2019s still in its infancy and doesn\u2019t feel right to have all our data in there without any backup options currently available. &lt;/p&gt;\n\n&lt;p&gt;My main idea is to use Azure Files or Azure Storage Explorer to upload the files to, I\u2019ve not used this but I\u2019m sure we can sort security for each folder as this needs to be exposed to the end user without building a web app. &lt;/p&gt;\n\n&lt;p&gt;Python function app to fetch data from an API or any logic that needs to be implemented. &lt;/p&gt;\n\n&lt;p&gt;ADF to connect to APIs, Function App and Databases &lt;/p&gt;\n\n&lt;p&gt;Azure SQL Server to store data. &lt;/p&gt;\n\n&lt;p&gt;Power BI as the reporting layer.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t believe there is a vast amount of data, my previous organisation had a lot of transactional data  whereas the main issue here is just having a central place for getting data from that supports end users putting their csv files in.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17g8nds", "is_robot_indexable": true, "report_reasons": null, "author": "12Eerc", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g8nds/what_azure_data_storage_should_we_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g8nds/what_azure_data_storage_should_we_use/", "subreddit_subscribers": 135900, "created_utc": 1698250950.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nFor a hobby project, I currently have a pipeline, where each 20 minutes, some JSON files are added to an S3 bucket. I would like to run some analytics on the data in the S3 bucket.\n\nIn the past, I have used a Glue crawler that analyze the schema and add new data (using subfolder partitioning) into a table in AWS Data Catalog. This however costs me over 6$ running hourly for just three days (about 2000 small new files in three days). As this is a hobby project, I would like to keep the costs to a minimum.\n\nI decided that I would update the Athena table manually, after uploading the files to S3, I would invoke  a lambda function to add the data into the Athena table. This should be way more cost-effective. I don't know however if Athena accepts INSERT statements.\n\n**What is the best way to insert new data into Athena manually (using a Python Lambda function for example) ?**", "author_fullname": "t2_nq65wse7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add data into Athena table without a Glue Crawler ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6i51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698245278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For a hobby project, I currently have a pipeline, where each 20 minutes, some JSON files are added to an S3 bucket. I would like to run some analytics on the data in the S3 bucket.&lt;/p&gt;\n\n&lt;p&gt;In the past, I have used a Glue crawler that analyze the schema and add new data (using subfolder partitioning) into a table in AWS Data Catalog. This however costs me over 6$ running hourly for just three days (about 2000 small new files in three days). As this is a hobby project, I would like to keep the costs to a minimum.&lt;/p&gt;\n\n&lt;p&gt;I decided that I would update the Athena table manually, after uploading the files to S3, I would invoke  a lambda function to add the data into the Athena table. This should be way more cost-effective. I don&amp;#39;t know however if Athena accepts INSERT statements.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What is the best way to insert new data into Athena manually (using a Python Lambda function for example) ?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17g6i51", "is_robot_indexable": true, "report_reasons": null, "author": "lancelot_of_camelot", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g6i51/how_to_add_data_into_athena_table_without_a_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g6i51/how_to_add_data_into_athena_table_without_a_glue/", "subreddit_subscribers": 135900, "created_utc": 1698245278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any recs on tooling? Or an overview on a plausible process?\n\nSay we need to add a new column to an existing table. \n\n- Alter table, add column with a default\n- Re-load all data to correct the new column value to the correct value\n\nOr is something like inserting all values of the single column by matching row IDs/PKs viable. \n\nAnd can anyone confirm - CH doesn't have transactions, so if a migration fails part way through the change won't be rolled back?\n\nThanks for any and all pointers.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema migration for Clickhouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6da9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698244900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any recs on tooling? Or an overview on a plausible process?&lt;/p&gt;\n\n&lt;p&gt;Say we need to add a new column to an existing table. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Alter table, add column with a default&lt;/li&gt;\n&lt;li&gt;Re-load all data to correct the new column value to the correct value&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Or is something like inserting all values of the single column by matching row IDs/PKs viable. &lt;/p&gt;\n\n&lt;p&gt;And can anyone confirm - CH doesn&amp;#39;t have transactions, so if a migration fails part way through the change won&amp;#39;t be rolled back?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any and all pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17g6da9", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17g6da9/schema_migration_for_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17g6da9/schema_migration_for_clickhouse/", "subreddit_subscribers": 135900, "created_utc": 1698244900.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}