{"kind": "Listing", "data": {"after": "t3_17g5cyv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "PSA, if anyone here is hoarding software you might want to mirror https://flings.vmware.com before 26.10. Full announcement from a VMware employee https://twitter.com/lamw/status/1716909940345045276?s=20 and https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;utm_medium=web2x&amp;context=3\n\nContext: Broadcom is taking over VMware so the flings website would be down either permanently (speculation) or for an extended period of time. Many people rely on Flings to achieve some niche solutions using VMware software. If you hoard such data you might be interested. Unfortunately I don\u2019t have the resources currently to mirror the entire thing and given the short period I can\u2019t realistically buy enough storage on time. \n\nEnjoy and thanks to however preserves those invaluable resources!\n\nEdit: Now available here: https://archive.org/download/flings.vmware.com", "author_fullname": "t2_hhere4lkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: VMware Flings website to be taken offline indefinitely", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fvy4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698235934.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698205663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PSA, if anyone here is hoarding software you might want to mirror &lt;a href=\"https://flings.vmware.com\"&gt;https://flings.vmware.com&lt;/a&gt; before 26.10. Full announcement from a VMware employee &lt;a href=\"https://twitter.com/lamw/status/1716909940345045276?s=20\"&gt;https://twitter.com/lamw/status/1716909940345045276?s=20&lt;/a&gt; and &lt;a href=\"https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/vmware/comments/17fniqp/psa_vmware_fling_will_be_down_on_thur_1026_please/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context: Broadcom is taking over VMware so the flings website would be down either permanently (speculation) or for an extended period of time. Many people rely on Flings to achieve some niche solutions using VMware software. If you hoard such data you might be interested. Unfortunately I don\u2019t have the resources currently to mirror the entire thing and given the short period I can\u2019t realistically buy enough storage on time. &lt;/p&gt;\n\n&lt;p&gt;Enjoy and thanks to however preserves those invaluable resources!&lt;/p&gt;\n\n&lt;p&gt;Edit: Now available here: &lt;a href=\"https://archive.org/download/flings.vmware.com\"&gt;https://archive.org/download/flings.vmware.com&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?auto=webp&amp;s=d073d57db5cc8bf41fdb15957274b08142891bd0", "width": 743, "height": 743}, "resolutions": [{"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19eee4059fa03c7321a40b81b3212a9c6b48e6ee", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e725264f4f4a6044d97d7c67c1423120e63532b6", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b6bd3655d4ce931ab1927be3c55703319c389f8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/LFTMds0SDIoUDl_WDxgwXJkKa28lAavtal8huebgu2Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=84572871cf288f66a0b0b8f5e4e86416bd073c5c", "width": 640, "height": 640}], "variants": {}, "id": "zlmL2m712SPSuZaEjTxkb2xX_TxZkQ2wNjwtx7OhFE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fvy4p", "is_robot_indexable": true, "report_reasons": null, "author": "Is-Not-El", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fvy4p/psa_vmware_flings_website_to_be_taken_offline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fvy4p/psa_vmware_flings_website_to_be_taken_offline/", "subreddit_subscribers": 708592, "created_utc": 1698205663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's time to ditch all my 2TB WD Green Drives. They are dying one by one...and I am losing my data.   \nI have been wanting to build a 200TB NAS with data protections for a long time. \n\nIt will be mainly used to HOARD all 4K remux Movies, TV Series, and Music.   \nAnd, It also will be used as Plex server, 4-6 family members will be using it with plex simultaneously. \n\nI have been lurking around forums and reddit posts. \n\nHardware I am thinking of getting is listed below   \nCPU: Intel W-1290P (Integrated GPU for plex transcoding)   \nMobo: Asus W480 ACE   \nRam: 128GB ECC DDR4 2933MHZ   \nSSD: 256GB M.2   \nHDD: Seagate EXOS 18TB   \nController Card:  LSI SAS 9300-16I    \nNIC: Asus 10gbps   \nCASE: Fractal Design 7 XL  \n\nSystem: TrueNas Scale   \nRaid: Raidz2 x2 \n\nSome Questions in my mind  \n\\- Does it need L2ARC ?  If so, what size ?   \n\\- Is 128GB Ram enough for this set up ?    \n\\- Will it hold 1000+ torrents seeding at the same time ? \n\nOr any suggestion to build it better would be appreciated !!", "author_fullname": "t2_4qm6o5ih", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Suggestion - Building 200TB NAS mainly for Media Hoarder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fzrmy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698221206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s time to ditch all my 2TB WD Green Drives. They are dying one by one...and I am losing my data.&lt;br/&gt;\nI have been wanting to build a 200TB NAS with data protections for a long time. &lt;/p&gt;\n\n&lt;p&gt;It will be mainly used to HOARD all 4K remux Movies, TV Series, and Music.&lt;br/&gt;\nAnd, It also will be used as Plex server, 4-6 family members will be using it with plex simultaneously. &lt;/p&gt;\n\n&lt;p&gt;I have been lurking around forums and reddit posts. &lt;/p&gt;\n\n&lt;p&gt;Hardware I am thinking of getting is listed below&lt;br/&gt;\nCPU: Intel W-1290P (Integrated GPU for plex transcoding)&lt;br/&gt;\nMobo: Asus W480 ACE&lt;br/&gt;\nRam: 128GB ECC DDR4 2933MHZ&lt;br/&gt;\nSSD: 256GB M.2&lt;br/&gt;\nHDD: Seagate EXOS 18TB&lt;br/&gt;\nController Card:  LSI SAS 9300-16I&lt;br/&gt;\nNIC: Asus 10gbps&lt;br/&gt;\nCASE: Fractal Design 7 XL  &lt;/p&gt;\n\n&lt;p&gt;System: TrueNas Scale&lt;br/&gt;\nRaid: Raidz2 x2 &lt;/p&gt;\n\n&lt;p&gt;Some Questions in my mind&lt;br/&gt;\n- Does it need L2ARC ?  If so, what size ?&lt;br/&gt;\n- Is 128GB Ram enough for this set up ?&lt;br/&gt;\n- Will it hold 1000+ torrents seeding at the same time ? &lt;/p&gt;\n\n&lt;p&gt;Or any suggestion to build it better would be appreciated !!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fzrmy", "is_robot_indexable": true, "report_reasons": null, "author": "Local-Bag-1045", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fzrmy/need_suggestion_building_200tb_nas_mainly_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fzrmy/need_suggestion_building_200tb_nas_mainly_for/", "subreddit_subscribers": 708592, "created_utc": 1698221206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently running Synology and I love the OS and everything around it, but it is getting really expensive to actually increase the size.\n\nI have about 110 TB right now, and the prices of expanding are getting a bit out of hand.\n\nSeriously considering dumping Syno all together and moving to something cheaper and friendlier storage expansion wise.\n\nLet's say I am planning on 500TB or so. How would that work? Can I even add 15+ drives to one motherboard? Will I need multiple motherboards? What CPU/GPU will I need to make Plex transcode and work properly on multiple devices at once? I assume I will need an external rack as well for the drives + cooling.\n\nI was hoping someone could maybe make simple list on [https://pcpartpicker.com/list/](https://pcpartpicker.com/list/) of stuff I need to actually do it.\n\nI will do it in small steps when things maybe go on discount so pricing is not a deal breaker, I just want something that can be easily expandable and only limited by the amount of drives I have. So if I want 30 drives, being able to actually have 30 of them.\n\nI build my last few PCs so I can start working on this as I go, and then when it is complete I would just need to move all the drives from my Syno to the new build.\n\nI have these parts from my old PC - [https://pcpartpicker.com/list/KqzPTB](https://pcpartpicker.com/list/KqzPTB).\n\nIf any of those can be reused, even better.", "author_fullname": "t2_ia3wt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hardware list for a 500TB+ media server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fpm6c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698187102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently running Synology and I love the OS and everything around it, but it is getting really expensive to actually increase the size.&lt;/p&gt;\n\n&lt;p&gt;I have about 110 TB right now, and the prices of expanding are getting a bit out of hand.&lt;/p&gt;\n\n&lt;p&gt;Seriously considering dumping Syno all together and moving to something cheaper and friendlier storage expansion wise.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I am planning on 500TB or so. How would that work? Can I even add 15+ drives to one motherboard? Will I need multiple motherboards? What CPU/GPU will I need to make Plex transcode and work properly on multiple devices at once? I assume I will need an external rack as well for the drives + cooling.&lt;/p&gt;\n\n&lt;p&gt;I was hoping someone could maybe make simple list on &lt;a href=\"https://pcpartpicker.com/list/\"&gt;https://pcpartpicker.com/list/&lt;/a&gt; of stuff I need to actually do it.&lt;/p&gt;\n\n&lt;p&gt;I will do it in small steps when things maybe go on discount so pricing is not a deal breaker, I just want something that can be easily expandable and only limited by the amount of drives I have. So if I want 30 drives, being able to actually have 30 of them.&lt;/p&gt;\n\n&lt;p&gt;I build my last few PCs so I can start working on this as I go, and then when it is complete I would just need to move all the drives from my Syno to the new build.&lt;/p&gt;\n\n&lt;p&gt;I have these parts from my old PC - &lt;a href=\"https://pcpartpicker.com/list/KqzPTB\"&gt;https://pcpartpicker.com/list/KqzPTB&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;If any of those can be reused, even better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fpm6c", "is_robot_indexable": true, "report_reasons": null, "author": "nsoifer", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fpm6c/hardware_list_for_a_500tb_media_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fpm6c/hardware_list_for_a_500tb_media_server/", "subreddit_subscribers": 708592, "created_utc": 1698187102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought my Lsi controller about 5yrs ago never had any issues until yesterday when I lost power. None of my drives start up anymore. I tried to troubleshoot and seen I should have 1 of the lights flashing like a heartbeat. They all stay solid. Is my card dead? Any help would be appreciated.", "author_fullname": "t2_4w2o2ea4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSI 9201-8I lights.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g3sex", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698237494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought my Lsi controller about 5yrs ago never had any issues until yesterday when I lost power. None of my drives start up anymore. I tried to troubleshoot and seen I should have 1 of the lights flashing like a heartbeat. They all stay solid. Is my card dead? Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g3sex", "is_robot_indexable": true, "report_reasons": null, "author": "jsandy83", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g3sex/lsi_92018i_lights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g3sex/lsi_92018i_lights/", "subreddit_subscribers": 708592, "created_utc": 1698237494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am looking for a medium capacity scanner with high degree of OCR accuracy.  5,000 pages per day is fine, at least 50 ADP capacity.\n\nThe documents are medical-legal, so HIPAA compliance and the ability to retain the original image just in case, because the goal is to not have to keep 1,000 pages of paper in storage for years (per patient...it adds up).\n\nThis is mainly for QME medical record review - a lot of these documents are copies of copies, poor quality faxes, etc.\n\nHow important is it for a scanner to have accurate OCR, versus running it through an OCR program?\n\nBudget is under $700. \n\nI already checked out other threads on scanners and am looking at the Raven Pro and Canon Image formula RS40, but I still have questions about OCR accuracy mainly.", "author_fullname": "t2_fdqlsef1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Document scanning &amp; OCR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g754o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698246999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a medium capacity scanner with high degree of OCR accuracy.  5,000 pages per day is fine, at least 50 ADP capacity.&lt;/p&gt;\n\n&lt;p&gt;The documents are medical-legal, so HIPAA compliance and the ability to retain the original image just in case, because the goal is to not have to keep 1,000 pages of paper in storage for years (per patient...it adds up).&lt;/p&gt;\n\n&lt;p&gt;This is mainly for QME medical record review - a lot of these documents are copies of copies, poor quality faxes, etc.&lt;/p&gt;\n\n&lt;p&gt;How important is it for a scanner to have accurate OCR, versus running it through an OCR program?&lt;/p&gt;\n\n&lt;p&gt;Budget is under $700. &lt;/p&gt;\n\n&lt;p&gt;I already checked out other threads on scanners and am looking at the Raven Pro and Canon Image formula RS40, but I still have questions about OCR accuracy mainly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g754o", "is_robot_indexable": true, "report_reasons": null, "author": "CBRit33", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g754o/document_scanning_ocr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g754o/document_scanning_ocr/", "subreddit_subscribers": 708592, "created_utc": 1698246999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have got 50k photos in Amazon Photos and I am looking to cancel Amazon Prime. I have everything on a local drives plus Google Photos for mobile phone pics, but want to pull everything from Amazon just in case.\n\nAny methods to bulk download so I can dump to a local drive connected to PC running Win10?\n \nBackblaze still best value for deep archive (1-2TB)?", "author_fullname": "t2_w6q5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk downloading from Amazon Photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g0ey2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698224200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have got 50k photos in Amazon Photos and I am looking to cancel Amazon Prime. I have everything on a local drives plus Google Photos for mobile phone pics, but want to pull everything from Amazon just in case.&lt;/p&gt;\n\n&lt;p&gt;Any methods to bulk download so I can dump to a local drive connected to PC running Win10?&lt;/p&gt;\n\n&lt;p&gt;Backblaze still best value for deep archive (1-2TB)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g0ey2", "is_robot_indexable": true, "report_reasons": null, "author": "hd1080ts", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g0ey2/bulk_downloading_from_amazon_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g0ey2/bulk_downloading_from_amazon_photos/", "subreddit_subscribers": 708592, "created_utc": 1698224200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My mum has at least a document box worth of photo albums and film store packets that we want to scan. Most of them don't have the date overlay, but I'm sure she would know a rough time range for each set of photos. The current plan is to simply store the scans in rough year range folders, and in album folders within those year folders if they're clearly from a certain event. Then once most stuff is scanned bulk set the date taken property, and maybe the description field for some of them.\n\nIs there anyway this could be done better?\nMy main concern is having them as a folder structure at the end, with the date taken set to something roughly correct so I don't have to worry about applications being required. (am planning on throwing them in as an external immich library though)", "author_fullname": "t2_15trlf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to organise photos without definite dates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fzqiz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698221050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My mum has at least a document box worth of photo albums and film store packets that we want to scan. Most of them don&amp;#39;t have the date overlay, but I&amp;#39;m sure she would know a rough time range for each set of photos. The current plan is to simply store the scans in rough year range folders, and in album folders within those year folders if they&amp;#39;re clearly from a certain event. Then once most stuff is scanned bulk set the date taken property, and maybe the description field for some of them.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway this could be done better?\nMy main concern is having them as a folder structure at the end, with the date taken set to something roughly correct so I don&amp;#39;t have to worry about applications being required. (am planning on throwing them in as an external immich library though)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "17TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fzqiz", "is_robot_indexable": true, "report_reasons": null, "author": "JayBigGuy10", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17fzqiz/best_way_to_organise_photos_without_definite_dates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fzqiz/best_way_to_organise_photos_without_definite_dates/", "subreddit_subscribers": 708592, "created_utc": 1698221050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It seems to me Exos X is superior on every front, price: capacity, AFR, power consumption.\n\nWhat is the use case for Exos E? Why does it cost more? I don't understand.\n\n&amp;#x200B;\n\nDrives in question are here [https://www.seagate.com/products/enterprise-drives/](https://www.seagate.com/products/enterprise-drives/)\n\nI know their site says \"low TCO\" (total cost of ownership) but I don't see how that makes sense given X series is superior in terms of AFR (annualized failure rate) and MTBF", "author_fullname": "t2_13eqfc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos E vs Exos X", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fspmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698195774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems to me Exos X is superior on every front, price: capacity, AFR, power consumption.&lt;/p&gt;\n\n&lt;p&gt;What is the use case for Exos E? Why does it cost more? I don&amp;#39;t understand.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Drives in question are here &lt;a href=\"https://www.seagate.com/products/enterprise-drives/\"&gt;https://www.seagate.com/products/enterprise-drives/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I know their site says &amp;quot;low TCO&amp;quot; (total cost of ownership) but I don&amp;#39;t see how that makes sense given X series is superior in terms of AFR (annualized failure rate) and MTBF&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?auto=webp&amp;s=b98f58f088fdf8fbeb225a485466816520892a66", "width": 1440, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=800aeac928e4d6917cdf3db161d37c173da92af5", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a9549b2c27b55813f6941439e0246cac408ff1d", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=708b52a8cc3161e776c554b1f4a488231d8bfce6", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93eb129bea186541cdd32a88a0cb2d619f9a954c", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ad5e3bcc25ba7cdf8dd591fb8e247b839fe5aa7", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/pyk-9B2ErLhMX2ZNE7n3iEQrhNSAXceYQSAeV3d7G6Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b52b8bf108499e8a6d3bd58f428c8f644af3085", "width": 1080, "height": 675}], "variants": {}, "id": "GJ9K1o7VNo6J4JXg3IrZBPizWfgWXRc6b6FmSaP5cNc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17fspmn", "is_robot_indexable": true, "report_reasons": null, "author": "tronicdude6", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fspmn/seagate_exos_e_vs_exos_x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fspmn/seagate_exos_e_vs_exos_x/", "subreddit_subscribers": 708592, "created_utc": 1698195774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have Norco 4u server, and I think I used the wrong screw and used screws for 3.5inch HDDs into 2.5inch. Now I got to replace my SSDs, but none of the screws are coming out (I may have forced them in, it's been 5-6 years, I can't remember what I did).\n\nI don't want to waste screw bitz anymore, so I'm looking to get a replacement tray... Is there one available?? ", "author_fullname": "t2_144dwn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any hot-swap drive tray replacement for Norco 24 bay?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g9c7z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698252734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have Norco 4u server, and I think I used the wrong screw and used screws for 3.5inch HDDs into 2.5inch. Now I got to replace my SSDs, but none of the screws are coming out (I may have forced them in, it&amp;#39;s been 5-6 years, I can&amp;#39;t remember what I did).&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to waste screw bitz anymore, so I&amp;#39;m looking to get a replacement tray... Is there one available?? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g9c7z", "is_robot_indexable": true, "report_reasons": null, "author": "reallionkiller", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g9c7z/is_there_any_hotswap_drive_tray_replacement_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g9c7z/is_there_any_hotswap_drive_tray_replacement_for/", "subreddit_subscribers": 708592, "created_utc": 1698252734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using Hitachi Deskstar 5K3000 2TB drives for many years and they are starting to fail.  They are used in a ZFS RAID-Z2 configuration so it's difficult to upgrade the entire setup.\n\nCan anybody give me any advice?\n\nIdeally I'd like to upgrade each drive to a WD RED PLUS/PRO (6TB+?) but that's going to cost a ton of money...", "author_fullname": "t2_8noa5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to upgrade/replace 10x 2TB drives? (re: ZFS, RAID-Z2)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g94jb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698252181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Hitachi Deskstar 5K3000 2TB drives for many years and they are starting to fail.  They are used in a ZFS RAID-Z2 configuration so it&amp;#39;s difficult to upgrade the entire setup.&lt;/p&gt;\n\n&lt;p&gt;Can anybody give me any advice?&lt;/p&gt;\n\n&lt;p&gt;Ideally I&amp;#39;d like to upgrade each drive to a WD RED PLUS/PRO (6TB+?) but that&amp;#39;s going to cost a ton of money...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g94jb", "is_robot_indexable": true, "report_reasons": null, "author": "sofakng", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g94jb/how_to_upgradereplace_10x_2tb_drives_re_zfs_raidz2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g94jb/how_to_upgradereplace_10x_2tb_drives_re_zfs_raidz2/", "subreddit_subscribers": 708592, "created_utc": 1698252181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hya Hoarders,\n\nI could really use some help. I am having to move to local storage by the end of the year. I've never really considered a home server or NAS type solution. Now that I have too, I'm incredibly overwhelmed....\n\nI have to store about 120TB of media, which I'm trying to cut down to save on hard drive costs. I have a PC acting as my Plex media server right now but can't cram enough drives into it to hold everything. What would be the best route to go? \n\nTimes are tight as we get close to the holidays but I need to try and make something work since the cloud is a no go now. \n\nShould I just get a NAS that has 6+ bays and hook it to my PC? That seems the simplest thing to do but I know it might not be the smartest...\n\n&amp;#x200B;\n\nI have been looking over the Wiki and past posts for hours and am not getting anywhere myself. There are a lot of really talented people here and if anyone could help I would be very appreciative!", "author_fullname": "t2_16260q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving Local and Need Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g91z0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698251997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hya Hoarders,&lt;/p&gt;\n\n&lt;p&gt;I could really use some help. I am having to move to local storage by the end of the year. I&amp;#39;ve never really considered a home server or NAS type solution. Now that I have too, I&amp;#39;m incredibly overwhelmed....&lt;/p&gt;\n\n&lt;p&gt;I have to store about 120TB of media, which I&amp;#39;m trying to cut down to save on hard drive costs. I have a PC acting as my Plex media server right now but can&amp;#39;t cram enough drives into it to hold everything. What would be the best route to go? &lt;/p&gt;\n\n&lt;p&gt;Times are tight as we get close to the holidays but I need to try and make something work since the cloud is a no go now. &lt;/p&gt;\n\n&lt;p&gt;Should I just get a NAS that has 6+ bays and hook it to my PC? That seems the simplest thing to do but I know it might not be the smartest...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have been looking over the Wiki and past posts for hours and am not getting anywhere myself. There are a lot of really talented people here and if anyone could help I would be very appreciative!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100TB | Local and Reliable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g91z0", "is_robot_indexable": true, "report_reasons": null, "author": "rowdya22", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17g91z0/moving_local_and_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g91z0/moving_local_and_need_help/", "subreddit_subscribers": 708592, "created_utc": 1698251997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! Over the years I've managed to hoard up quite a lot of drives, ranging from 120GB all the way up to 5TB. However, having such a fractured archive resulted in data being utterly disorganized - nothing short of complete chaos. As I'm going to have a lot of free time next week, I've decided to finally organize this gigantic mess. Since all of the biggest drives are nearly full, I decided to buy a new external HDD with &gt;8TB of storage, around 200$ being my budget.\n\nI plan to use it to organize the data first and then as a backup for the most important files. So far, I've found [this one](https://www.amazon.co.uk/WD-Elements-Desktop-Hard-Drive/dp/B07FNK6QMT?th=1) to be quite fitting to my criteria (7200RPM and good reliability), are there any better ones that you'd buy for similar price?", "author_fullname": "t2_vo38f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best 8+TB external HDD nowadays?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g8w83", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698251581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Over the years I&amp;#39;ve managed to hoard up quite a lot of drives, ranging from 120GB all the way up to 5TB. However, having such a fractured archive resulted in data being utterly disorganized - nothing short of complete chaos. As I&amp;#39;m going to have a lot of free time next week, I&amp;#39;ve decided to finally organize this gigantic mess. Since all of the biggest drives are nearly full, I decided to buy a new external HDD with &amp;gt;8TB of storage, around 200$ being my budget.&lt;/p&gt;\n\n&lt;p&gt;I plan to use it to organize the data first and then as a backup for the most important files. So far, I&amp;#39;ve found &lt;a href=\"https://www.amazon.co.uk/WD-Elements-Desktop-Hard-Drive/dp/B07FNK6QMT?th=1\"&gt;this one&lt;/a&gt; to be quite fitting to my criteria (7200RPM and good reliability), are there any better ones that you&amp;#39;d buy for similar price?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g8w83", "is_robot_indexable": true, "report_reasons": null, "author": "PKM1111", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17g8w83/best_8tb_external_hdd_nowadays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g8w83/best_8tb_external_hdd_nowadays/", "subreddit_subscribers": 708592, "created_utc": 1698251581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys, I've made this thread in hopes of getting some opinions and advice. I used to work in a rather small company where we used LTO tapes, rotating them weekly. I believe we used LTO-6. Since then, I've been interested in that backup method.\n\nMy home server runs inside a Fractal Define R5, which has two 5,25 slots. It's a consumer board (Z270-K, i7 7700k), and has been running as a home server with OpenMediaVault. It currently has two 2TB drives in raid1, a big, 8TB spare drive that I also use for my desktop backups, and two SSDs, one for docker containers, the other for the system itself. I'm currently starting to run out of space, and while I know hard drives are affordable, I'm not touching the data on them, so I want some good offline solution to keep working on my projects.\n\nI'm planning to store photography (preservation of negatives and slides, about 600MB per frame in most cases), video (digitized media for archival, sometimes lost media, again, for preservation), and audio, alongside huge server files I've been archiving since 2010.\n\nI've been thinking about getting a LTO drive for backing up my data, and so far, the info I've gathered is:\n\n* LTO-5 seems to be the most appropriate for my use case, having LTFS and about a terabyte per tape.\n* I would need a PCI-E SAS controller of some kind (this is where I have doubts).\n* Tapes are a good method for storing data over a long period of time, where as BD-R discs (my current method) are limited to 25GB (or 23,3GiB), forcing me to split backups into a few discs, and re-recording every 3-5 years.\n* Initial investment is higher but it's worth on the long run.\n* Should be able to check the drive's age and wear through software, although I'm unsure of how.\n* It should be able to run on linux, but this I will look up later when I need to test things out.\n\nSo, my doubts are in the following:\n\nWhat kind of PCI-E SAS controller card should I get or look up? I've seen RAID cards, which I assume are NOT the same thing. I've never used one, so I would need advice in this. This is where I'm completely lost.\n\nAnd about LTO-5 drives, any particular brand, or something specific I should look into? I'm primarily using linux on both server and desktop, I don't know if that's a problem compatibility-wise. I could deploy a windows VM on my server if necessary, or dual-boot into windows on my desktop.\n\nI don't care about the budget, I want the drive regardless. The data I'm storing is far more important, I wouldn't be here asking otherwise. Anyway, I hope this clarifies my doubts as much as possible.\n\nThanks in advance, guys.", "author_fullname": "t2_86u1b08y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some advice about LTO storage.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g4r68", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698240345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I&amp;#39;ve made this thread in hopes of getting some opinions and advice. I used to work in a rather small company where we used LTO tapes, rotating them weekly. I believe we used LTO-6. Since then, I&amp;#39;ve been interested in that backup method.&lt;/p&gt;\n\n&lt;p&gt;My home server runs inside a Fractal Define R5, which has two 5,25 slots. It&amp;#39;s a consumer board (Z270-K, i7 7700k), and has been running as a home server with OpenMediaVault. It currently has two 2TB drives in raid1, a big, 8TB spare drive that I also use for my desktop backups, and two SSDs, one for docker containers, the other for the system itself. I&amp;#39;m currently starting to run out of space, and while I know hard drives are affordable, I&amp;#39;m not touching the data on them, so I want some good offline solution to keep working on my projects.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to store photography (preservation of negatives and slides, about 600MB per frame in most cases), video (digitized media for archival, sometimes lost media, again, for preservation), and audio, alongside huge server files I&amp;#39;ve been archiving since 2010.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking about getting a LTO drive for backing up my data, and so far, the info I&amp;#39;ve gathered is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;LTO-5 seems to be the most appropriate for my use case, having LTFS and about a terabyte per tape.&lt;/li&gt;\n&lt;li&gt;I would need a PCI-E SAS controller of some kind (this is where I have doubts).&lt;/li&gt;\n&lt;li&gt;Tapes are a good method for storing data over a long period of time, where as BD-R discs (my current method) are limited to 25GB (or 23,3GiB), forcing me to split backups into a few discs, and re-recording every 3-5 years.&lt;/li&gt;\n&lt;li&gt;Initial investment is higher but it&amp;#39;s worth on the long run.&lt;/li&gt;\n&lt;li&gt;Should be able to check the drive&amp;#39;s age and wear through software, although I&amp;#39;m unsure of how.&lt;/li&gt;\n&lt;li&gt;It should be able to run on linux, but this I will look up later when I need to test things out.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So, my doubts are in the following:&lt;/p&gt;\n\n&lt;p&gt;What kind of PCI-E SAS controller card should I get or look up? I&amp;#39;ve seen RAID cards, which I assume are NOT the same thing. I&amp;#39;ve never used one, so I would need advice in this. This is where I&amp;#39;m completely lost.&lt;/p&gt;\n\n&lt;p&gt;And about LTO-5 drives, any particular brand, or something specific I should look into? I&amp;#39;m primarily using linux on both server and desktop, I don&amp;#39;t know if that&amp;#39;s a problem compatibility-wise. I could deploy a windows VM on my server if necessary, or dual-boot into windows on my desktop.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care about the budget, I want the drive regardless. The data I&amp;#39;m storing is far more important, I wouldn&amp;#39;t be here asking otherwise. Anyway, I hope this clarifies my doubts as much as possible.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance, guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "16TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g4r68", "is_robot_indexable": true, "report_reasons": null, "author": "AirPlenty", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17g4r68/looking_for_some_advice_about_lto_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g4r68/looking_for_some_advice_about_lto_storage/", "subreddit_subscribers": 708592, "created_utc": 1698240345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a website \"mywebsite.com\" that requires login and complex verification method. I want to make a offline copy of it for personal use.\n\nHttrack, wget or any other method fails during login. I have logged on with chrome and attached webdriver. I want to clone it/ download all pages under the domain [www.mywebsite.com/\\\\](http://www.mywebsite.com/%5C)\\* for offline use. Using selenium I can only download one page. How to download the whole website that includes all the links under that domain in relative relation .\n\n**ps: any tool that could do that will also work**.\n\n    import time\n    from selenium import webdriver\n    from selenium.webdriver.chrome.options import Options \n    options = webdriver.ChromeOptions()\n    options.add_argument(\"user-data-dir=C:\\\\Users\\\\mrmin\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n    options.add_experimental_option(\"detach\", True)\n    driver = webdriver.Chrome(options=options)\n    driver.get(\"https://www.mywebsite.com\")```", "author_fullname": "t2_a3rs29dtt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download whole website through chrome using selenium or any tool.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fwbh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698206952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a website &amp;quot;mywebsite.com&amp;quot; that requires login and complex verification method. I want to make a offline copy of it for personal use.&lt;/p&gt;\n\n&lt;p&gt;Httrack, wget or any other method fails during login. I have logged on with chrome and attached webdriver. I want to clone it/ download all pages under the domain [&lt;a href=\"http://www.mywebsite.com/%5C%5C%5D(http://www.mywebsite.com/%5C)%5C*\"&gt;www.mywebsite.com/\\\\](http://www.mywebsite.com/%5C)\\*&lt;/a&gt; for offline use. Using selenium I can only download one page. How to download the whole website that includes all the links under that domain in relative relation .&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;ps: any tool that could do that will also work&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import time\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options \noptions = webdriver.ChromeOptions()\noptions.add_argument(&amp;quot;user-data-dir=C:\\\\Users\\\\mrmin\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data&amp;quot;)\noptions.add_experimental_option(&amp;quot;detach&amp;quot;, True)\ndriver = webdriver.Chrome(options=options)\ndriver.get(&amp;quot;https://www.mywebsite.com&amp;quot;)```\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fwbh7", "is_robot_indexable": true, "report_reasons": null, "author": "mrmin87", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fwbh7/download_whole_website_through_chrome_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fwbh7/download_whole_website_through_chrome_using/", "subreddit_subscribers": 708592, "created_utc": 1698206952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been on google, github, stack overflow, and reddit for the last few days and have not been able to figure out how to accomplish what I am trying to do.   \n\n\nI have an index of a site I am trying to download. There are hundreds of folders, each folder has at least one subfolder, and some of the subfolders have subfolders. Files, mostly pdf's in all levels. The issue is, no matter what I use, best case scenario I am getting the main folders and the files in those. The subfolder will download, but it will be empty, or have an index of that subfolder.   \n\n\nManually downloading this is not an option. It would take me days to go one by one. My goal is to download as is, with all the folders, subfolder, files, etc... all in their place as it is listed in the sites index page.   \n\n\nSo far I have tried a few gui's like visualwget, jdownloader and a few chrome extensions.   \n\n\nOn my linux VM I have used wget with about every combination of flags I can think ok. Nothing has been able to work so far.  \n\n\nIs there any advice I can get from you guys?", "author_fullname": "t2_pyc58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading from Index of", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ftd0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698197680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been on google, github, stack overflow, and reddit for the last few days and have not been able to figure out how to accomplish what I am trying to do.   &lt;/p&gt;\n\n&lt;p&gt;I have an index of a site I am trying to download. There are hundreds of folders, each folder has at least one subfolder, and some of the subfolders have subfolders. Files, mostly pdf&amp;#39;s in all levels. The issue is, no matter what I use, best case scenario I am getting the main folders and the files in those. The subfolder will download, but it will be empty, or have an index of that subfolder.   &lt;/p&gt;\n\n&lt;p&gt;Manually downloading this is not an option. It would take me days to go one by one. My goal is to download as is, with all the folders, subfolder, files, etc... all in their place as it is listed in the sites index page.   &lt;/p&gt;\n\n&lt;p&gt;So far I have tried a few gui&amp;#39;s like visualwget, jdownloader and a few chrome extensions.   &lt;/p&gt;\n\n&lt;p&gt;On my linux VM I have used wget with about every combination of flags I can think ok. Nothing has been able to work so far.  &lt;/p&gt;\n\n&lt;p&gt;Is there any advice I can get from you guys?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ftd0y", "is_robot_indexable": true, "report_reasons": null, "author": "Kazelob", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ftd0y/downloading_from_index_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ftd0y/downloading_from_index_of/", "subreddit_subscribers": 708592, "created_utc": 1698197680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm an image hoarder... while most have a porn collection I have tons &amp; tons of pretty or cool anime illustrations since I just absolutely love them...\n\nHowever, I'm at a loss! Twitter has amazing artists but the worst ever set up for hosting art! There's no tag system agreed upon &amp; following means nothing since my timeline is flooded with every post &amp; retweet.\n\nIs there an easier way to keep track of the media tab per artist?\n\nI would have asked anywhere else if I knew where to ask, but I'm not even sure where I would ask this outside of here...\n\nI've googled but got nothing.\nI've been to GitHub &amp; FMHY as well.\nDanbooru &amp; Gelbooru have forums but I doubt you can ask this there...\n\nIf anyone knows of any tricks please I'm at a complete loss.\n\nThank you so much for your time \u208d\u208d (\u0328\u0321,,&gt; &lt;,, )\u0327\u0322 \u208e\u208e", "author_fullname": "t2_hyxmzsxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best &amp; Easiest Way to Keep Track of Twitter Artists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17flnd9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698177189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an image hoarder... while most have a porn collection I have tons &amp;amp; tons of pretty or cool anime illustrations since I just absolutely love them...&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m at a loss! Twitter has amazing artists but the worst ever set up for hosting art! There&amp;#39;s no tag system agreed upon &amp;amp; following means nothing since my timeline is flooded with every post &amp;amp; retweet.&lt;/p&gt;\n\n&lt;p&gt;Is there an easier way to keep track of the media tab per artist?&lt;/p&gt;\n\n&lt;p&gt;I would have asked anywhere else if I knew where to ask, but I&amp;#39;m not even sure where I would ask this outside of here...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve googled but got nothing.\nI&amp;#39;ve been to GitHub &amp;amp; FMHY as well.\nDanbooru &amp;amp; Gelbooru have forums but I doubt you can ask this there...&lt;/p&gt;\n\n&lt;p&gt;If anyone knows of any tricks please I&amp;#39;m at a complete loss.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for your time \u208d\u208d (\u0328\u0321,,&amp;gt; &amp;lt;,, )\u0327\u0322 \u208e\u208e&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17flnd9", "is_robot_indexable": true, "report_reasons": null, "author": "nyaahilism", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17flnd9/best_easiest_way_to_keep_track_of_twitter_artists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17flnd9/best_easiest_way_to_keep_track_of_twitter_artists/", "subreddit_subscribers": 708592, "created_utc": 1698177189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got one on the way. Any reason to cancel? \n\nI need to rip [this video](https://www.amazon.com/White-Mare-Blu-ray-Gy%C3%B6rgy-Cserhalmi/dp/B08Z2BZ566/ref=sr_1_1?crid=2P81TFLPRFJ70&amp;keywords=son+of+the+white+mare+blu+ray&amp;qid=1698250667&amp;sprefix=white+mare+%2Caps%2C172&amp;sr=8-1) so I can watch it on my TV.", "author_fullname": "t2_313be", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick check on Pinoneer BDR-XD08UMB-S for Blu-Ray ripping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17g8k7r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698250727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got one on the way. Any reason to cancel? &lt;/p&gt;\n\n&lt;p&gt;I need to rip &lt;a href=\"https://www.amazon.com/White-Mare-Blu-ray-Gy%C3%B6rgy-Cserhalmi/dp/B08Z2BZ566/ref=sr_1_1?crid=2P81TFLPRFJ70&amp;amp;keywords=son+of+the+white+mare+blu+ray&amp;amp;qid=1698250667&amp;amp;sprefix=white+mare+%2Caps%2C172&amp;amp;sr=8-1\"&gt;this video&lt;/a&gt; so I can watch it on my TV.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g8k7r", "is_robot_indexable": true, "report_reasons": null, "author": "onebit", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g8k7r/quick_check_on_pinoneer_bdrxd08umbs_for_bluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g8k7r/quick_check_on_pinoneer_bdrxd08umbs_for_bluray/", "subreddit_subscribers": 708592, "created_utc": 1698250727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 3x RAID1 pairs of 6 disks of the same size. I buy 2 more disks (same size). What's the best option for converting to a better array layout, while keeping the data (without passing through an external backup \ud83d\ude41), and ending up with more capacity and decent redundancy?\n\nSome options I've considered:\n\n1. Just add a 4th 2d RAID1 and call it a day. 4d capacity overall, 1 redundancy per set, and the option to upgrade any of the RAID1's to a larger disk size independently.\n2. 8d RAID6. I force-degrade the RAID1 arrays, use the 5 free disks to create a 5d RAID6, copy the data over, then add the 3 remaining disks to the RAID6 and grow the array to 8d. Big con: if the growth sequence is interrupted the whole thing is lost? Although I've read you can use a 16MB state file on a separate disk to recover. End up with 6d capacity and 2 drive redundancy.\n3. 5d RAID6 and 3d RAID5. Same as above but I leave the 5d RAID6 alone and make a 3d RAID5. End up with 5d capacity overall and 2 and 1 drive redundancy respectively.\n4. 2x 4d RAID6. Degrade the RAID1's as above but make a 4d RAID6 and then another. End up with 4d capacity overall and 2 drive redundancy respectively per RAID6. Big con: would need to trim some data during the copy from 3xDISK to 2xDISK capacity.\n5. 8d RAID60 or 8d RAID10. I would make up degraded RAID0 and expand it with new RAID6 or RAID1 mirrors. Not sure there's any benefit in this one, the RAID0 expansion(s) are vulnerable, and would end up with only 4d capacity and 2 or 1 redundancy per set.\n\nIs any of these a good idea? Do you have another? #4 and #5 in particular seem like too much hassle vs #1. The #2 option is interesting for capacity but risky and not that much redundancy. #3 seems like the safest.\n\nBonus question, can any of the new arrays use zfs strictly for the filesystem (self-check benefits), rather than volume management?\n\nAnd, related: can a RAID1 be converted from ext4 to zfs filesystem by degrading the array, using the removed disk to make a degraded RAID1 with zfs filesystem, copying the data over, then adding the 1st disk to the 2nd array?\n\nTIA for any insight and ideas.", "author_fullname": "t2_4o56qcsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting RAID1 arrays to something better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g6fcq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698245063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3x RAID1 pairs of 6 disks of the same size. I buy 2 more disks (same size). What&amp;#39;s the best option for converting to a better array layout, while keeping the data (without passing through an external backup \ud83d\ude41), and ending up with more capacity and decent redundancy?&lt;/p&gt;\n\n&lt;p&gt;Some options I&amp;#39;ve considered:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Just add a 4th 2d RAID1 and call it a day. 4d capacity overall, 1 redundancy per set, and the option to upgrade any of the RAID1&amp;#39;s to a larger disk size independently.&lt;/li&gt;\n&lt;li&gt;8d RAID6. I force-degrade the RAID1 arrays, use the 5 free disks to create a 5d RAID6, copy the data over, then add the 3 remaining disks to the RAID6 and grow the array to 8d. Big con: if the growth sequence is interrupted the whole thing is lost? Although I&amp;#39;ve read you can use a 16MB state file on a separate disk to recover. End up with 6d capacity and 2 drive redundancy.&lt;/li&gt;\n&lt;li&gt;5d RAID6 and 3d RAID5. Same as above but I leave the 5d RAID6 alone and make a 3d RAID5. End up with 5d capacity overall and 2 and 1 drive redundancy respectively.&lt;/li&gt;\n&lt;li&gt;2x 4d RAID6. Degrade the RAID1&amp;#39;s as above but make a 4d RAID6 and then another. End up with 4d capacity overall and 2 drive redundancy respectively per RAID6. Big con: would need to trim some data during the copy from 3xDISK to 2xDISK capacity.&lt;/li&gt;\n&lt;li&gt;8d RAID60 or 8d RAID10. I would make up degraded RAID0 and expand it with new RAID6 or RAID1 mirrors. Not sure there&amp;#39;s any benefit in this one, the RAID0 expansion(s) are vulnerable, and would end up with only 4d capacity and 2 or 1 redundancy per set.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is any of these a good idea? Do you have another? #4 and #5 in particular seem like too much hassle vs #1. The #2 option is interesting for capacity but risky and not that much redundancy. #3 seems like the safest.&lt;/p&gt;\n\n&lt;p&gt;Bonus question, can any of the new arrays use zfs strictly for the filesystem (self-check benefits), rather than volume management?&lt;/p&gt;\n\n&lt;p&gt;And, related: can a RAID1 be converted from ext4 to zfs filesystem by degrading the array, using the removed disk to make a degraded RAID1 with zfs filesystem, copying the data over, then adding the 1st disk to the 2nd array?&lt;/p&gt;\n\n&lt;p&gt;TIA for any insight and ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g6fcq", "is_robot_indexable": true, "report_reasons": null, "author": "GolemancerVekk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g6fcq/converting_raid1_arrays_to_something_better/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g6fcq/converting_raid1_arrays_to_something_better/", "subreddit_subscribers": 708592, "created_utc": 1698245063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For the last few years I have been downloading artwork. Mostly oil paintings from museum collections, Japanese Woodblock Prints, and 70s-90s pulp scifi/fantasy illustrations. \n\nI use them for a desktop background slideshow and inspiration in my own projects.\n\nI was thinking though, what all could I use this collection for? \n\nObviously I can upload the collection which I will do once I organize all the files but what else? \n\nI imagine there might be some interesting things I can do using image related AI but I know very little about that world other than using a few free text prompt image generators. Open to non-ai related suggestions as well. ", "author_fullname": "t2_bb7bm04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interesting uses for digital art collection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g693l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698244573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the last few years I have been downloading artwork. Mostly oil paintings from museum collections, Japanese Woodblock Prints, and 70s-90s pulp scifi/fantasy illustrations. &lt;/p&gt;\n\n&lt;p&gt;I use them for a desktop background slideshow and inspiration in my own projects.&lt;/p&gt;\n\n&lt;p&gt;I was thinking though, what all could I use this collection for? &lt;/p&gt;\n\n&lt;p&gt;Obviously I can upload the collection which I will do once I organize all the files but what else? &lt;/p&gt;\n\n&lt;p&gt;I imagine there might be some interesting things I can do using image related AI but I know very little about that world other than using a few free text prompt image generators. Open to non-ai related suggestions as well. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "11TB of nonsense", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g693l", "is_robot_indexable": true, "report_reasons": null, "author": "TrashVHS", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17g693l/interesting_uses_for_digital_art_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g693l/interesting_uses_for_digital_art_collection/", "subreddit_subscribers": 708592, "created_utc": 1698244573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to check that a HBA is functioning correctly is there a way to do it ?  I'm thinking of something along the likes of having a 1MB seed and writing pseudo random data to the disk until it is full, then verifying it was written correctly using the seed to verify the data.\n\nThis shouldn't be the same data repeated using dd\n\nIs this possible ?  How would you verify a storage controller ?\n\nSorry if this isn't an often asked question, but I wrote a firmware update to a HBA and want to ensure that it does what it is supposed to.\n\n&amp;#x200B;", "author_fullname": "t2_1w5zqtt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "testing a HBA and disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g2kyy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698233361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to check that a HBA is functioning correctly is there a way to do it ?  I&amp;#39;m thinking of something along the likes of having a 1MB seed and writing pseudo random data to the disk until it is full, then verifying it was written correctly using the seed to verify the data.&lt;/p&gt;\n\n&lt;p&gt;This shouldn&amp;#39;t be the same data repeated using dd&lt;/p&gt;\n\n&lt;p&gt;Is this possible ?  How would you verify a storage controller ?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this isn&amp;#39;t an often asked question, but I wrote a firmware update to a HBA and want to ensure that it does what it is supposed to.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g2kyy", "is_robot_indexable": true, "report_reasons": null, "author": "simonmcnair", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g2kyy/testing_a_hba_and_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g2kyy/testing_a_hba_and_disks/", "subreddit_subscribers": 708592, "created_utc": 1698233361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've currently got four 16TB WD Elements USB drives and two 5TB USB drives hooked up to a RPi 4 with a USB expansion hat and it's a bit of a mess with all the PSUs and USB leads, so I want to tidy it up by shucking the 16TB drives into a single enclosure. I'll probably try to sell or repurpose the 5TB drives, partly because it's a lot easier to find a 4-bay enclosure than a 6-bay one but also it's not very cost/power efficient to run these relatively small drives.\n\nI'm running a Chia farm at the moment but I'll probably switch it to a Storj node soon. Either way, I don't think the drives will need to be constantly active, so whatever enclosure I use should allow them to sleep, and it should have isolation mounts to reduce vibration noise.\n\nI don't want to spend a lot of money. If there's a compact PC case with 4+ drive bays that I could use, that could be an option. I'm not wedded to the RPi, so I can buy a Rockpi if that's better for this, and I could mount either inside the PC case. I'd need something to convert the drives from SATA to USB either way. If it's better/cheaper I could use two adapters that convert two drives each, rather than one for all four, or even one per drive with the RPi as I've got plenty of USB ports with the hat.\n\nAnother benefit with a PC case is being able to use quieter fans and I've got a few decent 120mm and 80mm ones spare. I'm not sure if any compact PC cases can accommodate 120mm fans though.\n\nI've got a spare 800W Gold PC PSU, but it would be a bit wasted on this and I should probably keep that in reserve for if my main PC's one dies. I've also got a passive PC power board which is powered by a Dell laptop PSU, but I'm not sure that can supply enough power for 4 3.5\" HDDs, so I might need to buy a PSU.", "author_fullname": "t2_4g7q3nm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need PC case or enclosure for 4+ drives and RPi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fnasy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698181446.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve currently got four 16TB WD Elements USB drives and two 5TB USB drives hooked up to a RPi 4 with a USB expansion hat and it&amp;#39;s a bit of a mess with all the PSUs and USB leads, so I want to tidy it up by shucking the 16TB drives into a single enclosure. I&amp;#39;ll probably try to sell or repurpose the 5TB drives, partly because it&amp;#39;s a lot easier to find a 4-bay enclosure than a 6-bay one but also it&amp;#39;s not very cost/power efficient to run these relatively small drives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a Chia farm at the moment but I&amp;#39;ll probably switch it to a Storj node soon. Either way, I don&amp;#39;t think the drives will need to be constantly active, so whatever enclosure I use should allow them to sleep, and it should have isolation mounts to reduce vibration noise.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to spend a lot of money. If there&amp;#39;s a compact PC case with 4+ drive bays that I could use, that could be an option. I&amp;#39;m not wedded to the RPi, so I can buy a Rockpi if that&amp;#39;s better for this, and I could mount either inside the PC case. I&amp;#39;d need something to convert the drives from SATA to USB either way. If it&amp;#39;s better/cheaper I could use two adapters that convert two drives each, rather than one for all four, or even one per drive with the RPi as I&amp;#39;ve got plenty of USB ports with the hat.&lt;/p&gt;\n\n&lt;p&gt;Another benefit with a PC case is being able to use quieter fans and I&amp;#39;ve got a few decent 120mm and 80mm ones spare. I&amp;#39;m not sure if any compact PC cases can accommodate 120mm fans though.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a spare 800W Gold PC PSU, but it would be a bit wasted on this and I should probably keep that in reserve for if my main PC&amp;#39;s one dies. I&amp;#39;ve also got a passive PC power board which is powered by a Dell laptop PSU, but I&amp;#39;m not sure that can supply enough power for 4 3.5&amp;quot; HDDs, so I might need to buy a PSU.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fnasy", "is_robot_indexable": true, "report_reasons": null, "author": "Big-Finding2976", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fnasy/need_pc_case_or_enclosure_for_4_drives_and_rpi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fnasy/need_pc_case_or_enclosure_for_4_drives_and_rpi/", "subreddit_subscribers": 708592, "created_utc": 1698181446.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i'm building an ios app that needs to get the transcript and description of a tikotk video, given a link to the video. what should my approach be? is it possible to use a selenium script and not face any limit issues?\n\nthanks", "author_fullname": "t2_4awfqs53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scraping tiktok", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fmgss", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698179277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m building an ios app that needs to get the transcript and description of a tikotk video, given a link to the video. what should my approach be? is it possible to use a selenium script and not face any limit issues?&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fmgss", "is_robot_indexable": true, "report_reasons": null, "author": "lavendercandy19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fmgss/scraping_tiktok/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fmgss/scraping_tiktok/", "subreddit_subscribers": 708592, "created_utc": 1698179277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I just bought three 1TB 2,5\u201d drives from a guy that replaced them with SSDs, and was selling these really cheap and near me. I plan to use one of these on my ps3 and got the other two just because was really cheap. \nOne WD drive was from 2018 with 2000 hours with no bad blocks, other was Toshiba from 2015 with no bad blocks and 2500 hours, and the last one was Toshiba from 2015 too but with 8000 hours and 49 bad blocks. I used easeus partition master to check all hdds. \n\nSo should I stop using this hdd? Is there a way to fix the bad blocks? How can I monitor if bad blocks are increasing without the need to fully scan the disk? Because it took a really long time to scan it.\n\nEDIT: I gave back the HDD and he gave me a newer one from 2017 with no bad blocks. \nI also tested the other hdds overnight and they have no bad blocks. ", "author_fullname": "t2_astog32v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bad sectors on used HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fm9mj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698241734.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698178768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I just bought three 1TB 2,5\u201d drives from a guy that replaced them with SSDs, and was selling these really cheap and near me. I plan to use one of these on my ps3 and got the other two just because was really cheap. \nOne WD drive was from 2018 with 2000 hours with no bad blocks, other was Toshiba from 2015 with no bad blocks and 2500 hours, and the last one was Toshiba from 2015 too but with 8000 hours and 49 bad blocks. I used easeus partition master to check all hdds. &lt;/p&gt;\n\n&lt;p&gt;So should I stop using this hdd? Is there a way to fix the bad blocks? How can I monitor if bad blocks are increasing without the need to fully scan the disk? Because it took a really long time to scan it.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I gave back the HDD and he gave me a newer one from 2017 with no bad blocks. \nI also tested the other hdds overnight and they have no bad blocks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fm9mj", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Pick8773", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fm9mj/bad_sectors_on_used_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fm9mj/bad_sectors_on_used_hdd/", "subreddit_subscribers": 708592, "created_utc": 1698178768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've tried versions of YouTube-dl but everything I've tried grabs the trailer, not the movie. \n\nCan anyone figure out how to grab the actual movie from this site?\n\n[https://moviesjoyhd.to/watch-movie/ruby-62392.5463016](https://moviesjoyhd.to/watch-movie/ruby-62392.5463016)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_rr4vx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for way to grab video from website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fhvtl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698167612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried versions of YouTube-dl but everything I&amp;#39;ve tried grabs the trailer, not the movie. &lt;/p&gt;\n\n&lt;p&gt;Can anyone figure out how to grab the actual movie from this site?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://moviesjoyhd.to/watch-movie/ruby-62392.5463016\"&gt;https://moviesjoyhd.to/watch-movie/ruby-62392.5463016&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?auto=webp&amp;s=72d667fcb43d7ca350405433370ab9981d4c2304", "width": 648, "height": 425}, "resolutions": [{"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=048a3d82c4cb13d203b46c3451cd0e8bb68775b0", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3cc81a74c95709658369e4919a6ea418ec810b9f", "width": 216, "height": 141}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8de5120ff70363441983db2e5688ebb8662ea370", "width": 320, "height": 209}, {"url": "https://external-preview.redd.it/OVOuVlAm9DsB3VffIOKFIUBa6Hl9c6zfrDrYDWeLJiQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=abe2e9c03f2495d51373c0dc2bdb2bffe9d26ebb", "width": 640, "height": 419}], "variants": {}, "id": "s6SBce-yDMS7QNV49GiC-FcN3ZvbxXmSnyMPJ7Ncx1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17fhvtl", "is_robot_indexable": true, "report_reasons": null, "author": "MarkPugnerIII", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17fhvtl/looking_for_way_to_grab_video_from_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17fhvtl/looking_for_way_to_grab_video_from_website/", "subreddit_subscribers": 708592, "created_utc": 1698167612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys. I just got three new used 1tb drives, deleted all data and formatted it. Tested for bad blocks overnight and they are healthy. \n\nOne of these hdds I will use on my ps3, I was wondering if zero fill would improve performance after I install on ps3. \nBecause I formatted the drive but the data is physically still there right? So if I install on ps3 and download games there, the data could be fragmented and have a performance penalty because of this. \n\nSo I was thinking if the drive is fully zeroed doesn\u2019t that mean that the file would get written more efficiently?\n\nOr should I just use it right now, and let ps3 do its thing?", "author_fullname": "t2_astog32v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Zero fill have any advantage besides destroying data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17g5cyv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698242027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I just got three new used 1tb drives, deleted all data and formatted it. Tested for bad blocks overnight and they are healthy. &lt;/p&gt;\n\n&lt;p&gt;One of these hdds I will use on my ps3, I was wondering if zero fill would improve performance after I install on ps3. \nBecause I formatted the drive but the data is physically still there right? So if I install on ps3 and download games there, the data could be fragmented and have a performance penalty because of this. &lt;/p&gt;\n\n&lt;p&gt;So I was thinking if the drive is fully zeroed doesn\u2019t that mean that the file would get written more efficiently?&lt;/p&gt;\n\n&lt;p&gt;Or should I just use it right now, and let ps3 do its thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17g5cyv", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Pick8773", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17g5cyv/does_zero_fill_have_any_advantage_besides/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17g5cyv/does_zero_fill_have_any_advantage_besides/", "subreddit_subscribers": 708592, "created_utc": 1698242027.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}