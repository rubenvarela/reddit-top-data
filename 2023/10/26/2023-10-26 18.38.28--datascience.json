{"kind": "Listing", "data": {"after": "t3_17gzyzp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was laid off from my startup in January so I took a job as a principal data scientist at a huge corporation. They exhibit every major red flag I can think of and I'm slowly losing my mind - any tips on how to survive long enough that it looks ok on my resume to leave?\n\nRed flags include:\n\n* No data / inaccessible data / data flying around in Excel\n* Management is not \"ML literate\"\n* More work dealing with red tape than actual work\n* 2x more managers than workers driving projects\n* Business consumers of our ML output do not trust it, and do not want it. They only like linear regression because they understand it\n* No version control. We run everything manually in prod. There is no dev/qa/prod separation. There is no deployment. There is no automation.\n* Because we work directly in prod, we don't have permission to save our processed data to tables or csv's - it must be done in memory every single day\n* No access to basic tools of the trade. We had to beg for basic file storage (s3) for 9 weeks. We can't download unapproved libraries or pre-trained models without security review (even just for exploration)\n\nMy career is jumpy recently - my first few roles were 3-4 years, but my last 2 roles were 1 year-ish, so trying to make it to Feb 2025", "author_fullname": "t2_am698", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to survive at nightmare employer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gfqqp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 120, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698270328.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698269551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was laid off from my startup in January so I took a job as a principal data scientist at a huge corporation. They exhibit every major red flag I can think of and I&amp;#39;m slowly losing my mind - any tips on how to survive long enough that it looks ok on my resume to leave?&lt;/p&gt;\n\n&lt;p&gt;Red flags include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;No data / inaccessible data / data flying around in Excel&lt;/li&gt;\n&lt;li&gt;Management is not &amp;quot;ML literate&amp;quot;&lt;/li&gt;\n&lt;li&gt;More work dealing with red tape than actual work&lt;/li&gt;\n&lt;li&gt;2x more managers than workers driving projects&lt;/li&gt;\n&lt;li&gt;Business consumers of our ML output do not trust it, and do not want it. They only like linear regression because they understand it&lt;/li&gt;\n&lt;li&gt;No version control. We run everything manually in prod. There is no dev/qa/prod separation. There is no deployment. There is no automation.&lt;/li&gt;\n&lt;li&gt;Because we work directly in prod, we don&amp;#39;t have permission to save our processed data to tables or csv&amp;#39;s - it must be done in memory every single day&lt;/li&gt;\n&lt;li&gt;No access to basic tools of the trade. We had to beg for basic file storage (s3) for 9 weeks. We can&amp;#39;t download unapproved libraries or pre-trained models without security review (even just for exploration)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My career is jumpy recently - my first few roles were 3-4 years, but my last 2 roles were 1 year-ish, so trying to make it to Feb 2025&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17gfqqp", "is_robot_indexable": true, "report_reasons": null, "author": "Mackelday", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gfqqp/how_to_survive_at_nightmare_employer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gfqqp/how_to_survive_at_nightmare_employer/", "subreddit_subscribers": 1100424, "created_utc": 1698269551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_898csv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Check out our GPT-4 powered CSV automations creator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gxa17", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 79, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 79, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1698329026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "squack.link", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "http://squack.link/dog-kangaroo-579", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17gxa17", "is_robot_indexable": true, "report_reasons": null, "author": "evilredpanda", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gxa17/check_out_our_gpt4_powered_csv_automations_creator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://squack.link/dog-kangaroo-579", "subreddit_subscribers": 1100424, "created_utc": 1698329026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My employer ran a rather expensive A/B test in its operations. The issue is that they failed to check for SRM and also didn't conduct A/A test beforehand. Now I inherited a test result that is poised with SRM. Due to their test setup, I could run a back A/A test, the result showed no significant difference between control and treatment. We tried to debug the SRM for a few weeks with no luck. I suppose with such SRM, the test results are not reliable. \n\nNow I am stuck between a rock and a hard place. Should we spend more time debugging the SRM or should we accept the cost and redesign and rerun the experiment. Is there a middle ground here?", "author_fullname": "t2_4oockqg5s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A/B test in real life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17go3pk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698293599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My employer ran a rather expensive A/B test in its operations. The issue is that they failed to check for SRM and also didn&amp;#39;t conduct A/A test beforehand. Now I inherited a test result that is poised with SRM. Due to their test setup, I could run a back A/A test, the result showed no significant difference between control and treatment. We tried to debug the SRM for a few weeks with no luck. I suppose with such SRM, the test results are not reliable. &lt;/p&gt;\n\n&lt;p&gt;Now I am stuck between a rock and a hard place. Should we spend more time debugging the SRM or should we accept the cost and redesign and rerun the experiment. Is there a middle ground here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17go3pk", "is_robot_indexable": true, "report_reasons": null, "author": "furioncruz", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17go3pk/ab_test_in_real_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17go3pk/ab_test_in_real_life/", "subreddit_subscribers": 1100424, "created_utc": 1698293599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've recently did some searches about AI failures, the most catastrophic failure I read about was when [Zillow had to fire 2000 employees](https://www.geekwire.com/2021/zillow-shutter-home-buying-business-lay-off-2k-employees-big-real-estate-bet-falters/). \n\nI also saw some articles like this [one](https://www.science.org/doi/10.1126/science.aax2342), about biases in health algorithms, but all in all I didn't see much examples that had a measure of how much damage was actually done.\n\nAre there more examples of AI failures on a large scale?", "author_fullname": "t2_3md3xoyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good examples of catastrophic AI failures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gujdu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698320521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently did some searches about AI failures, the most catastrophic failure I read about was when &lt;a href=\"https://www.geekwire.com/2021/zillow-shutter-home-buying-business-lay-off-2k-employees-big-real-estate-bet-falters/\"&gt;Zillow had to fire 2000 employees&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;I also saw some articles like this &lt;a href=\"https://www.science.org/doi/10.1126/science.aax2342\"&gt;one&lt;/a&gt;, about biases in health algorithms, but all in all I didn&amp;#39;t see much examples that had a measure of how much damage was actually done.&lt;/p&gt;\n\n&lt;p&gt;Are there more examples of AI failures on a large scale?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?auto=webp&amp;s=a6a2243892262e8393411d3fad75660c90d2408f", "width": 2952, "height": 2588}, "resolutions": [{"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e45f7c442f2e13ac08b03a554d8ce6d8d4f03a2", "width": 108, "height": 94}, {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=956fdece887482fdbec3fc2d8dee724093fbd1a0", "width": 216, "height": 189}, {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1ea13da7225f5d93bc97d1eb6224e4f2a47eae8", "width": 320, "height": 280}, {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd9651b89be1428872393f2df63bd1f92baaf791", "width": 640, "height": 561}, {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbb849a6674a127a742c174865a17e340a4ce3e2", "width": 960, "height": 841}, {"url": "https://external-preview.redd.it/zONBZXiaY3d4C_EEFRaGYL-SDpECz7GPhCfQBfSyA3k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31b8ca0737d15840b84b3ae5daa31a9affe68cf6", "width": 1080, "height": 946}], "variants": {}, "id": "vv1SQd3378N1QgX-P7PekhaXGgraU6Q3kKVMt9rPNGw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17gujdu", "is_robot_indexable": true, "report_reasons": null, "author": "Maimonatorz", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gujdu/what_are_some_good_examples_of_catastrophic_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gujdu/what_are_some_good_examples_of_catastrophic_ai/", "subreddit_subscribers": 1100424, "created_utc": 1698320521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " For context, I am a Master's student in CS and lurking in sub has made me realize that CS guys need more statistical background regarding DS positions. Hence, the motivation. However, I am already taking a course called Foundations course which feels like a quick Statistics walkthrough. I am also taking an Automated Learning course which basically follows the ISL contents. This course would be the third one? or the fourth one if I plan to audit this one.\n\nThis is what the course page says : \n\nStudent Learning Outcomes: \n\nMaster the essential tools of convex analysis, ability to characterize solutions to convex optimization problems, ability to formulate standard data science problems as convex optimization problems, and understanding the structure and implementation of the main classes of algorithms for solving optimization problems in data science.\n\nDetailed Content: \n\nIteration principles, fixed-point algorithms, convex sets and convex cones, best approximation paradigms, projection methods in convex feasibility problems \u2013 applications to data fusion and image recovery, convex functions, conjugation of convex functions, duality in convex optimization, subdifferential calculus, subgradient algorithms for convex feasibility and best approximation \u2013 applications in inverse problems, proximity operators, proximal calculus, forward-backward splitting and variants (Dykstra-like methods, Chambolle-Pock algorithm, dual ascent method, etc.), Douglas-Rachford splitting and variants (parallel proximal algorithm, alternating direction method of multipliers, composite primal-dual method, etc.), the monotone + skew decomposition principle \u2013 primal-dual algorithms, proximal modeling of statistical information, proximal information extraction, proximal sparsity enforcement, proximal data classification, proximal principal component analysis, proximal image reconstruction, proximal learning, proximal methods for matrix-based learning, scalability: proximal methods in big data problems, special topics.  \n\n\n  \nI was wondering if this would be something that could help with the day-to-day computations as a DS. I feel like real-world DS is more about optimization and less about using high-end ML/DL techniques. Any thoughts or suggestions?", "author_fullname": "t2_k6fzzm72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a Convex Optimization class good for Data Science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gc0b4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698259755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I am a Master&amp;#39;s student in CS and lurking in sub has made me realize that CS guys need more statistical background regarding DS positions. Hence, the motivation. However, I am already taking a course called Foundations course which feels like a quick Statistics walkthrough. I am also taking an Automated Learning course which basically follows the ISL contents. This course would be the third one? or the fourth one if I plan to audit this one.&lt;/p&gt;\n\n&lt;p&gt;This is what the course page says : &lt;/p&gt;\n\n&lt;p&gt;Student Learning Outcomes: &lt;/p&gt;\n\n&lt;p&gt;Master the essential tools of convex analysis, ability to characterize solutions to convex optimization problems, ability to formulate standard data science problems as convex optimization problems, and understanding the structure and implementation of the main classes of algorithms for solving optimization problems in data science.&lt;/p&gt;\n\n&lt;p&gt;Detailed Content: &lt;/p&gt;\n\n&lt;p&gt;Iteration principles, fixed-point algorithms, convex sets and convex cones, best approximation paradigms, projection methods in convex feasibility problems \u2013 applications to data fusion and image recovery, convex functions, conjugation of convex functions, duality in convex optimization, subdifferential calculus, subgradient algorithms for convex feasibility and best approximation \u2013 applications in inverse problems, proximity operators, proximal calculus, forward-backward splitting and variants (Dykstra-like methods, Chambolle-Pock algorithm, dual ascent method, etc.), Douglas-Rachford splitting and variants (parallel proximal algorithm, alternating direction method of multipliers, composite primal-dual method, etc.), the monotone + skew decomposition principle \u2013 primal-dual algorithms, proximal modeling of statistical information, proximal information extraction, proximal sparsity enforcement, proximal data classification, proximal principal component analysis, proximal image reconstruction, proximal learning, proximal methods for matrix-based learning, scalability: proximal methods in big data problems, special topics.  &lt;/p&gt;\n\n&lt;p&gt;I was wondering if this would be something that could help with the day-to-day computations as a DS. I feel like real-world DS is more about optimization and less about using high-end ML/DL techniques. Any thoughts or suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17gc0b4", "is_robot_indexable": true, "report_reasons": null, "author": "VastDragonfruit847", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gc0b4/is_a_convex_optimization_class_good_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gc0b4/is_a_convex_optimization_class_good_for_data/", "subreddit_subscribers": 1100424, "created_utc": 1698259755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "my first job was as a consultant, doing a mix of implementation and data analytics. \n\nthen i switched to a new job with the data analyst title, but I'm building production R scripts almost exclusively now; not a huge fan of wrangling with my team's complex/sparsely commented codebase and designing 'systems' (our scripts have to integrate with a variety of outside data sources).\n\nI miss doing 'investigations', eg how do we better optimize this product, make more revenue, etc. now it feels like I'm an underpaid backend software engineer (making 85k but seems most SWEs are earning 100k+).\n\nis data analytics in 2023 more similar to SWE? should I have expected this?", "author_fullname": "t2_gaa4jyzpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm a 'data analyst' who in practice is actually just a software engineer. Was I bamboozled, or did I misunderstand the role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gyevz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698332259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;my first job was as a consultant, doing a mix of implementation and data analytics. &lt;/p&gt;\n\n&lt;p&gt;then i switched to a new job with the data analyst title, but I&amp;#39;m building production R scripts almost exclusively now; not a huge fan of wrangling with my team&amp;#39;s complex/sparsely commented codebase and designing &amp;#39;systems&amp;#39; (our scripts have to integrate with a variety of outside data sources).&lt;/p&gt;\n\n&lt;p&gt;I miss doing &amp;#39;investigations&amp;#39;, eg how do we better optimize this product, make more revenue, etc. now it feels like I&amp;#39;m an underpaid backend software engineer (making 85k but seems most SWEs are earning 100k+).&lt;/p&gt;\n\n&lt;p&gt;is data analytics in 2023 more similar to SWE? should I have expected this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17gyevz", "is_robot_indexable": true, "report_reasons": null, "author": "Prestigious_Belt4965", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gyevz/im_a_data_analyst_who_in_practice_is_actually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gyevz/im_a_data_analyst_who_in_practice_is_actually/", "subreddit_subscribers": 1100424, "created_utc": 1698332259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Can be in work, as a passion project/academic project, or just an idea. Was it successful? If not, why not? Would love to be inspired &amp; motivated by all of your experiences, and who knows, maybe it\u2019ll help someone think about a current project in a new way.", "author_fullname": "t2_ct0r9u9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most unique, out of the box, or exciting application of DS you\u2019ve used/thought of?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gx6q4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698328791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can be in work, as a passion project/academic project, or just an idea. Was it successful? If not, why not? Would love to be inspired &amp;amp; motivated by all of your experiences, and who knows, maybe it\u2019ll help someone think about a current project in a new way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17gx6q4", "is_robot_indexable": true, "report_reasons": null, "author": "NewManufacturer3888", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gx6q4/what_is_the_most_unique_out_of_the_box_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gx6q4/what_is_the_most_unique_out_of_the_box_or/", "subreddit_subscribers": 1100424, "created_utc": 1698328791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_mjdr3ppgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are data science interviews for entry level different from senior level (L5-L6). How is the interview preparation different?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gtqqz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698317497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17gtqqz", "is_robot_indexable": true, "report_reasons": null, "author": "First_Beginning6365", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gtqqz/how_are_data_science_interviews_for_entry_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gtqqz/how_are_data_science_interviews_for_entry_level/", "subreddit_subscribers": 1100424, "created_utc": 1698317497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! I\u2019m applying to data science/ analytics jobs and internships right now. I have extensive personal and academic projects that I need to put in an online portfolio for hiring managers to see. What do you all recommend for this? Thanks!", "author_fullname": "t2_hhwehrl8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What website do I use to make a portfolio?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17h0uio", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698338865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I\u2019m applying to data science/ analytics jobs and internships right now. I have extensive personal and academic projects that I need to put in an online portfolio for hiring managers to see. What do you all recommend for this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17h0uio", "is_robot_indexable": true, "report_reasons": null, "author": "soupqueen6869", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h0uio/what_website_do_i_use_to_make_a_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17h0uio/what_website_do_i_use_to_make_a_portfolio/", "subreddit_subscribers": 1100424, "created_utc": 1698338865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello All,\n\nI am a student pursuing an MS in data science. I have done a few projects involving EDA and implemented a few ML algorithms. I am very enthusiastic about researching something and publishing a paper on it. However, I have no idea where to start or how to choose a research topic. Can someone among you guide me on this? At this point, I do not want to pursue a PhD but want to conduct independent research on a topic.", "author_fullname": "t2_650l3x8ha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance to publish a paper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gyvk2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698333564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\n\n&lt;p&gt;I am a student pursuing an MS in data science. I have done a few projects involving EDA and implemented a few ML algorithms. I am very enthusiastic about researching something and publishing a paper on it. However, I have no idea where to start or how to choose a research topic. Can someone among you guide me on this? At this point, I do not want to pursue a PhD but want to conduct independent research on a topic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17gyvk2", "is_robot_indexable": true, "report_reasons": null, "author": "Eastern-Habit6458", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gyvk2/need_guidance_to_publish_a_paper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gyvk2/need_guidance_to_publish_a_paper/", "subreddit_subscribers": 1100424, "created_utc": 1698333564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are the founders of Cursor Insight, the human motion experts. AMA!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gx81s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_gbypvyyx9", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "AI", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "IAmA", "selftext": "Hi r/IAmA,\n\nOur AMA participants:\r  \nCEO - Tamas Zelczer - u/tamas_z\r  \nCTO - Bence Golda - u/bencegolda\r  \nHead of R&amp;D - Gergely Hanczar - u/gergely_hanczar\r\r  \n\r  \nCursor Insight's technology can:\r  \n\ud83d\uddb1\ufe0f Authenticate people based on their cursor movement or how they interact with a mobile device\r  \n\ud83d\udeb6\u200d\u2640\ufe0f Identify people by the way they walk/move\r  \n\ud83c\udfe5 Detect the early symptoms of Parkinson's &amp; Alzheimer's Disease\r  \n... and even more.\r  \nOur solutions are built on a proprietary, ML-based human motion analysis toolchain.\ud83d\udc69\u200d\ud83d\udcbb\r  \nContinuous Biometric Authentication tool\r  \nGraboxy Sentinel runs invisibly in the background without interfering with the user\u2019s session. It builds and evaluates against biometric user profiles based on the uniqueness of cursor and/or mobile movement patterns, which are just as individual as fingerprints. If the real-time movement analysis shows a divergence from the user\u2019s biometric profile, Graboxy Sentinel flags the suspicious user session. Flagged users can be locked out or re-verified using traditional multi-factor authentication methods. \r  \nVideo: https://www.youtube.com/watch?v=Kz45xx9RDrM\r  \nMore information: https://sentinel.graboxy.com/\r\r  \n\r  \nWalking Recognition tool\r  \nCursor Insight\u2019s CCTV gait recognition technology analyzes how people move on live video footage. We use our machine learning models and motion analysis expertise to examine the unique gross motor coordination of people appearing on videos. We only process motion data and don\u2019t handle sensitive biometrics, such as facial images. \r  \nVideo: https://www.youtube.com/watch?v=TXEpk\\_6kO50\r  \nMore information: https://www.cursorinsight.com/walking-recognition\r\r  \n\r  \nPrediction of Parkinson\u2019s and Alzheimer\u2019s disease\r  \nWe have a fast and easy-to-use screening method for the early detection of mild cognitive impairment (MCI), which is often a precursor to Alzheimer\u2019s and Parkinson\u2019s disease. We partnered with the National Institute of Mental Health, Neurology and Neurosurgery (Budapest, Hungary). In our published study, our machine learning model detected MCI from a few seconds of cursor movement with an overall accuracy of 79.8%.\r  \nVideo: https://www.youtube.com/watch?v=o\\_UZ8g2JIxA\r  \nMore information: https://www.motionanalysis.health/\r  \n5 of our movement analysis research projects were published in scientific journals.\r  \n1\ufe0f\u20e3 [Feature space reduction method for ultrahigh-dimensional, multiclass data: random forest-based multiround screening (RFMS)](https://iopscience.iop.org/article/10.1088/2632-2153/ad020e)\r  \n2\ufe0f\u20e3 [Detection of mild cognitive impairment based on mouse movement data of trail making test](https://www.sciencedirect.com/science/article/pii/S235291482200257X)\r  \n3\ufe0f\u20e3 [BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space](https://www.sciencedirect.com/science/article/pii/S2352711023000626)\r  \n4\ufe0f\u20e3 [Motor Movement Data Collection from Older People with Tablets and Digital Pen-Based Input Devices](https://link.springer.com/chapter/10.1007/978-3-031-19745-1_18)\r  \n5\ufe0f\u20e3 [Anomalies in measuring speed and other dynamic properties with touchscreens and tablets](https://ieeexplore.ieee.org/document/8897249)\n\n\rProof:\r  \nTamas Zelczer: https://imgur.com/a/OM2GDhF\r  \nGergo Hanczar: https://imgur.com/a/Jc4baDw\r  \nBence Golda: https://imgur.com/a/iDYZZ5n\r  \n", "author_fullname": "t2_gbypvyyx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We are the founders of Cursor Insight, the human motion experts. AMA!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/IAmA", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gv9im", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698322962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.IAmA", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/IAmA\"&gt;r/IAmA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Our AMA participants:&lt;/p&gt;\n\n&lt;p&gt;CEO - Tamas Zelczer - &lt;a href=\"/u/tamas_z\"&gt;u/tamas_z&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;CTO - Bence Golda - &lt;a href=\"/u/bencegolda\"&gt;u/bencegolda&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Head of R&amp;amp;D - Gergely Hanczar - &lt;a href=\"/u/gergely_hanczar\"&gt;u/gergely_hanczar&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Cursor Insight&amp;#39;s technology can:&lt;/p&gt;\n\n&lt;p&gt;\ud83d\uddb1\ufe0f Authenticate people based on their cursor movement or how they interact with a mobile device&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udeb6\u200d\u2640\ufe0f Identify people by the way they walk/move&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udfe5 Detect the early symptoms of Parkinson&amp;#39;s &amp;amp; Alzheimer&amp;#39;s Disease&lt;/p&gt;\n\n&lt;p&gt;... and even more.&lt;/p&gt;\n\n&lt;p&gt;Our solutions are built on a proprietary, ML-based human motion analysis toolchain.\ud83d\udc69\u200d\ud83d\udcbb&lt;/p&gt;\n\n&lt;p&gt;Continuous Biometric Authentication tool&lt;/p&gt;\n\n&lt;p&gt;Graboxy Sentinel runs invisibly in the background without interfering with the user\u2019s session. It builds and evaluates against biometric user profiles based on the uniqueness of cursor and/or mobile movement patterns, which are just as individual as fingerprints. If the real-time movement analysis shows a divergence from the user\u2019s biometric profile, Graboxy Sentinel flags the suspicious user session. Flagged users can be locked out or re-verified using traditional multi-factor authentication methods. &lt;/p&gt;\n\n&lt;p&gt;Video: &lt;a href=\"https://www.youtube.com/watch?v=Kz45xx9RDrM\"&gt;https://www.youtube.com/watch?v=Kz45xx9RDrM&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More information: &lt;a href=\"https://sentinel.graboxy.com/\"&gt;https://sentinel.graboxy.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Walking Recognition tool&lt;/p&gt;\n\n&lt;p&gt;Cursor Insight\u2019s CCTV gait recognition technology analyzes how people move on live video footage. We use our machine learning models and motion analysis expertise to examine the unique gross motor coordination of people appearing on videos. We only process motion data and don\u2019t handle sensitive biometrics, such as facial images. &lt;/p&gt;\n\n&lt;p&gt;Video: &lt;a href=\"https://www.youtube.com/watch?v=TXEpk%5C_6kO50\"&gt;https://www.youtube.com/watch?v=TXEpk\\_6kO50&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More information: &lt;a href=\"https://www.cursorinsight.com/walking-recognition\"&gt;https://www.cursorinsight.com/walking-recognition&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Prediction of Parkinson\u2019s and Alzheimer\u2019s disease&lt;/p&gt;\n\n&lt;p&gt;We have a fast and easy-to-use screening method for the early detection of mild cognitive impairment (MCI), which is often a precursor to Alzheimer\u2019s and Parkinson\u2019s disease. We partnered with the National Institute of Mental Health, Neurology and Neurosurgery (Budapest, Hungary). In our published study, our machine learning model detected MCI from a few seconds of cursor movement with an overall accuracy of 79.8%.&lt;/p&gt;\n\n&lt;p&gt;Video: &lt;a href=\"https://www.youtube.com/watch?v=o%5C_UZ8g2JIxA\"&gt;https://www.youtube.com/watch?v=o\\_UZ8g2JIxA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;More information: &lt;a href=\"https://www.motionanalysis.health/\"&gt;https://www.motionanalysis.health/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;5 of our movement analysis research projects were published in scientific journals.&lt;/p&gt;\n\n&lt;p&gt;1\ufe0f\u20e3 &lt;a href=\"https://iopscience.iop.org/article/10.1088/2632-2153/ad020e\"&gt;Feature space reduction method for ultrahigh-dimensional, multiclass data: random forest-based multiround screening (RFMS)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;2\ufe0f\u20e3 &lt;a href=\"https://www.sciencedirect.com/science/article/pii/S235291482200257X\"&gt;Detection of mild cognitive impairment based on mouse movement data of trail making test&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;3\ufe0f\u20e3 &lt;a href=\"https://www.sciencedirect.com/science/article/pii/S2352711023000626\"&gt;BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;4\ufe0f\u20e3 &lt;a href=\"https://link.springer.com/chapter/10.1007/978-3-031-19745-1_18\"&gt;Motor Movement Data Collection from Older People with Tablets and Digital Pen-Based Input Devices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;5\ufe0f\u20e3 &lt;a href=\"https://ieeexplore.ieee.org/document/8897249\"&gt;Anomalies in measuring speed and other dynamic properties with touchscreens and tablets&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Proof:&lt;/p&gt;\n\n&lt;p&gt;Tamas Zelczer: &lt;a href=\"https://imgur.com/a/OM2GDhF\"&gt;https://imgur.com/a/OM2GDhF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Gergo Hanczar: &lt;a href=\"https://imgur.com/a/Jc4baDw\"&gt;https://imgur.com/a/Jc4baDw&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Bence Golda: &lt;a href=\"https://imgur.com/a/iDYZZ5n\"&gt;https://imgur.com/a/iDYZZ5n&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?auto=webp&amp;s=24b882d63ef1ab013e16a46ac729a756a9e8b976", "width": 1158, "height": 1544}, "resolutions": [{"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c78d8c20ed667f2f00bea530f2cc22b24fdaf55", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d7b7bdd8c9eeb9006a0e0c01a3148b21753e366", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93c2242a2bcf95fb8b927a690d066b2eaa2e3069", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=727f229d8985b3374ff871ce7039509d17f1a1b4", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=815e142f82784ed8950f43055157df3193c39e3d", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30fabcb510fc6931c8b6a2a033867a1dd0802624", "width": 1080, "height": 1440}], "variants": {}, "id": "ZPam9ugMptsf7YVU_v-9wAhybHrsoOb1KaKq10QYZJ8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qzb6", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17gv9im", "is_robot_indexable": true, "report_reasons": null, "author": "CursorInsight", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/IAmA/comments/17gv9im/we_are_the_founders_of_cursor_insight_the_human/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/IAmA/comments/17gv9im/we_are_the_founders_of_cursor_insight_the_human/", "subreddit_subscribers": 22556709, "created_utc": 1698322962.0, "num_crossposts": 7, "media": null, "is_video": false}], "created": 1698328882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.IAmA", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/IAmA/comments/17gv9im/we_are_the_founders_of_cursor_insight_the_human/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?auto=webp&amp;s=24b882d63ef1ab013e16a46ac729a756a9e8b976", "width": 1158, "height": 1544}, "resolutions": [{"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c78d8c20ed667f2f00bea530f2cc22b24fdaf55", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d7b7bdd8c9eeb9006a0e0c01a3148b21753e366", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93c2242a2bcf95fb8b927a690d066b2eaa2e3069", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=727f229d8985b3374ff871ce7039509d17f1a1b4", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=815e142f82784ed8950f43055157df3193c39e3d", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/i1d3f2jkvCD2cAKhw0oroi1dDsWRr_jYM78Ymh20rI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30fabcb510fc6931c8b6a2a033867a1dd0802624", "width": 1080, "height": 1440}], "variants": {}, "id": "ZPam9ugMptsf7YVU_v-9wAhybHrsoOb1KaKq10QYZJ8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2f731e52-70eb-11ee-bec5-5a5142e6a4d2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "17gx81s", "is_robot_indexable": true, "report_reasons": null, "author": "CursorInsight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_17gv9im", "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gx81s/we_are_the_founders_of_cursor_insight_the_human/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/IAmA/comments/17gv9im/we_are_the_founders_of_cursor_insight_the_human/", "subreddit_subscribers": 1100424, "created_utc": 1698328882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My brother-in-law(call him James) co-owns a small business with two of his colleagues who provide services to small businesses. These services include website design, marketing(everything from SEO optimization to email lists to whatever), and graphic design. James recently reached out to me to ask if I would do part-time work for them as a data analyst/data scientist. My background is in quantitative political science. I know how to do pretty much everything data scientists do at a low level (ML algorithms, acquiring data, cleaning data, etc.) but I don't know very well how to apply these techniques to a business. So from that, I have two questions: \n\n1. How are ML algorithms used for businesses? I'll give some examples of how I imagine it working. K-means clustering can be used for targeted advertisements based on the groups customers are put in. Linear regression can be used to predict sales based on some other independent variable. Decision trees can be used to determine what factors might lead to a customer discontinuing the use of a service. Am I on the correct track? Are these incorrect or are there others I am missing? I would love to hear about ways you guys use ML in your job. I know how to do A/B testing conceptually and do a ton of hypothesis testing in my work so that part of the job I am not worried about (and honestly looking at these two methods it seems they will be used more often than ML). \n2. Can data science even be done with small businesses? My main concern is about the quality of the data. It may require me to organize the data which could take a considerable amount of time and might venture into some data engineering spheres in which I really don't have experience. And then will there even be enough data? Is there some critical mass of sales that is needed before one can begin analyzing a company's metrics? I believe most of the people this service works with a smaller companies that might not have the robust data that F500 companies do. \n\nI hope these two questions make sense. I'm not trying to get quick and dirty information about data science. If I'm pointed in the direction of how to use these algorithms I can research them on my own. I just wanted some advice from people in the field. For reference, I use mostly Stata in my poli sci work, but I can do most of it in Python as well. Stata is just better for the small studies I do lol. ", "author_fullname": "t2_2i4xkeus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Freelance Data Science in small businesses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gwjgc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698326916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My brother-in-law(call him James) co-owns a small business with two of his colleagues who provide services to small businesses. These services include website design, marketing(everything from SEO optimization to email lists to whatever), and graphic design. James recently reached out to me to ask if I would do part-time work for them as a data analyst/data scientist. My background is in quantitative political science. I know how to do pretty much everything data scientists do at a low level (ML algorithms, acquiring data, cleaning data, etc.) but I don&amp;#39;t know very well how to apply these techniques to a business. So from that, I have two questions: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How are ML algorithms used for businesses? I&amp;#39;ll give some examples of how I imagine it working. K-means clustering can be used for targeted advertisements based on the groups customers are put in. Linear regression can be used to predict sales based on some other independent variable. Decision trees can be used to determine what factors might lead to a customer discontinuing the use of a service. Am I on the correct track? Are these incorrect or are there others I am missing? I would love to hear about ways you guys use ML in your job. I know how to do A/B testing conceptually and do a ton of hypothesis testing in my work so that part of the job I am not worried about (and honestly looking at these two methods it seems they will be used more often than ML). &lt;/li&gt;\n&lt;li&gt;Can data science even be done with small businesses? My main concern is about the quality of the data. It may require me to organize the data which could take a considerable amount of time and might venture into some data engineering spheres in which I really don&amp;#39;t have experience. And then will there even be enough data? Is there some critical mass of sales that is needed before one can begin analyzing a company&amp;#39;s metrics? I believe most of the people this service works with a smaller companies that might not have the robust data that F500 companies do. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I hope these two questions make sense. I&amp;#39;m not trying to get quick and dirty information about data science. If I&amp;#39;m pointed in the direction of how to use these algorithms I can research them on my own. I just wanted some advice from people in the field. For reference, I use mostly Stata in my poli sci work, but I can do most of it in Python as well. Stata is just better for the small studies I do lol. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17gwjgc", "is_robot_indexable": true, "report_reasons": null, "author": "KamdynS7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gwjgc/freelance_data_science_in_small_businesses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gwjgc/freelance_data_science_in_small_businesses/", "subreddit_subscribers": 1100424, "created_utc": 1698326916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m building Underhive, a collaboration platform for ML Teams. I\u2019ve just put out the first product up which helps you use your own storage backend for Git-LFS.\n\nPlease email me at: support@underhive.in.\nIf you want to help and be one of the first beta clients.\nWe\u2019re also giving free usage for upto 200GBs for the next 6 months to beta clients.\r  \nTry out: https://underhive.in (please use on Desktop, the mobile version is broken right now)", "author_fullname": "t2_6qimiaok", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git Version Controlled Datasets in your own S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 66, "top_awarded_type": null, "hide_score": false, "name": "t3_17gwd7z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/f1OlSBSNNEJZAmwlPIi3HjCS-4n4jYT0TRbIIieQ8qU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698326392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building Underhive, a collaboration platform for ML Teams. I\u2019ve just put out the first product up which helps you use your own storage backend for Git-LFS.&lt;/p&gt;\n\n&lt;p&gt;Please email me at: &lt;a href=\"mailto:support@underhive.in\"&gt;support@underhive.in&lt;/a&gt;.\nIf you want to help and be one of the first beta clients.\nWe\u2019re also giving free usage for upto 200GBs for the next 6 months to beta clients.&lt;/p&gt;\n\n&lt;p&gt;Try out: &lt;a href=\"https://underhive.in\"&gt;https://underhive.in&lt;/a&gt; (please use on Desktop, the mobile version is broken right now)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pkoix1xasjwb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?auto=webp&amp;s=0cfb2bb0fcde155afa9212aadee76fe66d733781", "width": 1919, "height": 910}, "resolutions": [{"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea7444673e01e20b699edc7c4d0ee9950959ff7c", "width": 108, "height": 51}, {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06cbd8c2e12a6eae38c22adc6b0fe5b362dca112", "width": 216, "height": 102}, {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=caa9d8b538f17b51c3a7f8624746177663ada6e7", "width": 320, "height": 151}, {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14aca97e42576634fd0195abe4f21dde48e2caba", "width": 640, "height": 303}, {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62bc861e0f50a91e90f9bee7c5786af81c8b9d79", "width": 960, "height": 455}, {"url": "https://preview.redd.it/pkoix1xasjwb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1b6bb9096dfb70ffce217de88741e7a37c77018", "width": 1080, "height": 512}], "variants": {}, "id": "RwTIUFrxOqlzdg1MtSnUTZXi5rfvLnvbYHcTXoFYW4g"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17gwd7z", "is_robot_indexable": true, "report_reasons": null, "author": "kaisoma", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gwd7z/git_version_controlled_datasets_in_your_own_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pkoix1xasjwb1.jpg", "subreddit_subscribers": 1100424, "created_utc": 1698326392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Context: I have been looking at learning resources for data science interview preparations. Most are targeted at entry level and contains SQL/Coding questions as preparation guide. But then often interviews are asking questions like:  **Say you work at a major credit card company and are given a dataset of 600,000 credit card transactions. Use this dataset to build a fraud detection model.**   \n\n\nI have seen and found this framework for answering such questions:  \n\n\nStep1: Ask clarifying questions on problems and constraints \n\nStep 2: Establish Metrics \n\nStep 3: Understand your data sources \n\nStep 4: Explore your data \n\nStep 5: Data Cleanup \n\nStep 6: Feature Engineering \n\nStep 7: Model Selection and training \n\nStep 8: Deployment \n\nStep 9: Iterate \n\nI would love to get inputs on need and usefulness of such frameworks?\n\n&amp;#x200B;", "author_fullname": "t2_mjdr3ppgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data science answering frameworks helpful?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gtvwl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698318064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: I have been looking at learning resources for data science interview preparations. Most are targeted at entry level and contains SQL/Coding questions as preparation guide. But then often interviews are asking questions like:  &lt;strong&gt;Say you work at a major credit card company and are given a dataset of 600,000 credit card transactions. Use this dataset to build a fraud detection model.&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;I have seen and found this framework for answering such questions:  &lt;/p&gt;\n\n&lt;p&gt;Step1: Ask clarifying questions on problems and constraints &lt;/p&gt;\n\n&lt;p&gt;Step 2: Establish Metrics &lt;/p&gt;\n\n&lt;p&gt;Step 3: Understand your data sources &lt;/p&gt;\n\n&lt;p&gt;Step 4: Explore your data &lt;/p&gt;\n\n&lt;p&gt;Step 5: Data Cleanup &lt;/p&gt;\n\n&lt;p&gt;Step 6: Feature Engineering &lt;/p&gt;\n\n&lt;p&gt;Step 7: Model Selection and training &lt;/p&gt;\n\n&lt;p&gt;Step 8: Deployment &lt;/p&gt;\n\n&lt;p&gt;Step 9: Iterate &lt;/p&gt;\n\n&lt;p&gt;I would love to get inputs on need and usefulness of such frameworks?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17gtvwl", "is_robot_indexable": true, "report_reasons": null, "author": "First_Beginning6365", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gtvwl/are_data_science_answering_frameworks_helpful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gtvwl/are_data_science_answering_frameworks_helpful/", "subreddit_subscribers": 1100424, "created_utc": 1698318064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all, I encountered this data analytics / data science challenge at work, and I was quite perplexed about how to approach it. I'm curious to hear from those with more experience (if this is more of a stack overflow question, I apologize).\n\nIn order to not reveal confidential information, I have changed the details, but the problem statement remains the same.\n\n**Background:**\n\nI was working for an online platform that showcased products from various vendors, and our objective was to pinpoint which features contribute to user engagement (likes, shares, purchases, etc.) with a product listing.\n\nGiven that we weren't producing the product descriptions ourselves, our focus was on **features we could influence**. This meant we **did not include** aspects such as:\n\n* product origin / vendor, \n* brand reputation, \n* type of product, \n* price \n\n, even if they were vital factors driving user engagement. --&gt; Naturally, a product from a renowned brand priced competitively will receive much more attention than a niche article from an unknown maker priced higher than market average.\n\nOur attention was instead directed at a few controllable features:\n\n* whether or not the descriptions and product names exceeded a certain length (we could provide feedback on these to vendors)\n* whether or not our in-house ML model could categorize the product (affecting its searchability)\n* the presence of vendor ratings,\n* etc.\n\nTo clarify, every feature we identified was binary. That is, the listing either met the criteria or it didn't. So, my dataset consisted of rows representing all product listings from a 6 month period, around 10 feature columns with binary values, and an engagement metric.\n\n**Approach:**\n\nMy next steps? I initiated numerous student t-tests. \n\nFor instance, how do product listings with names shorter than 80 characters fare against those longer than 80 characters? What's the engagement disparity between products that were categorized versus those that weren't? \n\nGiven the presence of three distinct engagement metrics and three different product listing styles, each significance test focused on a single feature, metric, and style. I conducted over 100 tests, applying the Bonferroni correction to address the multiple comparisons problem. \n\nNote: while A/B testing was on my mind, I did not see an easy possibility of performing A/B testing on short vs. long product descriptions and titles, since every additional word also influences the content and meaning (adding certain words could have a beneficial effect, others a detrimental one). Some features (like presence of vendor ratings) likely could have been A/B tested, but weren't for UX / political reasons.\n\n**Results:**\n\nWith extensive data at hand, I observed significant differences in engagement for nearly all features for the primary engagement metric, which was encouraging (I of course also had a few false negatives due to the Bonferroni correction).\n\nYet, the findings weren't consistent. While some features demonstrated consistent engagement patterns across all listing styles, most varied. Without the structure of an A/B testing framework, it became evident that multiple confounding variables were in action. For instance, certain products and vendors were more prevalent in specific listing styles than others.\n\nMy next idea was to devise a regression model to predict engagement based on these diverse features. However, I was unsure what type of model to use considering that the features were binary, and I was also aware that multi-collinearity would impact the coefficients for a linear regression model. Also, my ultimate goal was not to develop a predictive model, but rather to have a solid understanding of the extent to which each feature influenced engagement.\n\nI never was able to fully explore this avenue because the project was called off -  the achievable bottom-line impact seemed less than that which could be achieved through other means.\n\n**What could I have done differently?**\n\nIn retrospect, I wonder what I could have done differently / better. Given the lack of an A/B testing environment, was it even possible to draw any conclusions? If yes, what kind of methods or approaches could have been better? Were the significance tests the correct way to go? Should I have tried a certain predictive model type? How and at what point do I determine that this is an avenue worth / not worth exploring further?\n\nI would love to hear your thoughts!", "author_fullname": "t2_ap3spmczk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with features of questionable predictive power and confounding variables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gtslo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698318586.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698317713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I encountered this data analytics / data science challenge at work, and I was quite perplexed about how to approach it. I&amp;#39;m curious to hear from those with more experience (if this is more of a stack overflow question, I apologize).&lt;/p&gt;\n\n&lt;p&gt;In order to not reveal confidential information, I have changed the details, but the problem statement remains the same.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was working for an online platform that showcased products from various vendors, and our objective was to pinpoint which features contribute to user engagement (likes, shares, purchases, etc.) with a product listing.&lt;/p&gt;\n\n&lt;p&gt;Given that we weren&amp;#39;t producing the product descriptions ourselves, our focus was on &lt;strong&gt;features we could influence&lt;/strong&gt;. This meant we &lt;strong&gt;did not include&lt;/strong&gt; aspects such as:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;product origin / vendor, &lt;/li&gt;\n&lt;li&gt;brand reputation, &lt;/li&gt;\n&lt;li&gt;type of product, &lt;/li&gt;\n&lt;li&gt;price &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;, even if they were vital factors driving user engagement. --&amp;gt; Naturally, a product from a renowned brand priced competitively will receive much more attention than a niche article from an unknown maker priced higher than market average.&lt;/p&gt;\n\n&lt;p&gt;Our attention was instead directed at a few controllable features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;whether or not the descriptions and product names exceeded a certain length (we could provide feedback on these to vendors)&lt;/li&gt;\n&lt;li&gt;whether or not our in-house ML model could categorize the product (affecting its searchability)&lt;/li&gt;\n&lt;li&gt;the presence of vendor ratings,&lt;/li&gt;\n&lt;li&gt;etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To clarify, every feature we identified was binary. That is, the listing either met the criteria or it didn&amp;#39;t. So, my dataset consisted of rows representing all product listings from a 6 month period, around 10 feature columns with binary values, and an engagement metric.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Approach:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My next steps? I initiated numerous student t-tests. &lt;/p&gt;\n\n&lt;p&gt;For instance, how do product listings with names shorter than 80 characters fare against those longer than 80 characters? What&amp;#39;s the engagement disparity between products that were categorized versus those that weren&amp;#39;t? &lt;/p&gt;\n\n&lt;p&gt;Given the presence of three distinct engagement metrics and three different product listing styles, each significance test focused on a single feature, metric, and style. I conducted over 100 tests, applying the Bonferroni correction to address the multiple comparisons problem. &lt;/p&gt;\n\n&lt;p&gt;Note: while A/B testing was on my mind, I did not see an easy possibility of performing A/B testing on short vs. long product descriptions and titles, since every additional word also influences the content and meaning (adding certain words could have a beneficial effect, others a detrimental one). Some features (like presence of vendor ratings) likely could have been A/B tested, but weren&amp;#39;t for UX / political reasons.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;With extensive data at hand, I observed significant differences in engagement for nearly all features for the primary engagement metric, which was encouraging (I of course also had a few false negatives due to the Bonferroni correction).&lt;/p&gt;\n\n&lt;p&gt;Yet, the findings weren&amp;#39;t consistent. While some features demonstrated consistent engagement patterns across all listing styles, most varied. Without the structure of an A/B testing framework, it became evident that multiple confounding variables were in action. For instance, certain products and vendors were more prevalent in specific listing styles than others.&lt;/p&gt;\n\n&lt;p&gt;My next idea was to devise a regression model to predict engagement based on these diverse features. However, I was unsure what type of model to use considering that the features were binary, and I was also aware that multi-collinearity would impact the coefficients for a linear regression model. Also, my ultimate goal was not to develop a predictive model, but rather to have a solid understanding of the extent to which each feature influenced engagement.&lt;/p&gt;\n\n&lt;p&gt;I never was able to fully explore this avenue because the project was called off -  the achievable bottom-line impact seemed less than that which could be achieved through other means.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What could I have done differently?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In retrospect, I wonder what I could have done differently / better. Given the lack of an A/B testing environment, was it even possible to draw any conclusions? If yes, what kind of methods or approaches could have been better? Were the significance tests the correct way to go? Should I have tried a certain predictive model type? How and at what point do I determine that this is an avenue worth / not worth exploring further?&lt;/p&gt;\n\n&lt;p&gt;I would love to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17gtslo", "is_robot_indexable": true, "report_reasons": null, "author": "Glum-Bat8771", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gtslo/dealing_with_features_of_questionable_predictive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gtslo/dealing_with_features_of_questionable_predictive/", "subreddit_subscribers": 1100424, "created_utc": 1698317713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently working as a Research Specialist at a private company where I conduct trials and experimental data analysis. I have a biological science background with experience in R and JMP and thinking of jumping careers in Data Science. \n\nAny additional skills I need to learn?", "author_fullname": "t2_m26htnjnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to qualify for a job in Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gtgb0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698316319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a Research Specialist at a private company where I conduct trials and experimental data analysis. I have a biological science background with experience in R and JMP and thinking of jumping careers in Data Science. &lt;/p&gt;\n\n&lt;p&gt;Any additional skills I need to learn?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17gtgb0", "is_robot_indexable": true, "report_reasons": null, "author": "cinderbl0ckgardener", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gtgb0/how_to_qualify_for_a_job_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gtgb0/how_to_qualify_for_a_job_in_data_science/", "subreddit_subscribers": 1100424, "created_utc": 1698316319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I currently work for a government contractor. I have an opportunity to take a job with a technology consulting company as a consultant. I am on W2 at the first job and would be on a 1099 for the second. Could the first job still be able to find out about the second?  If I'm doing 20 hrs/wk at the second job, would that still be a problem for employers since I would be able to do it after hours?", "author_fullname": "t2_jnnvdz39c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having a second job on 1099", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17goc84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698294457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work for a government contractor. I have an opportunity to take a job with a technology consulting company as a consultant. I am on W2 at the first job and would be on a 1099 for the second. Could the first job still be able to find out about the second?  If I&amp;#39;m doing 20 hrs/wk at the second job, would that still be a problem for employers since I would be able to do it after hours?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17goc84", "is_robot_indexable": true, "report_reasons": null, "author": "daufoi21", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17goc84/having_a_second_job_on_1099/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17goc84/having_a_second_job_on_1099/", "subreddit_subscribers": 1100424, "created_utc": 1698294457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\nI made this little doodle below as a good-faith effort at trying to lay out a reasonable decision tree for choosing an appropriate model evaluation metric for the sort of basic cases of predictive analytics. I'm ignoring numerous more complex cases of like like NLP, Computer Vision, and even arguably some simpler cases like forecasting, but trying to still cover the bulk of the entry gauntlet. There's obviously innumerable choices one could make for metrics, so the bias here is picking ones that are \"less wrong\" (avoid as many pitfalls as possible), fairly interpretable (e.g. a value of 1 or 0 \"means something\"), and have some popular acceptance. Sharing here in case it's helpful, and also I'm interested in others poking holes in the choices I made (if something seems egregious enough)!\n\nMy motivation here was mostly out of internal frustration of often seeing folks (online, friends, colleagues) fall into fairly rough pitfalls in their eval choices, and just seeing whether something like this could be reasonably written out. Not for anything else in a sense than the jollies.\n\nhttps://preview.redd.it/7nplhw2npgwb1.png?width=7162&amp;format=png&amp;auto=webp&amp;s=9bf42afad02bccdb791e88016a78862c7d7faa32", "author_fullname": "t2_9yzncc2e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluation Metric Flowchart (possibly handy, interested in feedback!)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7nplhw2npgwb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=030c67234c4f1eab22019470c0c1ca689e587b78"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4778be88b74770c29cfba8eb985952477d7e668d"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2575007f8ef0e519f6657b3807e6a8eef7520277"}, {"y": 356, "x": 640, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6807bc711553c64a5d8a096d501ce856e4c4cb53"}, {"y": 534, "x": 960, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b91e51da19d7ecac8dd06a18ef275289f9ba9fa"}, {"y": 601, "x": 1080, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c8caebe3c3b31aa7899791907ab558296754139"}], "s": {"y": 3990, "x": 7162, "u": "https://preview.redd.it/7nplhw2npgwb1.png?width=7162&amp;format=png&amp;auto=webp&amp;s=9bf42afad02bccdb791e88016a78862c7d7faa32"}, "id": "7nplhw2npgwb1"}}, "name": "t3_17gn08a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/C6FsdcbioThGINeCCFjlbVQ1pb184byUS-UeLLkTbLs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698289874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I made this little doodle below as a good-faith effort at trying to lay out a reasonable decision tree for choosing an appropriate model evaluation metric for the sort of basic cases of predictive analytics. I&amp;#39;m ignoring numerous more complex cases of like like NLP, Computer Vision, and even arguably some simpler cases like forecasting, but trying to still cover the bulk of the entry gauntlet. There&amp;#39;s obviously innumerable choices one could make for metrics, so the bias here is picking ones that are &amp;quot;less wrong&amp;quot; (avoid as many pitfalls as possible), fairly interpretable (e.g. a value of 1 or 0 &amp;quot;means something&amp;quot;), and have some popular acceptance. Sharing here in case it&amp;#39;s helpful, and also I&amp;#39;m interested in others poking holes in the choices I made (if something seems egregious enough)!&lt;/p&gt;\n\n&lt;p&gt;My motivation here was mostly out of internal frustration of often seeing folks (online, friends, colleagues) fall into fairly rough pitfalls in their eval choices, and just seeing whether something like this could be reasonably written out. Not for anything else in a sense than the jollies.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7nplhw2npgwb1.png?width=7162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9bf42afad02bccdb791e88016a78862c7d7faa32\"&gt;https://preview.redd.it/7nplhw2npgwb1.png?width=7162&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9bf42afad02bccdb791e88016a78862c7d7faa32&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17gn08a", "is_robot_indexable": true, "report_reasons": null, "author": "jshkk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gn08a/evaluation_metric_flowchart_possibly_handy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gn08a/evaluation_metric_flowchart_possibly_handy/", "subreddit_subscribers": 1100424, "created_utc": 1698289874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone have a good feel of how to formulate the task of predicting if a company will have to relocate in following X months? Say I can construct a dataset with info on employees, area, building details and some financial numbers. I know the months in which they move or not. So zeros and ones here for the label. I haven't managed to find any relevant literature or code or blog post on a similar topic. Is this a binary classification problem? What algorithms to use? How to account for class imbalance that case? Any pointers would be much appreciated.", "author_fullname": "t2_genrcwic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to predict office relocation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17h2gwb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698343297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a good feel of how to formulate the task of predicting if a company will have to relocate in following X months? Say I can construct a dataset with info on employees, area, building details and some financial numbers. I know the months in which they move or not. So zeros and ones here for the label. I haven&amp;#39;t managed to find any relevant literature or code or blog post on a similar topic. Is this a binary classification problem? What algorithms to use? How to account for class imbalance that case? Any pointers would be much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17h2gwb", "is_robot_indexable": true, "report_reasons": null, "author": "nacho_biznis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h2gwb/how_to_predict_office_relocation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17h2gwb/how_to_predict_office_relocation/", "subreddit_subscribers": 1100424, "created_utc": 1698343297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello guys,\n\nI am currently doing a project for my internship. It involves image detection which I have more or less dealt with. The main thing now for me to do is that I have to compare the mass or brightness of each of the blue holes with the reference chart circled red. The blue dots in the red circle have varying uniform opacity and I have to see how the outside blue dots compare with the reference dots. I cannot seem to figure out how to go about doing this. I was thinking of a graph, but it does not seem convenient or maybe a 3d graph(?). I would be grateful if you guys can give me suggestions.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jaefgmgh4lwb1.png?width=717&amp;format=png&amp;auto=webp&amp;s=8c03ed5cc6732c8748f548c5742824c952815156", "author_fullname": "t2_pqlshwwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions for my internship project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 134, "top_awarded_type": null, "hide_score": true, "media_metadata": {"jaefgmgh4lwb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 104, "x": 108, "u": "https://preview.redd.it/jaefgmgh4lwb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b831488d5ad4ff0af1a4e9427d18b39337c872c"}, {"y": 208, "x": 216, "u": "https://preview.redd.it/jaefgmgh4lwb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=766367769ec8e749a750a6cd37e2611eff11cf7b"}, {"y": 308, "x": 320, "u": "https://preview.redd.it/jaefgmgh4lwb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=50463a3966e9a2dc9c6bb54a24c772cf263b9d64"}, {"y": 616, "x": 640, "u": "https://preview.redd.it/jaefgmgh4lwb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3269c9dea05931421fa9e68c927d1a94d8d2b595"}], "s": {"y": 691, "x": 717, "u": "https://preview.redd.it/jaefgmgh4lwb1.png?width=717&amp;format=png&amp;auto=webp&amp;s=8c03ed5cc6732c8748f548c5742824c952815156"}, "id": "jaefgmgh4lwb1"}}, "name": "t3_17h2bxq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/CV9HMHpaNUuJHNcwn8d7b_bHJoNjdcs4Y5Ugfmxqnd8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698342923.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I am currently doing a project for my internship. It involves image detection which I have more or less dealt with. The main thing now for me to do is that I have to compare the mass or brightness of each of the blue holes with the reference chart circled red. The blue dots in the red circle have varying uniform opacity and I have to see how the outside blue dots compare with the reference dots. I cannot seem to figure out how to go about doing this. I was thinking of a graph, but it does not seem convenient or maybe a 3d graph(?). I would be grateful if you guys can give me suggestions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jaefgmgh4lwb1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c03ed5cc6732c8748f548c5742824c952815156\"&gt;https://preview.redd.it/jaefgmgh4lwb1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c03ed5cc6732c8748f548c5742824c952815156&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17h2bxq", "is_robot_indexable": true, "report_reasons": null, "author": "SussyAutist", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h2bxq/suggestions_for_my_internship_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17h2bxq/suggestions_for_my_internship_project/", "subreddit_subscribers": 1100424, "created_utc": 1698342923.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I find the data specifications / requirements process to be awful. It's legit one of my least favorite aspects of this job.\n\nFor context, I work in an academia-adjacent industry, and I'm typically working with subject matter research experts. They are responsible for writing programming specifications for our projects, which are supposed to serve as an outline for programming and development for the data science team. Sometimes PMs will write them as well. For ex. something like:\n\n\n    1. Load data from [data source]\n    2. Confirm variables `x`, `y`, and `z` are correct data types\n    3. Merge data (outer join) with [other dataset]\n    \ta. output a table of merge %\n    4. Deduplicate on ID variable\n    5. Filter by ...\n    6. etc.\n    7. export files to [server location]\n\nAs a data scientist, I am supposed to generally follow these steps to produce the result we are looking for. If I disagree with a step, or need to add some logic, I'll go in to the document and edit it. So it's a shared responsibility between my team and the research / project management team. The above steps are a very simplified example - sometimes these types of requirement documents can be like 15 pages long, with a ton of rules and nested logic / requests.\n\nThese documents tend to be written in Microsoft Word, which is messy and hard to version control when working across large teams. It's very easy to miss updates and lose track of which specifications have changed.\n\nI can't help but think this process could be so much cleaner and efficient.", "author_fullname": "t2_1xtk67f5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it just me, or is the requirements / specifications process a complete nightmare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17h23wx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698342308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find the data specifications / requirements process to be awful. It&amp;#39;s legit one of my least favorite aspects of this job.&lt;/p&gt;\n\n&lt;p&gt;For context, I work in an academia-adjacent industry, and I&amp;#39;m typically working with subject matter research experts. They are responsible for writing programming specifications for our projects, which are supposed to serve as an outline for programming and development for the data science team. Sometimes PMs will write them as well. For ex. something like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1. Load data from [data source]\n2. Confirm variables `x`, `y`, and `z` are correct data types\n3. Merge data (outer join) with [other dataset]\n    a. output a table of merge %\n4. Deduplicate on ID variable\n5. Filter by ...\n6. etc.\n7. export files to [server location]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;As a data scientist, I am supposed to generally follow these steps to produce the result we are looking for. If I disagree with a step, or need to add some logic, I&amp;#39;ll go in to the document and edit it. So it&amp;#39;s a shared responsibility between my team and the research / project management team. The above steps are a very simplified example - sometimes these types of requirement documents can be like 15 pages long, with a ton of rules and nested logic / requests.&lt;/p&gt;\n\n&lt;p&gt;These documents tend to be written in Microsoft Word, which is messy and hard to version control when working across large teams. It&amp;#39;s very easy to miss updates and lose track of which specifications have changed.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t help but think this process could be so much cleaner and efficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17h23wx", "is_robot_indexable": true, "report_reasons": null, "author": "donhuell", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h23wx/is_it_just_me_or_is_the_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17h23wx/is_it_just_me_or_is_the_requirements/", "subreddit_subscribers": 1100424, "created_utc": 1698342308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_uvp5yieu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a shiny app for data scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_17h1y9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0CMyKLo3ar_Fa2W4xP2FUdt6HJaU1LSmqtd6W1yWo7Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698341884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/AdrianAntico/Quantico", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?auto=webp&amp;s=d57e35095e0b630892f6f55d9d8876989fee23be", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64ee66110847a082bcc581fa4ccb58c9a9da70f9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa80206fc3808972dfaa1aff85007a1d5fc83582", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a5ceba7fd4b66fa31d0be58327257c3b9656518", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4a7aa8d386f6fb7d244c9c1500e5e396e992503d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4c4ecc794d70515daf3c8e0ea85b2ee76e97ac1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/5KUA20Dg_UpMh90iqUWBY1hWSMfpcBRH1XV79o-gZvg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb7514d257a19b6f7d8e8c4e3215da0eec8f1b41", "width": 1080, "height": 540}], "variants": {}, "id": "aWyT6o3LntwFYRLyiWK3GsMjBs0k543cr9WZyGcqc2E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17h1y9t", "is_robot_indexable": true, "report_reasons": null, "author": "house_lite", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h1y9t/i_created_a_shiny_app_for_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/AdrianAntico/Quantico", "subreddit_subscribers": 1100424, "created_utc": 1698341884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks, I just published an article where I shared some of the tips I've learned based on my research and experience for building a data strategy and leveling up your business. Curious to learn more? Dive in here", "author_fullname": "t2_8wu7bwpq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Strategy Mastery: Valuable Tips for Data Pros and Companies Aiming to Level Up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_17h10e0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6NCDxcVZbPJuuikhFhFJheI3PH2Bps0lCqJi_WQC9tA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698339307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "meysamraz.medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, I just published an article where I shared some of the tips I&amp;#39;ve learned based on my research and experience for building a data strategy and leveling up your business. Curious to learn more? Dive in here&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://meysamraz.medium.com/data-strategy-mastery-valuable-tips-for-data-pros-and-companies-aiming-to-level-up-69b17606e7e4", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?auto=webp&amp;s=b6ed06eeb97a52c1945e6e9d540b002638ef3431", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ecad36e43614cb0fe0bcc1d2678c0c66cd8ae58", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=877b6496ae0b916b6272bc7fb7cc2d7e9210b7d5", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddc732be9859fc9a287ff8ac4030bd03b84399c6", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9ef82f0cd8526b45b172ad98684ff28ee7744cc7", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed7c8ccc3abaf0053d939bceaea9f3dab53a6cb4", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/J7x_4TdrpYRbC5Tk0KQMklivvv7HAaUVsEclotWlcj0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc05c9cba09344cc372822dcbd2bd73ce06f31b4", "width": 1080, "height": 720}], "variants": {}, "id": "tSK7y3dr6jSBXFMMLvFAne_vgCXbc2lAR8kEfcGpaNs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17h10e0", "is_robot_indexable": true, "report_reasons": null, "author": "Alert_Pea_4855", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h10e0/data_strategy_mastery_valuable_tips_for_data_pros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://meysamraz.medium.com/data-strategy-mastery-valuable-tips-for-data-pros-and-companies-aiming-to-level-up-69b17606e7e4", "subreddit_subscribers": 1100424, "created_utc": 1698339307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to get the residuals to white noise but there are two different behaviors on residuals. Any ideas on how I should transform this? Or what should I do. I tried log/sqrt. Doesn\u2019t really do shit. Dataset is a hourly data for a couple years. The graph behavior is seasonal yearly, and daily aswell. But right now I just care about the yearly. Any advice?", "author_fullname": "t2_16pd0l0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Residuals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": 28, "top_awarded_type": null, "hide_score": true, "name": "t3_17h0yms", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JqicF8FCnFbGkcNdxJqsEQrCsTjgmwENcWOYsrmYWB0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698339171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to get the residuals to white noise but there are two different behaviors on residuals. Any ideas on how I should transform this? Or what should I do. I tried log/sqrt. Doesn\u2019t really do shit. Dataset is a hourly data for a couple years. The graph behavior is seasonal yearly, and daily aswell. But right now I just care about the yearly. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4f064j4bukwb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?auto=webp&amp;s=fdbe72f2999111d8b6e56938f1feb2811ff6a079", "width": 4030, "height": 826}, "resolutions": [{"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27e1f99a799ac1e7fb10210dab0828ea45cc0491", "width": 108, "height": 22}, {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6cc1f8a75a65eae682a5df4ccab0daaefff5749a", "width": 216, "height": 44}, {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ea862e2a595668ccd44ea89673f9690ec57a9e1", "width": 320, "height": 65}, {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f200b1d941601db4fe62a98e0b24b47718e83ea0", "width": 640, "height": 131}, {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f59e4be220cca0a8595a98ccb3ce1eb9e5f45a1", "width": 960, "height": 196}, {"url": "https://preview.redd.it/4f064j4bukwb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49a7e391586ae92e8a7bbf19542aa95060618838", "width": 1080, "height": 221}], "variants": {}, "id": "zDjGcBOQynDw_5XnMbw8Dp-gI-3Y0ILAc5peC5tpX9g"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17h0yms", "is_robot_indexable": true, "report_reasons": null, "author": "battleaxe37", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17h0yms/residuals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4f064j4bukwb1.jpg", "subreddit_subscribers": 1100424, "created_utc": 1698339171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, can anyone help me out. I want to convert a huge .dta file(~3GB) to .csv file but I am not able to do so using python due to its large size. I also tried on kaggle but it said memory limit exceeded. Can anyone help me out?", "author_fullname": "t2_tvg76tfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert Stata(.DTA) files to .csv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17gzyzp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698336565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, can anyone help me out. I want to convert a huge .dta file(~3GB) to .csv file but I am not able to do so using python due to its large size. I also tried on kaggle but it said memory limit exceeded. Can anyone help me out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17gzyzp", "is_robot_indexable": true, "report_reasons": null, "author": "smokeyScraper", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17gzyzp/convert_statadta_files_to_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17gzyzp/convert_statadta_files_to_csv/", "subreddit_subscribers": 1100424, "created_utc": 1698336565.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}