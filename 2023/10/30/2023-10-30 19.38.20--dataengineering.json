{"kind": "Listing", "data": {"after": "t3_17ju8s8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, i've done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?", "author_fullname": "t2_daehbbsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i'm seeing less star schemas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jg1x5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698623091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, i&amp;#39;ve done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jg1x5", "is_robot_indexable": true, "report_reasons": null, "author": "Tough-Error520", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jg1x5/im_seeing_less_star_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jg1x5/im_seeing_less_star_schemas/", "subreddit_subscribers": 136969, "created_utc": 1698623091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are prepping for the data engineering interview, you should include the data modeling loop as a key section of your prep. Here is a two part series that includes an overview of the signal that they are looking for, practice problems, and a deeb dive into some of the technical aspects you will need to know.  \n\n\n[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938)  \n\n\n[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca)", "author_fullname": "t2_4fpl974m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cracking the Data Modeling Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jh22q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698626042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are prepping for the data engineering interview, you should include the data modeling loop as a key section of your prep. Here is a two part series that includes an overview of the signal that they are looking for, practice problems, and a deeb dive into some of the technical aspects you will need to know.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938\"&gt;https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca\"&gt;https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?auto=webp&amp;s=4090b055bdc86a803c1f6b2c73b9004f54ddba8e", "width": 1200, "height": 557}, "resolutions": [{"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2db5992e420edc82f4e1dc1f1849565ff085d58", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef17ec983bf531cd47ffe8c475fd585bedc4ed40", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7eb2456cff1d6d17a83d2b90861504e91330e8c", "width": 320, "height": 148}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58336696dcdfbfa2f0306d9fef6cb38b5acc1882", "width": 640, "height": 297}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8ecae39f65c5ee5f3d6c9196d657381264e1de", "width": 960, "height": 445}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2070f6d3d66ac2ee727fd392976a77d070e92a80", "width": 1080, "height": 501}], "variants": {}, "id": "oNoLwNsn-uU3Qx0q4ZAFYrBNCcGCimMegvkILgS8w3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17jh22q", "is_robot_indexable": true, "report_reasons": null, "author": "coyne_operated", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jh22q/cracking_the_data_modeling_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jh22q/cracking_the_data_modeling_interview/", "subreddit_subscribers": 136969, "created_utc": 1698626042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a cs student looking to become a data engineer at some point down the line - should I take a databases class? My school offers one next semester however it conflicts w another class I need to graduate time wise. I graduate next year so the database class won\u2019t be offered again before I graduate. Is it worth potentially putting off graduation by a semester or are databases something I can teach myself?", "author_fullname": "t2_kc7mg3akr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take a databases class?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ji5fm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698629454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a cs student looking to become a data engineer at some point down the line - should I take a databases class? My school offers one next semester however it conflicts w another class I need to graduate time wise. I graduate next year so the database class won\u2019t be offered again before I graduate. Is it worth potentially putting off graduation by a semester or are databases something I can teach myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ji5fm", "is_robot_indexable": true, "report_reasons": null, "author": "Brief-Union-3493", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ji5fm/should_i_take_a_databases_class/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ji5fm/should_i_take_a_databases_class/", "subreddit_subscribers": 136969, "created_utc": 1698629454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Pentagram Schema for Evaluating Haunted Data Warehouses**\n\nIn the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the \"Pentagram Schema.\" This schema is not just about data; it's about the eerie, the paranormal, and the mystical.\n\n**Pentagram Schema Overview:**\n\nAt the heart of the Pentagram Schema lies the **\"Haunted Data WareHouse Access\" fact table.** This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.\n\n**The Five Satanic Dimensions:**\n\n1. **Visitor (Dimension 1):** In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.\n2. **Query (Dimension 2):** Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they're regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.\n3. **Outcome (Dimension 3):** The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.\n4. **Tables Hit (Dimension 4):** Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.\n5. **Maintainer (Dimension 5):** In any haunted attraction, there's a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.\n\n&amp;#x200B;\n\n**Why the Pentagram Schema:**\n\nThese five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.\n\nHappy Haunting!\n\n  \n(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haunted data warehouse pentagram schema architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvmmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Pentagram Schema for Evaluating Haunted Data Warehouses&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the &amp;quot;Pentagram Schema.&amp;quot; This schema is not just about data; it&amp;#39;s about the eerie, the paranormal, and the mystical.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pentagram Schema Overview:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At the heart of the Pentagram Schema lies the &lt;strong&gt;&amp;quot;Haunted Data WareHouse Access&amp;quot; fact table.&lt;/strong&gt; This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Five Satanic Dimensions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Visitor (Dimension 1):&lt;/strong&gt; In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Query (Dimension 2):&lt;/strong&gt; Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they&amp;#39;re regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Outcome (Dimension 3):&lt;/strong&gt; The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tables Hit (Dimension 4):&lt;/strong&gt; Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintainer (Dimension 5):&lt;/strong&gt; In any haunted attraction, there&amp;#39;s a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why the Pentagram Schema:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.&lt;/p&gt;\n\n&lt;p&gt;Happy Haunting!&lt;/p&gt;\n\n&lt;p&gt;(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "17jvmmf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "subreddit_subscribers": 136969, "created_utc": 1698678690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nI\u2019m sorry if this has already been talked about here, but I couldn\u2019t really find older posts on this. I\u2019m trying to get into DE coming from a SWE background. I thought about doing so firstly through a job as a DA, which seems the most common path where I\u2019m from, at least. My question is: what are the first certifications to get if I wanted to break into DE/DA? My main goal is to not come across as if this is a nine-day wonder. What are the ones that make me stand out for entry positions?\n\nThank you.", "author_fullname": "t2_fgp3mdj67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certifications for someone trying to land an entrance position in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jnb1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698648277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sorry if this has already been talked about here, but I couldn\u2019t really find older posts on this. I\u2019m trying to get into DE coming from a SWE background. I thought about doing so firstly through a job as a DA, which seems the most common path where I\u2019m from, at least. My question is: what are the first certifications to get if I wanted to break into DE/DA? My main goal is to not come across as if this is a nine-day wonder. What are the ones that make me stand out for entry positions?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jnb1u", "is_robot_indexable": true, "report_reasons": null, "author": "LusoDev", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jnb1u/certifications_for_someone_trying_to_land_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jnb1u/certifications_for_someone_trying_to_land_an/", "subreddit_subscribers": 136969, "created_utc": 1698648277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.\n\nWe have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.\n\nOur latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.\n\nSo a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster \"Integrations\" is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don't really know these platforms either. Perhaps we should simply be looking into these?\n\nAre we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?", "author_fullname": "t2_7no9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect&amp;Flask as a Hub and Spoke", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqhvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698671728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698662486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.&lt;/p&gt;\n\n&lt;p&gt;We have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.&lt;/p&gt;\n\n&lt;p&gt;Our latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.&lt;/p&gt;\n\n&lt;p&gt;So a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster &amp;quot;Integrations&amp;quot; is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don&amp;#39;t really know these platforms either. Perhaps we should simply be looking into these?&lt;/p&gt;\n\n&lt;p&gt;Are we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqhvd", "is_robot_indexable": true, "report_reasons": null, "author": "YourOldBuddy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "subreddit_subscribers": 136969, "created_utc": 1698662486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some help to in understanding the dbricks cluster loading.\n\nSay I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. \n\nIf I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? \n\nIf I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?\n\nThanks for checking.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens when we submit a lot of spark jobs on a Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvo4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some help to in understanding the dbricks cluster loading.&lt;/p&gt;\n\n&lt;p&gt;Say I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. &lt;/p&gt;\n\n&lt;p&gt;If I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? &lt;/p&gt;\n\n&lt;p&gt;If I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?&lt;/p&gt;\n\n&lt;p&gt;Thanks for checking.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jvo4e", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "subreddit_subscribers": 136969, "created_utc": 1698678796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "13 Crucial Steps for End-to-End File Testing by iceDQ \ud83d\udcdd\ud83d\ude80", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nlboefgoocxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7ece2ed0cf3a498074596648a071778bf681c2"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ee8561b09888eb5bc2a6593f2ec4373b8f990c"}, {"y": 452, "x": 320, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5957f37ce22e5d778db788c12dc7a7f8cac848b9"}, {"y": 905, "x": 640, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d77c6b9c42c21a0a5e0b3b61e95c074e6e834b"}, {"y": 1357, "x": 960, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd3a1bc09bcf33a0a54c8774b4d9e9d4af04ef4c"}, {"y": 1527, "x": 1080, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c38ef27a54dc9a38c537ba668b899c2976a581ef"}], "s": {"y": 3508, "x": 2480, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390"}, "id": "nlboefgoocxb1"}}, "name": "t3_17juzns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bee6HOYrBoKyF9T9pqtVygDL1FKfzYwDr-vght6rGOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390\"&gt;https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17juzns", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "subreddit_subscribers": 136969, "created_utc": 1698677009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I'm curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.\n\nFor some context, I'm looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.", "author_fullname": "t2_gn8s9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse / Lakehouse / Data Set Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jv50c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I&amp;#39;m curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.&lt;/p&gt;\n\n&lt;p&gt;For some context, I&amp;#39;m looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jv50c", "is_robot_indexable": true, "report_reasons": null, "author": "Thiseffingguy2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "subreddit_subscribers": 136969, "created_utc": 1698677418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?", "author_fullname": "t2_52cbaf2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Privacy Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtq6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698677124.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jtq6w", "is_robot_indexable": true, "report_reasons": null, "author": "yoquierodata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "subreddit_subscribers": 136969, "created_utc": 1698673478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAnyone has been involved near or far with the decision of wether or not to build a warehouse in a **nascent** startup that is **just launching its product** ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?", "author_fullname": "t2_8kenyeuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse in a small startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqoei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698663233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Anyone has been involved near or far with the decision of wether or not to build a warehouse in a &lt;strong&gt;nascent&lt;/strong&gt; startup that is &lt;strong&gt;just launching its product&lt;/strong&gt; ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqoei", "is_robot_indexable": true, "report_reasons": null, "author": "btenami", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "subreddit_subscribers": 136969, "created_utc": 1698663233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For an environment with high stakes that necessitates proactive communication if a pipeline fails , how do you setup a safe way to promote new code / code changes and also maintain and monitor. \n\nIn a less data intensive and less pipeline stack , having a messaging system that pushed to slack and PagerDuty for errors , typically broken out by extraction then transformation jobs , typically catching api timeout errors and schema change errors , seems effective. But have a larger volume of data , a real sla , and using both data bricks / pyspark and snowflake. Any ideas on how to setup the system to scale and easily be monitored?", "author_fullname": "t2_32eyna18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement scalable code deployment and monitoring system for a company that ingests pedabytes of data daily , 100 different pipelines while using airflow , snowflake , pyspark/databricks and AWS ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jmxzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698646600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For an environment with high stakes that necessitates proactive communication if a pipeline fails , how do you setup a safe way to promote new code / code changes and also maintain and monitor. &lt;/p&gt;\n\n&lt;p&gt;In a less data intensive and less pipeline stack , having a messaging system that pushed to slack and PagerDuty for errors , typically broken out by extraction then transformation jobs , typically catching api timeout errors and schema change errors , seems effective. But have a larger volume of data , a real sla , and using both data bricks / pyspark and snowflake. Any ideas on how to setup the system to scale and easily be monitored?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jmxzw", "is_robot_indexable": true, "report_reasons": null, "author": "acceptedcitizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jmxzw/how_to_implement_scalable_code_deployment_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jmxzw/how_to_implement_scalable_code_deployment_and/", "subreddit_subscribers": 136969, "created_utc": 1698646600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I just uploaded a Python Pandas course on YouTube. I covered the introduction and installation of pandas, series and series operations, dataframes and basic dataframe creation, creating dataframes from various file formats, dataframe operations, identifying and handling missing data, data manipulation using loc and iloc, sorting and ranking data, combining and merging dataframes, data cleaning techniques, handling categorical data, data transformation techniques, handling date and time data, group by operations, aggregating data using functions, time series data visualization, advanced data manipulation techniques (apply, map, and apply map), data visualization with pandas tools, working with multi-index dataframes and text manipulation methods topics. I am leaving the course link below, have a great day!\n\nhttps://www.youtube.com/watch?v=KvFZf3cL_IY", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I shared a Python Pandas course (1.5 Hrs) on YouTube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvf6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I just uploaded a Python Pandas course on YouTube. I covered the introduction and installation of pandas, series and series operations, dataframes and basic dataframe creation, creating dataframes from various file formats, dataframe operations, identifying and handling missing data, data manipulation using loc and iloc, sorting and ranking data, combining and merging dataframes, data cleaning techniques, handling categorical data, data transformation techniques, handling date and time data, group by operations, aggregating data using functions, time series data visualization, advanced data manipulation techniques (apply, map, and apply map), data visualization with pandas tools, working with multi-index dataframes and text manipulation methods topics. I am leaving the course link below, have a great day!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=KvFZf3cL_IY\"&gt;https://www.youtube.com/watch?v=KvFZf3cL_IY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?auto=webp&amp;s=f1439e666963311860ed2bbc6d18d58138ab20ab", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1160dcf6aa25a9ba5c8b7c522c6001b5a93c5bde", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9047f27c215633f01e07e0e4e2dadf6e306dd0bd", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/l3w_x32sW1ndSFuhwAmk4hpXmMXlpQcLmA7Cg0_N-UQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31180cc898c560b7db600f56a3d2fa228fdaabff", "width": 320, "height": 240}], "variants": {}, "id": "l-wENCMcE_8QSoQ-KSZE0RDTI-ogPC-MOsWTs7k42RI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17jvf6p", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvf6p/i_shared_a_python_pandas_course_15_hrs_on_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvf6p/i_shared_a_python_pandas_course_15_hrs_on_youtube/", "subreddit_subscribers": 136969, "created_utc": 1698678171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).\n\nThe dataset is the \"final\" result of a research project and is contained in a SAS dataset:\n\n* The main data file \"data.sas7dat\" comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.\n* The formats file \"formats.sas7bcat\" contains value labels for categorical variables, such as 1 is \"male\" and 2 is \"female\".\n\nChallenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we've actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.\n\nThe main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).\n\nI have attempted the following solutions:\n\n1. Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.\n2. Melting the Dataset: I transformed the columns into rows to create a \"long\" format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.\n\nThe users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all \\~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.\n\nI am considering a star schema but am unsure if that is the most effective approach given the dataset's complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!", "author_fullname": "t2_s3w1zbjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring an extremely wide dataset in an RDBMS: Seeking advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtonq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).&lt;/p&gt;\n\n&lt;p&gt;The dataset is the &amp;quot;final&amp;quot; result of a research project and is contained in a SAS dataset:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The main data file &amp;quot;data.sas7dat&amp;quot; comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.&lt;/li&gt;\n&lt;li&gt;The formats file &amp;quot;formats.sas7bcat&amp;quot; contains value labels for categorical variables, such as 1 is &amp;quot;male&amp;quot; and 2 is &amp;quot;female&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Challenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we&amp;#39;ve actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.&lt;/p&gt;\n\n&lt;p&gt;The main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).&lt;/p&gt;\n\n&lt;p&gt;I have attempted the following solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.&lt;/li&gt;\n&lt;li&gt;Melting the Dataset: I transformed the columns into rows to create a &amp;quot;long&amp;quot; format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all ~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.&lt;/p&gt;\n\n&lt;p&gt;I am considering a star schema but am unsure if that is the most effective approach given the dataset&amp;#39;s complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jtonq", "is_robot_indexable": true, "report_reasons": null, "author": "BudgetAd1030", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "subreddit_subscribers": 136969, "created_utc": 1698673351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I recently learned about the data platform Promethium.ai. Does anyone have any reviews or insights to share about it?", "author_fullname": "t2_kgk078ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Promethium Reviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jmwjn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698646416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I recently learned about the data platform Promethium.ai. Does anyone have any reviews or insights to share about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jmwjn", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning-Forever597", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jmwjn/promethium_reviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jmwjn/promethium_reviews/", "subreddit_subscribers": 136969, "created_utc": 1698646416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! \n\nI\u2019m doing my last co-op (hence very important since I\u2019m not going to have any internships experience anymore) in Canada and I have the option to go with Samsung or P&amp;G? Both are for Data Engineer roles so I was wondering if anyone has any insights on this? Samsung is in the city I am in (Vancouver), and I have to relocate to North York Ontario if I end up with P&amp;G. I\u2019m in a big dilemma what to do in this case. Thank you so much! :\u201d)", "author_fullname": "t2_e5537ck7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Internship: P&amp;G or Samsung?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jafhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698607486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m doing my last co-op (hence very important since I\u2019m not going to have any internships experience anymore) in Canada and I have the option to go with Samsung or P&amp;amp;G? Both are for Data Engineer roles so I was wondering if anyone has any insights on this? Samsung is in the city I am in (Vancouver), and I have to relocate to North York Ontario if I end up with P&amp;amp;G. I\u2019m in a big dilemma what to do in this case. Thank you so much! :\u201d)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jafhq", "is_robot_indexable": true, "report_reasons": null, "author": "angelicsmiless", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jafhq/data_engineer_internship_pg_or_samsung/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jafhq/data_engineer_internship_pg_or_samsung/", "subreddit_subscribers": 136969, "created_utc": 1698607486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI'm a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don't know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere's also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? \n\nHow realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?\n\nCan you recommend some resources or action plan?\nThanks", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How hard is it to pick up java?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17k04oj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698690651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI&amp;#39;m a mid, my main stack is airflow, pyspark, SQL. My python is very mediocre, I don&amp;#39;t know oop etc. I mainly write pyspark scripts and use them in airflow dags. \nThere&amp;#39;s also streaming done in my company, but in order to engage in that I would need to pick up java. How difficult is it for a person with my background? &lt;/p&gt;\n\n&lt;p&gt;How realistic is it to pick it up in a few months to the point of understanding the code in streaming workflows, and perhaps be able to then start learning kafka, beam etc?&lt;/p&gt;\n\n&lt;p&gt;Can you recommend some resources or action plan?\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17k04oj", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17k04oj/how_hard_is_it_to_pick_up_java/", "subreddit_subscribers": 136969, "created_utc": 1698690651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working on a [personal project](https://github.com/digitalghost-dev/premier-league) where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:\n\n1. Extract data with a Python script.\n2. Load a new row into a PostgreSQL database with zero transformations, just raw.\n3. Use Google Cloud's Datastream to stream the data to BigQuery.\n4. **dbt step** \\- I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.\n\nHere is a flowchart. I'm talking about **Data Pipeline 1**.\n\n[Diagram of my project's data pipeline architecture.](https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833)\n\n&amp;#x200B;\n\nIs this the proper use case and the idea behind dbt?", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I using (and understanding) dbt correctly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "media_metadata": {"ua5lnpuetdxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b83e07a70a8b3c2189410caff557ae4aa3799e1"}, {"y": 212, "x": 216, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b6e174c7da8eb4697581acb5c1c80a2ac010cc1"}, {"y": 314, "x": 320, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbbe14518af6f879f0b5253956f5c4fa8a33b3bb"}, {"y": 629, "x": 640, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=51e88c787254f71fcd46c39ad9dab95ca43e290e"}, {"y": 943, "x": 960, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5da25a96ef19a0b7d508c5a34fcc014019671da9"}, {"y": 1061, "x": 1080, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3785607b6b617389cc5ca8507935e194c725034"}], "s": {"y": 1846, "x": 1878, "u": "https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;format=png&amp;auto=webp&amp;s=1f406427ef9cdf428514e9be35e56db2922cf833"}, "id": "ua5lnpuetdxb1"}}, "name": "t3_17jzx1n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/U_Hm0yHJlubQWG9uED_0PP_WpXFeOORip1YigNcAVeM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698690066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;personal project&lt;/a&gt; where I am starting to implement dbt as a transformation step. One of my data pipelines runs in the following steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Extract data with a Python script.&lt;/li&gt;\n&lt;li&gt;Load a new row into a PostgreSQL database with zero transformations, just raw.&lt;/li&gt;\n&lt;li&gt;Use Google Cloud&amp;#39;s Datastream to stream the data to BigQuery.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;dbt step&lt;/strong&gt; - I have a SQL model I built that transforms the data for me and loads it as a view in BigQuery. I am planning on creating Jobs in dbt Cloud to run these jobs once a night. I had to create a production environment in dbt Cloud to run Jobs where I had to specify a dataset to write in BigQuery so now my Streamlit app reads from there.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Here is a flowchart. I&amp;#39;m talking about &lt;strong&gt;Data Pipeline 1&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ua5lnpuetdxb1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f406427ef9cdf428514e9be35e56db2922cf833\"&gt;Diagram of my project&amp;#39;s data pipeline architecture.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this the proper use case and the idea behind dbt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?auto=webp&amp;s=e3224c394c241c64ad5e53c355431ac0f8ab8526", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2070931ceff1caacdb448277ad5d4f117bd987", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31fa2166140c915e95065317d8cfb91907b2a034", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13bfb84f156729c537e946581469eae84eeb2d20", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=907ff5e42af2eb56467f097761c3511733513afc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fdbc94d7d480cf2be7812bcad28f4cb5c8518ac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tVtiInrttsScJ8yn3_Iq4X0n0a8PeGEvNsE6alZIL2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=304945d38c5ad77b0f7f149beef56e3dd120bfa5", "width": 1080, "height": 540}], "variants": {}, "id": "KPMQgVo0A9r8AVnlQp0ErSoXWIJswqtQdpS0EBWkAlQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jzx1n", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jzx1n/am_i_using_and_understanding_dbt_correctly/", "subreddit_subscribers": 136969, "created_utc": 1698690066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've switched roles recently from a company where the DE org didn't have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege's, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more \"normal\" end of things, but just curious if others have similar processes.", "author_fullname": "t2_mut7fkytm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does the deploy process look like in your role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jynbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698686684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve switched roles recently from a company where the DE org didn&amp;#39;t have much regulation to a company that has a ton.  Previously I was able to cut a PR and get it approved by anyone on my team with tech review privilege&amp;#39;s, but at my new company we have a much much longer process.  To get one line of code shipped it can touch as many as 7 different people from development to deploy.  I assume that latter is on the more &amp;quot;normal&amp;quot; end of things, but just curious if others have similar processes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jynbe", "is_robot_indexable": true, "report_reasons": null, "author": "MrCheezle47", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jynbe/what_does_the_deploy_process_look_like_in_your/", "subreddit_subscribers": 136969, "created_utc": 1698686684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, everything good?\n\n&amp;#x200B;\n\nI'm falling in love with the data area, because I'm a computer science student and I'm told to study first and delve deeper, as I see a lot of people saying different things around me, \"study cloud computing\", \"study apache spark\", \"azure\"... and so on.\n\n&amp;#x200B;\n\nBut those of you who are already in the area, what should I study? to improve and enter the data market, what is the market lacking?", "author_fullname": "t2_dhhqxp4io", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If it were you starting today with your experience, what would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jxy0k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698684852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, everything good?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m falling in love with the data area, because I&amp;#39;m a computer science student and I&amp;#39;m told to study first and delve deeper, as I see a lot of people saying different things around me, &amp;quot;study cloud computing&amp;quot;, &amp;quot;study apache spark&amp;quot;, &amp;quot;azure&amp;quot;... and so on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But those of you who are already in the area, what should I study? to improve and enter the data market, what is the market lacking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jxy0k", "is_robot_indexable": true, "report_reasons": null, "author": "TIandSadness", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jxy0k/if_it_were_you_starting_today_with_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jxy0k/if_it_were_you_starting_today_with_your/", "subreddit_subscribers": 136969, "created_utc": 1698684852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. \n\nWould it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.", "author_fullname": "t2_fd5v0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Preparation with ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jxjbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698683782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting a cloud setup at my organisation and looking at ingesting data in Data Lake Gen2. We\u2019re not going to have the amount of data for Synapse or Databricks to be worthwhile and will push this into a SQL DB for reporting. &lt;/p&gt;\n\n&lt;p&gt;Would it be a good idea to explore Azure Machine Learning to help with data preparation before running our pipelines. Would like to know if any of you have had any experiences with this or any alternatives we could use as a MS shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jxjbv", "is_robot_indexable": true, "report_reasons": null, "author": "12Eerc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jxjbv/data_preparation_with_ml/", "subreddit_subscribers": 136969, "created_utc": 1698683782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi redditors, \n\nI am posting this after a failed interview process for Senior Analytics position.\n\nI would like to share with you my situation as I've been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.\n\n\nI worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)\n\nAll of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)\n\n\nAfter that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).\n\nI \"succeeded\" in some of the projects and \"failed\" in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)\n\n\nI don't know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!\n\n\nTL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-\n\n\nPD: wrote this from cellphone, sorry for the grammar and styling", "author_fullname": "t2_13if99", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career advice / rant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jx3of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698682610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors, &lt;/p&gt;\n\n&lt;p&gt;I am posting this after a failed interview process for Senior Analytics position.&lt;/p&gt;\n\n&lt;p&gt;I would like to share with you my situation as I&amp;#39;ve been reading a lot this sub and felt a nice vibe regarding feedback so, any of it is appreciated.&lt;/p&gt;\n\n&lt;p&gt;I worked in this field since mid 2011 as a Business intelligence consultant (until 2016), small team lead (2017) and head of BI (2018-19)&lt;/p&gt;\n\n&lt;p&gt;All of their projects were on premises back then and I got a solid data modeling foundation and end to end BI project understanding (with many of them deployed just by myself)&lt;/p&gt;\n\n&lt;p&gt;After that, life happened, and I was disconnected from the data field. Until late 2021, where I got a job in a company that outsources data services (as my early career).&lt;/p&gt;\n\n&lt;p&gt;I &amp;quot;succeeded&amp;quot; in some of the projects and &amp;quot;failed&amp;quot; in the last two ones, mainly because my role was as of Data Architect, which was new for me (we know it as infrastructure)&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know even what I want to ask, just overshare and hoping to get some insight from you. If you read this long, thanks a lot!!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: rejected today as senior data Engineer, wants input in current professional situation as the impostor syndrome is pretty damaging me -besides nirmal interview outcomes-&lt;/p&gt;\n\n&lt;p&gt;PD: wrote this from cellphone, sorry for the grammar and styling&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jx3of", "is_robot_indexable": true, "report_reasons": null, "author": "ratacarnic", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jx3of/career_advice_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jx3of/career_advice_rant/", "subreddit_subscribers": 136969, "created_utc": 1698682610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).\n\nI've boiled the ocean and have come to the issue is how pyodbc is passing the password.\n\nThe password contains 2 escape characters, 'abc;abc\\[abc', I think the issue is the ; and the \\[. I've tried {} the password and user, I've tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can't get the pw changed sadly because bureaucracy.", "author_fullname": "t2_kg3va7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having issues connecting pyodbc to sql server because escape characters in pw.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jw6q4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698680167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve boiled the ocean and have come to the issue is how pyodbc is passing the password.&lt;/p&gt;\n\n&lt;p&gt;The password contains 2 escape characters, &amp;#39;abc;abc[abc&amp;#39;, I think the issue is the ; and the [. I&amp;#39;ve tried {} the password and user, I&amp;#39;ve tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can&amp;#39;t get the pw changed sadly because bureaucracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jw6q4", "is_robot_indexable": true, "report_reasons": null, "author": "IIndAmendmentJesus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "subreddit_subscribers": 136969, "created_utc": 1698680167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone take a good online course/mooc for this cert? \n\nAny bit would help! Thanks in advance", "author_fullname": "t2_16k0ev", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake snowpro core cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jvbd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone take a good online course/mooc for this cert? &lt;/p&gt;\n\n&lt;p&gt;Any bit would help! Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jvbd6", "is_robot_indexable": true, "report_reasons": null, "author": "jovalabs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "subreddit_subscribers": 136969, "created_utc": 1698677901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\r\n\r\nI'm in my first FlutterFlow course and we're using Firestore. The course suggests duplicating some data from a popularDoctors collection to a bookings collection upon creating a booking. So, for each booking, we copy the doctor's info from popularDoctors to bookings.\r\n\r\nIs this a common practice? Or is it better to just reference the doctor in the bookings collection and pull the doctor's info from popularDoctors when needed? Concerned about data consistency and storage efficiency.\r\n\r\nWould love to hear your thoughts! \ud83d\ude4f", "author_fullname": "t2_w5eiso48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalization vs Normalization in Firestore:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ju8s8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698674910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in my first FlutterFlow course and we&amp;#39;re using Firestore. The course suggests duplicating some data from a popularDoctors collection to a bookings collection upon creating a booking. So, for each booking, we copy the doctor&amp;#39;s info from popularDoctors to bookings.&lt;/p&gt;\n\n&lt;p&gt;Is this a common practice? Or is it better to just reference the doctor in the bookings collection and pull the doctor&amp;#39;s info from popularDoctors when needed? Concerned about data consistency and storage efficiency.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts! \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ju8s8", "is_robot_indexable": true, "report_reasons": null, "author": "Tom5542131845", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ju8s8/denormalization_vs_normalization_in_firestore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ju8s8/denormalization_vs_normalization_in_firestore/", "subreddit_subscribers": 136969, "created_utc": 1698674910.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}