{"kind": "Listing", "data": {"after": "t3_17jkkfc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Quick fact: 95% of the world is not eligible for the 95% of the jobs on this subreddit. This is to show my frustration with an existing job market for data engineers. Being a third world national I feel it is hard to get a decent job in a field of DE. Is the only way to become good DE is to migrate to the US, cause I feel there are small chances of getting good project here. I was outstaffed to a US company, but my work was without challenges. Sometimes I feel like third world nationals don't get interesting things. Am I the only one who feels frustrated on my DE path? Getting through textbooks may be interesting, but outcomes are unknown and it is very time consuming. Getting certified is extremely boring and also it's imo useless without hands on experience. Has anyone managed to break this wall? **What are the steps a third world national should take to become a distinguished data engineer?**", "author_fullname": "t2_g6ziwt5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Third world data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17j7o18", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698599886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quick fact: 95% of the world is not eligible for the 95% of the jobs on this subreddit. This is to show my frustration with an existing job market for data engineers. Being a third world national I feel it is hard to get a decent job in a field of DE. Is the only way to become good DE is to migrate to the US, cause I feel there are small chances of getting good project here. I was outstaffed to a US company, but my work was without challenges. Sometimes I feel like third world nationals don&amp;#39;t get interesting things. Am I the only one who feels frustrated on my DE path? Getting through textbooks may be interesting, but outcomes are unknown and it is very time consuming. Getting certified is extremely boring and also it&amp;#39;s imo useless without hands on experience. Has anyone managed to break this wall? &lt;strong&gt;What are the steps a third world national should take to become a distinguished data engineer?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17j7o18", "is_robot_indexable": true, "report_reasons": null, "author": "fire_air", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17j7o18/third_world_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17j7o18/third_world_data_engineering/", "subreddit_subscribers": 136932, "created_utc": 1698599886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lately, i've done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?", "author_fullname": "t2_daehbbsip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i'm seeing less star schemas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jg1x5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698623091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lately, i&amp;#39;ve done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jg1x5", "is_robot_indexable": true, "report_reasons": null, "author": "Tough-Error520", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jg1x5/im_seeing_less_star_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jg1x5/im_seeing_less_star_schemas/", "subreddit_subscribers": 136932, "created_utc": 1698623091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are prepping for the data engineering interview, you should include the data modeling loop as a key section of your prep. Here is a two part series that includes an overview of the signal that they are looking for, practice problems, and a deeb dive into some of the technical aspects you will need to know.  \n\n\n[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938)  \n\n\n[https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca)", "author_fullname": "t2_4fpl974m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cracking the Data Modeling Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jh22q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698626042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are prepping for the data engineering interview, you should include the data modeling loop as a key section of your prep. Here is a two part series that includes an overview of the signal that they are looking for, practice problems, and a deeb dive into some of the technical aspects you will need to know.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938\"&gt;https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-1-an-overview-b09e7d5a7938&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca\"&gt;https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-2-normalization-indexes-and-partitioning-fac334d767ca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?auto=webp&amp;s=4090b055bdc86a803c1f6b2c73b9004f54ddba8e", "width": 1200, "height": 557}, "resolutions": [{"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2db5992e420edc82f4e1dc1f1849565ff085d58", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef17ec983bf531cd47ffe8c475fd585bedc4ed40", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7eb2456cff1d6d17a83d2b90861504e91330e8c", "width": 320, "height": 148}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58336696dcdfbfa2f0306d9fef6cb38b5acc1882", "width": 640, "height": 297}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e8ecae39f65c5ee5f3d6c9196d657381264e1de", "width": 960, "height": 445}, {"url": "https://external-preview.redd.it/g_dhuJAfmQ1iNTX-NeX0K7pz1QFPNjgG39XY0gr5XRo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2070f6d3d66ac2ee727fd392976a77d070e92a80", "width": 1080, "height": 501}], "variants": {}, "id": "oNoLwNsn-uU3Qx0q4ZAFYrBNCcGCimMegvkILgS8w3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17jh22q", "is_robot_indexable": true, "report_reasons": null, "author": "coyne_operated", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jh22q/cracking_the_data_modeling_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jh22q/cracking_the_data_modeling_interview/", "subreddit_subscribers": 136932, "created_utc": 1698626042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a cs student looking to become a data engineer at some point down the line - should I take a databases class? My school offers one next semester however it conflicts w another class I need to graduate time wise. I graduate next year so the database class won\u2019t be offered again before I graduate. Is it worth potentially putting off graduation by a semester or are databases something I can teach myself?", "author_fullname": "t2_kc7mg3akr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take a databases class?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ji5fm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698629454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a cs student looking to become a data engineer at some point down the line - should I take a databases class? My school offers one next semester however it conflicts w another class I need to graduate time wise. I graduate next year so the database class won\u2019t be offered again before I graduate. Is it worth potentially putting off graduation by a semester or are databases something I can teach myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ji5fm", "is_robot_indexable": true, "report_reasons": null, "author": "Brief-Union-3493", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ji5fm/should_i_take_a_databases_class/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ji5fm/should_i_take_a_databases_class/", "subreddit_subscribers": 136932, "created_utc": 1698629454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the same vein as this [question](https://www.reddit.com/r/vim/comments/17ffelc/whats_the_vim_pareto_for_an_it_professional/) I asked for VIM, I'd like to know what according to you is the Data Engineering [Pareto](https://en.wikipedia.org/wiki/Pareto_principle)?\n\n20% of XYZ(concepts, frameworks, language constructs, libraries etc.) that are sufficient to accomplish 80% of DE tasks.", "author_fullname": "t2_g4v8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What, according to you, is the Data Engineering Pareto?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17j85hu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698601242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the same vein as this &lt;a href=\"https://www.reddit.com/r/vim/comments/17ffelc/whats_the_vim_pareto_for_an_it_professional/\"&gt;question&lt;/a&gt; I asked for VIM, I&amp;#39;d like to know what according to you is the Data Engineering &lt;a href=\"https://en.wikipedia.org/wiki/Pareto_principle\"&gt;Pareto&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;20% of XYZ(concepts, frameworks, language constructs, libraries etc.) that are sufficient to accomplish 80% of DE tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z0KhRy4dT59epvVdLEHyL1O4pBEJiH6-5Byq7mgeKaY.jpg?auto=webp&amp;s=77108879ce1467c6179a7684cfc856c49520e8aa", "width": 539, "height": 560}, "resolutions": [{"url": "https://external-preview.redd.it/z0KhRy4dT59epvVdLEHyL1O4pBEJiH6-5Byq7mgeKaY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59ee074ebffc935b18d0b1a063c2e363c75ad619", "width": 108, "height": 112}, {"url": "https://external-preview.redd.it/z0KhRy4dT59epvVdLEHyL1O4pBEJiH6-5Byq7mgeKaY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e328266b558161fbc599f3a37e6129abf3340a6d", "width": 216, "height": 224}, {"url": "https://external-preview.redd.it/z0KhRy4dT59epvVdLEHyL1O4pBEJiH6-5Byq7mgeKaY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cc77d7f54428ccd453adf9eea490bea7dc6925e", "width": 320, "height": 332}], "variants": {}, "id": "dA1Z5W9CE92S0TuslnqeO50Jk-gBx9H3l63KikMQOaA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17j85hu", "is_robot_indexable": true, "report_reasons": null, "author": "swapripper", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17j85hu/what_according_to_you_is_the_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17j85hu/what_according_to_you_is_the_data_engineering/", "subreddit_subscribers": 136932, "created_utc": 1698601242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.\n\nI\u2019m sorry if this has already been talked about here, but I couldn\u2019t really find older posts on this. I\u2019m trying to get into DE coming from a SWE background. I thought about doing so firstly through a job as a DA, which seems the most common path where I\u2019m from, at least. My question is: what are the first certifications to get if I wanted to break into DE/DA? My main goal is to not come across as if this is a nine-day wonder. What are the ones that make me stand out for entry positions?\n\nThank you.", "author_fullname": "t2_fgp3mdj67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certifications for someone trying to land an entrance position in DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jnb1u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698648277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m sorry if this has already been talked about here, but I couldn\u2019t really find older posts on this. I\u2019m trying to get into DE coming from a SWE background. I thought about doing so firstly through a job as a DA, which seems the most common path where I\u2019m from, at least. My question is: what are the first certifications to get if I wanted to break into DE/DA? My main goal is to not come across as if this is a nine-day wonder. What are the ones that make me stand out for entry positions?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jnb1u", "is_robot_indexable": true, "report_reasons": null, "author": "LusoDev", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jnb1u/certifications_for_someone_trying_to_land_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jnb1u/certifications_for_someone_trying_to_land_an/", "subreddit_subscribers": 136932, "created_utc": 1698648277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Pentagram Schema for Evaluating Haunted Data Warehouses**\n\nIn the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the \"Pentagram Schema.\" This schema is not just about data; it's about the eerie, the paranormal, and the mystical.\n\n**Pentagram Schema Overview:**\n\nAt the heart of the Pentagram Schema lies the **\"Haunted Data WareHouse Access\" fact table.** This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.\n\n**The Five Satanic Dimensions:**\n\n1. **Visitor (Dimension 1):** In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.\n2. **Query (Dimension 2):** Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they're regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.\n3. **Outcome (Dimension 3):** The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.\n4. **Tables Hit (Dimension 4):** Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.\n5. **Maintainer (Dimension 5):** In any haunted attraction, there's a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.\n\n&amp;#x200B;\n\n**Why the Pentagram Schema:**\n\nThese five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.\n\nHappy Haunting!\n\n  \n(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haunted data warehouse pentagram schema architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17jvmmf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Pentagram Schema for Evaluating Haunted Data Warehouses&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In the spirit of Halloween, I present a unique and playful approach to evaluating data warehouses in the world of the supernatural. I call it the &amp;quot;Pentagram Schema.&amp;quot; This schema is not just about data; it&amp;#39;s about the eerie, the paranormal, and the mystical.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pentagram Schema Overview:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;At the heart of the Pentagram Schema lies the &lt;strong&gt;&amp;quot;Haunted Data WareHouse Access&amp;quot; fact table.&lt;/strong&gt; This central table embodies the core interactions within the supernatural realm. Visitors are at the center of the paranormal activity, and the Pentagram Schema seeks to understand their experiences in haunted houses.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Five Satanic Dimensions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Visitor (Dimension 1):&lt;/strong&gt; In the eerie world of haunted data, understanding the visitors is paramount. This dimension focuses on identifying and categorizing the entities or users who dare to explore the spectral data. It delves into user roles, permissions, and their interactions with the data, ensuring a comprehensive understanding of the spectral visitors.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Query (Dimension 2):&lt;/strong&gt; Queries are the incantations of the data world, and this dimension classifies the different types of queries used to summon insights from the haunted data warehouse. Whether they&amp;#39;re regular SQL queries, cryptic data requests, or magical spells to extract information, Dimension 2 captures and analyzes these spectral interactions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Outcome (Dimension 3):&lt;/strong&gt; The outcome is the ultimate manifestation of data sorcery. Dimension 3 seeks to answer two fundamental questions: Did the spectral ritual work, and how quickly did the requested insights materialize? By scrutinizing the outcomes, data conjurers can refine their data spells for more efficient spectral results.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tables Hit (Dimension 4):&lt;/strong&gt; Just like chambers in a haunted mansion, data in a warehouse is stored in different tables, each with its unique properties. This dimension uncovers which spectral data chambers were accessed, how frequently, and to what extent, allowing for a comprehensive understanding of the spectral data landscape.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Maintainer (Dimension 5):&lt;/strong&gt; In any haunted attraction, there&amp;#39;s a shadowy team behind the scenes keeping the paranormal elements in order. Dimension 5 profiles these custodians of the arcane, revealing their roles and contributions. These unsung heroes ensure the spectral operations run smoothly, shining a light on the often-hidden aspects of haunted data warehouse management.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why the Pentagram Schema:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;These five satanic dimensions combine to create a holistic evaluation framework for haunted data warehouse usage. By analyzing visitors, queries, outcomes, tables hit, and maintainers, data sorcerers can uncover the mysteries within the spectral data, optimize their data spells, and ensure a seamless and enchanting data experience in the world of the supernatural. As you navigate this unique schema, remember to approach it with the spirit of Halloween\u2014fearless, curious, and ready for some unearthly revelations.&lt;/p&gt;\n\n&lt;p&gt;Happy Haunting!&lt;/p&gt;\n\n&lt;p&gt;(edit, just having fun with how i evaluate usage + halloween + gpt for words :) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "17jvmmf", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvmmf/haunted_data_warehouse_pentagram_schema/", "subreddit_subscribers": 136932, "created_utc": 1698678690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.\n\nWe have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.\n\nOur latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.\n\nSo a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster \"Integrations\" is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don't really know these platforms either. Perhaps we should simply be looking into these?\n\nAre we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?", "author_fullname": "t2_7no9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect&amp;Flask as a Hub and Spoke", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqhvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698671728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698662486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Small community owned non-profit with a number of functions.  Two part time and one jr. developer. We prefer Python but we also know a bit of JS and C# but we are getting rusty with Java. Ops/IT has a bias against Java. As an organization we are not very cost averse but we could use the money on other very worthy things.&lt;/p&gt;\n\n&lt;p&gt;We have many different systems for billing, maintenance, finance, inventory, a few different analysis and metering systems and more. We need a system that will help us with cross system communications, sometimes with very different methods such as text files, XML, REST API and direct SQL selects, inserts and merges between different PostGres and MS SQL Server databases.  Some on a daily schedule and other information instantaneously or instantaneously using persisted data from older systems. This may have to run on-prem.&lt;/p&gt;\n\n&lt;p&gt;Our latest plan is to make Prefect and Flask run together and just script the retries and logging into flask. ... or Dagster and FastAPI to do the same or another combination of similar platforms.&lt;/p&gt;\n\n&lt;p&gt;So a Hub and Spoke with a data orchestration tool and a simple as possible API.  I have had some exposure to Camel but it would probably take us a while to started and doing Prefect/Dagster &amp;quot;Integrations&amp;quot; is so much simpler. Companies/Organizations with overlapping functions and we are familiar with are using Azure servicebuses, Timextender, n-ServiceBus, Mule and I was hoping for something a bit more lightweight/easier, on-prem and avoid lock-in but we don&amp;#39;t really know these platforms either. Perhaps we should simply be looking into these?&lt;/p&gt;\n\n&lt;p&gt;Are we heading towards certain doom here or has someone here done anything similar? Alternatives we should be looking at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqhvd", "is_robot_indexable": true, "report_reasons": null, "author": "YourOldBuddy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqhvd/prefectflask_as_a_hub_and_spoke/", "subreddit_subscribers": 136932, "created_utc": 1698662486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390", "author_fullname": "t2_sw2luf69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "13 Crucial Steps for End-to-End File Testing by iceDQ \ud83d\udcdd\ud83d\ude80", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"nlboefgoocxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e7ece2ed0cf3a498074596648a071778bf681c2"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55ee8561b09888eb5bc2a6593f2ec4373b8f990c"}, {"y": 452, "x": 320, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5957f37ce22e5d778db788c12dc7a7f8cac848b9"}, {"y": 905, "x": 640, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3d77c6b9c42c21a0a5e0b3b61e95c074e6e834b"}, {"y": 1357, "x": 960, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fd3a1bc09bcf33a0a54c8774b4d9e9d4af04ef4c"}, {"y": 1527, "x": 1080, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c38ef27a54dc9a38c537ba668b899c2976a581ef"}], "s": {"y": 3508, "x": 2480, "u": "https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;format=png&amp;auto=webp&amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390"}, "id": "nlboefgoocxb1"}}, "name": "t3_17juzns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bee6HOYrBoKyF9T9pqtVygDL1FKfzYwDr-vght6rGOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390\"&gt;https://preview.redd.it/nlboefgoocxb1.png?width=2480&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52a729f80c075a9f6fd64c43286f8ed2c8489390&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17juzns", "is_robot_indexable": true, "report_reasons": null, "author": "icedqengineer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17juzns/13_crucial_steps_for_endtoend_file_testing_by/", "subreddit_subscribers": 136932, "created_utc": 1698677009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?", "author_fullname": "t2_52cbaf2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Privacy Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtq6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698677124.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the Head of Data for a tech startup that provides services and SaaS-based tools for customers in the real estate industry. I need to start designing a governance framework for our data (and therefore determine the solution architecture). There is of course CCPA and the big names, but does anyone have advice on how I \u201cdiscover\u201d which laws, regulations, standards apply to the data of our business?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jtq6w", "is_robot_indexable": true, "report_reasons": null, "author": "yoquierodata", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtq6w/data_privacy_resources/", "subreddit_subscribers": 136932, "created_utc": 1698673478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need some help to in understanding the dbricks cluster loading.\n\nSay I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. \n\nIf I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? \n\nIf I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?\n\nThanks for checking.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4kvf695m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What happens when we submit a lot of spark jobs on a Databricks cluster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17jvo4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698678796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need some help to in understanding the dbricks cluster loading.&lt;/p&gt;\n\n&lt;p&gt;Say I have a databricks cluster (E4ds v4) with 1 driver + 1worker and a given job takes 5 minutes to complete. &lt;/p&gt;\n\n&lt;p&gt;If I submit 10 jobs/notebooks using a forEach loop of the ADF pipeline, I believe jobs will be executed seuentially and  I can expect the ADF pipeline to be running for 50 minutes. Is this correct? &lt;/p&gt;\n\n&lt;p&gt;If I submit all 10 jobs together using some Rest API service, will those jobs get accumulated in the driver node and gets executed one by one ?&lt;/p&gt;\n\n&lt;p&gt;Thanks for checking.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jvo4e", "is_robot_indexable": true, "report_reasons": null, "author": "rasviz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvo4e/what_happens_when_we_submit_a_lot_of_spark_jobs/", "subreddit_subscribers": 136932, "created_utc": 1698678796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I'm curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.\n\nFor some context, I'm looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.", "author_fullname": "t2_gn8s9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse / Lakehouse / Data Set Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17jv50c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in a bit of a silo at my company without many other human resources to brainstorm data solutions with, so... I&amp;#39;m curious to know about what kind of current, most important best practices there are out there for setting up a wholesale data stack. Not looking for every best practice at every part of the pipe, just the considerations that have been the most impactful to your workflows.&lt;/p&gt;\n\n&lt;p&gt;For some context, I&amp;#39;m looking at implementing Microsoft Fabric to replace our current system of exporting .xls, .xlsx, or .csv files from our handful of sources, manually wrangling each week/month/quarter, reporting via PDF. Our company deals with primarily financial and operational data in the staffing/contracting sector, no scientific data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jv50c", "is_robot_indexable": true, "report_reasons": null, "author": "Thiseffingguy2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jv50c/warehouse_lakehouse_data_set_best_practices/", "subreddit_subscribers": 136932, "created_utc": 1698677418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nAnyone has been involved near or far with the decision of wether or not to build a warehouse in a **nascent** startup that is **just launching its product** ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?", "author_fullname": "t2_8kenyeuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Warehouse in a small startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqoei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698663233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Anyone has been involved near or far with the decision of wether or not to build a warehouse in a &lt;strong&gt;nascent&lt;/strong&gt; startup that is &lt;strong&gt;just launching its product&lt;/strong&gt; ? Was the decision to go with the warehouse or not ? Which technical setup was chosen ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jqoei", "is_robot_indexable": true, "report_reasons": null, "author": "btenami", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqoei/warehouse_in_a_small_startup/", "subreddit_subscribers": 136932, "created_utc": 1698663233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For an environment with high stakes that necessitates proactive communication if a pipeline fails , how do you setup a safe way to promote new code / code changes and also maintain and monitor. \n\nIn a less data intensive and less pipeline stack , having a messaging system that pushed to slack and PagerDuty for errors , typically broken out by extraction then transformation jobs , typically catching api timeout errors and schema change errors , seems effective. But have a larger volume of data , a real sla , and using both data bricks / pyspark and snowflake. Any ideas on how to setup the system to scale and easily be monitored?", "author_fullname": "t2_32eyna18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to implement scalable code deployment and monitoring system for a company that ingests pedabytes of data daily , 100 different pipelines while using airflow , snowflake , pyspark/databricks and AWS ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jmxzw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698646600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For an environment with high stakes that necessitates proactive communication if a pipeline fails , how do you setup a safe way to promote new code / code changes and also maintain and monitor. &lt;/p&gt;\n\n&lt;p&gt;In a less data intensive and less pipeline stack , having a messaging system that pushed to slack and PagerDuty for errors , typically broken out by extraction then transformation jobs , typically catching api timeout errors and schema change errors , seems effective. But have a larger volume of data , a real sla , and using both data bricks / pyspark and snowflake. Any ideas on how to setup the system to scale and easily be monitored?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jmxzw", "is_robot_indexable": true, "report_reasons": null, "author": "acceptedcitizen", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jmxzw/how_to_implement_scalable_code_deployment_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jmxzw/how_to_implement_scalable_code_deployment_and/", "subreddit_subscribers": 136932, "created_utc": 1698646600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! \n\nI\u2019m doing my last co-op (hence very important since I\u2019m not going to have any internships experience anymore) in Canada and I have the option to go with Samsung or P&amp;G? Both are for Data Engineer roles so I was wondering if anyone has any insights on this? Samsung is in the city I am in (Vancouver), and I have to relocate to North York Ontario if I end up with P&amp;G. I\u2019m in a big dilemma what to do in this case. Thank you so much! :\u201d)", "author_fullname": "t2_e5537ck7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer Internship: P&amp;G or Samsung?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jafhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698607486.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m doing my last co-op (hence very important since I\u2019m not going to have any internships experience anymore) in Canada and I have the option to go with Samsung or P&amp;amp;G? Both are for Data Engineer roles so I was wondering if anyone has any insights on this? Samsung is in the city I am in (Vancouver), and I have to relocate to North York Ontario if I end up with P&amp;amp;G. I\u2019m in a big dilemma what to do in this case. Thank you so much! :\u201d)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jafhq", "is_robot_indexable": true, "report_reasons": null, "author": "angelicsmiless", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jafhq/data_engineer_internship_pg_or_samsung/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jafhq/data_engineer_internship_pg_or_samsung/", "subreddit_subscribers": 136932, "created_utc": 1698607486.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm looking to switch to full time data engineering role in cloud from my current data analyst role ( decent dev experience with SSIS, informatica and Talend ). I have good knowledge in SQL and Python, but no exp with PYSPARK/Scala. Can you all suggest some good material where I can directly get into the pyskark/Scala code for data manipulation, wrangling ,cleaning and transformation. Personally I tend to learn any language by looking at a code and do my own research on each line l. Thanks in advance.", "author_fullname": "t2_62wpf6l3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scala requirements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17j8sq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698603025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking to switch to full time data engineering role in cloud from my current data analyst role ( decent dev experience with SSIS, informatica and Talend ). I have good knowledge in SQL and Python, but no exp with PYSPARK/Scala. Can you all suggest some good material where I can directly get into the pyskark/Scala code for data manipulation, wrangling ,cleaning and transformation. Personally I tend to learn any language by looking at a code and do my own research on each line l. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17j8sq3", "is_robot_indexable": true, "report_reasons": null, "author": "iksftw1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17j8sq3/scala_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17j8sq3/scala_requirements/", "subreddit_subscribers": 136932, "created_utc": 1698603025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).\n\nI've boiled the ocean and have come to the issue is how pyodbc is passing the password.\n\nThe password contains 2 escape characters, 'abc;abc\\[abc', I think the issue is the ; and the \\[. I've tried {} the password and user, I've tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can't get the pw changed sadly because bureaucracy.", "author_fullname": "t2_kg3va7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "having issues connecting pyodbc to sql server because escape characters in pw.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17jw6q4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698680167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a probono project for an animal shelter, I have to pull data from sql server, process it and kick it to another db. I am having no issue connecting to other servers with my python script and I can connect to the problem server using several SQL Clients(DBeaver, ss sms, toad, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve boiled the ocean and have come to the issue is how pyodbc is passing the password.&lt;/p&gt;\n\n&lt;p&gt;The password contains 2 escape characters, &amp;#39;abc;abc[abc&amp;#39;, I think the issue is the ; and the [. I&amp;#39;ve tried {} the password and user, I&amp;#39;ve tried {} and doubling up both and one of the escape characters with no success, any suggestion would be appreciated but I can&amp;#39;t get the pw changed sadly because bureaucracy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jw6q4", "is_robot_indexable": true, "report_reasons": null, "author": "IIndAmendmentJesus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jw6q4/having_issues_connecting_pyodbc_to_sql_server/", "subreddit_subscribers": 136932, "created_utc": 1698680167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone take a good online course/mooc for this cert? \n\nAny bit would help! Thanks in advance", "author_fullname": "t2_16k0ev", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake snowpro core cert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17jvbd6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698677901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone take a good online course/mooc for this cert? &lt;/p&gt;\n\n&lt;p&gt;Any bit would help! Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17jvbd6", "is_robot_indexable": true, "report_reasons": null, "author": "jovalabs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jvbd6/snowflake_snowpro_core_cert/", "subreddit_subscribers": 136932, "created_utc": 1698677901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python vs Rust. Memory Usage and Speed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_17juqsx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ei0yUSDt2NwuqwwV5FcLfnU_XTJgOq2QCgpTdbPzkw8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698676320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/python-vs-rust-memory-usage-and-speed", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?auto=webp&amp;s=e6209acdd529ea86a157d4730d9f7089844d2a51", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0398daaa752f40478b31d1ae988bf3e769338356", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb14e5cb3ae0cd1482f90c46e0168b8619e8fef7", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5554654cd57de3891185599d2fd3ce02435756e5", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09940ba97769d8b4bf1795e63172c693b4d64673", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/9ULxTgrddoVu3zdQCPuSbTZIYxps6zdrdavgj4kVnSg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=224300bedb4245a712d39fb24dfeeaa6ddaa7d5c", "width": 960, "height": 562}], "variants": {}, "id": "jade9nZ_x7rhjjZazxzCNWWCeriynsT88Rz_VolVEkA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17juqsx", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17juqsx/python_vs_rust_memory_usage_and_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/python-vs-rust-memory-usage-and-speed", "subreddit_subscribers": 136932, "created_utc": 1698676320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,\r\n\r\nI'm in my first FlutterFlow course and we're using Firestore. The course suggests duplicating some data from a popularDoctors collection to a bookings collection upon creating a booking. So, for each booking, we copy the doctor's info from popularDoctors to bookings.\r\n\r\nIs this a common practice? Or is it better to just reference the doctor in the bookings collection and pull the doctor's info from popularDoctors when needed? Concerned about data consistency and storage efficiency.\r\n\r\nWould love to hear your thoughts! \ud83d\ude4f", "author_fullname": "t2_w5eiso48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Denormalization vs Normalization in Firestore:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ju8s8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698674910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in my first FlutterFlow course and we&amp;#39;re using Firestore. The course suggests duplicating some data from a popularDoctors collection to a bookings collection upon creating a booking. So, for each booking, we copy the doctor&amp;#39;s info from popularDoctors to bookings.&lt;/p&gt;\n\n&lt;p&gt;Is this a common practice? Or is it better to just reference the doctor in the bookings collection and pull the doctor&amp;#39;s info from popularDoctors when needed? Concerned about data consistency and storage efficiency.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts! \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ju8s8", "is_robot_indexable": true, "report_reasons": null, "author": "Tom5542131845", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ju8s8/denormalization_vs_normalization_in_firestore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ju8s8/denormalization_vs_normalization_in_firestore/", "subreddit_subscribers": 136932, "created_utc": 1698674910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).\n\nThe dataset is the \"final\" result of a research project and is contained in a SAS dataset:\n\n* The main data file \"data.sas7dat\" comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.\n* The formats file \"formats.sas7bcat\" contains value labels for categorical variables, such as 1 is \"male\" and 2 is \"female\".\n\nChallenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we've actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.\n\nThe main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).\n\nI have attempted the following solutions:\n\n1. Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.\n2. Melting the Dataset: I transformed the columns into rows to create a \"long\" format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.\n\nThe users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all \\~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.\n\nI am considering a star schema but am unsure if that is the most effective approach given the dataset's complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!", "author_fullname": "t2_s3w1zbjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Structuring an extremely wide dataset in an RDBMS: Seeking advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jtonq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698673351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am seeking advice on how to structure a database for an extremely wide dataset in a relational database management system (RDBMS).&lt;/p&gt;\n\n&lt;p&gt;The dataset is the &amp;quot;final&amp;quot; result of a research project and is contained in a SAS dataset:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The main data file &amp;quot;data.sas7dat&amp;quot; comprises approximately 3,000 columns and 4,000 rows, with each row representing a participant. This file includes demographic information, questionnaire responses, and average nutrient intake values. It also contains variable labels (column descriptions) as metadata.&lt;/li&gt;\n&lt;li&gt;The formats file &amp;quot;formats.sas7bcat&amp;quot; contains value labels for categorical variables, such as 1 is &amp;quot;male&amp;quot; and 2 is &amp;quot;female&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Challenges I am facing include the size of the dataset, which makes it difficult to manage within most RDBMS, as we&amp;#39;ve actually hit the upper column limit and row sizes in RDBMS like MariaDB/MySQL, MSSQL, and PostgreSQL. Its current structure, while facilitating analysis, is too wide for effective database management.&lt;/p&gt;\n\n&lt;p&gt;The main reason I want to put this data into an RDBMS is to easily connect various analytical and statistical tools, like SPSS, Apache Superset, or programming languages used for data analysis like R or Python, most of which interact seamlessly with SQL. However, I cannot use cloud services like Google, Azure, or Amazon, only plain old self-hosted platforms (preferably open source).&lt;/p&gt;\n\n&lt;p&gt;I have attempted the following solutions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Vertical Partitioning: I divided the dataset by subject into different dimension and fact tables (e.g., responses table, nutrients table, etc.), resulting in semi-wide tables with hundreds of columns. This created multiple fact tables with the same grain and a few dimension tables, with a 1:1 relationship between dimensions and facts, which essentially forms a fact constellation schema.&lt;/li&gt;\n&lt;li&gt;Melting the Dataset: I transformed the columns into rows to create a &amp;quot;long&amp;quot; format, but this led to loss of data types, as categorical (integers), measurements (floating point numbers), and text responses (strings) ended up in the same column.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The users of this dataset would expect to be able to retrieve the data in its original columnar format. While they are not usually interested in all ~3,000 columns at once, it is important that the database can reconstruct the long format efficiently, especially if the data needs to be exported to another system or analytical platform.&lt;/p&gt;\n\n&lt;p&gt;I am considering a star schema but am unsure if that is the most effective approach given the dataset&amp;#39;s complexity and the challenge of incorporating metadata. Any suggestions or advice on how to tackle this issue would be greatly appreciated. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jtonq", "is_robot_indexable": true, "report_reasons": null, "author": "BudgetAd1030", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jtonq/structuring_an_extremely_wide_dataset_in_an_rdbms/", "subreddit_subscribers": 136932, "created_utc": 1698673351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nI am looking for an apache beam I/O connector that makes use of HTTPS. And also one that makes use of server sent events(SSE's). If these do not exist how would i go about creating one?", "author_fullname": "t2_fcsujw3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I/O connectors apache beam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqt39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698663725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am looking for an apache beam I/O connector that makes use of HTTPS. And also one that makes use of server sent events(SSE&amp;#39;s). If these do not exist how would i go about creating one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jqt39", "is_robot_indexable": true, "report_reasons": null, "author": "Abject-Battle1215", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jqt39/io_connectors_apache_beam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jqt39/io_connectors_apache_beam/", "subreddit_subscribers": 136932, "created_utc": 1698663725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_129ag6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Free E-Book] Vector Databases and AI Applications For Dummies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17jqd0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M_w6LCVKtZRxewkvpnjsbkZKjr4UMxwhA9ewz38LFIY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698661933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "singlestore.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.singlestore.com/resources/vector-databases-and-ai-applications-for-dummies/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?auto=webp&amp;s=000c0c7064f64ca2b0cac6827b14d12e37d2d232", "width": 1201, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c12f0fce9010e3ce2b216f8fc5be7a47e302c73", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3170483cb05e990313b8d326715900e87a6118ff", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46198771b44a5e88f556c346a5319e49d8f4d97a", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ace9f2c49ee51a37dc87fb35943cf69b5994e62", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e29ac98a284e4cd1b2cc955e9610520b852f2439", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/k9XLworLSn61Y1j3SAYp5kZzoypcnytvGRa4HiVT-48.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1a4c949414fc446017cc9a9c455e43df2938369", "width": 1080, "height": 606}], "variants": {}, "id": "CV6vqaEWO_WXiT53Pnv-OIpG-MTUtHAMri_KogAZW3k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17jqd0e", "is_robot_indexable": true, "report_reasons": null, "author": "PavanBelagatti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jqd0e/free_ebook_vector_databases_and_ai_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.singlestore.com/resources/vector-databases-and-ai-applications-for-dummies/", "subreddit_subscribers": 136932, "created_utc": 1698661933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I recently learned about the data platform Promethium.ai. Does anyone have any reviews or insights to share about it?", "author_fullname": "t2_kgk078ub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Promethium Reviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jmwjn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698646416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I recently learned about the data platform Promethium.ai. Does anyone have any reviews or insights to share about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17jmwjn", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning-Forever597", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jmwjn/promethium_reviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jmwjn/promethium_reviews/", "subreddit_subscribers": 136932, "created_utc": 1698646416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've tried googling up on it, but it's often non-conclusive and I don't quite get much from it. They would say that it can be moved between storages and systems, but they don't ever give a counter example, or what would make something be non-portable.\n\nIt would help to have some point of reference, on what makes a database portable or not. \n\n&amp;#x200B;", "author_fullname": "t2_mdkpy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What makes a database portable or not.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17jkkfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698637149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried googling up on it, but it&amp;#39;s often non-conclusive and I don&amp;#39;t quite get much from it. They would say that it can be moved between storages and systems, but they don&amp;#39;t ever give a counter example, or what would make something be non-portable.&lt;/p&gt;\n\n&lt;p&gt;It would help to have some point of reference, on what makes a database portable or not. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17jkkfc", "is_robot_indexable": true, "report_reasons": null, "author": "WaifuMasterRace", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17jkkfc/what_makes_a_database_portable_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17jkkfc/what_makes_a_database_portable_or_not/", "subreddit_subscribers": 136932, "created_utc": 1698637149.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}