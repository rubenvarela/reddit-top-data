{"kind": "Listing", "data": {"after": "t3_17dy2zb", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_w2ys9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital and Kioxia to Announce Merge This Month: Report", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwqq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_HjNwUP_ErZdeswEWVIkT9veOtbwLgjUkMrqlIofI6E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697990951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?auto=webp&amp;s=0a039feef35030b1ed90389fb053276881b1f6eb", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8045b31dfe1fca2d265b727545f0599704f7dd0", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d282422022ced40625bd6fc43b3653b626c5bdc", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5079d484539d9125775eae8c1c43c7f62583f050", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a01dea74e9b5552cb000be63f5f62341ffc2652e", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=370b433d16b978661a0e2cff60c015bcda96410b", "width": 960, "height": 640}], "variants": {}, "id": "i4fwqt6AascksQGPmda9WPjiAdzc2Ub9m3c54W9xlPQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwqq4", "is_robot_indexable": true, "report_reasons": null, "author": "777fer", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwqq4/western_digital_and_kioxia_to_announce_merge_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "subreddit_subscribers": 708038, "created_utc": 1697990951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sry for my english but I am so amazed of how much data we can store.\n\nI feel like, that I can store everything.\n\nBut then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)\n\nLike imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.", "author_fullname": "t2_oq2e3f8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much does it theoretically cost to record your WHOLE life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dtp2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697982502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sry for my english but I am so amazed of how much data we can store.&lt;/p&gt;\n\n&lt;p&gt;I feel like, that I can store everything.&lt;/p&gt;\n\n&lt;p&gt;But then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)&lt;/p&gt;\n\n&lt;p&gt;Like imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dtp2m", "is_robot_indexable": true, "report_reasons": null, "author": "Sorita_", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "subreddit_subscribers": 708038, "created_utc": 1697982502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've grown interested in ripping my blu ray/DVDs to conserve them digitally. Pretty much every guide/tutorial I found use makeMKV to extract the main movie. What I am looking for is to extract the entire disc raw. Basically get the Blu-ray structure itself aka the BDMV and Certificate files. This is because I want to also obtain all the dic special features and to make into an image so I can play the bluray menu even if I don't have it on me. I've also heard of some people doing it \"manually\". What does that mean? I don't mind if the process is complicated, as a matter of fact, I find it fun to do things manually rather than automatically.", "author_fullname": "t2_8963hup3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to FULLY rip you bluray?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e3dfh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698008934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve grown interested in ripping my blu ray/DVDs to conserve them digitally. Pretty much every guide/tutorial I found use makeMKV to extract the main movie. What I am looking for is to extract the entire disc raw. Basically get the Blu-ray structure itself aka the BDMV and Certificate files. This is because I want to also obtain all the dic special features and to make into an image so I can play the bluray menu even if I don&amp;#39;t have it on me. I&amp;#39;ve also heard of some people doing it &amp;quot;manually&amp;quot;. What does that mean? I don&amp;#39;t mind if the process is complicated, as a matter of fact, I find it fun to do things manually rather than automatically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e3dfh", "is_robot_indexable": true, "report_reasons": null, "author": "Lord-_-Kirito", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e3dfh/how_to_fully_rip_you_bluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e3dfh/how_to_fully_rip_you_bluray/", "subreddit_subscribers": 708038, "created_utc": 1698008934.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am basically planning to use a mini pc like an optiplex to use as a media server for Plex and running other stuff to go with it for home media. \n\nI already have a 16TB Ironwolf which I am currently transferring my contents to another 32 OTW. \n\nNow the issue I am facing is since it being a mini pc there is no way to directly attach the drives to the system. \n\nSomeone suggested to buy a Nas, but the Nas prices even 2nd hand here (Thailand) are exceptionally high. Is there anyway else one can connect these drives to build such a home media system? I saw some Orico enclosures but I heard HDDs over USB is a bad idea so I am dropping that thought. \n\nMaybe building a mini Nas? Or some kind of home storage. I am quite lost.", "author_fullname": "t2_155d4nhk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long time (data)hoarder on here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dpof4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697967948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am basically planning to use a mini pc like an optiplex to use as a media server for Plex and running other stuff to go with it for home media. &lt;/p&gt;\n\n&lt;p&gt;I already have a 16TB Ironwolf which I am currently transferring my contents to another 32 OTW. &lt;/p&gt;\n\n&lt;p&gt;Now the issue I am facing is since it being a mini pc there is no way to directly attach the drives to the system. &lt;/p&gt;\n\n&lt;p&gt;Someone suggested to buy a Nas, but the Nas prices even 2nd hand here (Thailand) are exceptionally high. Is there anyway else one can connect these drives to build such a home media system? I saw some Orico enclosures but I heard HDDs over USB is a bad idea so I am dropping that thought. &lt;/p&gt;\n\n&lt;p&gt;Maybe building a mini Nas? Or some kind of home storage. I am quite lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dpof4", "is_robot_indexable": true, "report_reasons": null, "author": "qwertyuiop1158", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dpof4/long_time_datahoarder_on_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dpof4/long_time_datahoarder_on_here/", "subreddit_subscribers": 708038, "created_utc": 1697967948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://github.com/Obscurely/Pbthal-Archive-Manager\n\nI've made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.\n\nAny Instructions you need are in the repo's readme. If you have any questions let me know.\n\nThis is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.", "author_fullname": "t2_38ga09q5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python CLI to make downloading music from PBTHAL's archive easier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dskl2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Obscurely/Pbthal-Archive-Manager\"&gt;https://github.com/Obscurely/Pbthal-Archive-Manager&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.&lt;/p&gt;\n\n&lt;p&gt;Any Instructions you need are in the repo&amp;#39;s readme. If you have any questions let me know.&lt;/p&gt;\n\n&lt;p&gt;This is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?auto=webp&amp;s=9c1d67d21f9fbd196762f514de41d499aec9608b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b1e81a571b06809f8210158292ba9ccebcdd459", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b02a07f7ba574446ff90564afab245b5538e04c7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff993a9b399bad4efecf21deaf72738c6f0d2361", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d6fd13ae82424b7d6ba1481d34196f9ab0c4c14", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b0add97f962e11c0e295390342fd18ecb30321b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56881814dbdb3162653e1bac7374b40d666eb710", "width": 1080, "height": 540}], "variants": {}, "id": "Polxcak_Uw013SprAbMfapj9eIxjp1_aTOvkDEj48iM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dskl2", "is_robot_indexable": true, "report_reasons": null, "author": "CrismarucAdrian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "subreddit_subscribers": 708038, "created_utc": 1697979072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I haven\u2019t ordered a drive in nearly two years and I\u2019m kind of lost - I have one open slot in my NAS build and I need to add another 14tb+ for space. I used to crack open WD EasyStore drives from Best Buy, but I noticed they don\u2019t stock much on that front anymore.", "author_fullname": "t2_c6ft8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the latest deal on large (14tb+) drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e2o88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698007060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven\u2019t ordered a drive in nearly two years and I\u2019m kind of lost - I have one open slot in my NAS build and I need to add another 14tb+ for space. I used to crack open WD EasyStore drives from Best Buy, but I noticed they don\u2019t stock much on that front anymore.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e2o88", "is_robot_indexable": true, "report_reasons": null, "author": "AboutToSnap", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e2o88/whats_the_latest_deal_on_large_14tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e2o88/whats_the_latest_deal_on_large_14tb_drives/", "subreddit_subscribers": 708038, "created_utc": 1698007060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt's currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don't currently have a backup. I've been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.\n\nI want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I'll have once the hard drives are all upgraded. I don't need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.\n\n...please help!...", "author_fullname": "t2_coe35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading 10Tb NAS to 30Tb, need backup solution.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17du9xi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697984201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt&amp;#39;s currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don&amp;#39;t currently have a backup. I&amp;#39;ve been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I&amp;#39;ll have once the hard drives are all upgraded. I don&amp;#39;t need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.&lt;/p&gt;\n\n&lt;p&gt;...please help!...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17du9xi", "is_robot_indexable": true, "report_reasons": null, "author": "braedan51", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "subreddit_subscribers": 708038, "created_utc": 1697984201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have read at several boards (even here), that people had bad experience with BD-Rs:\n\n\" I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.\n\nOn  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?\"\n\n\"  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  \"\n\netc...\n\nI personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.\n\nI am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!", "author_fullname": "t2_c0w7qv19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does BD-R really have shorter lifespan than DVD-R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dseim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697978519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have read at several boards (even here), that people had bad experience with BD-Rs:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot; I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.&lt;/p&gt;\n\n&lt;p&gt;On  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;etc...&lt;/p&gt;\n\n&lt;p&gt;I personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dseim", "is_robot_indexable": true, "report_reasons": null, "author": "neidhardtzx", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "subreddit_subscribers": 708038, "created_utc": 1697978519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I have been figuring out for hours how to mirror the entirety of some very complicated websites such as [this](https://www.aten7.com/),  [this](https://www.sprite.com/zerolimits), and [this](https://kprverse.com/). I tried to use wget and HTTrack and obviously is not working.\n\nI have heard about [Offline Explorer](https://metaproducts.com/products/offline-explorer) but it is pricey. How should I go about this or is it just impossible since it is too complicated?", "author_fullname": "t2_e3bkxcn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring a very complex website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17drfx8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697975142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been figuring out for hours how to mirror the entirety of some very complicated websites such as &lt;a href=\"https://www.aten7.com/\"&gt;this&lt;/a&gt;,  &lt;a href=\"https://www.sprite.com/zerolimits\"&gt;this&lt;/a&gt;, and &lt;a href=\"https://kprverse.com/\"&gt;this&lt;/a&gt;. I tried to use wget and HTTrack and obviously is not working.&lt;/p&gt;\n\n&lt;p&gt;I have heard about &lt;a href=\"https://metaproducts.com/products/offline-explorer\"&gt;Offline Explorer&lt;/a&gt; but it is pricey. How should I go about this or is it just impossible since it is too complicated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?auto=webp&amp;s=019fe2a6bebec7b7501a65412fc5a85171d4770c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=87b01fedeea8b795318a4882c2a4f6970676f5e2", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b0680511a543384ddb6d1cf5b7292e7d25796e0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a44c9d2b1df85a9da679573c4111606cf093fb83", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=53f0b20d451ec30506a6dc85f293ce6a39d2ed0d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b09996b49469e80858cae90db5ebf424c56f779c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uT62XYn8KgBMMpxVMSYkcC-M-2hAb9SNNixiCEHKu5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d98591ca3ed0593040f1869fd4a0881e6631e8b", "width": 1080, "height": 567}], "variants": {}, "id": "Ri93iwPEsuQ9QqVgtArPKG-9kfxAbbyT82muhhicR04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17drfx8", "is_robot_indexable": true, "report_reasons": null, "author": "-Meme-Lord-", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17drfx8/mirroring_a_very_complex_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17drfx8/mirroring_a_very_complex_website/", "subreddit_subscribers": 708038, "created_utc": 1697975142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 3TB Internal HDD, and I would like to use my 3TB External HDD as a backup drive.\n\nFor example, I could connect the external hdd once per month, or something like that, and backup the changes. Is there a good tool to do this? I am using Windows.", "author_fullname": "t2_2xftem1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD as Backup for my Internal HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17drevb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697975029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3TB Internal HDD, and I would like to use my 3TB External HDD as a backup drive.&lt;/p&gt;\n\n&lt;p&gt;For example, I could connect the external hdd once per month, or something like that, and backup the changes. Is there a good tool to do this? I am using Windows.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17drevb", "is_robot_indexable": true, "report_reasons": null, "author": "Leonhardt90", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17drevb/external_hdd_as_backup_for_my_internal_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17drevb/external_hdd_as_backup_for_my_internal_hdd/", "subreddit_subscribers": 708038, "created_utc": 1697975029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought the t7 regular SSD would stay cooler than the t7 shield because the shield has so much extra casing on it. Do I have it backwards?  Does the shield have more cooling pads?\n\nI just got a t7 (non shield) and after only about 10 minutes of usage it was steaming hot, I keep a fan blowing at it, I was worried about it overheating. \n\n&amp;#x200B;\n\nI'm wondering if I should have gotten a Crucial or PNY equivalent external SSD, or maybe they get just as hot too.", "author_fullname": "t2_bib2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should the T7 or T7 Shield Run Cooler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e1ird", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698003951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought the t7 regular SSD would stay cooler than the t7 shield because the shield has so much extra casing on it. Do I have it backwards?  Does the shield have more cooling pads?&lt;/p&gt;\n\n&lt;p&gt;I just got a t7 (non shield) and after only about 10 minutes of usage it was steaming hot, I keep a fan blowing at it, I was worried about it overheating. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if I should have gotten a Crucial or PNY equivalent external SSD, or maybe they get just as hot too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17e1ird", "is_robot_indexable": true, "report_reasons": null, "author": "LeoWitt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e1ird/should_the_t7_or_t7_shield_run_cooler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e1ird/should_the_t7_or_t7_shield_run_cooler/", "subreddit_subscribers": 708038, "created_utc": 1698003951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# TLDR:\n\n***I have a python program for downloading twitch streams and I'm looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!***\n\n...\n\nBack when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.^(1)\n\nThis program was created solely to archive a single streamer's content and, if i wanted to archive someone else's, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I've been trying to get back into archiving Twitch streams again, but now there's more than one streamer I'd like an archive for.\n\nWhen I searched on Google, however, it seems as though yt-dlp doesn't support this via normal command line usage and so I'll have to come up with a solution in python instead.\n\nCurrently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.\n\n**I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here's those solutions:**\n\n* **Brute force:** just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I'd like to avoid it at all cost.\n* **Add multi-threading to current script:** while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven't done any testing, but I assume I'd run into some kind of issue here.\n* **Add multi-threading + a ffmpeg library:** while this also seems like a good solution, I'm once again unsure if the ffmpeg library would solve those issues with these threads all using it.\n\n...\n\n1. *My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.*", "author_fullname": "t2_vjs2k4um", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using yt-dlp + Python to Archive Multiple Twitch Streams Simultaneously", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dv9a9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697986939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;TLDR:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I have a python program for downloading twitch streams and I&amp;#39;m looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;p&gt;Back when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;This program was created solely to archive a single streamer&amp;#39;s content and, if i wanted to archive someone else&amp;#39;s, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I&amp;#39;ve been trying to get back into archiving Twitch streams again, but now there&amp;#39;s more than one streamer I&amp;#39;d like an archive for.&lt;/p&gt;\n\n&lt;p&gt;When I searched on Google, however, it seems as though yt-dlp doesn&amp;#39;t support this via normal command line usage and so I&amp;#39;ll have to come up with a solution in python instead.&lt;/p&gt;\n\n&lt;p&gt;Currently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here&amp;#39;s those solutions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Brute force:&lt;/strong&gt; just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I&amp;#39;d like to avoid it at all cost.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading to current script:&lt;/strong&gt; while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven&amp;#39;t done any testing, but I assume I&amp;#39;d run into some kind of issue here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading + a ffmpeg library:&lt;/strong&gt; while this also seems like a good solution, I&amp;#39;m once again unsure if the ffmpeg library would solve those issues with these threads all using it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dv9a9", "is_robot_indexable": true, "report_reasons": null, "author": "GothicMutt", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "subreddit_subscribers": 708038, "created_utc": 1697986939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "While running this:\n\n    # badblocks -b 512 -wsv -o /root/badblocks.txt /dev/sdb\n    Checking for bad blocks in read-write mode\n    From block 0 to 1953525167\n    Testing with pattern 0xaa: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0x55: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0xff: done                                                 \n    Reading and comparing: done                                                 \n    Testing with pattern 0x00: done                                                 \n    Reading and comparing: done                                                 \n    Pass completed, 0 bad blocks found. (0/0/0 errors)\n\nThis appears in dmesg:\n\n    [19837.677357] ata5.00: exception Emask 0x0 SAct 0x4000 SErr 0x0 action 0x0\n    [19837.677367] ata5.00: irq_stat 0x40000008\n    [19837.677372] ata5.00: failed command: READ FPDMA QUEUED\n    [19837.677374] ata5.00: cmd 60/00:70:00:2a:11/02:00:38:00:00/40 tag 14 ncq dma 262144 in\n                            res 41/40:00:01:2a:11/00:00:38:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [19837.677383] ata5.00: status: { DRDY ERR }\n    [19837.677386] ata5.00: error: { UNC }\n    [19837.681847] ata5.00: configured for UDMA/133\n    [19837.681867] sd 4:0:0:0: [sdb] tag#14 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [19837.681872] sd 4:0:0:0: [sdb] tag#14 Sense Key : Medium Error [current] \n    [19837.681875] sd 4:0:0:0: [sdb] tag#14 Add. Sense: Unrecovered read error - auto reallocate failed\n    [19837.681879] sd 4:0:0:0: [sdb] tag#14 CDB: Read(10) 28 00 38 11 2a 00 00 02 00 00\n    [19837.681881] I/O error, dev sdb, sector 940648961 op 0x0:(READ) flags 0x800 phys_seg 64 prio class 2\n    [19837.681893] ata5: EH complete\n    [82326.997668] ata5.00: exception Emask 0x0 SAct 0x40000 SErr 0x0 action 0x0\n    [82326.997678] ata5.00: irq_stat 0x40000008\n    [82326.997684] ata5.00: failed command: READ FPDMA QUEUED\n    [82326.997686] ata5.00: cmd 60/40:90:c0:9d:46/00:00:4a:00:00/40 tag 18 ncq dma 32768 in\n                            res 41/40:00:fd:9d:46/00:00:4a:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [82326.997695] ata5.00: status: { DRDY ERR }\n    [82326.997698] ata5.00: error: { UNC }\n    [82327.003076] ata5.00: configured for UDMA/133\n    [82327.003096] sd 4:0:0:0: [sdb] tag#18 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [82327.003101] sd 4:0:0:0: [sdb] tag#18 Sense Key : Medium Error [current] \n    [82327.003104] sd 4:0:0:0: [sdb] tag#18 Add. Sense: Unrecovered read error - auto reallocate failed\n    [82327.003108] sd 4:0:0:0: [sdb] tag#18 CDB: Read(10) 28 00 4a 46 9d c0 00 00 40 00\n    [82327.003110] I/O error, dev sdb, sector 1246141949 op 0x0:(READ) flags 0x800 phys_seg 1 prio class 2\n    [82327.003125] ata5: EH complete\n    [82419.252924] ata5.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x0\n    [82419.252934] ata5.00: irq_stat 0x40000008\n    [82419.252940] ata5.00: failed command: READ FPDMA QUEUED\n    [82419.252942] ata5.00: cmd 60/40:98:c0:92:94/00:00:4a:00:00/40 tag 19 ncq dma 32768 in\n                            res 41/40:00:c3:92:94/00:00:4a:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n    [82419.252951] ata5.00: status: { DRDY ERR }\n    [82419.252954] ata5.00: error: { UNC }\n    [82419.258166] ata5.00: configured for UDMA/133\n    [82419.258185] sd 4:0:0:0: [sdb] tag#19 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n    [82419.258190] sd 4:0:0:0: [sdb] tag#19 Sense Key : Medium Error [current] \n    [82419.258193] sd 4:0:0:0: [sdb] tag#19 Add. Sense: Unrecovered read error - auto reallocate failed\n    [82419.258197] sd 4:0:0:0: [sdb] tag#19 CDB: Read(10) 28 00 4a 94 92 c0 00 00 40 00\n    [82419.258199] I/O error, dev sdb, sector 1251250883 op 0x0:(READ) flags 0x800 phys_seg 8 prio class 2\n    [82419.258213] ata5: EH complete\n\nI don't know who to trust.", "author_fullname": "t2_aox1z74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This isn't making any sense. (badblocks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17djsic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697944186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While running this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# badblocks -b 512 -wsv -o /root/badblocks.txt /dev/sdb\nChecking for bad blocks in read-write mode\nFrom block 0 to 1953525167\nTesting with pattern 0xaa: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0x55: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0xff: done                                                 \nReading and comparing: done                                                 \nTesting with pattern 0x00: done                                                 \nReading and comparing: done                                                 \nPass completed, 0 bad blocks found. (0/0/0 errors)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This appears in dmesg:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[19837.677357] ata5.00: exception Emask 0x0 SAct 0x4000 SErr 0x0 action 0x0\n[19837.677367] ata5.00: irq_stat 0x40000008\n[19837.677372] ata5.00: failed command: READ FPDMA QUEUED\n[19837.677374] ata5.00: cmd 60/00:70:00:2a:11/02:00:38:00:00/40 tag 14 ncq dma 262144 in\n                        res 41/40:00:01:2a:11/00:00:38:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[19837.677383] ata5.00: status: { DRDY ERR }\n[19837.677386] ata5.00: error: { UNC }\n[19837.681847] ata5.00: configured for UDMA/133\n[19837.681867] sd 4:0:0:0: [sdb] tag#14 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[19837.681872] sd 4:0:0:0: [sdb] tag#14 Sense Key : Medium Error [current] \n[19837.681875] sd 4:0:0:0: [sdb] tag#14 Add. Sense: Unrecovered read error - auto reallocate failed\n[19837.681879] sd 4:0:0:0: [sdb] tag#14 CDB: Read(10) 28 00 38 11 2a 00 00 02 00 00\n[19837.681881] I/O error, dev sdb, sector 940648961 op 0x0:(READ) flags 0x800 phys_seg 64 prio class 2\n[19837.681893] ata5: EH complete\n[82326.997668] ata5.00: exception Emask 0x0 SAct 0x40000 SErr 0x0 action 0x0\n[82326.997678] ata5.00: irq_stat 0x40000008\n[82326.997684] ata5.00: failed command: READ FPDMA QUEUED\n[82326.997686] ata5.00: cmd 60/40:90:c0:9d:46/00:00:4a:00:00/40 tag 18 ncq dma 32768 in\n                        res 41/40:00:fd:9d:46/00:00:4a:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[82326.997695] ata5.00: status: { DRDY ERR }\n[82326.997698] ata5.00: error: { UNC }\n[82327.003076] ata5.00: configured for UDMA/133\n[82327.003096] sd 4:0:0:0: [sdb] tag#18 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[82327.003101] sd 4:0:0:0: [sdb] tag#18 Sense Key : Medium Error [current] \n[82327.003104] sd 4:0:0:0: [sdb] tag#18 Add. Sense: Unrecovered read error - auto reallocate failed\n[82327.003108] sd 4:0:0:0: [sdb] tag#18 CDB: Read(10) 28 00 4a 46 9d c0 00 00 40 00\n[82327.003110] I/O error, dev sdb, sector 1246141949 op 0x0:(READ) flags 0x800 phys_seg 1 prio class 2\n[82327.003125] ata5: EH complete\n[82419.252924] ata5.00: exception Emask 0x0 SAct 0x80000 SErr 0x0 action 0x0\n[82419.252934] ata5.00: irq_stat 0x40000008\n[82419.252940] ata5.00: failed command: READ FPDMA QUEUED\n[82419.252942] ata5.00: cmd 60/40:98:c0:92:94/00:00:4a:00:00/40 tag 19 ncq dma 32768 in\n                        res 41/40:00:c3:92:94/00:00:4a:00:00/40 Emask 0x409 (media error) &amp;lt;F&amp;gt;\n[82419.252951] ata5.00: status: { DRDY ERR }\n[82419.252954] ata5.00: error: { UNC }\n[82419.258166] ata5.00: configured for UDMA/133\n[82419.258185] sd 4:0:0:0: [sdb] tag#19 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=1s\n[82419.258190] sd 4:0:0:0: [sdb] tag#19 Sense Key : Medium Error [current] \n[82419.258193] sd 4:0:0:0: [sdb] tag#19 Add. Sense: Unrecovered read error - auto reallocate failed\n[82419.258197] sd 4:0:0:0: [sdb] tag#19 CDB: Read(10) 28 00 4a 94 92 c0 00 00 40 00\n[82419.258199] I/O error, dev sdb, sector 1251250883 op 0x0:(READ) flags 0x800 phys_seg 8 prio class 2\n[82419.258213] ata5: EH complete\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I don&amp;#39;t know who to trust.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17djsic", "is_robot_indexable": true, "report_reasons": null, "author": "ppw0", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17djsic/this_isnt_making_any_sense_badblocks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17djsic/this_isnt_making_any_sense_badblocks/", "subreddit_subscribers": 708038, "created_utc": 1697944186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had guided sent to me when I queried  how to digitize the mini-DVs, though this particular model wasn't listed.\n\nI just want to double check with people more clued up on this as to whether there's something obvious or not-so that I could be missing here.\n\nI've found a really good secondhand deal on this camera so want to check before I buy.", "author_fullname": "t2_6cr2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any issues with using SAMSUNG VP-D362 video camera to transfer mini-dvs to PC at best quality (for source)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e5hcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698014525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had guided sent to me when I queried  how to digitize the mini-DVs, though this particular model wasn&amp;#39;t listed.&lt;/p&gt;\n\n&lt;p&gt;I just want to double check with people more clued up on this as to whether there&amp;#39;s something obvious or not-so that I could be missing here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found a really good secondhand deal on this camera so want to check before I buy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e5hcw", "is_robot_indexable": true, "report_reasons": null, "author": "JiggaRob", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e5hcw/any_issues_with_using_samsung_vpd362_video_camera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e5hcw/any_issues_with_using_samsung_vpd362_video_camera/", "subreddit_subscribers": 708038, "created_utc": 1698014525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys. I know a lot of people here are pretty fond of NetApp. So I was looking at the net app 4246, but it's about 25 in deep and I have a pretty shallow closet and I think it'll be a tight fit\n\nI was looking at some 3U 16 Bay super micro chassis, but they're also about 25 in deep\n\nThe closet with my structured media enclosure is about 18 in deep. My hope was that by separating the disk shelf and the computer chassis I could pick things that were shallow and stack them in a portable rack. Anyone have any recommendations?\n\nThe requirement would be front load/top load, probably 16 drives total. Currently looking to migrate about 120 terabytes from Google drive", "author_fullname": "t2_49vmur3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a shallow disk shelf / chassis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e52xl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698013395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. I know a lot of people here are pretty fond of NetApp. So I was looking at the net app 4246, but it&amp;#39;s about 25 in deep and I have a pretty shallow closet and I think it&amp;#39;ll be a tight fit&lt;/p&gt;\n\n&lt;p&gt;I was looking at some 3U 16 Bay super micro chassis, but they&amp;#39;re also about 25 in deep&lt;/p&gt;\n\n&lt;p&gt;The closet with my structured media enclosure is about 18 in deep. My hope was that by separating the disk shelf and the computer chassis I could pick things that were shallow and stack them in a portable rack. Anyone have any recommendations?&lt;/p&gt;\n\n&lt;p&gt;The requirement would be front load/top load, probably 16 drives total. Currently looking to migrate about 120 terabytes from Google drive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e52xl", "is_robot_indexable": true, "report_reasons": null, "author": "afm1191", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e52xl/looking_for_a_shallow_disk_shelf_chassis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e52xl/looking_for_a_shallow_disk_shelf_chassis/", "subreddit_subscribers": 708038, "created_utc": 1698013395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The amount of hard drives I\u2019ve had just for back up just in case something fails has been too many to count. Looking for the best most reliable drives etc. would love to just store all my stuff in the cloud and know it\u2019s safe and secure but that will never happen. Not to mention the time it would take to download it off the cloud. It\u2019s just a mess having lots of data. Just venting because I never feel like my data is safe despite having back ups.", "author_fullname": "t2_2o8t3n2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With the advance in technology I wish I could just rely fully on cloud storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e40dv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698010577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The amount of hard drives I\u2019ve had just for back up just in case something fails has been too many to count. Looking for the best most reliable drives etc. would love to just store all my stuff in the cloud and know it\u2019s safe and secure but that will never happen. Not to mention the time it would take to download it off the cloud. It\u2019s just a mess having lots of data. Just venting because I never feel like my data is safe despite having back ups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e40dv", "is_robot_indexable": true, "report_reasons": null, "author": "QualitySound96", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e40dv/with_the_advance_in_technology_i_wish_i_could/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e40dv/with_the_advance_in_technology_i_wish_i_could/", "subreddit_subscribers": 708038, "created_utc": 1698010577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "tl;dr is in the title already...\n\n- My family has Windows machines and I need to backup them. \n- The most frequently recommended tool is Veeam.\n- The backup target I am trying to use is a Hetzner storage box\n- Veeam \"free community edition for Windows\" can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.\n- However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). \n\nSeems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.\n\nAm I missing something? Has Samba become more secure in the recent years?", "author_fullname": "t2_vqur7zf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Machines Backup - everyone recommends Veeam - but the only protocol available is Samba, should I use this over the open internet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwmpz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697990645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr is in the title already...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My family has Windows machines and I need to backup them. &lt;/li&gt;\n&lt;li&gt;The most frequently recommended tool is Veeam.&lt;/li&gt;\n&lt;li&gt;The backup target I am trying to use is a Hetzner storage box&lt;/li&gt;\n&lt;li&gt;Veeam &amp;quot;free community edition for Windows&amp;quot; can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.&lt;/li&gt;\n&lt;li&gt;However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Seems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Has Samba become more secure in the recent years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwmpz", "is_robot_indexable": true, "report_reasons": null, "author": "AlpineGuy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "subreddit_subscribers": 708038, "created_utc": 1697990645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I'm looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a \"drive\" or a \"sync\" feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?", "author_fullname": "t2_sh0pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Storage Service with Shared Access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dss1a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I&amp;#39;m looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a &amp;quot;drive&amp;quot; or a &amp;quot;sync&amp;quot; feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dss1a", "is_robot_indexable": true, "report_reasons": null, "author": "WeWillFigureItOut", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "subreddit_subscribers": 708038, "created_utc": 1697979728.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The title. But it's showing up on YouTube. But when it comes to Facebook videos, it's not showing up. Can anyone help?", "author_fullname": "t2_6hchm85h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Idm download panel not showing up in facebook.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17do6wn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697961395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title. But it&amp;#39;s showing up on YouTube. But when it comes to Facebook videos, it&amp;#39;s not showing up. Can anyone help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17do6wn", "is_robot_indexable": true, "report_reasons": null, "author": "abrisham200", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17do6wn/idm_download_panel_not_showing_up_in_facebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17do6wn/idm_download_panel_not_showing_up_in_facebook/", "subreddit_subscribers": 708038, "created_utc": 1697961395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've picked up a couple of SAS drives for pretty cheap, but don't really know the most effective way to use them. I don't have any spare PCIe slots on my machine, and tbh no real idea what I'm doing. I kinda know about HBAs and I could buy a PCIe splitter, or are there cables for mini-SBA to the SATA ports on my motherboard?", "author_fullname": "t2_q13ihcw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest way to use SAS drives in a personal machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dijos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697940160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve picked up a couple of SAS drives for pretty cheap, but don&amp;#39;t really know the most effective way to use them. I don&amp;#39;t have any spare PCIe slots on my machine, and tbh no real idea what I&amp;#39;m doing. I kinda know about HBAs and I could buy a PCIe splitter, or are there cables for mini-SBA to the SATA ports on my motherboard?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dijos", "is_robot_indexable": true, "report_reasons": null, "author": "DirpyToes1315", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dijos/cheapest_way_to_use_sas_drives_in_a_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dijos/cheapest_way_to_use_sas_drives_in_a_personal/", "subreddit_subscribers": 708038, "created_utc": 1697940160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Current state is a very nice but dead 5+ year old Dell T630 with eight 6T drives.   Running software RAID using mdadm.   This family of Dell servers has a high mobo failure rate (I've had 4 die) and this one has also died.  Hence, my array is probably fine.   This box has dual 10 core Xeons and 384GB of ram, it has served me well.     I see three paths forward:\n\n* **REPAIR -** Buy a replacement motherboard on Ebay for maybe $200.    I'm taking a risk that the problem is actually the mobo, and that I would be able to successfully install it.    Good money after bad?\n* **KLUDGE -** Use a spare system, with a single i5 10th gen 6 core (12 thread).    Buy a \"cheap\" HBA and somehow jam all the drives into a full size case and hope that mdadm can reassemble the array (pretty high confidence, I've done this before).    Operate like this going forward.\n* **MIGRATE -** Invest in 3 or 4 20TB drives for a new array, additionally probably also do the Kludge to recover the array and just copy it over to the new array.  \n\nInterested in anybodys thoughts, plus a good choice for HBA cards.", "author_fullname": "t2_7osuk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Repair It, Kludge It, or Move It?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17e6zl1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698018918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current state is a very nice but dead 5+ year old Dell T630 with eight 6T drives.   Running software RAID using mdadm.   This family of Dell servers has a high mobo failure rate (I&amp;#39;ve had 4 die) and this one has also died.  Hence, my array is probably fine.   This box has dual 10 core Xeons and 384GB of ram, it has served me well.     I see three paths forward:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;REPAIR -&lt;/strong&gt; Buy a replacement motherboard on Ebay for maybe $200.    I&amp;#39;m taking a risk that the problem is actually the mobo, and that I would be able to successfully install it.    Good money after bad?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;KLUDGE -&lt;/strong&gt; Use a spare system, with a single i5 10th gen 6 core (12 thread).    Buy a &amp;quot;cheap&amp;quot; HBA and somehow jam all the drives into a full size case and hope that mdadm can reassemble the array (pretty high confidence, I&amp;#39;ve done this before).    Operate like this going forward.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MIGRATE -&lt;/strong&gt; Invest in 3 or 4 20TB drives for a new array, additionally probably also do the Kludge to recover the array and just copy it over to the new array.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Interested in anybodys thoughts, plus a good choice for HBA cards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e6zl1", "is_robot_indexable": true, "report_reasons": null, "author": "Simusid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e6zl1/repair_it_kludge_it_or_move_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e6zl1/repair_it_kludge_it_or_move_it/", "subreddit_subscribers": 708038, "created_utc": 1698018918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there an existing tool out there that will allow me to summarize all of my liked/saved posts?", "author_fullname": "t2_7ayx50k9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool that summarizes my saved/liked posts on different social media sites (fb, twitter, tiktok, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e2ohd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698007082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an existing tool out there that will allow me to summarize all of my liked/saved posts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e2ohd", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy_Sherbert9151", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e2ohd/tool_that_summarizes_my_savedliked_posts_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e2ohd/tool_that_summarizes_my_savedliked_posts_on/", "subreddit_subscribers": 708038, "created_utc": 1698007082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nI recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  \n\n\nIn specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.\n\n&amp;#x200B;\n\nNow my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don't touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?\n\n&amp;#x200B;\n\nResources for SSD data loss:\n\n[https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will\\_ssd\\_lose\\_data\\_if\\_left\\_unpowered\\_for\\_extended/](https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/)\n\n[https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd\\_lose\\_data\\_wo\\_power\\_in\\_a\\_year\\_myth\\_or\\_truth/](https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/)\n\n[https://www.reddit.com/r/DataHoarder/comments/10fn86x/do\\_ssd\\_drives\\_really\\_lose\\_data\\_if\\_left\\_unpowered/](https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/)\n\n  \nThanks a lot for your expert opinion.", "author_fullname": "t2_491r3ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you mitigate/avoid SSD data loss?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dzg27", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697998306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  &lt;/p&gt;\n\n&lt;p&gt;In specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don&amp;#39;t touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Resources for SSD data loss:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/\"&gt;https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/\"&gt;https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your expert opinion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dzg27", "is_robot_indexable": true, "report_reasons": null, "author": "ghac101", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "subreddit_subscribers": 708038, "created_utc": 1697998306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a little under 3tb of porn across 90k+ files. Realistically I've been using a basic folder system to organize things but it makes it difficult to really differentiate or find stuff beyond a certain point. Like I have to make decisions on do I sort things by the website they came from, the performers in question, specific acts/scenes I like in that clip, or what?\n\nI'm already planning on going through and seeing what I can cut out (because, realistically, I don't need this much porn and there's large swathes that I don't even watch) which will help, but previous attempts to get it organized have proven difficult and fruitless. \n\nI've tried using Pornganizer (which, is that even around anymore?) as well as Picasa and also just plain redoing the folder structure. But none of these feel quite as clean or easy to use as I'd like. I do often find myself looking for something specific and not being able to find it, even when I know I have it on my computer.\n\nSo, what do y'all do?", "author_fullname": "t2_mbeizvm17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing porn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dz7r2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697997703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a little under 3tb of porn across 90k+ files. Realistically I&amp;#39;ve been using a basic folder system to organize things but it makes it difficult to really differentiate or find stuff beyond a certain point. Like I have to make decisions on do I sort things by the website they came from, the performers in question, specific acts/scenes I like in that clip, or what?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m already planning on going through and seeing what I can cut out (because, realistically, I don&amp;#39;t need this much porn and there&amp;#39;s large swathes that I don&amp;#39;t even watch) which will help, but previous attempts to get it organized have proven difficult and fruitless. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using Pornganizer (which, is that even around anymore?) as well as Picasa and also just plain redoing the folder structure. But none of these feel quite as clean or easy to use as I&amp;#39;d like. I do often find myself looking for something specific and not being able to find it, even when I know I have it on my computer.&lt;/p&gt;\n\n&lt;p&gt;So, what do y&amp;#39;all do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dz7r2", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantArmy6030", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dz7r2/organizing_porn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dz7r2/organizing_porn/", "subreddit_subscribers": 708038, "created_utc": 1697997703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.\n\nI initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.", "author_fullname": "t2_1gvgwz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video files on USB not opening", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dy2zb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697994645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dy2zb", "is_robot_indexable": true, "report_reasons": null, "author": "Parker813", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "subreddit_subscribers": 708038, "created_utc": 1697994645.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}