{"kind": "Listing", "data": {"after": "t3_17e5hcw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_w2ys9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital and Kioxia to Announce Merge This Month: Report", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwqq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 114, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 114, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_HjNwUP_ErZdeswEWVIkT9veOtbwLgjUkMrqlIofI6E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697990951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?auto=webp&amp;s=0a039feef35030b1ed90389fb053276881b1f6eb", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8045b31dfe1fca2d265b727545f0599704f7dd0", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d282422022ced40625bd6fc43b3653b626c5bdc", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5079d484539d9125775eae8c1c43c7f62583f050", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a01dea74e9b5552cb000be63f5f62341ffc2652e", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/dIAOONMJjJo7ZF3kWtPX0qSaEmRCj7UMISf-1AWnSPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=370b433d16b978661a0e2cff60c015bcda96410b", "width": 960, "height": 640}], "variants": {}, "id": "i4fwqt6AascksQGPmda9WPjiAdzc2Ub9m3c54W9xlPQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwqq4", "is_robot_indexable": true, "report_reasons": null, "author": "777fer", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwqq4/western_digital_and_kioxia_to_announce_merge_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/western-digital-and-kioxia-to-announce-merge-this-month-report", "subreddit_subscribers": 708142, "created_utc": 1697990951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I haven't appreciated how much magic is now involved in keeping HDDs reliable. Western Digital recently had a [blog post that showed me a little of what's behind the curtain.](https://blog.westerndigital.com/optinand/) It's a lot!\n\nI came of age back when personal computers had 5 MB or 10 MB hard drives. That technology was simple to understand.\n\nBut now, so many things. For example, there's something called Adjacent Track Interference, where, when a drive writes a data track, that slightly interferes with adjacent data tracks. This is for CMR, not SMR.\n\nHere's some interesting snippets from that blog post:\n\nHowever, a hard drive can only write so many times before there is a risk of magnetic interference on adjacent sectors to the one being written. ... the drive will go back and re-read the data, and then rewrite tracks to prevent any data corruption.\n\n\u201cIt used to be, not that many generations ago, that you could write 10,000 times before needing to refresh sectors on either side,\u201d Hall said. **\u201cAnd then as we pushed the tracks closer and closer together, it went to 100 then 50 then 10, and now for some sectors, it\u2019s as low as six.\u201d**", "author_fullname": "t2_r8b2hsfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The latest hard drives might not be less \"reliable\", but it takes a lot of behind the scenes black magic.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ebmd6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698033494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t appreciated how much magic is now involved in keeping HDDs reliable. Western Digital recently had a &lt;a href=\"https://blog.westerndigital.com/optinand/\"&gt;blog post that showed me a little of what&amp;#39;s behind the curtain.&lt;/a&gt; It&amp;#39;s a lot!&lt;/p&gt;\n\n&lt;p&gt;I came of age back when personal computers had 5 MB or 10 MB hard drives. That technology was simple to understand.&lt;/p&gt;\n\n&lt;p&gt;But now, so many things. For example, there&amp;#39;s something called Adjacent Track Interference, where, when a drive writes a data track, that slightly interferes with adjacent data tracks. This is for CMR, not SMR.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s some interesting snippets from that blog post:&lt;/p&gt;\n\n&lt;p&gt;However, a hard drive can only write so many times before there is a risk of magnetic interference on adjacent sectors to the one being written. ... the drive will go back and re-read the data, and then rewrite tracks to prevent any data corruption.&lt;/p&gt;\n\n&lt;p&gt;\u201cIt used to be, not that many generations ago, that you could write 10,000 times before needing to refresh sectors on either side,\u201d Hall said. &lt;strong&gt;\u201cAnd then as we pushed the tracks closer and closer together, it went to 100 then 50 then 10, and now for some sectors, it\u2019s as low as six.\u201d&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?auto=webp&amp;s=f2771685c9f9e4162fa11659dd80f73222dfea23", "width": 1024, "height": 536}, "resolutions": [{"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1bbef16ed8f0a9eca5eb33090ab84c174513031", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4773317f131559526436b46ed1dc58f6487eb803", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9198687b82ff0e6434ad5f7eebde398e3479b88d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=166106f965e43c791734cdfe5cc7306df51e2567", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/7ldgQZVNns2dm060CeM3qMui9l-hpWO105A3AlialV4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea76dddc1cd6b524e8cb85ca17a8d151b9ed6e77", "width": 960, "height": 502}], "variants": {}, "id": "sdPQ3kehKK3g_R9XTlGBScCieF4hJAsYhkcpKdav7_Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17ebmd6", "is_robot_indexable": true, "report_reasons": null, "author": "old_knurd", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ebmd6/the_latest_hard_drives_might_not_be_less_reliable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ebmd6/the_latest_hard_drives_might_not_be_less_reliable/", "subreddit_subscribers": 708142, "created_utc": 1698033494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sry for my english but I am so amazed of how much data we can store.\n\nI feel like, that I can store everything.\n\nBut then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)\n\nLike imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.", "author_fullname": "t2_oq2e3f8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much does it theoretically cost to record your WHOLE life", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dtp2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697982502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sry for my english but I am so amazed of how much data we can store.&lt;/p&gt;\n\n&lt;p&gt;I feel like, that I can store everything.&lt;/p&gt;\n\n&lt;p&gt;But then I had the question: If we could record like 80years of content, how much would that cost? (Medium video quality)&lt;/p&gt;\n\n&lt;p&gt;Like imagine you have a hidden camera and you could record all memories.\nAnd after 100 years the grand children or someone else can look up for a whole life in the past of 100 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17dtp2m", "is_robot_indexable": true, "report_reasons": null, "author": "Sorita_", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dtp2m/how_much_does_it_theoretically_cost_to_record/", "subreddit_subscribers": 708142, "created_utc": 1697982502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've grown interested in ripping my blu ray/DVDs to conserve them digitally. Pretty much every guide/tutorial I found use makeMKV to extract the main movie. What I am looking for is to extract the entire disc raw. Basically get the Blu-ray structure itself aka the BDMV and Certificate files. This is because I want to also obtain all the dic special features and to make into an image so I can play the bluray menu even if I don't have it on me. I've also heard of some people doing it \"manually\". What does that mean? I don't mind if the process is complicated, as a matter of fact, I find it fun to do things manually rather than automatically.", "author_fullname": "t2_8963hup3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to FULLY rip you bluray?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e3dfh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698008934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve grown interested in ripping my blu ray/DVDs to conserve them digitally. Pretty much every guide/tutorial I found use makeMKV to extract the main movie. What I am looking for is to extract the entire disc raw. Basically get the Blu-ray structure itself aka the BDMV and Certificate files. This is because I want to also obtain all the dic special features and to make into an image so I can play the bluray menu even if I don&amp;#39;t have it on me. I&amp;#39;ve also heard of some people doing it &amp;quot;manually&amp;quot;. What does that mean? I don&amp;#39;t mind if the process is complicated, as a matter of fact, I find it fun to do things manually rather than automatically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e3dfh", "is_robot_indexable": true, "report_reasons": null, "author": "Lord-_-Kirito", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e3dfh/how_to_fully_rip_you_bluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e3dfh/how_to_fully_rip_you_bluray/", "subreddit_subscribers": 708142, "created_utc": 1698008934.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The Jonsbo N3 is a fairly new NAS case offering that released sometime 23H2.\n\nImages Here: https://imgur.com/a/MPgqI5F\n\n**DESCRIPTION**\n\nIt is an 8 bay hot swap \"compact\" NAS case that accommodates Mini ITX motherboards and requires an SFX PSU. The case itself is all matte black painted metal except for the easy to remove plastic front panel secured by magnets to the chassis which conceals the drives.\n\nOn the front there is a single Type A USB 3.0 port, a combo headphone/audio out 3.5mm jack, and a USB C port next to the small round power switch. Eight tiny blue status LED's run along the front next to the power button. There are four circular feet on the bottom with foam at the base to support the case and keep it from sliding or vibrating.\n\nThe disks are housed in the bottom half of the chassis and the Mini ITX motherboard and PSU are mounted on the top. Four hex head bolts secure the top lid which upon removal allows the lid to be easily slid off exposing the top compartment. There is support for a dual wide full height PCIe card, and mounting provisions for two 80mm fans. There's ample room for a monstrously tall CPU cooler as well. Two 2.5\" disks can be mounted on either side of the chassis rail.\n\nCables from the case include a 20-pin USB 3.0 connector, a USB C header connector, front panel audio, and a single connector combining the power, reset, HDD status lights. There is also a cable running directly from the backplane to the front panel to accommodate the eight individual disk status lights.\n\nAs noted before, a diagonally slatted plastic panel in front is easily removable to expose the drive bays. Looking inside you can see the front face of the backplane accommodating eight SAS or SATA drives. Two 100mm x 25mm fans come with the case and are mounted on a back panel behind the hard drive bay secured by two thumb screws. It is very easy to remove the panel which exposes the back of the backplane that contains two 4-pin Molex connectors and one SATA power connector which are used to power all eight disks. Two 4-pin fan headers are also there to power the rear fans, however the fans provided are only 3-pin. And then of course the eight SATA data connectors span across the backplane.\n\n**ACCESSORIES**\n\nA couple hex tools are provided to remove the screws for the top lid. There's ample screws and insulating washers to mount the motherboard to the standoffs, eight fan screws for the two rear 80mm fan mounts, and some screws for mounting 2.5\" HDDs.\n\nThe disks don't mount by use of a traditional tray like in a QNAP or Synology NAS. Instead they provide a bunch of flat headed philips shoulder screws which mount to the hard drives through circular rubber grommets allowing them to slide into the case rail. There are rubber straps that mount to the back of the drives for something to grab onto when removing the disks.\n\n**BUILDING IN THE CASE**\n\nBuilding components in the case is pretty simple. Removal of the top lid gives full easy access install the motherboard with no issues. There are tiedown provisions for wiring in the motherboard bay, and a large opening to snake power and data cables down to the backplane.\n\nThe biggest issue is with the power supply. A power plug already exists in the back of the case which routes through a cable to plug into your SFX PSU mounted internally in the case (similar to the Fractal Design Node 304 if you're familiar with that one). I'm not a big fan of that design because then you don't have access to a switch to power off the PSU, you have to pull the power plug.\n\nAdditionally in order to install the PSU, you need to remove a bracket to mount to the PSU then mount the PSU with the bracket to the case. However, in order to remove the bracket you need a long shaft Philips head screwdriver with a shaft at least about 140mm long.\n\nAn LSI 9211-8i HBA was used to control the disks through the backplane.\n\nI was able to build up a case with decent wire management in about 30 minutes.\n\n**HARD DRIVE COOLING**\n\nI mounted three sets of disks in this case and used OpenMediaVault to manage the disks:\n\n* 8x Seagate Barracuda SATA ST2000DM001 2TB (CMR) in RAID 6\n* 4x HGST SAS 4TB (CMR) in RAID 5\n* 5x 12TB Western Digital White + 3x 14TB Western Digital White (CMR) in RAID 6\n\nThe two rear fans provided by Jonsbo to cool the hard drive bay have Jonsbo labelling on them. I didn't find any other labels to see if it was manufactured by some third party and didn't recognize them otherwise either.\n\nI did test each of the above three configurations with the fans run at two speeds:\n\n* One at max 12V speed as connected to the backplane headers would only provide it\n* Connected to the motherboard at a lower fan speed (8.8V) adjusted through the BIOS\n\nRemember these are 3-pin DC voltage controlled fans, there is no PWM.\n\nIn each situation I wrote a simple script to record the drive temps in two situations:\n\n* a three hour timespan while idle (near 0% utilization)\n* six hour timespan while building the RAID arrays (near 100% utilization)\n\nAmbient room temperature is about 24C.\n\nResults from these tests are as follows:\n\n    High Fan Speed Disk Temperature (deg C):\n    8x 2TB RAID 6 IDLE:         29 to 32\n    8x 2TB RAID 6 RAID 6 BUILD: 31 to 34\n    4x HGST SAS RAID 5 BUILD:   37 to 38\n    8x 12TB WD RAID 6 IDLE:     35 to 40\n    8x 12TB WD RAID 6 BUILD:    35 to 41\n    \n    Low (8.8V) Fan Speed Disk Temperature (deg C):\n    8x 2TB RAID 6 IDLE:         31 to 35\n    8x 2TB RAID 6 RAID 6 BUILD: 33 to 38\n    8x 12TB WD RAID 6 IDLE:     35 to 40\n    8x 12TB WD RAID 6 BUILD:    35 to 41\n\n**FAN NOISE**\n\nNoise measurements were also taken (values in dB):\n\n    Ambient:               40.5\n    Low Fan No HDD:        42.6\n    Low Fan 8x  2TB Idle:  43.2\n    Low Fan 8x 12TB Idle:  47.9\n    High Fan No HDD:       45.1\n    High Fan 8x  2TB Idle: 46.4\n    High Fan 8x 12TB Idle: 48.3\n\n**ASSESSMENT**\n\nSo a few things we can glean from this data:\n\n* SAS disks are supported\n* The noise levels between low fan speed and max fan speed are fairly negligible\n* The fans are more than adequate to cool eight high capacity HDD's during high utilization scenarios\n\nThe fan noise is also a low tone whoosh, no different from other PC fans I have running in the room.\n\nAdditionally, 8x Samsung 850 EVO 250GB 2.5\" SATA SSD's were installed just to ensure the backplane was functioning properly up to SATA III (600 MB/sec) speeds, or minimally not gimped to SATA II speeds for some reason (I've seen that in some cases). The sustained 1GB read speed maintained approximately 500 MB/sec for each SSD, well exceeding the 300 MB/sec SATA II threshold, so it seems to be fine.\n\n**FINAL THOUGHTS**\n\nWhat I liked:\n\n* Good fit and finish overall, solid build with no noticeable buzzes or rattles while in use.\n* Easy to build in with exception of PSU bracket requires long Philips head shaft screwdriver to remove.\n* Ample clearance for large CPU cooler and full height dual width PCIe card.\n* Included 100mm fans provide adequate hdd cooling at reasonable sound levels, keeps large capacity disks under 40C at load.\n* Disks are super easy to access and fit snugly.\n* Front panel pins are in a singular connector.\n* SAS disks supported, with proper HBA card support.\n\nWhat could be improved/changed, mainly questionable design decisions, otherwise a solid case:\n\n* Change cover screws from hex to Philips. Hex tools aren\u2019t as common.\n* Doesn\u2019t need to be so tall, half height cards are fine and probably no massive CPU or cooler needed.\n* Mini ITX limits to single PCIe slot. Most Mini ITX boards don\u2019t have more than 4 SATA ports so PCIe card required. Can\u2019t install faster network card like a 10G then.\n* I\u2019d rather see 2-3 inch wider to support Micro ATX with PSU to the side of the drive bays, and chop a couple inches off the height. A more squat form factor would look nicer IMHO.\n* USB C connector is not supported on many motherboards. Would rather see two USB type A ports than one A and one C.\n* Not a fan of the internal power plug. No way to manually switch off power without pulling plug or removing cover.\n\nYou can see my video review here: https://youtu.be/3tCIAE_luFY?si=xBB22Mtaf2QtxJDD\n\nedit: grammar, clarification.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jonsbo N3 8 Bay NAS Case Quick Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eatg6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Review", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1698033544.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698030763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Jonsbo N3 is a fairly new NAS case offering that released sometime 23H2.&lt;/p&gt;\n\n&lt;p&gt;Images Here: &lt;a href=\"https://imgur.com/a/MPgqI5F\"&gt;https://imgur.com/a/MPgqI5F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;DESCRIPTION&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It is an 8 bay hot swap &amp;quot;compact&amp;quot; NAS case that accommodates Mini ITX motherboards and requires an SFX PSU. The case itself is all matte black painted metal except for the easy to remove plastic front panel secured by magnets to the chassis which conceals the drives.&lt;/p&gt;\n\n&lt;p&gt;On the front there is a single Type A USB 3.0 port, a combo headphone/audio out 3.5mm jack, and a USB C port next to the small round power switch. Eight tiny blue status LED&amp;#39;s run along the front next to the power button. There are four circular feet on the bottom with foam at the base to support the case and keep it from sliding or vibrating.&lt;/p&gt;\n\n&lt;p&gt;The disks are housed in the bottom half of the chassis and the Mini ITX motherboard and PSU are mounted on the top. Four hex head bolts secure the top lid which upon removal allows the lid to be easily slid off exposing the top compartment. There is support for a dual wide full height PCIe card, and mounting provisions for two 80mm fans. There&amp;#39;s ample room for a monstrously tall CPU cooler as well. Two 2.5&amp;quot; disks can be mounted on either side of the chassis rail.&lt;/p&gt;\n\n&lt;p&gt;Cables from the case include a 20-pin USB 3.0 connector, a USB C header connector, front panel audio, and a single connector combining the power, reset, HDD status lights. There is also a cable running directly from the backplane to the front panel to accommodate the eight individual disk status lights.&lt;/p&gt;\n\n&lt;p&gt;As noted before, a diagonally slatted plastic panel in front is easily removable to expose the drive bays. Looking inside you can see the front face of the backplane accommodating eight SAS or SATA drives. Two 100mm x 25mm fans come with the case and are mounted on a back panel behind the hard drive bay secured by two thumb screws. It is very easy to remove the panel which exposes the back of the backplane that contains two 4-pin Molex connectors and one SATA power connector which are used to power all eight disks. Two 4-pin fan headers are also there to power the rear fans, however the fans provided are only 3-pin. And then of course the eight SATA data connectors span across the backplane.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;ACCESSORIES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A couple hex tools are provided to remove the screws for the top lid. There&amp;#39;s ample screws and insulating washers to mount the motherboard to the standoffs, eight fan screws for the two rear 80mm fan mounts, and some screws for mounting 2.5&amp;quot; HDDs.&lt;/p&gt;\n\n&lt;p&gt;The disks don&amp;#39;t mount by use of a traditional tray like in a QNAP or Synology NAS. Instead they provide a bunch of flat headed philips shoulder screws which mount to the hard drives through circular rubber grommets allowing them to slide into the case rail. There are rubber straps that mount to the back of the drives for something to grab onto when removing the disks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BUILDING IN THE CASE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Building components in the case is pretty simple. Removal of the top lid gives full easy access install the motherboard with no issues. There are tiedown provisions for wiring in the motherboard bay, and a large opening to snake power and data cables down to the backplane.&lt;/p&gt;\n\n&lt;p&gt;The biggest issue is with the power supply. A power plug already exists in the back of the case which routes through a cable to plug into your SFX PSU mounted internally in the case (similar to the Fractal Design Node 304 if you&amp;#39;re familiar with that one). I&amp;#39;m not a big fan of that design because then you don&amp;#39;t have access to a switch to power off the PSU, you have to pull the power plug.&lt;/p&gt;\n\n&lt;p&gt;Additionally in order to install the PSU, you need to remove a bracket to mount to the PSU then mount the PSU with the bracket to the case. However, in order to remove the bracket you need a long shaft Philips head screwdriver with a shaft at least about 140mm long.&lt;/p&gt;\n\n&lt;p&gt;An LSI 9211-8i HBA was used to control the disks through the backplane.&lt;/p&gt;\n\n&lt;p&gt;I was able to build up a case with decent wire management in about 30 minutes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;HARD DRIVE COOLING&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I mounted three sets of disks in this case and used OpenMediaVault to manage the disks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8x Seagate Barracuda SATA ST2000DM001 2TB (CMR) in RAID 6&lt;/li&gt;\n&lt;li&gt;4x HGST SAS 4TB (CMR) in RAID 5&lt;/li&gt;\n&lt;li&gt;5x 12TB Western Digital White + 3x 14TB Western Digital White (CMR) in RAID 6&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The two rear fans provided by Jonsbo to cool the hard drive bay have Jonsbo labelling on them. I didn&amp;#39;t find any other labels to see if it was manufactured by some third party and didn&amp;#39;t recognize them otherwise either.&lt;/p&gt;\n\n&lt;p&gt;I did test each of the above three configurations with the fans run at two speeds:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;One at max 12V speed as connected to the backplane headers would only provide it&lt;/li&gt;\n&lt;li&gt;Connected to the motherboard at a lower fan speed (8.8V) adjusted through the BIOS&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Remember these are 3-pin DC voltage controlled fans, there is no PWM.&lt;/p&gt;\n\n&lt;p&gt;In each situation I wrote a simple script to record the drive temps in two situations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a three hour timespan while idle (near 0% utilization)&lt;/li&gt;\n&lt;li&gt;six hour timespan while building the RAID arrays (near 100% utilization)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Ambient room temperature is about 24C.&lt;/p&gt;\n\n&lt;p&gt;Results from these tests are as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;High Fan Speed Disk Temperature (deg C):\n8x 2TB RAID 6 IDLE:         29 to 32\n8x 2TB RAID 6 RAID 6 BUILD: 31 to 34\n4x HGST SAS RAID 5 BUILD:   37 to 38\n8x 12TB WD RAID 6 IDLE:     35 to 40\n8x 12TB WD RAID 6 BUILD:    35 to 41\n\nLow (8.8V) Fan Speed Disk Temperature (deg C):\n8x 2TB RAID 6 IDLE:         31 to 35\n8x 2TB RAID 6 RAID 6 BUILD: 33 to 38\n8x 12TB WD RAID 6 IDLE:     35 to 40\n8x 12TB WD RAID 6 BUILD:    35 to 41\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;FAN NOISE&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Noise measurements were also taken (values in dB):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Ambient:               40.5\nLow Fan No HDD:        42.6\nLow Fan 8x  2TB Idle:  43.2\nLow Fan 8x 12TB Idle:  47.9\nHigh Fan No HDD:       45.1\nHigh Fan 8x  2TB Idle: 46.4\nHigh Fan 8x 12TB Idle: 48.3\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;ASSESSMENT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So a few things we can glean from this data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SAS disks are supported&lt;/li&gt;\n&lt;li&gt;The noise levels between low fan speed and max fan speed are fairly negligible&lt;/li&gt;\n&lt;li&gt;The fans are more than adequate to cool eight high capacity HDD&amp;#39;s during high utilization scenarios&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The fan noise is also a low tone whoosh, no different from other PC fans I have running in the room.&lt;/p&gt;\n\n&lt;p&gt;Additionally, 8x Samsung 850 EVO 250GB 2.5&amp;quot; SATA SSD&amp;#39;s were installed just to ensure the backplane was functioning properly up to SATA III (600 MB/sec) speeds, or minimally not gimped to SATA II speeds for some reason (I&amp;#39;ve seen that in some cases). The sustained 1GB read speed maintained approximately 500 MB/sec for each SSD, well exceeding the 300 MB/sec SATA II threshold, so it seems to be fine.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;FINAL THOUGHTS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;What I liked:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Good fit and finish overall, solid build with no noticeable buzzes or rattles while in use.&lt;/li&gt;\n&lt;li&gt;Easy to build in with exception of PSU bracket requires long Philips head shaft screwdriver to remove.&lt;/li&gt;\n&lt;li&gt;Ample clearance for large CPU cooler and full height dual width PCIe card.&lt;/li&gt;\n&lt;li&gt;Included 100mm fans provide adequate hdd cooling at reasonable sound levels, keeps large capacity disks under 40C at load.&lt;/li&gt;\n&lt;li&gt;Disks are super easy to access and fit snugly.&lt;/li&gt;\n&lt;li&gt;Front panel pins are in a singular connector.&lt;/li&gt;\n&lt;li&gt;SAS disks supported, with proper HBA card support.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What could be improved/changed, mainly questionable design decisions, otherwise a solid case:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Change cover screws from hex to Philips. Hex tools aren\u2019t as common.&lt;/li&gt;\n&lt;li&gt;Doesn\u2019t need to be so tall, half height cards are fine and probably no massive CPU or cooler needed.&lt;/li&gt;\n&lt;li&gt;Mini ITX limits to single PCIe slot. Most Mini ITX boards don\u2019t have more than 4 SATA ports so PCIe card required. Can\u2019t install faster network card like a 10G then.&lt;/li&gt;\n&lt;li&gt;I\u2019d rather see 2-3 inch wider to support Micro ATX with PSU to the side of the drive bays, and chop a couple inches off the height. A more squat form factor would look nicer IMHO.&lt;/li&gt;\n&lt;li&gt;USB C connector is not supported on many motherboards. Would rather see two USB type A ports than one A and one C.&lt;/li&gt;\n&lt;li&gt;Not a fan of the internal power plug. No way to manually switch off power without pulling plug or removing cover.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You can see my video review here: &lt;a href=\"https://youtu.be/3tCIAE_luFY?si=xBB22Mtaf2QtxJDD\"&gt;https://youtu.be/3tCIAE_luFY?si=xBB22Mtaf2QtxJDD&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;edit: grammar, clarification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?auto=webp&amp;s=2924dc28fc15af4c8ac6f8c0f38da8b83d921652", "width": 4032, "height": 2268}, "resolutions": [{"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0edbbc46031c9f1b6511989bb9508046ba30b3cf", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf9bcf38403fde05206066941ac7cf2982197318", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1f3786b798407bee63598b2815536bc1f38aac7", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d59a5209f509f74424e1a9ba11b4db77fea39a90", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79b4b1002fb54cd5154c67a6daf2f0abe0dbbdc5", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/9_bwIHmad_iYW5w5aQ7tCr0fVtMMPkM1U3E0upDVwco.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=99968b8fbaf134b075dc8ba28685e2e37b3adebb", "width": 1080, "height": 607}], "variants": {}, "id": "xai93e0gOggbG5vEwW49aYU9XQqR7ZwLGmbPGy9uYHs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17eatg6", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17eatg6/jonsbo_n3_8_bay_nas_case_quick_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17eatg6/jonsbo_n3_8_bay_nas_case_quick_review/", "subreddit_subscribers": 708142, "created_utc": 1698030763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I haven\u2019t ordered a drive in nearly two years and I\u2019m kind of lost - I have one open slot in my NAS build and I need to add another 14tb+ for space. I used to crack open WD EasyStore drives from Best Buy, but I noticed they don\u2019t stock much on that front anymore.", "author_fullname": "t2_c6ft8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the latest deal on large (14tb+) drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e2o88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698007060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven\u2019t ordered a drive in nearly two years and I\u2019m kind of lost - I have one open slot in my NAS build and I need to add another 14tb+ for space. I used to crack open WD EasyStore drives from Best Buy, but I noticed they don\u2019t stock much on that front anymore.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e2o88", "is_robot_indexable": true, "report_reasons": null, "author": "AboutToSnap", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e2o88/whats_the_latest_deal_on_large_14tb_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e2o88/whats_the_latest_deal_on_large_14tb_drives/", "subreddit_subscribers": 708142, "created_utc": 1698007060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://github.com/Obscurely/Pbthal-Archive-Manager\n\nI've made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.\n\nAny Instructions you need are in the repo's readme. If you have any questions let me know.\n\nThis is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.", "author_fullname": "t2_38ga09q5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python CLI to make downloading music from PBTHAL's archive easier", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dskl2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/Obscurely/Pbthal-Archive-Manager\"&gt;https://github.com/Obscurely/Pbthal-Archive-Manager&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve made this script a while ago and recenty used it again. What it esentially does is the following: download (using real debrid), extract, mass rename, make proper albums by changing the metadata tags, create spectrograms (I like to have them) and download albums covers (this one is broken for now). This saved me a whole, whole lot of time.&lt;/p&gt;\n\n&lt;p&gt;Any Instructions you need are in the repo&amp;#39;s readme. If you have any questions let me know.&lt;/p&gt;\n\n&lt;p&gt;This is more of a nieche thing for someone to need, but I was like maybe I can help one person in the world save some time so why not showcase it. If this is actually helpful to you maybe... give it a star, it would make me really happy :) to help someone out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?auto=webp&amp;s=9c1d67d21f9fbd196762f514de41d499aec9608b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b1e81a571b06809f8210158292ba9ccebcdd459", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b02a07f7ba574446ff90564afab245b5538e04c7", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff993a9b399bad4efecf21deaf72738c6f0d2361", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d6fd13ae82424b7d6ba1481d34196f9ab0c4c14", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b0add97f962e11c0e295390342fd18ecb30321b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/J7IhWTh74KG9db27Na5Q64CgiCLmg7tnI73Lg-0gThg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=56881814dbdb3162653e1bac7374b40d666eb710", "width": 1080, "height": 540}], "variants": {}, "id": "Polxcak_Uw013SprAbMfapj9eIxjp1_aTOvkDEj48iM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dskl2", "is_robot_indexable": true, "report_reasons": null, "author": "CrismarucAdrian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dskl2/python_cli_to_make_downloading_music_from_pbthals/", "subreddit_subscribers": 708142, "created_utc": 1697979072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt's currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don't currently have a backup. I've been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.\n\nI want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I'll have once the hard drives are all upgraded. I don't need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.\n\n...please help!...", "author_fullname": "t2_coe35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upgrading 10Tb NAS to 30Tb, need backup solution.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17du9xi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697984201.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 16Tb (10.9Tb usable) Synology DS418 NAS.\nIt&amp;#39;s currently configured with (4) 4Tb WD Red drives in a Synology Hybrid Raid (SHR). I just found out what SMR is and these are SMR drives. I don&amp;#39;t currently have a backup. I&amp;#39;ve been relying on the redundancy provided by the raid as protection (I know this is foolish). The NAS is about 3 years old.&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade to (4) 16Tb Segate EXOS 7200 RPM, 256Mb cache with SATA 6Gb/s interface and I want to plan some kind of backup solution. This is a media server and the absolutely critical storage is 12Tb for my film collection, but Ideally I would like the ability to be back up the entire 43.6Tb that I&amp;#39;ll have once the hard drives are all upgraded. I don&amp;#39;t need all that backup capacity now, 20Tb will cover everything I have plus at least 24 months of expansion. A scalable backup solution would be great if such a thing exists.&lt;/p&gt;\n\n&lt;p&gt;...please help!...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17du9xi", "is_robot_indexable": true, "report_reasons": null, "author": "braedan51", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17du9xi/upgrading_10tb_nas_to_30tb_need_backup_solution/", "subreddit_subscribers": 708142, "created_utc": 1697984201.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_derpe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nanofiche: Laser Engraving Nickel at 300 Nanometers to Store Text and Images on the Moon for 50 Million Years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17efku9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a_WRfPRIPcYyhOSXtz707cFthTUV20vCe2PKf-GqycU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698049993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "archmission.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.archmission.org/nanofiche", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?auto=webp&amp;s=0395e3568cafa2b0652b3caae9b0f87be124e917", "width": 1500, "height": 1500}, "resolutions": [{"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dc1f02709bcf48b569de3fd04fb5610f6e8efc6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c75fe26750da0845fbe6c9a092ea5bd4e24553c4", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94f28921c83099560a9f4602d2fe79527cadd276", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=448277888145f00f0637e2b35625d36a92729188", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf4bad0b40fda07416dc34f8c0411667ab2d4165", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/j13DHR7myB9mGv1-LIYXaEnBcyn2yAzykeBQh5LN8uQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=94281293138d47deb74dbbb288c2d142bb0dee5c", "width": 1080, "height": 1080}], "variants": {}, "id": "gqSGCew_SphAB7ArkjJkYy_GmCXv4bFyvfPgzd8FBvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17efku9", "is_robot_indexable": true, "report_reasons": null, "author": "CosmosisQ", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17efku9/nanofiche_laser_engraving_nickel_at_300/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.archmission.org/nanofiche", "subreddit_subscribers": 708142, "created_utc": 1698049993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a DIY home server that runs Debian 11 (upgrading to Debian 12 at the moment). In the past, I did backups in this manner:\n\n* Inside the home server, there were two HDDs - a main one, and a backup one. I manually ran rsync every now and then to copy over data to the backup drive.\n* Additionally, there were two HDDs in external 3.5\" drive enclosures. One of them I placed off-site. That way, at some point, I copied over data from the backup drive with rsync to the external drive that I had at home, and then went to the off-site location to swap external drives.\n\nI am now upgrading the drives and Debian itself, and as part of that, I also want a smarter backup solution. I think I'll keep the rsync based approach for the external drives, since these I always run manually anyway. And that way, I can always fully access the data - I am not risking data loss due to corruption in some backup tool specific archives / images / snapshots. I think it is a good idea to not use the same backup tool and format for _all_ drives. If there's corruption due to a backup tool bug, then at least the external drives can still be accessed, and data loss is migitated.\n\nAlso, I use btrfs on all drives mainly for its checksum feature, on top of LUKS.\n\nWhat I want to specifically change is what I do with the internal backup drive. For *that* I want to move towards an automated solution with versioning.\n\nHowever, there are three questions that I haven't found answers to yet.\n\nFirst, in the past, I rsync'd the contents of the _backup_ drive to external drives, and not the contents of the main drive. The reason for that was that I wanted to offload some of the IO load. The main drive is getting used the most of all of the drives already. By first rsync'ing the main drive contents to the internal backup drive, and then from the internal backup drive to the external drive, I hoped to reduce IO on the main disk (since the internal drives are closer in sync than an internal and an external drive are). If I use an actual backup tool, this means that I somehow have to rsync the contents of the backup tool specific archive/image/snapshot. I suppose one way to do that would be to be able to mount the archive/image/snapshot in Linux?\n\nSecond, it seems to me that relying on LUKS to do the encryption is more secure and robust since LUKS is much more tested than any custom made encryption by a backup tool. Would you agree?\n\nThird, I am still researching what backup tool is best suited for me. I do not need any cloud backup support, or anything that isn't local for that matter. As said, off-site backups I do by transporting the physical external disk anyway. My constraints are (a) reliability and (b) resource consumption. For (a), if there is corruption in the archive/image/snapshot, it must not cause total loss of the backup. For (b), I use a Supermicro mainboard with an Intel Atom C3558, and the machine has 16 GB ECC RAM. I do not want a backup solution that is extremely RAM hungry. I have about 10 TB of data and &gt; 4 million files.\n\nTo summarize what I do _not_ need:\n* Compression\n* Encryption (unless it really is better to use backup tool based encrypting instead of relying on LUKS)\n* Cloud storage support\n* Ability to use the same archive to backup data from multiple computers\n* Anything related to commercial licenses (this is for private use only)\n* Windows support (I only use Linux)\n\nThe main tools I found and looked at thus far:\n\n## Restic\n\nPro: Versioned snapshots, very popular, good performance.\n\nCon: Apparently, very high resource consumption with large datasets (see for example [here](https://forum.restic.net/t/restic-uses-more-than-4gb-ram-on-14gb-backup-than-crashes/4584), [here](https://github.com/restic/restic/issues/2519), or [here](https://forum.restic.net/t/high-memory-usage-on-backup-prune-and-check/1253)). This seems to be mainly caused by Go's garbage collector, which needs finetuning (see the `GOGC=20` recommendatins).\n\nWith regards to reliability, I found [this report](https://www.reddit.com/r/DataHoarder/comments/16rnnji/comment/k26i0p5/) about data corruption, but this seems to have been caused by RAM corruption. [This one](https://www.reddit.com/r/DataHoarder/comments/f40fa2/restic_alternative_for_backups/) is more worrisome, but it is also 4 years old. Same applies to [this one](https://forum.restic.net/t/recovery-options-for-damaged-repositories/1571). [This post](https://www.reddit.com/r/DataHoarder/comments/169tawo/comment/jz3ss6e/) claims that Restic does not have bit rot protection, but according to [this](https://github.com/restic/restic/issues/1620), Restic does use SHA256 and stores the checksum in blob filenames.\n\n## Borg\n\nPro: Versioned snapshots, also very popular, seems very mature. Did not find any RAM usage issues. Well established ecosystem from what I've seen.\n\nCon: Unclear about performance. [This post](https://www.reddit.com/r/DataHoarder/comments/izq0mo/comment/g9jmqak/) claims that larger datasets like mine would cause Borg to be very slow - but, it is 3 years old.\n\nAlso, I am not sure about a Python based solution. It does come with a large amounts of Python library dependencies, and I am typically skeptical about using a dynamically typed language for software that must be rock solid and highly reliable. But Borg has been around for a long time, so I am less worried about bugs.\n\n## Duplicacy\n\nPro: No need for a special index database. The chunks are placed in the file system. [This](https://github.com/gilbertchen/duplicacy/wiki/Lock-Free-Deduplication) explains it in greater detail. Seems to place great emphasis on reliability, which is important for me. Versioning is also supported.\n\nCon: Apparently, much less popular than Restic or Borg - I had a harder time researching about experiences others have had with it. And, similar to Restic, RAM usage seems to be potentially a problem. I found [this post](https://forum.duplicacy.com/t/memory-usage/623) that states: \"The memory usage is highly related to the number of files to be backed up. Duplicacy loads the entire file list into the memory during the indexing phase so you may run out of memory if there are too many files.\" With &gt;4 million files, this worries me. But, this is an old post. [This post](https://lowendtalk.com/discussion/comment/3401965/#Comment_3401965) also reports high RAM usage, but indicates that my file count could be handled with 16 GB RAM.\n\nI also briefly looked at Kopia. Overall quite interesting, with very good performance, but I am unsure about its reliability. [This thread](https://www.reddit.com/r/selfhosted/comments/tz0ad6/kopia_vs_borg/) (which admittely is 2 years old) did not inspire confidence. [This post](https://www.reddit.com/r/DataHoarder/comments/16rnnji/comment/k24e70x/) however is much more recent, and also rather alarming.\n\nThus far, my favorite is Duplicacy due to it not needing a special index database and putting such emphasis on reliability. Close second is Borg due to its maturity and ecosystem.\n\nWhat are your thoughts?", "author_fullname": "t2_v3szwqll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Researching what to use for purely local Linux home server backup (no cloud backups)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ec8s3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698035663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a DIY home server that runs Debian 11 (upgrading to Debian 12 at the moment). In the past, I did backups in this manner:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Inside the home server, there were two HDDs - a main one, and a backup one. I manually ran rsync every now and then to copy over data to the backup drive.&lt;/li&gt;\n&lt;li&gt;Additionally, there were two HDDs in external 3.5&amp;quot; drive enclosures. One of them I placed off-site. That way, at some point, I copied over data from the backup drive with rsync to the external drive that I had at home, and then went to the off-site location to swap external drives.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am now upgrading the drives and Debian itself, and as part of that, I also want a smarter backup solution. I think I&amp;#39;ll keep the rsync based approach for the external drives, since these I always run manually anyway. And that way, I can always fully access the data - I am not risking data loss due to corruption in some backup tool specific archives / images / snapshots. I think it is a good idea to not use the same backup tool and format for &lt;em&gt;all&lt;/em&gt; drives. If there&amp;#39;s corruption due to a backup tool bug, then at least the external drives can still be accessed, and data loss is migitated.&lt;/p&gt;\n\n&lt;p&gt;Also, I use btrfs on all drives mainly for its checksum feature, on top of LUKS.&lt;/p&gt;\n\n&lt;p&gt;What I want to specifically change is what I do with the internal backup drive. For &lt;em&gt;that&lt;/em&gt; I want to move towards an automated solution with versioning.&lt;/p&gt;\n\n&lt;p&gt;However, there are three questions that I haven&amp;#39;t found answers to yet.&lt;/p&gt;\n\n&lt;p&gt;First, in the past, I rsync&amp;#39;d the contents of the &lt;em&gt;backup&lt;/em&gt; drive to external drives, and not the contents of the main drive. The reason for that was that I wanted to offload some of the IO load. The main drive is getting used the most of all of the drives already. By first rsync&amp;#39;ing the main drive contents to the internal backup drive, and then from the internal backup drive to the external drive, I hoped to reduce IO on the main disk (since the internal drives are closer in sync than an internal and an external drive are). If I use an actual backup tool, this means that I somehow have to rsync the contents of the backup tool specific archive/image/snapshot. I suppose one way to do that would be to be able to mount the archive/image/snapshot in Linux?&lt;/p&gt;\n\n&lt;p&gt;Second, it seems to me that relying on LUKS to do the encryption is more secure and robust since LUKS is much more tested than any custom made encryption by a backup tool. Would you agree?&lt;/p&gt;\n\n&lt;p&gt;Third, I am still researching what backup tool is best suited for me. I do not need any cloud backup support, or anything that isn&amp;#39;t local for that matter. As said, off-site backups I do by transporting the physical external disk anyway. My constraints are (a) reliability and (b) resource consumption. For (a), if there is corruption in the archive/image/snapshot, it must not cause total loss of the backup. For (b), I use a Supermicro mainboard with an Intel Atom C3558, and the machine has 16 GB ECC RAM. I do not want a backup solution that is extremely RAM hungry. I have about 10 TB of data and &amp;gt; 4 million files.&lt;/p&gt;\n\n&lt;p&gt;To summarize what I do &lt;em&gt;not&lt;/em&gt; need:\n* Compression\n* Encryption (unless it really is better to use backup tool based encrypting instead of relying on LUKS)\n* Cloud storage support\n* Ability to use the same archive to backup data from multiple computers\n* Anything related to commercial licenses (this is for private use only)\n* Windows support (I only use Linux)&lt;/p&gt;\n\n&lt;p&gt;The main tools I found and looked at thus far:&lt;/p&gt;\n\n&lt;h2&gt;Restic&lt;/h2&gt;\n\n&lt;p&gt;Pro: Versioned snapshots, very popular, good performance.&lt;/p&gt;\n\n&lt;p&gt;Con: Apparently, very high resource consumption with large datasets (see for example &lt;a href=\"https://forum.restic.net/t/restic-uses-more-than-4gb-ram-on-14gb-backup-than-crashes/4584\"&gt;here&lt;/a&gt;, &lt;a href=\"https://github.com/restic/restic/issues/2519\"&gt;here&lt;/a&gt;, or &lt;a href=\"https://forum.restic.net/t/high-memory-usage-on-backup-prune-and-check/1253\"&gt;here&lt;/a&gt;). This seems to be mainly caused by Go&amp;#39;s garbage collector, which needs finetuning (see the &lt;code&gt;GOGC=20&lt;/code&gt; recommendatins).&lt;/p&gt;\n\n&lt;p&gt;With regards to reliability, I found &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/16rnnji/comment/k26i0p5/\"&gt;this report&lt;/a&gt; about data corruption, but this seems to have been caused by RAM corruption. &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/f40fa2/restic_alternative_for_backups/\"&gt;This one&lt;/a&gt; is more worrisome, but it is also 4 years old. Same applies to &lt;a href=\"https://forum.restic.net/t/recovery-options-for-damaged-repositories/1571\"&gt;this one&lt;/a&gt;. &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/169tawo/comment/jz3ss6e/\"&gt;This post&lt;/a&gt; claims that Restic does not have bit rot protection, but according to &lt;a href=\"https://github.com/restic/restic/issues/1620\"&gt;this&lt;/a&gt;, Restic does use SHA256 and stores the checksum in blob filenames.&lt;/p&gt;\n\n&lt;h2&gt;Borg&lt;/h2&gt;\n\n&lt;p&gt;Pro: Versioned snapshots, also very popular, seems very mature. Did not find any RAM usage issues. Well established ecosystem from what I&amp;#39;ve seen.&lt;/p&gt;\n\n&lt;p&gt;Con: Unclear about performance. &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/izq0mo/comment/g9jmqak/\"&gt;This post&lt;/a&gt; claims that larger datasets like mine would cause Borg to be very slow - but, it is 3 years old.&lt;/p&gt;\n\n&lt;p&gt;Also, I am not sure about a Python based solution. It does come with a large amounts of Python library dependencies, and I am typically skeptical about using a dynamically typed language for software that must be rock solid and highly reliable. But Borg has been around for a long time, so I am less worried about bugs.&lt;/p&gt;\n\n&lt;h2&gt;Duplicacy&lt;/h2&gt;\n\n&lt;p&gt;Pro: No need for a special index database. The chunks are placed in the file system. &lt;a href=\"https://github.com/gilbertchen/duplicacy/wiki/Lock-Free-Deduplication\"&gt;This&lt;/a&gt; explains it in greater detail. Seems to place great emphasis on reliability, which is important for me. Versioning is also supported.&lt;/p&gt;\n\n&lt;p&gt;Con: Apparently, much less popular than Restic or Borg - I had a harder time researching about experiences others have had with it. And, similar to Restic, RAM usage seems to be potentially a problem. I found &lt;a href=\"https://forum.duplicacy.com/t/memory-usage/623\"&gt;this post&lt;/a&gt; that states: &amp;quot;The memory usage is highly related to the number of files to be backed up. Duplicacy loads the entire file list into the memory during the indexing phase so you may run out of memory if there are too many files.&amp;quot; With &amp;gt;4 million files, this worries me. But, this is an old post. &lt;a href=\"https://lowendtalk.com/discussion/comment/3401965/#Comment_3401965\"&gt;This post&lt;/a&gt; also reports high RAM usage, but indicates that my file count could be handled with 16 GB RAM.&lt;/p&gt;\n\n&lt;p&gt;I also briefly looked at Kopia. Overall quite interesting, with very good performance, but I am unsure about its reliability. &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/tz0ad6/kopia_vs_borg/\"&gt;This thread&lt;/a&gt; (which admittely is 2 years old) did not inspire confidence. &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/16rnnji/comment/k24e70x/\"&gt;This post&lt;/a&gt; however is much more recent, and also rather alarming.&lt;/p&gt;\n\n&lt;p&gt;Thus far, my favorite is Duplicacy due to it not needing a special index database and putting such emphasis on reliability. Close second is Borg due to its maturity and ecosystem.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MSVCaSP7fZgUkwQEgEpPPTAlmD-fGLoTXmF5r5VVbMk.jpg?auto=webp&amp;s=2881d5ee3d193f9007724d692b07ae06360b8989", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/MSVCaSP7fZgUkwQEgEpPPTAlmD-fGLoTXmF5r5VVbMk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=022e706941b08a3ef6626f5e3ade84699697fcc3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MSVCaSP7fZgUkwQEgEpPPTAlmD-fGLoTXmF5r5VVbMk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8df4e178c47240b62150ea4d5166e0be89bc2ef1", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/MSVCaSP7fZgUkwQEgEpPPTAlmD-fGLoTXmF5r5VVbMk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a7e43fa504aa164cf5eb00e1f26ba34a910bb15", "width": 320, "height": 320}], "variants": {}, "id": "WkRB1sPGhejue7S5yRJAUgbnxbszsDYT31U0A-AGhBw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ec8s3", "is_robot_indexable": true, "report_reasons": null, "author": "FourDimensionalTaco", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ec8s3/researching_what_to_use_for_purely_local_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ec8s3/researching_what_to_use_for_purely_local_linux/", "subreddit_subscribers": 708142, "created_utc": 1698035663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# TLDR:\n\n***I have a python program for downloading twitch streams and I'm looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!***\n\n...\n\nBack when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.^(1)\n\nThis program was created solely to archive a single streamer's content and, if i wanted to archive someone else's, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I've been trying to get back into archiving Twitch streams again, but now there's more than one streamer I'd like an archive for.\n\nWhen I searched on Google, however, it seems as though yt-dlp doesn't support this via normal command line usage and so I'll have to come up with a solution in python instead.\n\nCurrently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.\n\n**I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here's those solutions:**\n\n* **Brute force:** just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I'd like to avoid it at all cost.\n* **Add multi-threading to current script:** while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven't done any testing, but I assume I'd run into some kind of issue here.\n* **Add multi-threading + a ffmpeg library:** while this also seems like a good solution, I'm once again unsure if the ffmpeg library would solve those issues with these threads all using it.\n\n...\n\n1. *My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.*", "author_fullname": "t2_vjs2k4um", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using yt-dlp + Python to Archive Multiple Twitch Streams Simultaneously", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dv9a9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697986939.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;TLDR:&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;I have a python program for downloading twitch streams and I&amp;#39;m looking into expanding this script to support downloading multiple streams at once. Listed below in bold are the three solutions I currently have in mind and I would appreciate feedback from anyone who has experience doing something similar. Thanks!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;p&gt;Back when I was first learning python about 4 years ago, I quickly whipped together a program that would repeatedly check whether or not a specified Twitch channel was live and, if it was, it would download said live stream.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;This program was created solely to archive a single streamer&amp;#39;s content and, if i wanted to archive someone else&amp;#39;s, I would just manually change a variable in the file and it would work. After a few years of use, however, that streamer stopped streaming and the script went into a period of disuse. I&amp;#39;ve been trying to get back into archiving Twitch streams again, but now there&amp;#39;s more than one streamer I&amp;#39;d like an archive for.&lt;/p&gt;\n\n&lt;p&gt;When I searched on Google, however, it seems as though yt-dlp doesn&amp;#39;t support this via normal command line usage and so I&amp;#39;ll have to come up with a solution in python instead.&lt;/p&gt;\n\n&lt;p&gt;Currently, my program does not import a ffmpeg library and so the script has to placed in a directory where ffmpeg is present.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I have a couple solutions in mind, but I wanted to ask if anyone else has tried something similar before I make any major changes. Here&amp;#39;s those solutions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Brute force:&lt;/strong&gt; just have multiple scripts, one for each streamer, each in their own folder with ffmpeg. This would work, but it seems wildly inefficient and so I&amp;#39;d like to avoid it at all cost.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading to current script:&lt;/strong&gt; while this would be a more elegant solution, the problem would come down to how these threads would share ffmpeg. I haven&amp;#39;t done any testing, but I assume I&amp;#39;d run into some kind of issue here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Add multi-threading + a ffmpeg library:&lt;/strong&gt; while this also seems like a good solution, I&amp;#39;m once again unsure if the ffmpeg library would solve those issues with these threads all using it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;My reason for doing this comes down to the fact that copyrighted music can cause vods to be muted in certain parts, therefore just downloading vods is insufficient. I can get a more complete archive by first having this script download the stream as it happens, then I can download the vod later by hand, and lastly, i can edit the two together to make a single complete backup. This process could definitely be improved, but on average it takes me no more than 5 minutes to edit the two videos together and so i never got around to really improving it.&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dv9a9", "is_robot_indexable": true, "report_reasons": null, "author": "GothicMutt", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dv9a9/using_ytdlp_python_to_archive_multiple_twitch/", "subreddit_subscribers": 708142, "created_utc": 1697986939.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have read at several boards (even here), that people had bad experience with BD-Rs:\n\n\" I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.\n\nOn  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?\"\n\n\"  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  \"\n\netc...\n\nI personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.\n\nI am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!", "author_fullname": "t2_c0w7qv19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does BD-R really have shorter lifespan than DVD-R?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dseim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697978519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have read at several boards (even here), that people had bad experience with BD-Rs:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot; I have used virtually every DVD-R  known to mankind, and they are (almost) all still working... I guess  data rot has killed about 0.05% of large files backed up.&lt;/p&gt;\n\n&lt;p&gt;On  the contrary, I have burned 5 BD-R discs (at slowest speed) and have  lost 100% of data backed up. I think it comes down to the dye in the  substrate?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;  what I am seeing is that 95% of the movies on DVD-R are readable (even  some as old as 15 years), but only about 50% of the movies on BD-R are  readable.  &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;etc...&lt;/p&gt;\n\n&lt;p&gt;I personally have only used CD/DVDs so far, and most of them works, including those I burned 15-20 years ago.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of getting a BD Drive now, because I can fit more content on a Disc, but if it really is less reliable I will just stay with DVD. I would like to hear your opinion and experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dseim", "is_robot_indexable": true, "report_reasons": null, "author": "neidhardtzx", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dseim/does_bdr_really_have_shorter_lifespan_than_dvdr/", "subreddit_subscribers": 708142, "created_utc": 1697978519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Current state is a very nice but dead 5+ year old Dell T630 with eight 6T drives.   Running software RAID using mdadm.   This family of Dell servers has a high mobo failure rate (I've had 4 die) and this one has also died.  Hence, my array is probably fine.   This box has dual 10 core Xeons and 384GB of ram, it has served me well.     I see three paths forward:\n\n* **REPAIR -** Buy a replacement motherboard on Ebay for maybe $200.    I'm taking a risk that the problem is actually the mobo, and that I would be able to successfully install it.    Good money after bad?\n* **KLUDGE -** Use a spare system, with a single i5 10th gen 6 core (12 thread).    Buy a \"cheap\" HBA and somehow jam all the drives into a full size case and hope that mdadm can reassemble the array (pretty high confidence, I've done this before).    Operate like this going forward.\n* **MIGRATE -** Invest in 3 or 4 20TB drives for a new array, additionally probably also do the Kludge to recover the array and just copy it over to the new array.  \n\nInterested in anybodys thoughts, plus a good choice for HBA cards.", "author_fullname": "t2_7osuk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Repair It, Kludge It, or Move It?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e6zl1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698018918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current state is a very nice but dead 5+ year old Dell T630 with eight 6T drives.   Running software RAID using mdadm.   This family of Dell servers has a high mobo failure rate (I&amp;#39;ve had 4 die) and this one has also died.  Hence, my array is probably fine.   This box has dual 10 core Xeons and 384GB of ram, it has served me well.     I see three paths forward:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;REPAIR -&lt;/strong&gt; Buy a replacement motherboard on Ebay for maybe $200.    I&amp;#39;m taking a risk that the problem is actually the mobo, and that I would be able to successfully install it.    Good money after bad?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;KLUDGE -&lt;/strong&gt; Use a spare system, with a single i5 10th gen 6 core (12 thread).    Buy a &amp;quot;cheap&amp;quot; HBA and somehow jam all the drives into a full size case and hope that mdadm can reassemble the array (pretty high confidence, I&amp;#39;ve done this before).    Operate like this going forward.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;MIGRATE -&lt;/strong&gt; Invest in 3 or 4 20TB drives for a new array, additionally probably also do the Kludge to recover the array and just copy it over to the new array.&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Interested in anybodys thoughts, plus a good choice for HBA cards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e6zl1", "is_robot_indexable": true, "report_reasons": null, "author": "Simusid", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e6zl1/repair_it_kludge_it_or_move_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e6zl1/repair_it_kludge_it_or_move_it/", "subreddit_subscribers": 708142, "created_utc": 1698018918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought the t7 regular SSD would stay cooler than the t7 shield because the shield has so much extra casing on it. Do I have it backwards?  Does the shield have more cooling pads?\n\nI just got a t7 (non shield) and after only about 10 minutes of usage it was steaming hot, I keep a fan blowing at it, I was worried about it overheating. \n\n&amp;#x200B;\n\nI'm wondering if I should have gotten a Crucial or PNY equivalent external SSD, or maybe they get just as hot too.", "author_fullname": "t2_bib2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should the T7 or T7 Shield Run Cooler?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e1ird", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698003951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought the t7 regular SSD would stay cooler than the t7 shield because the shield has so much extra casing on it. Do I have it backwards?  Does the shield have more cooling pads?&lt;/p&gt;\n\n&lt;p&gt;I just got a t7 (non shield) and after only about 10 minutes of usage it was steaming hot, I keep a fan blowing at it, I was worried about it overheating. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if I should have gotten a Crucial or PNY equivalent external SSD, or maybe they get just as hot too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17e1ird", "is_robot_indexable": true, "report_reasons": null, "author": "LeoWitt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e1ird/should_the_t7_or_t7_shield_run_cooler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e1ird/should_the_t7_or_t7_shield_run_cooler/", "subreddit_subscribers": 708142, "created_utc": 1698003951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently using Syncthing and the lack of selective sync just ruins it for me.  \nYes, I know there are ignore patterns but they don't do a very good job and for some reason some files I ignore get synced anyway.", "author_fullname": "t2_o6oit", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's everyone's opinion on Resilio?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17egy2h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698056023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using Syncthing and the lack of selective sync just ruins it for me.&lt;br/&gt;\nYes, I know there are ignore patterns but they don&amp;#39;t do a very good job and for some reason some files I ignore get synced anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17egy2h", "is_robot_indexable": true, "report_reasons": null, "author": "etay080", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17egy2h/whats_everyones_opinion_on_resilio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17egy2h/whats_everyones_opinion_on_resilio/", "subreddit_subscribers": 708142, "created_utc": 1698056023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone know where a good place to archive these is? I have them in the same .app format that Apple did back in 2021/2022", "author_fullname": "t2_ilut4bvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MacOS Monterey and Ventura Dev Beta 1", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eeinp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698045047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone know where a good place to archive these is? I have them in the same .app format that Apple did back in 2021/2022&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17eeinp", "is_robot_indexable": true, "report_reasons": null, "author": "CosmicEternityCD", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17eeinp/macos_monterey_and_ventura_dev_beta_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17eeinp/macos_monterey_and_ventura_dev_beta_1/", "subreddit_subscribers": 708142, "created_utc": 1698045047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "tl;dr is in the title already...\n\n- My family has Windows machines and I need to backup them. \n- The most frequently recommended tool is Veeam.\n- The backup target I am trying to use is a Hetzner storage box\n- Veeam \"free community edition for Windows\" can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.\n- However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). \n\nSeems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.\n\nAm I missing something? Has Samba become more secure in the recent years?", "author_fullname": "t2_vqur7zf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows Machines Backup - everyone recommends Veeam - but the only protocol available is Samba, should I use this over the open internet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dwmpz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697990645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;tl;dr is in the title already...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My family has Windows machines and I need to backup them. &lt;/li&gt;\n&lt;li&gt;The most frequently recommended tool is Veeam.&lt;/li&gt;\n&lt;li&gt;The backup target I am trying to use is a Hetzner storage box&lt;/li&gt;\n&lt;li&gt;Veeam &amp;quot;free community edition for Windows&amp;quot; can only do backups to local machine, Samba or its proprietary server. My storage box can do Samba.&lt;/li&gt;\n&lt;li&gt;However, I am confused about this - I have always been told that Samba is not secure for use on the open internet (and every source I found online also says that; however most of those discussions are several years old). &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Seems this Samba setup would always make sense if I use a local server - or - add a server with VPN on the Hetzner side (they are offering this as a packaged solution) and switch the storage box to local network mode.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? Has Samba become more secure in the recent years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dwmpz", "is_robot_indexable": true, "report_reasons": null, "author": "AlpineGuy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dwmpz/windows_machines_backup_everyone_recommends_veeam/", "subreddit_subscribers": 708142, "created_utc": 1697990645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Update**: Looks like czkawka is that magical app. Still using it but few things I wanted to achieve resulted in exactly what I wanted it. Checking now whether I can see preview for similar images or not.\n\nI have some folders on my Mac which has duplicate files among them (sometimes whole folders). \n\ne.g. Folder A has 40GB and B has 57GB and there might be some overlapping data. \n\nor there could also be C of 7GB and then maybe D of 50GB and they could also have some overlapping files.\n\n- How can I control this as in make sure there are simply no duplicate files?\n- How can I ensure that in one \"du-dup\" operation I decide that e.g. any duplicates other than A (assume that is my base/reference folder) should simply be deleted?\n  - I am fine with doing it multiple times as long as every time I have a base/ref folder I can delete dupes from everywhere else.\n- How can I do this at batch level? \n- Any tools or apps you recommend - preferably GUI? (I just know of dupeguru but last time I used it it was not very intuitive)", "author_fullname": "t2_ijrtwri24", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manage duplicate files across multiple folders on a Mac?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dtrsa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698054737.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697982732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Looks like czkawka is that magical app. Still using it but few things I wanted to achieve resulted in exactly what I wanted it. Checking now whether I can see preview for similar images or not.&lt;/p&gt;\n\n&lt;p&gt;I have some folders on my Mac which has duplicate files among them (sometimes whole folders). &lt;/p&gt;\n\n&lt;p&gt;e.g. Folder A has 40GB and B has 57GB and there might be some overlapping data. &lt;/p&gt;\n\n&lt;p&gt;or there could also be C of 7GB and then maybe D of 50GB and they could also have some overlapping files.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How can I control this as in make sure there are simply no duplicate files?&lt;/li&gt;\n&lt;li&gt;How can I ensure that in one &amp;quot;du-dup&amp;quot; operation I decide that e.g. any duplicates other than A (assume that is my base/reference folder) should simply be deleted?\n\n&lt;ul&gt;\n&lt;li&gt;I am fine with doing it multiple times as long as every time I have a base/ref folder I can delete dupes from everywhere else.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;How can I do this at batch level? &lt;/li&gt;\n&lt;li&gt;Any tools or apps you recommend - preferably GUI? (I just know of dupeguru but last time I used it it was not very intuitive)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dtrsa", "is_robot_indexable": true, "report_reasons": null, "author": "No-Anybody-692", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dtrsa/how_to_manage_duplicate_files_across_multiple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dtrsa/how_to_manage_duplicate_files_across_multiple/", "subreddit_subscribers": 708142, "created_utc": 1697982732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I'm looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a \"drive\" or a \"sync\" feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?", "author_fullname": "t2_sh0pb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Cloud Storage Service with Shared Access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dss1a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697979728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any services designed for personal used that offer shared access to folders, similar to Egynte or box.com?  I&amp;#39;m looking for something for my wife and I, we would both have accounts, where shared folders could be accessed via a &amp;quot;drive&amp;quot; or a &amp;quot;sync&amp;quot; feature.  I think we would be looking at $45 per month with box.com, which is a lot more than we planned on paying.  Can anyone make a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dss1a", "is_robot_indexable": true, "report_reasons": null, "author": "WeWillFigureItOut", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dss1a/personal_cloud_storage_service_with_shared_access/", "subreddit_subscribers": 708142, "created_utc": 1697979728.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm gonna keep this as blunt as I can. I have reason to believe that a Twitter account is going to be deleted soon, naturally I want to save its data. What's the quickest way I can go about this? Time is of the essence and I want everything, tweets, pictures, videos. ", "author_fullname": "t2_kff4gv4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quickest way to archive a Twitter account?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eh6dl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698056956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m gonna keep this as blunt as I can. I have reason to believe that a Twitter account is going to be deleted soon, naturally I want to save its data. What&amp;#39;s the quickest way I can go about this? Time is of the essence and I want everything, tweets, pictures, videos. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17eh6dl", "is_robot_indexable": true, "report_reasons": null, "author": "7LayeredUp", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17eh6dl/quickest_way_to_archive_a_twitter_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17eh6dl/quickest_way_to_archive_a_twitter_account/", "subreddit_subscribers": 708142, "created_utc": 1698056956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there an existing tool out there that will allow me to summarize all of my liked/saved posts?", "author_fullname": "t2_7ayx50k9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool that summarizes my saved/liked posts on different social media sites (fb, twitter, tiktok, etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e2ohd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698007082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an existing tool out there that will allow me to summarize all of my liked/saved posts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e2ohd", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy_Sherbert9151", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e2ohd/tool_that_summarizes_my_savedliked_posts_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e2ohd/tool_that_summarizes_my_savedliked_posts_on/", "subreddit_subscribers": 708142, "created_utc": 1698007082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nI recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  \n\n\nIn specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.\n\n&amp;#x200B;\n\nNow my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don't touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?\n\n&amp;#x200B;\n\nResources for SSD data loss:\n\n[https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will\\_ssd\\_lose\\_data\\_if\\_left\\_unpowered\\_for\\_extended/](https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/)\n\n[https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd\\_lose\\_data\\_wo\\_power\\_in\\_a\\_year\\_myth\\_or\\_truth/](https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/)\n\n[https://www.reddit.com/r/DataHoarder/comments/10fn86x/do\\_ssd\\_drives\\_really\\_lose\\_data\\_if\\_left\\_unpowered/](https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/)\n\n  \nThanks a lot for your expert opinion.", "author_fullname": "t2_491r3ws", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you mitigate/avoid SSD data loss?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dzg27", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697998306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently read that with an SSD compared to HDD there is a risk of data loss, if the SSD is not powered on for some time and the data is kind of re-written to the storage (also highly depending on the temperature when the data was written and the temperature the ssd drive is stored).  &lt;/p&gt;\n\n&lt;p&gt;In specific I have the Samsung T7 shield with 4TB and my plan was to use this as my primary place where the data is stored. A 1:1 copy with rsync is done to a WD 4TB and in parallel a copy of the original data is made to an online server with restic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now my question is: How can I ensure that my primary data is still considered fully working and reliable if I use this as the basis for all my future backups? Especially how is it if some of the data stored I don&amp;#39;t touch for many years, even if the SSD is plugged in (e.g. pictures of my childhood I may not touch for 5 years, even if the SSD is plugged in and other files are changed)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Resources for SSD data loss:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/\"&gt;https://www.reddit.com/r/DataHoarder/comments/ba8o0b/will_ssd_lose_data_if_left_unpowered_for_extended/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/\"&gt;https://www.reddit.com/r/DataHoarder/comments/nyor7h/ssd_lose_data_wo_power_in_a_year_myth_or_truth/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/\"&gt;https://www.reddit.com/r/DataHoarder/comments/10fn86x/do_ssd_drives_really_lose_data_if_left_unpowered/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for your expert opinion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dzg27", "is_robot_indexable": true, "report_reasons": null, "author": "ghac101", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dzg27/how_do_you_mitigateavoid_ssd_data_loss/", "subreddit_subscribers": 708142, "created_utc": 1697998306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.\n\nI initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.", "author_fullname": "t2_1gvgwz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video files on USB not opening", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dy2zb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697994645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve had something strange happen.  I transferred some video files onto a usb stick recently.  One is a 50 ep series, one movie in mp4 format and a vlc file movie.  The strange thing is that mp4 files of the 50 episodes is working perfectly fine, but the movie files aren\u2019t opening at all. VLC window opens but it\u2019s a blank.  It will only open on the original location on the desktop.  Same goes for the mp4 file of the movie on the USB.  Stranger is that the QuickTime Player doesn\u2019t recognize the file, saying it isn\u2019t compatible, even it\u2019s fine with the original file.&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was because the usb is the Fat32 format, which has a limit on 4gb files, but the usb format is exFat, which doesn\u2019t have that problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dy2zb", "is_robot_indexable": true, "report_reasons": null, "author": "Parker813", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dy2zb/video_files_on_usb_not_opening/", "subreddit_subscribers": 708142, "created_utc": 1697994645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried Googling and not much can be seen on the ES18's passthrough capability.", "author_fullname": "t2_2ufw7ltl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the DMR-ES18 similar to the ES15?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17dv6sb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697986768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried Googling and not much can be seen on the ES18&amp;#39;s passthrough capability.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17dv6sb", "is_robot_indexable": true, "report_reasons": null, "author": "Pandaemonae0n", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17dv6sb/is_the_dmres18_similar_to_the_es15/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17dv6sb/is_the_dmres18_similar_to_the_es15/", "subreddit_subscribers": 708142, "created_utc": 1697986768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had guided sent to me when I queried  how to digitize the mini-DVs, though this particular model wasn't listed.\n\nI just want to double check with people more clued up on this as to whether there's something obvious or not-so that I could be missing here.\n\nI've found a really good secondhand deal on this camera so want to check before I buy.", "author_fullname": "t2_6cr2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any issues with using SAMSUNG VP-D362 video camera to transfer mini-dvs to PC at best quality (for source)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e5hcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698014525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had guided sent to me when I queried  how to digitize the mini-DVs, though this particular model wasn&amp;#39;t listed.&lt;/p&gt;\n\n&lt;p&gt;I just want to double check with people more clued up on this as to whether there&amp;#39;s something obvious or not-so that I could be missing here.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found a really good secondhand deal on this camera so want to check before I buy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17e5hcw", "is_robot_indexable": true, "report_reasons": null, "author": "JiggaRob", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17e5hcw/any_issues_with_using_samsung_vpd362_video_camera/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17e5hcw/any_issues_with_using_samsung_vpd362_video_camera/", "subreddit_subscribers": 708142, "created_utc": 1698014525.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}