{"kind": "Listing", "data": {"after": "t3_17ee6ao", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I see a lot of people posting here either venting because they are going through a tough time, seeking advice on a situation that they're facing or something they are confused on, a particular experience they've had in this industry... And you get these people that have a chip on their shoulder coming out of the woodwork to insult them, talk to them in a condescending way as if they are dumb, inadequate, not good enough, like they should feel bad about themselves.\n\n\nFor example, I saw a post Where someone was expressing how they were really struggling with TensorFlow and the department they were in, there was no training for it, manager expected them to know basically everything there was to know about it and treated them like they were dumb for not being completely brilliant at it. People were coming here and just commenting and ragging on the dude saying that he shouldn't even be a data scientist if he doesn't know TensorFlow, How can he possibly be a data scientist not knowing something so elementary, that it would take him all of 20 minutes to go and Google it, and worst of all, people saying that he sounded like an arrogant prick for being offended for the way his manager had treated him. \n\n\nLike, I'm not expecting this to be a support community, we are not data scientists anonymous or something like that. But like damn, what is wrong with some of these people? Who hurt you? Is your job or life really that miserable you have to come here and rag on other people to make yourself feel better? You think you're doing some good deed for the community?  Some people are legit struggling and others are just getting started in this industry, they haven't been doing this for 10 or 15 years, haven't been a software developer or someone with a PhD in math, like damn", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's sad how toxic the community has been lately here", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eos8w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 152, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 152, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of people posting here either venting because they are going through a tough time, seeking advice on a situation that they&amp;#39;re facing or something they are confused on, a particular experience they&amp;#39;ve had in this industry... And you get these people that have a chip on their shoulder coming out of the woodwork to insult them, talk to them in a condescending way as if they are dumb, inadequate, not good enough, like they should feel bad about themselves.&lt;/p&gt;\n\n&lt;p&gt;For example, I saw a post Where someone was expressing how they were really struggling with TensorFlow and the department they were in, there was no training for it, manager expected them to know basically everything there was to know about it and treated them like they were dumb for not being completely brilliant at it. People were coming here and just commenting and ragging on the dude saying that he shouldn&amp;#39;t even be a data scientist if he doesn&amp;#39;t know TensorFlow, How can he possibly be a data scientist not knowing something so elementary, that it would take him all of 20 minutes to go and Google it, and worst of all, people saying that he sounded like an arrogant prick for being offended for the way his manager had treated him. &lt;/p&gt;\n\n&lt;p&gt;Like, I&amp;#39;m not expecting this to be a support community, we are not data scientists anonymous or something like that. But like damn, what is wrong with some of these people? Who hurt you? Is your job or life really that miserable you have to come here and rag on other people to make yourself feel better? You think you&amp;#39;re doing some good deed for the community?  Some people are legit struggling and others are just getting started in this industry, they haven&amp;#39;t been doing this for 10 or 15 years, haven&amp;#39;t been a software developer or someone with a PhD in math, like damn&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17eos8w", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eos8w/its_sad_how_toxic_the_community_has_been_lately/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eos8w/its_sad_how_toxic_the_community_has_been_lately/", "subreddit_subscribers": 1096498, "created_utc": 1698079102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a director and I feel like I barely do \"Data Science\" any more. My job is mostly about working with engineers and architects to facilitate data collection and data tools (python, spark) for my team. Is this relevant for career advancement or do I need to refocus more on hard skills and learning new stuff.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do fellow Data Science Directors do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17el93s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698070046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a director and I feel like I barely do &amp;quot;Data Science&amp;quot; any more. My job is mostly about working with engineers and architects to facilitate data collection and data tools (python, spark) for my team. Is this relevant for career advancement or do I need to refocus more on hard skills and learning new stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17el93s", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17el93s/what_do_fellow_data_science_directors_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17el93s/what_do_fellow_data_science_directors_do/", "subreddit_subscribers": 1096498, "created_utc": 1698070046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_1yjldyeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas-based library for graphing emotion events with LMs for in-depth sentiment analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17eiqbl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BU0fAANwIMR57EGmmleNI78KoM_wCoeHt1otjTk7UFY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698062621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/hdls4kgrzxvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?auto=webp&amp;s=d3222101a81f485e597fb2eb79cd097d49297aeb", "width": 1150, "height": 768}, "resolutions": [{"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dff2d78d3fd6edaaf19223d754d9fd559bacedc", "width": 108, "height": 72}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83dea6f55fa8676296c8ae66c64117cfd02ae04d", "width": 216, "height": 144}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e37743083741785d5ae0554750102754d503d6a9", "width": 320, "height": 213}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaae51500e62cefb4fe77530e21e7b847fe75f90", "width": 640, "height": 427}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=de8137f2681ebbfae39897b3a8028ab208fa78b5", "width": 960, "height": 641}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da785ad2dbe896d0589134bd0bbb6fc8373ae552", "width": 1080, "height": 721}], "variants": {}, "id": "OFW8OTEOtv794Im1BEzy9osCxAL3qmYG3gzh1yd1b3o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17eiqbl", "is_robot_indexable": true, "report_reasons": null, "author": "helliun", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eiqbl/pandasbased_library_for_graphing_emotion_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/hdls4kgrzxvb1.jpg", "subreddit_subscribers": 1096498, "created_utc": 1698062621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the non-data scientist tasks that you still do in your data scientist role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17efkcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698049934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17efkcz", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17efkcz/what_are_the_nondata_scientist_tasks_that_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17efkcz/what_are_the_nondata_scientist_tasks_that_you/", "subreddit_subscribers": 1096498, "created_utc": 1698049934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a data scientist looking to solve a problem that you have. My experience is on regressions, classification and scores for credit. Could it be somehing that exist and its expensive, something that it's not out there, etc. Looking to help :)", "author_fullname": "t2_80ydgoh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What problems would you like to be solved?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eafwm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698030264.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698029544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist looking to solve a problem that you have. My experience is on regressions, classification and scores for credit. Could it be somehing that exist and its expensive, something that it&amp;#39;s not out there, etc. Looking to help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17eafwm", "is_robot_indexable": true, "report_reasons": null, "author": "ResponsibleGazelle76", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eafwm/what_problems_would_you_like_to_be_solved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eafwm/what_problems_would_you_like_to_be_solved/", "subreddit_subscribers": 1096498, "created_utc": 1698029544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Not talking folks who work off linux servers or VMs, I'm talking about those of us who work on a linux install running on our local hardware that might also run other things (games, media, etc)\n\nI do all my work through windows (corporate laptop) but sometimes I want to try out toy problems and other things on a personal machine.\n\nI was using Anaconda, but something about the conda shell caused Arch to try to compile system packages within the conda environment and things went haywire.\n\nRolling my own python virtual env just feels like work, and again, I broke my window manager (qtile, runs on python) by setting it up.\n\nNot against going back to Anaconda, but I'm curious what other folks in my situation (daily drive linux on their primary personal machine, on which they also do some data work) do to keep a working data science environment going.", "author_fullname": "t2_rrh4y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Native Linux Users: How do you setup your DS Environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e7m1p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698020773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not talking folks who work off linux servers or VMs, I&amp;#39;m talking about those of us who work on a linux install running on our local hardware that might also run other things (games, media, etc)&lt;/p&gt;\n\n&lt;p&gt;I do all my work through windows (corporate laptop) but sometimes I want to try out toy problems and other things on a personal machine.&lt;/p&gt;\n\n&lt;p&gt;I was using Anaconda, but something about the conda shell caused Arch to try to compile system packages within the conda environment and things went haywire.&lt;/p&gt;\n\n&lt;p&gt;Rolling my own python virtual env just feels like work, and again, I broke my window manager (qtile, runs on python) by setting it up.&lt;/p&gt;\n\n&lt;p&gt;Not against going back to Anaconda, but I&amp;#39;m curious what other folks in my situation (daily drive linux on their primary personal machine, on which they also do some data work) do to keep a working data science environment going.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17e7m1p", "is_robot_indexable": true, "report_reasons": null, "author": "feldomatic", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17e7m1p/native_linux_users_how_do_you_setup_your_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17e7m1p/native_linux_users_how_do_you_setup_your_ds/", "subreddit_subscribers": 1096498, "created_utc": 1698020773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My work primarily stores data in a full databases. Pandas has a lot of similar functionality to SQL in regards to the ability to group data and preform calculations, even being able to take full on SQL queries to import data. Do you guys do all your calculations in the query itself, or in python after the data has been imported? What about with grouping data?", "author_fullname": "t2_2o12zv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do in SQL vs Pandas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eot80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My work primarily stores data in a full databases. Pandas has a lot of similar functionality to SQL in regards to the ability to group data and preform calculations, even being able to take full on SQL queries to import data. Do you guys do all your calculations in the query itself, or in python after the data has been imported? What about with grouping data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17eot80", "is_robot_indexable": true, "report_reasons": null, "author": "Alucard2051", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eot80/what_do_you_do_in_sql_vs_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eot80/what_do_you_do_in_sql_vs_pandas/", "subreddit_subscribers": 1096498, "created_utc": 1698079172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my present company we are just chasing ad hoc analytical  work - these never gets into production. The processes are very ad hoc, not streamlined, no structure to it, running from personal notebooks. It\u2019s very demoralizing to see models developed from 2017 that are in production and have not been refreshed thought the data it used for inference is constantly changing as my company looks at market finance data. \n\nI\u2019m wondering what are other good companies to look out for that are either applying best practices in DS/ML and not just the talk or building product/services. \n\nI understand recent news in GenAI is sparking lot of conversations but which companies out there are grabbing it by the horns and taking the lead? Perhaps if you are fortunate to work for one such company you may want to share your story. Appreciate your insights very much!", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Besides FAANG, what other companies out there are doing actual DS or MLE work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17er8bt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698085365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my present company we are just chasing ad hoc analytical  work - these never gets into production. The processes are very ad hoc, not streamlined, no structure to it, running from personal notebooks. It\u2019s very demoralizing to see models developed from 2017 that are in production and have not been refreshed thought the data it used for inference is constantly changing as my company looks at market finance data. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what are other good companies to look out for that are either applying best practices in DS/ML and not just the talk or building product/services. &lt;/p&gt;\n\n&lt;p&gt;I understand recent news in GenAI is sparking lot of conversations but which companies out there are grabbing it by the horns and taking the lead? Perhaps if you are fortunate to work for one such company you may want to share your story. Appreciate your insights very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17er8bt", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17er8bt/besides_faang_what_other_companies_out_there_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17er8bt/besides_faang_what_other_companies_out_there_are/", "subreddit_subscribers": 1096498, "created_utc": 1698085365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I started to get frequent requests from marketing to do a quick hypothesis test and produce a single visual for a post related to a topic illustrating the findings.  \n\n\nWhat seemed like a quick Jupyter Notebook task quickly turned into:\n\nax.set\\_ylabels(fontsize='fuck me I am a designer now', linestyle='8====D')\n\nHow the hell do you produce visually appealing marketing-ready visuals with Python?  \n\n\nI checked all of these (Altair, plotly, ggplot, Pygal, Matplotlib, Seaborn, Bokeh) and they all are either web interaction oriented or science-oriented which means shitloads of fine-tuning to get the charts to look decent.", "author_fullname": "t2_13hucc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use for non-scientific data visualizations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ekq26", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698068569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started to get frequent requests from marketing to do a quick hypothesis test and produce a single visual for a post related to a topic illustrating the findings.  &lt;/p&gt;\n\n&lt;p&gt;What seemed like a quick Jupyter Notebook task quickly turned into:&lt;/p&gt;\n\n&lt;p&gt;ax.set_ylabels(fontsize=&amp;#39;fuck me I am a designer now&amp;#39;, linestyle=&amp;#39;8====D&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;How the hell do you produce visually appealing marketing-ready visuals with Python?  &lt;/p&gt;\n\n&lt;p&gt;I checked all of these (Altair, plotly, ggplot, Pygal, Matplotlib, Seaborn, Bokeh) and they all are either web interaction oriented or science-oriented which means shitloads of fine-tuning to get the charts to look decent.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17ekq26", "is_robot_indexable": true, "report_reasons": null, "author": "every_other_freackle", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ekq26/what_do_you_use_for_nonscientific_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ekq26/what_do_you_use_for_nonscientific_data/", "subreddit_subscribers": 1096498, "created_utc": 1698068569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Happy monday guys! \n\nQuick question \u2013 what do you do on light days where you don\u2019t have much(or any) work and want to maintain your productivity, especially when working from home? \n\nI would love to increase my theory/stress on learning new skills! So if you\u2019re one who reads books/blogs would love to know what you guys read or any book recommendations\n\nCheers guys, have a great week!", "author_fullname": "t2_7xxtza3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productivity help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eh6kb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698056978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy monday guys! &lt;/p&gt;\n\n&lt;p&gt;Quick question \u2013 what do you do on light days where you don\u2019t have much(or any) work and want to maintain your productivity, especially when working from home? &lt;/p&gt;\n\n&lt;p&gt;I would love to increase my theory/stress on learning new skills! So if you\u2019re one who reads books/blogs would love to know what you guys read or any book recommendations&lt;/p&gt;\n\n&lt;p&gt;Cheers guys, have a great week!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17eh6kb", "is_robot_indexable": true, "report_reasons": null, "author": "Asleep-Fun-6508", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eh6kb/productivity_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eh6kb/productivity_help/", "subreddit_subscribers": 1096498, "created_utc": 1698056978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a data scientist working for a healthcare company in Michigan with 5 years of experience (6 months in US rest in India). I make 90k pa. I'm interviewing with a mortgage company for Senior Data Scientist position. How much pay increase should I ask for? I am also interviewing for a couple more companies. Whats the ballpark number I should ask for every time  they ask me this question?", "author_fullname": "t2_j20e59lh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expected salary for a senior data scientist..", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17esfjq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698088397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data scientist working for a healthcare company in Michigan with 5 years of experience (6 months in US rest in India). I make 90k pa. I&amp;#39;m interviewing with a mortgage company for Senior Data Scientist position. How much pay increase should I ask for? I am also interviewing for a couple more companies. Whats the ballpark number I should ask for every time  they ask me this question?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17esfjq", "is_robot_indexable": true, "report_reasons": null, "author": "AdMediocre3090", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17esfjq/expected_salary_for_a_senior_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17esfjq/expected_salary_for_a_senior_data_scientist/", "subreddit_subscribers": 1096498, "created_utc": 1698088397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I need to explore an odd problem. We have an old dataset of interview sessions (its not our dataset). It works as follows.\n\nThe candidate comes in, goes through several rounds of interviews (from 1 - 5) each with its own interviewer. (We know the number of interviewers)\n\nAfter each round, the candidate rates the interviewer (score from 0 to 5). (We do not have this data)\n\nFinally, an overall score is calculated for the entire interview session based on the ratings for each round. (We know the overall score but we do not know how it was calculated)\n\nSo essentially, the dataset is roughly off the form:\n\nsession_id, score, [interviewer_id1, interviewer_id2, interviewer_id3 ...] (This list is unordered)\n\nThe question is: given a particular interviewer_id, is it possible to determine whether he generally got positive or negative ratings?\n\nFor context, I write software and don't know much beyond stats 101 so I would appreciate any and all pointers. I would ordinarily say no to the above question but I have met people who've been able to pull signals out of noise so it behoves me to ask.\n\nThanks.", "author_fullname": "t2_rz22aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wondering whether the following problem is workable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eirhz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I need to explore an odd problem. We have an old dataset of interview sessions (its not our dataset). It works as follows.&lt;/p&gt;\n\n&lt;p&gt;The candidate comes in, goes through several rounds of interviews (from 1 - 5) each with its own interviewer. (We know the number of interviewers)&lt;/p&gt;\n\n&lt;p&gt;After each round, the candidate rates the interviewer (score from 0 to 5). (We do not have this data)&lt;/p&gt;\n\n&lt;p&gt;Finally, an overall score is calculated for the entire interview session based on the ratings for each round. (We know the overall score but we do not know how it was calculated)&lt;/p&gt;\n\n&lt;p&gt;So essentially, the dataset is roughly off the form:&lt;/p&gt;\n\n&lt;p&gt;session_id, score, [interviewer_id1, interviewer_id2, interviewer_id3 ...] (This list is unordered)&lt;/p&gt;\n\n&lt;p&gt;The question is: given a particular interviewer_id, is it possible to determine whether he generally got positive or negative ratings?&lt;/p&gt;\n\n&lt;p&gt;For context, I write software and don&amp;#39;t know much beyond stats 101 so I would appreciate any and all pointers. I would ordinarily say no to the above question but I have met people who&amp;#39;ve been able to pull signals out of noise so it behoves me to ask.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17eirhz", "is_robot_indexable": true, "report_reasons": null, "author": "grchelp2018", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eirhz/wondering_whether_the_following_problem_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eirhz/wondering_whether_the_following_problem_is/", "subreddit_subscribers": 1096498, "created_utc": 1698062724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 23 Oct, 2023 - 30 Oct, 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eboh0", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698033686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eboh0", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eboh0/weekly_entering_transitioning_thread_23_oct_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/17eboh0/weekly_entering_transitioning_thread_23_oct_2023/", "subreddit_subscribers": 1096498, "created_utc": 1698033686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a final round interview in a few days and apparently I will be asked a Data Structures problem. Should I be reviewing some basic trees, queues, heaps , hash maps ,etc. like I would for a leetcode-type swe interview , or are there any other  niche data structures I should be prepped to use . This is my first time interviewing for this type of role, so any help would be appreciated!", "author_fullname": "t2_67jqs4r8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to expect for Data Structures Technical Interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17etbst", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Coding", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a final round interview in a few days and apparently I will be asked a Data Structures problem. Should I be reviewing some basic trees, queues, heaps , hash maps ,etc. like I would for a leetcode-type swe interview , or are there any other  niche data structures I should be prepped to use . This is my first time interviewing for this type of role, so any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4ab9c418-70eb-11ee-8a37-4a495429ae82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17etbst", "is_robot_indexable": true, "report_reasons": null, "author": "Imaginary-Ad1964", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17etbst/what_to_expect_for_data_structures_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17etbst/what_to_expect_for_data_structures_technical/", "subreddit_subscribers": 1096498, "created_utc": 1698090676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have talked this previously, that like, I am working as a data analyst but is it worth to learn graph database. I got some comments that saying master SQL first, then learn other tools. For me, learning a new fun tool is for my free time so I thought, OK, I will just try it. It is been a month almost and came back to think like,,, I don't feel the graph database is that much worth to learn especially if I consider the size of the market.\n\nHowever, maybe, if there's a PG extension that adds graph analytics to PG database, which I use everyday, it would be fun because I can actually utilize it with my PG data. Apache AGE is an open-source PG extension that really solves the problem that I'm having right now. I will leave the [github link](https://github.com/apache/age) and a [webinar link](https://us06web.zoom.us/webinar/register/2516980853755/WN_mzhlCggCQ_ytIxiGb9ioTg) that they (I guess Apache Foundation?) organize like bi-weekly. For those who are having same thought process with me, I think you guys also can just try? What do you think?", "author_fullname": "t2_59z60tud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PG extension (Apache AGE) for adding graph analytics functionality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eriso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698086112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have talked this previously, that like, I am working as a data analyst but is it worth to learn graph database. I got some comments that saying master SQL first, then learn other tools. For me, learning a new fun tool is for my free time so I thought, OK, I will just try it. It is been a month almost and came back to think like,,, I don&amp;#39;t feel the graph database is that much worth to learn especially if I consider the size of the market.&lt;/p&gt;\n\n&lt;p&gt;However, maybe, if there&amp;#39;s a PG extension that adds graph analytics to PG database, which I use everyday, it would be fun because I can actually utilize it with my PG data. Apache AGE is an open-source PG extension that really solves the problem that I&amp;#39;m having right now. I will leave the &lt;a href=\"https://github.com/apache/age\"&gt;github link&lt;/a&gt; and a &lt;a href=\"https://us06web.zoom.us/webinar/register/2516980853755/WN_mzhlCggCQ_ytIxiGb9ioTg\"&gt;webinar link&lt;/a&gt; that they (I guess Apache Foundation?) organize like bi-weekly. For those who are having same thought process with me, I think you guys also can just try? What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?auto=webp&amp;s=a4a9678ee6d4fee3ad114c4e3f4c60bd3a9878b9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c61ccf87e96ae7f1e62a23e0d3e0edaf1e9231ad", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c3d8c63135c41a9e9b540db64704378fc6d327a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc77ff3491686199af775da523847897c7960e45", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ced5be128d162387dc23847553096ba7a82d98e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=280b511ee72232f7f9af49618992ae1faf180cb0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=796899fab0653df124ea60ce9b1a3e08173916ad", "width": 1080, "height": 540}], "variants": {}, "id": "SJvZ9bFaaSdT-WOtIPVpyQVNRTxT1PIoiuA3-IrbPYc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17eriso", "is_robot_indexable": true, "report_reasons": null, "author": "oh5oh5", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eriso/pg_extension_apache_age_for_adding_graph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eriso/pg_extension_apache_age_for_adding_graph/", "subreddit_subscribers": 1096498, "created_utc": 1698086112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking to discover new relationships that exist in the relational database and then generate ingestion script to populate a graph database. Are there tools already exhausting for this and what are their limitations? Can we he new LLMs come to rescue?", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational database to graph database using NLP/LLM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17erhca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698086005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to discover new relationships that exist in the relational database and then generate ingestion script to populate a graph database. Are there tools already exhausting for this and what are their limitations? Can we he new LLMs come to rescue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17erhca", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17erhca/relational_database_to_graph_database_using_nlpllm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17erhca/relational_database_to_graph_database_using_nlpllm/", "subreddit_subscribers": 1096498, "created_utc": 1698086005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a text corpus of tweets written by users in a four month period, and I computed the sentiment of all words written by the same user in my data. I then calculated the share of negative words written by individual A across across all words written by individual A. Ex: individual A wrote 3000 words, and 1000 of which were classified as negative, then individual A's negative share would be \\~ 33%.\n\n I am then comparing how the share of negative words for each username before and after a certain event occurs, but I am not sure how to besd visualize this type of data using R.\n\nHere is a data example:\n\n\\`\\`\\`\n\ndput(sentiment\\_twitter\\[1186:1194,c(1,2,3)\\])\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nLooking at the output below, we can learn that the \"negative\\_word\\_share\" for id 1194 has increased after the \"treatment\\_implementation\", which refers to the event of interest.\n\nThe challenge here is that I have \\~3K observations, and I am not sure what's the best way to visualize this type of granular data?\n\n\\`\\`\\`\n\nstructure(list(id = c(912L, 912L, 913L, 914L, 915L, 916L, 917L, \n\n918L, 919L), treatment\\_implementation = c(0, 1, 1, 0, 0, 1, 1, \n\n1, 0), negative\\_word\\_share = c(20, 49.4252873563218, 0, 60, 50, \n\n0, 100, 88.8888888888889, 0)), class = c(\"grouped\\_df\", \"tbl\\_df\", \n\n\"tbl\", \"data.frame\"), row.names = c(NA, -9L), groups = structure(list(\n\nid = 912:919, .rows = structure(list(1:2, 3L, 4L, 5L, 6L, \n\n7L, 8L, 9L), ptype = integer(0), class = c(\"vctrs\\_list\\_of\", \n\n\"vctrs\\_vctr\", \"list\"))), class = c(\"tbl\\_df\", \"tbl\", \"data.frame\"\n\n), row.names = c(NA, -8L), .drop = TRUE))\n\n\\`\\`\\`", "author_fullname": "t2_hz4cyqnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for visualizing user-level sentiment before and after an event", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enpwe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a text corpus of tweets written by users in a four month period, and I computed the sentiment of all words written by the same user in my data. I then calculated the share of negative words written by individual A across across all words written by individual A. Ex: individual A wrote 3000 words, and 1000 of which were classified as negative, then individual A&amp;#39;s negative share would be ~ 33%.&lt;/p&gt;\n\n&lt;p&gt;I am then comparing how the share of negative words for each username before and after a certain event occurs, but I am not sure how to besd visualize this type of data using R.&lt;/p&gt;\n\n&lt;p&gt;Here is a data example:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;dput(sentiment_twitter[1186:1194,c(1,2,3)])&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Looking at the output below, we can learn that the &amp;quot;negative_word_share&amp;quot; for id 1194 has increased after the &amp;quot;treatment_implementation&amp;quot;, which refers to the event of interest.&lt;/p&gt;\n\n&lt;p&gt;The challenge here is that I have ~3K observations, and I am not sure what&amp;#39;s the best way to visualize this type of granular data?&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;structure(list(id = c(912L, 912L, 913L, 914L, 915L, 916L, 917L, &lt;/p&gt;\n\n&lt;p&gt;918L, 919L), treatment_implementation = c(0, 1, 1, 0, 0, 1, 1, &lt;/p&gt;\n\n&lt;p&gt;1, 0), negative_word_share = c(20, 49.4252873563218, 0, 60, 50, &lt;/p&gt;\n\n&lt;p&gt;0, 100, 88.8888888888889, 0)), class = c(&amp;quot;grouped_df&amp;quot;, &amp;quot;tbl_df&amp;quot;, &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;tbl&amp;quot;, &amp;quot;data.frame&amp;quot;), row.names = c(NA, -9L), groups = structure(list(&lt;/p&gt;\n\n&lt;p&gt;id = 912:919, .rows = structure(list(1:2, 3L, 4L, 5L, 6L, &lt;/p&gt;\n\n&lt;p&gt;7L, 8L, 9L), ptype = integer(0), class = c(&amp;quot;vctrs_list_of&amp;quot;, &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;vctrs_vctr&amp;quot;, &amp;quot;list&amp;quot;))), class = c(&amp;quot;tbl_df&amp;quot;, &amp;quot;tbl&amp;quot;, &amp;quot;data.frame&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;), row.names = c(NA, -8L), .drop = TRUE))&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17enpwe", "is_robot_indexable": true, "report_reasons": null, "author": "nesta1970", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17enpwe/ideas_for_visualizing_userlevel_sentiment_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17enpwe/ideas_for_visualizing_userlevel_sentiment_before/", "subreddit_subscribers": 1096498, "created_utc": 1698076456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi i am working on a i got this freelance project and its a module of a huge project where i have to write code parse address provided.\n\nI was first using Libpostal but the for provided data libpostal is not effiecient and i want to create my custom parsing.\n\nI am trying to use regex but it seems very complicated. Can anyone help me if there\u2019s any other way .\n\nI found it is possible using NLP with spaCy.\n\nPlease guide", "author_fullname": "t2_jprjjprw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Address parsing with NLP or with regex", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enm4q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi i am working on a i got this freelance project and its a module of a huge project where i have to write code parse address provided.&lt;/p&gt;\n\n&lt;p&gt;I was first using Libpostal but the for provided data libpostal is not effiecient and i want to create my custom parsing.&lt;/p&gt;\n\n&lt;p&gt;I am trying to use regex but it seems very complicated. Can anyone help me if there\u2019s any other way .&lt;/p&gt;\n\n&lt;p&gt;I found it is possible using NLP with spaCy.&lt;/p&gt;\n\n&lt;p&gt;Please guide&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17enm4q", "is_robot_indexable": true, "report_reasons": null, "author": "Pristine-Sound-484", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17enm4q/address_parsing_with_nlp_or_with_regex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17enm4q/address_parsing_with_nlp_or_with_regex/", "subreddit_subscribers": 1096498, "created_utc": 1698076188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nGetting back to DS and I'm updating my portfolio with new  projects, any one available for some mentorship or career advices please ? ", "author_fullname": "t2_fd92duw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mentorship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17en7as", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698075122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Getting back to DS and I&amp;#39;m updating my portfolio with new  projects, any one available for some mentorship or career advices please ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17en7as", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-School-07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17en7as/mentorship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17en7as/mentorship/", "subreddit_subscribers": 1096498, "created_utc": 1698075122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI'm updating my Portfolio to get back to DS. Working on a project I'd like to put the algorithm into an interface. Is it better to try and do it using other programming languages like JavaScript or Python is sufficient using Flask or Streamlit ? ", "author_fullname": "t2_fd92duw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Interface.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17en64n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698075041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m updating my Portfolio to get back to DS. Working on a project I&amp;#39;d like to put the algorithm into an interface. Is it better to try and do it using other programming languages like JavaScript or Python is sufficient using Flask or Streamlit ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17en64n", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-School-07", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17en64n/project_interface/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17en64n/project_interface/", "subreddit_subscribers": 1096498, "created_utc": 1698075041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a software engineer with 10yo experience. Can someone recommend a good Data Science program? I am willing to spend 3-6 months to get a deep understanding of the fundamentals.", "author_fullname": "t2_42tqv1mr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Data Science Program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17em9tn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698072836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a software engineer with 10yo experience. Can someone recommend a good Data Science program? I am willing to spend 3-6 months to get a deep understanding of the fundamentals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17em9tn", "is_robot_indexable": true, "report_reasons": null, "author": "h3dgyy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17em9tn/looking_for_a_data_science_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17em9tn/looking_for_a_data_science_program/", "subreddit_subscribers": 1096498, "created_utc": 1698072836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_6do37ngf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need your suggestions how build Knowledge graphs from given pdf file(more than 50 pages). Also able to perform inferencing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17em9b7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "AI", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698072794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2f731e52-70eb-11ee-bec5-5a5142e6a4d2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "17em9b7", "is_robot_indexable": true, "report_reasons": null, "author": "G7gamingarena", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17em9b7/i_need_your_suggestions_how_build_knowledge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17em9b7/i_need_your_suggestions_how_build_knowledge/", "subreddit_subscribers": 1096498, "created_utc": 1698072794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "# A real-world case study of performance optimization in Numpy\n\nThis article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/).\n\n&amp;#x200B;\n\n[ How to Optimize Multidimensional Numpy Array Operations with Numexpr. Photo Credit: Created by Author, Canva. ](https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;format=png&amp;auto=webp&amp;s=ab8950800797f55f538fdb1343df6d275bd07152)\n\nThis is a relatively brief article. In it, I will use a real-world scenario as an example to explain how to use [Numexpr expressions](https://numexpr.readthedocs.io/en/latest/user_guide.html?ref=dataleadsfuture.com#supported-functions) in multidimensional Numpy arrays to achieve substantial performance improvements.\n\nThere aren't many articles explaining how to use Numexpr in multidimensional Numpy arrays and how to use Numexpr expressions, so I hope this one will help you.\n\n# Introduction\n\nRecently, while reviewing some of my old work, I stumbled upon this piece of code:\n\n    def predict(X, w, b):\n        z = np.dot(X, w)\n        y_hat = sigmoid(z)\n        y_pred = np.zeros((y_hat.shape[0], 1))\n    \n        for i in range(y_hat.shape[0]):\n            if y_hat[i, 0] &lt; 0.5:\n                y_pred[i, 0] = 0\n            else:\n                y_pred[i, 0] = 1\n        return y_pred\n\nThis code transforms prediction results from probabilities to classification results of 0 or 1 in the logistic regression model of machine learning.\n\nBut heavens, who would use a `for loop` to iterate over Numpy ndarray?\n\nYou can foresee that when the data reaches a certain amount, it will not only occupy a lot of memory, but the performance will also be inferior.\n\nThat's right, the person who wrote this code was me when I was younger.\n\nWith a sense of responsibility, I plan to rewrite this code with the Numexpr library today.\n\nAlong the way, I will show you how to use Numexpr and Numexpr's `where` expression in multidimensional Numpy arrays to achieve significant performance improvements.\n\n## Code Implementation\n\nIf you are not familiar with the basic usage of Numexpr, you can refer to this article:\n\n[https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/)\n\nThis article uses a real-world example to demonstrate the specific usage of Numexpr's API and expressions in Numpy and Pandas.\n\n*where(bool, number1, number2): number* *- number1 if the bool condition is true, number2 otherwise.*\n\nThe above is the usage of the where expression in Numpy.\n\nWhen dealing with matrix data, you may used to using Pandas `DataFrame`. But since the `eval` method of Pandas does not support the `where` expression, you can only choose to use Numexpr in multidimensional Numpy ndarray.\n\nDon't worry, I'll explain it to you right away.\n\nBefore starting, we need to import the necessary packages and implement a `generate_ndarray` method to generate a specific size ndarray for testing:\n\n    from typing import Callable\n    import time\n    \n    import numpy as np\n    import numexpr as ne\n    import matplotlib.pyplot as plt\n    \n    rng = np.random.default_rng(seed=4000)\n    \n    def generate_ndarray(rows: int) -&gt; np.ndarray:\n        result_array = rng.random((rows, 1))\n        return result_array\n\nFirst, we generate a matrix of 200 rows to see if it is the test data we want:\n\n    In:  arr = generate_ndarray(200)\n         print(f\"The dimension of this array: {arr.ndim}\")\n         print(f\"The shape of this array: {arr.shape}\")\n    \n    \n    Out: The dimension of this array: 2\n         The shape of this array: (200, 1)\n\nTo be close to the actual situation of the logistic regression model, we generate an ndarray of the shape `(200, 1)` Of course, you can also test other shapes of ndarray according to your needs.\n\nThen, we start writing the specific use of Numexpr in the `numexpr_to_binary` method:\n\n* First, we use the index to separate the columns that need to be processed.\n* Then, use the where expression of Numexpr to process the values.\n* Finally, merge the processed columns with other columns to generate the required results.\n\nSince the ndarray's shape here is `(200, 1)`, there is only one column, so I add a new dimension.\n\nThe code is as follows:\n\n    def numexpr_to_binary(np_array: np.ndarray) -&gt; np.ndarray:\n        temp = np_array[:, 0]\n        temp = ne.evaluate(\"where(temp&lt;0.5, 0, 1)\")\n        return temp[:, np.newaxis]\n\nWe can test the result with an array of 10 rows to see if it is what I want:\n\n    arr = generate_ndarray(10)\n    result = numexpr_to_binary(arr)\n    \n    mapping = np.column_stack((arr, result))\n    mapping\n\n[ I test an array of 10 rows and the result is what I want. Image by Author ](https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;format=png&amp;auto=webp&amp;s=d6977cc422be66a8c37980554c1478e76d2d326c)\n\nLook, the match is correct. Our task is completed.\n\nThe entire process can be demonstrated with the following figure:\n\n&amp;#x200B;\n\n[ The entire process of how Numexpr transforms the multidimensional ndarray. Image by Author ](https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;format=png&amp;auto=webp&amp;s=df44481e44b2dc48b3acd522b51327b9030e2335)\n\n## Performance Comparison\n\nAfter the code implementation, we need to compare the Numexpr implementation version with the previous `for each` implementation version to confirm that there has been a performance improvement.\n\nFirst, we implement a `numexpr_example` method. This method is based on the implementation of Numexpr:\n\n    def numexpr_example(rows: int) -&gt; np.ndarray:\n        orig_arr = generate_ndarray(rows)\n        the_result = numexpr_to_binary(orig_arr)\n        return the_result\n\nThen, we need to supplement a `for_loop_example` method. This method refers to the original code I need to rewrite and is used as a performance benchmark:\n\n    def for_loop_example(rows: int) -&gt; np.ndarray:\n        the_arr = generate_ndarray(rows)\n        for i in range(the_arr.shape[0]):\n            if the_arr[i][0] &lt; 0.5:\n                the_arr[i][0] = 0\n            else:\n                the_arr[i][0] = 1\n        return the_arr\n\nThen, I wrote a test method `time_method`. This method will generate data from 10 to 10 to the 9th power rows separately, call the corresponding method, and finally save the time required for different data amounts:\n\n    def time_method(method: Callable):\n        time_dict = dict()\n        for i in range(9):\n            begin = time.perf_counter()\n            rows = 10 ** i\n            method(rows)\n            end = time.perf_counter()\n            time_dict[i] = end - begin\n        return time_dict\n\nWe test the numexpr version and the `for_loop` version separately, and use `matplotlib` to draw the time required for different amounts of data:\n\n    t_m = time_method(for_loop_example)\n    t_m_2 = time_method(numexpr_example)\n    plt.plot(t_m.keys(), t_m.values(), c=\"red\", linestyle=\"solid\")\n    plt.plot(t_m_2.keys(), t_m_2.values(), c=\"green\", linestyle=\"dashed\")\n    plt.legend([\"for loop\", \"numexpr\"])\n    plt.xlabel(\"exponent\")\n    plt.ylabel(\"time\")\n    plt.show()\n\n[ The Numexpr version of the implementation has a huge performance improvement. Image by Author ](https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;format=png&amp;auto=webp&amp;s=d508bddb500f8065c75921c1905f14e414ccf932)\n\nIt can be seen that when the number of rows of data is greater than 10 to the 6th power, the Numexpr version of the implementation has a huge performance improvement.\n\n## Conclusion\n\nAfter explaining the basic usage of Numexpr in the previous article, this article uses a specific example in actual work to explain how to use Numexpr to rewrite existing code to obtain performance improvement.\n\nThis article mainly uses two features of Numexpr:\n\n1. Numexpr allows calculations to be performed in a vectorized manner.\n2. During the calculation of Numexpr, no new arrays will be generated, thereby significantly reducing memory usage.\n\nThank you for reading. If you have other solutions, please feel free to leave a message and discuss them with me.\n\nThis article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/).", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Optimize Multidimensional Numpy Array Operations with Numexpr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"i5trs6h79xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c265d077c76d74f45c5cb125f9355561a7f5e03b"}, {"y": 156, "x": 216, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=543d74bdf357b325082cbe4cb60187df5cd783c0"}, {"y": 231, "x": 320, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed7c629584bc9b778eb0e7d678bb40a828d31a2f"}], "s": {"y": 430, "x": 595, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;format=png&amp;auto=webp&amp;s=d508bddb500f8065c75921c1905f14e414ccf932"}, "id": "i5trs6h79xvb1"}, "aw26lp8q8xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82706ba0390facc4e733375417b45445f7c662a1"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22228f5dad76a4b48ca4e453156a2901bb54b88e"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e786249aa06a8e8f13a6a851ec5fe5987fe8eac"}, {"y": 490, "x": 640, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8af89b6cbada268340bfd9195964ebdbf23bd67"}], "s": {"y": 701, "x": 915, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;format=png&amp;auto=webp&amp;s=df44481e44b2dc48b3acd522b51327b9030e2335"}, "id": "aw26lp8q8xvb1"}, "r24q1n674yvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aecb3555e64f8a91d18098d7a590df334e914973"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4887aa200906b3ad3351f555ed8ffb7cfebb0d3"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=633e830499b83a2b99e2d148d3b7d85b117e41af"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60662b1da6eee8d758e63f4038d3f808ff3fac73"}, {"y": 639, "x": 960, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55e3bee81e42e0c9aa309b45c5b16ba342f13c06"}, {"y": 719, "x": 1080, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b865d7cab7c9cf76cb713eef7c45433ff49d312"}], "s": {"y": 924, "x": 1387, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;format=png&amp;auto=webp&amp;s=ab8950800797f55f538fdb1343df6d275bd07152"}, "id": "r24q1n674yvb1"}, "o1h5bwdn8xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 73, "x": 108, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4122e67f9a4134bc9caf07a853c9778f36b525c"}, {"y": 146, "x": 216, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d2086eaad1fc13fb23f5e326b05ba6d0b3a44da"}, {"y": 216, "x": 320, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb3d5913b3730778257fc3e0713c166604658b00"}], "s": {"y": 238, "x": 351, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;format=png&amp;auto=webp&amp;s=d6977cc422be66a8c37980554c1478e76d2d326c"}, "id": "o1h5bwdn8xvb1"}}, "name": "t3_17egeux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Coding", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bVSQy67lM6KFb-o8nRKeJvIBP8TdwnEgDfNjXQbap2s.jpg", "edited": 1698064078.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698053752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;A real-world case study of performance optimization in Numpy&lt;/h1&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/\"&gt;Data Leads Future&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab8950800797f55f538fdb1343df6d275bd07152\"&gt; How to Optimize Multidimensional Numpy Array Operations with Numexpr. Photo Credit: Created by Author, Canva. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a relatively brief article. In it, I will use a real-world scenario as an example to explain how to use &lt;a href=\"https://numexpr.readthedocs.io/en/latest/user_guide.html?ref=dataleadsfuture.com#supported-functions\"&gt;Numexpr expressions&lt;/a&gt; in multidimensional Numpy arrays to achieve substantial performance improvements.&lt;/p&gt;\n\n&lt;p&gt;There aren&amp;#39;t many articles explaining how to use Numexpr in multidimensional Numpy arrays and how to use Numexpr expressions, so I hope this one will help you.&lt;/p&gt;\n\n&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;Recently, while reviewing some of my old work, I stumbled upon this piece of code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def predict(X, w, b):\n    z = np.dot(X, w)\n    y_hat = sigmoid(z)\n    y_pred = np.zeros((y_hat.shape[0], 1))\n\n    for i in range(y_hat.shape[0]):\n        if y_hat[i, 0] &amp;lt; 0.5:\n            y_pred[i, 0] = 0\n        else:\n            y_pred[i, 0] = 1\n    return y_pred\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This code transforms prediction results from probabilities to classification results of 0 or 1 in the logistic regression model of machine learning.&lt;/p&gt;\n\n&lt;p&gt;But heavens, who would use a &lt;code&gt;for loop&lt;/code&gt; to iterate over Numpy ndarray?&lt;/p&gt;\n\n&lt;p&gt;You can foresee that when the data reaches a certain amount, it will not only occupy a lot of memory, but the performance will also be inferior.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s right, the person who wrote this code was me when I was younger.&lt;/p&gt;\n\n&lt;p&gt;With a sense of responsibility, I plan to rewrite this code with the Numexpr library today.&lt;/p&gt;\n\n&lt;p&gt;Along the way, I will show you how to use Numexpr and Numexpr&amp;#39;s &lt;code&gt;where&lt;/code&gt; expression in multidimensional Numpy arrays to achieve significant performance improvements.&lt;/p&gt;\n\n&lt;h2&gt;Code Implementation&lt;/h2&gt;\n\n&lt;p&gt;If you are not familiar with the basic usage of Numexpr, you can refer to this article:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article uses a real-world example to demonstrate the specific usage of Numexpr&amp;#39;s API and expressions in Numpy and Pandas.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;where(bool, number1, number2): number&lt;/em&gt; &lt;em&gt;- number1 if the bool condition is true, number2 otherwise.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;The above is the usage of the where expression in Numpy.&lt;/p&gt;\n\n&lt;p&gt;When dealing with matrix data, you may used to using Pandas &lt;code&gt;DataFrame&lt;/code&gt;. But since the &lt;code&gt;eval&lt;/code&gt; method of Pandas does not support the &lt;code&gt;where&lt;/code&gt; expression, you can only choose to use Numexpr in multidimensional Numpy ndarray.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry, I&amp;#39;ll explain it to you right away.&lt;/p&gt;\n\n&lt;p&gt;Before starting, we need to import the necessary packages and implement a &lt;code&gt;generate_ndarray&lt;/code&gt; method to generate a specific size ndarray for testing:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from typing import Callable\nimport time\n\nimport numpy as np\nimport numexpr as ne\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(seed=4000)\n\ndef generate_ndarray(rows: int) -&amp;gt; np.ndarray:\n    result_array = rng.random((rows, 1))\n    return result_array\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;First, we generate a matrix of 200 rows to see if it is the test data we want:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  arr = generate_ndarray(200)\n     print(f&amp;quot;The dimension of this array: {arr.ndim}&amp;quot;)\n     print(f&amp;quot;The shape of this array: {arr.shape}&amp;quot;)\n\n\nOut: The dimension of this array: 2\n     The shape of this array: (200, 1)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be close to the actual situation of the logistic regression model, we generate an ndarray of the shape &lt;code&gt;(200, 1)&lt;/code&gt; Of course, you can also test other shapes of ndarray according to your needs.&lt;/p&gt;\n\n&lt;p&gt;Then, we start writing the specific use of Numexpr in the &lt;code&gt;numexpr_to_binary&lt;/code&gt; method:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;First, we use the index to separate the columns that need to be processed.&lt;/li&gt;\n&lt;li&gt;Then, use the where expression of Numexpr to process the values.&lt;/li&gt;\n&lt;li&gt;Finally, merge the processed columns with other columns to generate the required results.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Since the ndarray&amp;#39;s shape here is &lt;code&gt;(200, 1)&lt;/code&gt;, there is only one column, so I add a new dimension.&lt;/p&gt;\n\n&lt;p&gt;The code is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def numexpr_to_binary(np_array: np.ndarray) -&amp;gt; np.ndarray:\n    temp = np_array[:, 0]\n    temp = ne.evaluate(&amp;quot;where(temp&amp;lt;0.5, 0, 1)&amp;quot;)\n    return temp[:, np.newaxis]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We can test the result with an array of 10 rows to see if it is what I want:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;arr = generate_ndarray(10)\nresult = numexpr_to_binary(arr)\n\nmapping = np.column_stack((arr, result))\nmapping\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6977cc422be66a8c37980554c1478e76d2d326c\"&gt; I test an array of 10 rows and the result is what I want. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Look, the match is correct. Our task is completed.&lt;/p&gt;\n\n&lt;p&gt;The entire process can be demonstrated with the following figure:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df44481e44b2dc48b3acd522b51327b9030e2335\"&gt; The entire process of how Numexpr transforms the multidimensional ndarray. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Performance Comparison&lt;/h2&gt;\n\n&lt;p&gt;After the code implementation, we need to compare the Numexpr implementation version with the previous &lt;code&gt;for each&lt;/code&gt; implementation version to confirm that there has been a performance improvement.&lt;/p&gt;\n\n&lt;p&gt;First, we implement a &lt;code&gt;numexpr_example&lt;/code&gt; method. This method is based on the implementation of Numexpr:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def numexpr_example(rows: int) -&amp;gt; np.ndarray:\n    orig_arr = generate_ndarray(rows)\n    the_result = numexpr_to_binary(orig_arr)\n    return the_result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, we need to supplement a &lt;code&gt;for_loop_example&lt;/code&gt; method. This method refers to the original code I need to rewrite and is used as a performance benchmark:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def for_loop_example(rows: int) -&amp;gt; np.ndarray:\n    the_arr = generate_ndarray(rows)\n    for i in range(the_arr.shape[0]):\n        if the_arr[i][0] &amp;lt; 0.5:\n            the_arr[i][0] = 0\n        else:\n            the_arr[i][0] = 1\n    return the_arr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, I wrote a test method &lt;code&gt;time_method&lt;/code&gt;. This method will generate data from 10 to 10 to the 9th power rows separately, call the corresponding method, and finally save the time required for different data amounts:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def time_method(method: Callable):\n    time_dict = dict()\n    for i in range(9):\n        begin = time.perf_counter()\n        rows = 10 ** i\n        method(rows)\n        end = time.perf_counter()\n        time_dict[i] = end - begin\n    return time_dict\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We test the numexpr version and the &lt;code&gt;for_loop&lt;/code&gt; version separately, and use &lt;code&gt;matplotlib&lt;/code&gt; to draw the time required for different amounts of data:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;t_m = time_method(for_loop_example)\nt_m_2 = time_method(numexpr_example)\nplt.plot(t_m.keys(), t_m.values(), c=&amp;quot;red&amp;quot;, linestyle=&amp;quot;solid&amp;quot;)\nplt.plot(t_m_2.keys(), t_m_2.values(), c=&amp;quot;green&amp;quot;, linestyle=&amp;quot;dashed&amp;quot;)\nplt.legend([&amp;quot;for loop&amp;quot;, &amp;quot;numexpr&amp;quot;])\nplt.xlabel(&amp;quot;exponent&amp;quot;)\nplt.ylabel(&amp;quot;time&amp;quot;)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d508bddb500f8065c75921c1905f14e414ccf932\"&gt; The Numexpr version of the implementation has a huge performance improvement. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It can be seen that when the number of rows of data is greater than 10 to the 6th power, the Numexpr version of the implementation has a huge performance improvement.&lt;/p&gt;\n\n&lt;h2&gt;Conclusion&lt;/h2&gt;\n\n&lt;p&gt;After explaining the basic usage of Numexpr in the previous article, this article uses a specific example in actual work to explain how to use Numexpr to rewrite existing code to obtain performance improvement.&lt;/p&gt;\n\n&lt;p&gt;This article mainly uses two features of Numexpr:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Numexpr allows calculations to be performed in a vectorized manner.&lt;/li&gt;\n&lt;li&gt;During the calculation of Numexpr, no new arrays will be generated, thereby significantly reducing memory usage.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you for reading. If you have other solutions, please feel free to leave a message and discuss them with me.&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/\"&gt;Data Leads Future&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?auto=webp&amp;s=366216577b9b38ba41452286b2fcf4f2c68c9636", "width": 1387, "height": 924}, "resolutions": [{"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0120bd33f146716d9a9571e26276d1f7f86fb93", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=939e7da9c16e695695b1ad59dec1d4e6e8d683d6", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a4456f1d0041d99b553a235dbd71d4165addac3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b2b091715dcd26bde14ed92aaf499316d099b9", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57b2c9250bad68102634450d43d77c65d882e11e", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2393289abf0d2b8b91de65ffb3765a8ecc5bb9e", "width": 1080, "height": 719}], "variants": {}, "id": "i2JEOSQeihCf3kK3-z6ZS0cLg5Reifzq6bHfNWMCWZA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4ab9c418-70eb-11ee-8a37-4a495429ae82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17egeux", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17egeux/how_to_optimize_multidimensional_numpy_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17egeux/how_to_optimize_multidimensional_numpy_array/", "subreddit_subscribers": 1096498, "created_utc": 1698053752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am studying a master\u2019s in data science and working as a \u201cjunior data scientist\u201d as my first ever job at a start up. Problem is, even though I have ended the more \u201cdata science\u201d part of my degree (ML, advanced math/statistics etc.), at work, I\u2019m working more on reporting (power bi, excel, sql). I have never built or implemented any model, except for the finals I passed like 5 months ago. Sadly, I don\u2019t remember anything from them. \n\nI\u2019m approaching 1 year in experience, and my goal is to apply for junior/entry level jobs preferably in the UK or Netherlands. However, I fear that even if I land an interview, there\u2019s no way I can make it past any of them because of the discrepency between my title and actual experience.", "author_fullname": "t2_m826ekr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Title not matching tasks, am I making it a big deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eeucx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698046517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying a master\u2019s in data science and working as a \u201cjunior data scientist\u201d as my first ever job at a start up. Problem is, even though I have ended the more \u201cdata science\u201d part of my degree (ML, advanced math/statistics etc.), at work, I\u2019m working more on reporting (power bi, excel, sql). I have never built or implemented any model, except for the finals I passed like 5 months ago. Sadly, I don\u2019t remember anything from them. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m approaching 1 year in experience, and my goal is to apply for junior/entry level jobs preferably in the UK or Netherlands. However, I fear that even if I land an interview, there\u2019s no way I can make it past any of them because of the discrepency between my title and actual experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eeucx", "is_robot_indexable": true, "report_reasons": null, "author": "Utterizi", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eeucx/title_not_matching_tasks_am_i_making_it_a_big_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eeucx/title_not_matching_tasks_am_i_making_it_a_big_deal/", "subreddit_subscribers": 1096498, "created_utc": 1698046517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello together. I (M24) am studying Finance and I had a course in data science last semester. I really enjoyed and I know that I want to work in that area in my career.\n\nBut I am not sure if the standards I learn are enough to be successful. I basically just know R and I am learning Python right now. And at this point I kind of feel like I am not good enough in finance, because I am focusing on data science, but I am also not good enough at what I want to do, because others who studied computer science are much better then me. \n\nSo right now I am thinking about starting computer science after I am done with finance (with 25 then). Would it make sense or do you think I should maybe just continue to educate myself by myself or is a good knowledge in finance and a good knowledge in data science maybe even better?\n\n(I am not a native speaker btw, don\u2019t judge it)", "author_fullname": "t2_c00h03za", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career in computer science after finance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ee6ao", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698076085.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698043562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello together. I (M24) am studying Finance and I had a course in data science last semester. I really enjoyed and I know that I want to work in that area in my career.&lt;/p&gt;\n\n&lt;p&gt;But I am not sure if the standards I learn are enough to be successful. I basically just know R and I am learning Python right now. And at this point I kind of feel like I am not good enough in finance, because I am focusing on data science, but I am also not good enough at what I want to do, because others who studied computer science are much better then me. &lt;/p&gt;\n\n&lt;p&gt;So right now I am thinking about starting computer science after I am done with finance (with 25 then). Would it make sense or do you think I should maybe just continue to educate myself by myself or is a good knowledge in finance and a good knowledge in data science maybe even better?&lt;/p&gt;\n\n&lt;p&gt;(I am not a native speaker btw, don\u2019t judge it)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ee6ao", "is_robot_indexable": true, "report_reasons": null, "author": "DeutschlandHooligan", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ee6ao/career_in_computer_science_after_finance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ee6ao/career_in_computer_science_after_finance/", "subreddit_subscribers": 1096498, "created_utc": 1698043562.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}