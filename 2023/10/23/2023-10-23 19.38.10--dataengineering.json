{"kind": "Listing", "data": {"after": "t3_17ecs59", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if people are using copilot or ChatGPT in their everyday work? What do you use them for? Where do you find them lacking?\n\nPersonally, I use ChatGPT a lot, but don\u2019t find copilot nearly as useful. I wish there were more DE workflow specific AI tooling.", "author_fullname": "t2_87fao7nr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How useful do you find AI coding tools for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ebg8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698032891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if people are using copilot or ChatGPT in their everyday work? What do you use them for? Where do you find them lacking?&lt;/p&gt;\n\n&lt;p&gt;Personally, I use ChatGPT a lot, but don\u2019t find copilot nearly as useful. I wish there were more DE workflow specific AI tooling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ebg8x", "is_robot_indexable": true, "report_reasons": null, "author": "Psychling1", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ebg8x/how_useful_do_you_find_ai_coding_tools_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ebg8x/how_useful_do_you_find_ai_coding_tools_for_de/", "subreddit_subscribers": 135524, "created_utc": 1698032891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've been in the Big Data game for a while, and I've noticed that there are tons of tools and solutions out there from hyperscalers (like EMR), Snowflake, and Databricks. However, I've always had a soft spot for Hadoop and its ecosystem. There's something about ingesting data into HDFS using Sqoop, creating Hive raw tables, designing partitions and buckets, and building external tables for data warehousing workloads that I find fascinating.\n\nHadoop has been around for more than a decade now, and some big companies still use it. I'm curious to hear your thoughts on it. Do you prefer Hadoop in your data engineering stack, or have you shifted to newer tools and technologies?", "author_fullname": "t2_7fhzjcsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hadoop in the Era of Modern Data Tools: Your Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e6rol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698018259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been in the Big Data game for a while, and I&amp;#39;ve noticed that there are tons of tools and solutions out there from hyperscalers (like EMR), Snowflake, and Databricks. However, I&amp;#39;ve always had a soft spot for Hadoop and its ecosystem. There&amp;#39;s something about ingesting data into HDFS using Sqoop, creating Hive raw tables, designing partitions and buckets, and building external tables for data warehousing workloads that I find fascinating.&lt;/p&gt;\n\n&lt;p&gt;Hadoop has been around for more than a decade now, and some big companies still use it. I&amp;#39;m curious to hear your thoughts on it. Do you prefer Hadoop in your data engineering stack, or have you shifted to newer tools and technologies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17e6rol", "is_robot_indexable": true, "report_reasons": null, "author": "New-Ship-5404", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17e6rol/hadoop_in_the_era_of_modern_data_tools_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17e6rol/hadoop_in_the_era_of_modern_data_tools_your/", "subreddit_subscribers": 135524, "created_utc": 1698018259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " While observing an educational presentation, the speaker displayed a slide emphasizing the complexity before the advent of Databricks Delta. This prompted me to consider the approach to resolving the validation aspect. So, How do you do it?\n\nhttps://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;format=png&amp;auto=webp&amp;s=780a38c414c64843f79b605fe8f32e8c50b25541", "author_fullname": "t2_84ztczxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenge : How you people do the validation part below?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yz9jwi5cxxvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7b6a97e8aa5113418d2fbe26fb030b102bf922c"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=18c50ca1158fe005de1412893e482ac4cb1448ff"}, {"y": 182, "x": 320, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e398a0e1d97013a7ff2964a80377d6629c151f1"}, {"y": 365, "x": 640, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e879c6836e8891df77e2e0da37a8a6ada848ea1"}, {"y": 548, "x": 960, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494be702a880da93cfbf93598ddf0f6ffada72a0"}, {"y": 617, "x": 1080, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1bab5c744e5d620d9be2a9c9e29a32fe76ffe4f"}], "s": {"y": 635, "x": 1111, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;format=png&amp;auto=webp&amp;s=780a38c414c64843f79b605fe8f32e8c50b25541"}, "id": "yz9jwi5cxxvb1"}}, "name": "t3_17eijyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w8WCr6QrUV7_7kUCInkEyqC2ntRjW7rWZxAiCF1mKnE.jpg", "edited": 1698067340.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While observing an educational presentation, the speaker displayed a slide emphasizing the complexity before the advent of Databricks Delta. This prompted me to consider the approach to resolving the validation aspect. So, How do you do it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=780a38c414c64843f79b605fe8f32e8c50b25541\"&gt;https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=780a38c414c64843f79b605fe8f32e8c50b25541&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17eijyc", "is_robot_indexable": true, "report_reasons": null, "author": "chaachans", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eijyc/challenge_how_you_people_do_the_validation_part/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eijyc/challenge_how_you_people_do_the_validation_part/", "subreddit_subscribers": 135524, "created_utc": 1698062054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on defining our tech KPIs for our data engineering team. We don\u2019t have any at the moment. Can you please share the KPIs that you use and find it helpful in your teams? Also helpful resources would be nice. We are around 4 data engineers in the team.\n\nAbout the products:\n\nWe have couple of ETL jobs that we read order data and do some aggregation on top of it depends on the business requirements. Plus we share some of the output tables with the marketing teams.\n\nWe have some jobs that we consume APIs and run aggregation on top.\n\nWe have a job that we push our reports to a third party tool. \n\nWe also enable some BI teams with dbt.\n\nTech stack more or less: SQL (Bigquery), Airflow, dbt, python for some GDPR jobs, terraform for infra.", "author_fullname": "t2_4iz9ffyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tech KPIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e377n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698012709.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698008482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on defining our tech KPIs for our data engineering team. We don\u2019t have any at the moment. Can you please share the KPIs that you use and find it helpful in your teams? Also helpful resources would be nice. We are around 4 data engineers in the team.&lt;/p&gt;\n\n&lt;p&gt;About the products:&lt;/p&gt;\n\n&lt;p&gt;We have couple of ETL jobs that we read order data and do some aggregation on top of it depends on the business requirements. Plus we share some of the output tables with the marketing teams.&lt;/p&gt;\n\n&lt;p&gt;We have some jobs that we consume APIs and run aggregation on top.&lt;/p&gt;\n\n&lt;p&gt;We have a job that we push our reports to a third party tool. &lt;/p&gt;\n\n&lt;p&gt;We also enable some BI teams with dbt.&lt;/p&gt;\n\n&lt;p&gt;Tech stack more or less: SQL (Bigquery), Airflow, dbt, python for some GDPR jobs, terraform for infra.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17e377n", "is_robot_indexable": true, "report_reasons": null, "author": "rollingindata", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17e377n/tech_kpis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/", "subreddit_subscribers": 135524, "created_utc": 1698008482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the main differences between the different databricks tiers (standard and premium) ?\n\nI\u2019ve been looking for a capabilities comparison between different tiers but not able to find anything beside costs.\n\nIs it worth to have premium instead of standard?\n\nFor instance, I know data lineage it\u2019s only available for premium tier. Is there anything else?", "author_fullname": "t2_9l5ldlre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks standard vs premium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17effag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698049271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the main differences between the different databricks tiers (standard and premium) ?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been looking for a capabilities comparison between different tiers but not able to find anything beside costs.&lt;/p&gt;\n\n&lt;p&gt;Is it worth to have premium instead of standard?&lt;/p&gt;\n\n&lt;p&gt;For instance, I know data lineage it\u2019s only available for premium tier. Is there anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17effag", "is_robot_indexable": true, "report_reasons": null, "author": "RoundReveal", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17effag/databricks_standard_vs_premium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17effag/databricks_standard_vs_premium/", "subreddit_subscribers": 135524, "created_utc": 1698049271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i have a lakehouse built on delta tables which are being fed with multilple Spark pipelines. I recently got asked to build the streaming pipeline with a strict performance requirements that will feed one of already existing tables. The load of new pipeline is super small (just few rows) therefore i wonder if this is possibile to use some more lightweight framework than Spark. I've looked on few do far but they suport the standard append/overwrite methods only. Is there any framework that supports delta lake merge like Spark does?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Spark for lakehouse workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eemod", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698045537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have a lakehouse built on delta tables which are being fed with multilple Spark pipelines. I recently got asked to build the streaming pipeline with a strict performance requirements that will feed one of already existing tables. The load of new pipeline is super small (just few rows) therefore i wonder if this is possibile to use some more lightweight framework than Spark. I&amp;#39;ve looked on few do far but they suport the standard append/overwrite methods only. Is there any framework that supports delta lake merge like Spark does?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eemod", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eemod/alternative_to_spark_for_lakehouse_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eemod/alternative_to_spark_for_lakehouse_workflows/", "subreddit_subscribers": 135524, "created_utc": 1698045537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we're dealing with at work:\n\nMy goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.\n\nOther nuances is that the JSON nesting isnt always consistent-- right now, we see for example that 'dob' has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. \n\nTechnically, I can unnest without going through this complexity, but just wanted to be 100% sure I've explored all options\n\nThanks in advance for the help", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to unnest a json recursively", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_17enoxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cgWsf_KzQhOGBEpI2QRqwchmDAwtxl-R2Qu9XppzbOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698076385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we&amp;#39;re dealing with at work:&lt;/p&gt;\n\n&lt;p&gt;My goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.&lt;/p&gt;\n\n&lt;p&gt;Other nuances is that the JSON nesting isnt always consistent-- right now, we see for example that &amp;#39;dob&amp;#39; has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. &lt;/p&gt;\n\n&lt;p&gt;Technically, I can unnest without going through this complexity, but just wanted to be 100% sure I&amp;#39;ve explored all options&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f88mmvrw4zvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?auto=webp&amp;s=8328474a62e520df61153d0f2e28a86f65788cc1", "width": 792, "height": 515}, "resolutions": [{"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=984478729cb739bd5b66b9f6e7d9900fe45ff363", "width": 108, "height": 70}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed30fe70230d04b71b7dcb44525b676c51f9faa2", "width": 216, "height": 140}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d85c033e0523b07fb5ef8cb9f43619bf58a400cb", "width": 320, "height": 208}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b18022056383e51d51926fd2951f0bdc4e23635c", "width": 640, "height": 416}], "variants": {}, "id": "7B8x_aRgiSwSrrL4ftXRJ3VQggLC6QwoppjPOvJrBZQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17enoxk", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enoxk/how_to_unnest_a_json_recursively/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f88mmvrw4zvb1.jpg", "subreddit_subscribers": 135524, "created_utc": 1698076385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6g4yk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beam College: Online lessons for learning Apache Beam. Oct 23 to 26.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ed8sj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1698039518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "us.airmeet.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://us.airmeet.com/e/74658860-4887-11ee-9f2f-c790e4299545", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ed8sj", "is_robot_indexable": true, "report_reasons": null, "author": "pedrogk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ed8sj/beam_college_online_lessons_for_learning_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://us.airmeet.com/e/74658860-4887-11ee-9f2f-c790e4299545", "subreddit_subscribers": 135524, "created_utc": 1698039518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.  \nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  \n\n\nA little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  \n\n\nWould be great if someone could help with good reading material, or videos  \n", "author_fullname": "t2_be03utda", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to data engineering, need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ep162", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.&lt;br/&gt;\nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  &lt;/p&gt;\n\n&lt;p&gt;A little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  &lt;/p&gt;\n\n&lt;p&gt;Would be great if someone could help with good reading material, or videos  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ep162", "is_robot_indexable": true, "report_reasons": null, "author": "Full-Natural5932", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "subreddit_subscribers": 135524, "created_utc": 1698079729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. \n\nI am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. \n\nI have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. \n\nMy question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.\n\nThanks in advance!!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling - Modern Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eodmb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698078084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. &lt;/p&gt;\n\n&lt;p&gt;I am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. &lt;/p&gt;\n\n&lt;p&gt;I have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. &lt;/p&gt;\n\n&lt;p&gt;My question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eodmb", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "subreddit_subscribers": 135524, "created_utc": 1698078084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It's worked fairly well up until now, but there are several problems I'm hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.\n\nHere's what I'm envisioning:\n\n\\- custom tool (there's no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.\n\n\\- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  \n\n\\- \\[this is the part I'm most unclear on and SQL is failing the most\\].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central \"bronze\" table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  \n\n\\- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it's prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source's data, that describes more or less what I'm looking for.  So I don't know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I've got some ML in there as well, so moving the data offline is doable.\n\n\\- Using MERGE will make this more efficient without the overhead of SQL's locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)\n\n\\- Build gold/aggregate tables in data lake with \\[databricks?\\]\n\nPush the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.\n\nI have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We're not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I've got to have something in between for caching and refreshes.\n\n&amp;#x200B;", "author_fullname": "t2_gilvsz7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check architecture please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq1yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It&amp;#39;s worked fairly well up until now, but there are several problems I&amp;#39;m hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m envisioning:&lt;/p&gt;\n\n&lt;p&gt;- custom tool (there&amp;#39;s no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.&lt;/p&gt;\n\n&lt;p&gt;- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  &lt;/p&gt;\n\n&lt;p&gt;- [this is the part I&amp;#39;m most unclear on and SQL is failing the most].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central &amp;quot;bronze&amp;quot; table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  &lt;/p&gt;\n\n&lt;p&gt;- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it&amp;#39;s prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source&amp;#39;s data, that describes more or less what I&amp;#39;m looking for.  So I don&amp;#39;t know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I&amp;#39;ve got some ML in there as well, so moving the data offline is doable.&lt;/p&gt;\n\n&lt;p&gt;- Using MERGE will make this more efficient without the overhead of SQL&amp;#39;s locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)&lt;/p&gt;\n\n&lt;p&gt;- Build gold/aggregate tables in data lake with [databricks?]&lt;/p&gt;\n\n&lt;p&gt;Push the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.&lt;/p&gt;\n\n&lt;p&gt;I have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We&amp;#39;re not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I&amp;#39;ve got to have something in between for caching and refreshes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq1yd", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Log_8482", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "subreddit_subscribers": 135524, "created_utc": 1698082305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. \n\nMy job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. \n\nIs there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark sliding window within a window", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17emrm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698074072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. &lt;/p&gt;\n\n&lt;p&gt;My job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17emrm7", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "subreddit_subscribers": 135524, "created_utc": 1698074072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Configure VS Code to Develop Airflow DAGs in Docker at ease!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17elttv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17elttv", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yhN84w9nIeUQQpFrguMmWJid7Gh_zCqk3cid0v4svck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/fsMKV9A1B-I", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?auto=webp&amp;s=c578ca44010c7790f0bc08cf9ba73e7c60be5627", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d443dc814c8a8befeffcb24924974b7df32bdce2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d401374ee81fe607289fb56d722bfa0990f7f72", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4b864db7e7561a16ae0822bb500e174a66c4380", "width": 320, "height": 240}], "variants": {}, "id": "EjbV75W_SKL66g68oZtVMqNqSHRNtBErf-59fQyDhOA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17elttv", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17elttv/new_video_configure_vs_code_to_develop_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/fsMKV9A1B-I", "subreddit_subscribers": 135524, "created_utc": 1698071614.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2qof8m9w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transforming Realtime Air Quality and Asteroid Data into MIDI - [TouchDesigner + Ableton Live Project Files]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_17erlpl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NLmNGdSAPdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NLmNGdSAPdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live\"&gt;&lt;/iframe&gt;", "author_name": "uisato", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/NLmNGdSAPdA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@uisato_"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NLmNGdSAPdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17erlpl", "height": 200}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uNWMKiV1htG5Kxhst2UPy6UNOZ0ZomxDnIOKmOyYy-w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698086328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=NLmNGdSAPdA", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2aL2NemLnPbX6o83crw_v4lh-UCBiIXaGWRN8pY5f0k.jpg?auto=webp&amp;s=42d6bb8d5678e43d2f04e9c7fae084d363ce04b7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/2aL2NemLnPbX6o83crw_v4lh-UCBiIXaGWRN8pY5f0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd5661b082f26f03b5d26dee11187ab2fd854f72", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/2aL2NemLnPbX6o83crw_v4lh-UCBiIXaGWRN8pY5f0k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5790609b2b77beecdfba3173d9d1346b436e136d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/2aL2NemLnPbX6o83crw_v4lh-UCBiIXaGWRN8pY5f0k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddddbcef34e5d5f9355675040839cffdd5b54976", "width": 320, "height": 240}], "variants": {}, "id": "1gLmo-VyUO-f_cS2RVOrzD4oTspGIc9c4gL46KippOw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "17erlpl", "is_robot_indexable": true, "report_reasons": null, "author": "Chuka444", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17erlpl/transforming_realtime_air_quality_and_asteroid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=NLmNGdSAPdA", "subreddit_subscribers": 135524, "created_utc": 1698086328.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/NLmNGdSAPdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Transforming Realtime Air Quality and Asteroid Data into MIDI - TouchDesigner + Ableton Live\"&gt;&lt;/iframe&gt;", "author_name": "uisato", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/NLmNGdSAPdA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@uisato_"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.\n\nThe data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.\n\nGenerally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.\n\nI am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don't want to use a paginated report.\n\nDoes anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much detail is required for a Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17eq7fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.&lt;/p&gt;\n\n&lt;p&gt;The data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.&lt;/p&gt;\n\n&lt;p&gt;Generally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.&lt;/p&gt;\n\n&lt;p&gt;I am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don&amp;#39;t want to use a paginated report.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq7fw", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "subreddit_subscribers": 135524, "created_utc": 1698082725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can't find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.", "author_fullname": "t2_n52imw1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "export WhatsApp business group chat data in the structure format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eo0tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698077435.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698077193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can&amp;#39;t find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eo0tu", "is_robot_indexable": true, "report_reasons": null, "author": "SurpriseLopsided5536", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "subreddit_subscribers": 135524, "created_utc": 1698077193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Sai, CEO of PeerDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17ekedw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iZVPEagtKd4g-yZ1SN-JXfC_3KqR03WLkeqHwug84FI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698067631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-sai-ceo-of-peerdb?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/N_ro3wuF5xRpqIcQRIVm72fejyoOccTTLtmJEbOXvYQ.jpg?auto=webp&amp;s=932168d967dfe8ac8fd2930ad813a8f8b39a9c5c", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/N_ro3wuF5xRpqIcQRIVm72fejyoOccTTLtmJEbOXvYQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d97700e259c2beb2704b5939561eff65eae5015", "width": 108, "height": 108}], "variants": {}, "id": "-AnNYmf6lpdn2cWORPDWCAHghdmrGM0zk9Wk0x7XetI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ekedw", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ekedw/interview_with_sai_ceo_of_peerdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-sai-ceo-of-peerdb?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 135524, "created_utc": 1698067631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a postgres database with a materialized view that has some complex logic. We would like to \"stream\" changes/data from this materialized view every 10mins or so to other system. What would be the best approach? We firstly thought of doing backups every 10mins but backups apparently cannot capture materialized view state/data and capturing only MV source data does not make sense as we would need to reimplement the MV logic.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream changes from materialized view", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eires", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a postgres database with a materialized view that has some complex logic. We would like to &amp;quot;stream&amp;quot; changes/data from this materialized view every 10mins or so to other system. What would be the best approach? We firstly thought of doing backups every 10mins but backups apparently cannot capture materialized view state/data and capturing only MV source data does not make sense as we would need to reimplement the MV logic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17eires", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eires/stream_changes_from_materialized_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eires/stream_changes_from_materialized_view/", "subreddit_subscribers": 135524, "created_utc": 1698062717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any good tools for displaying and refreshing tables from a database, showing forms, and creating buttons that can pass form data to a python program on the server?\n\nMaybe something with a web-based GUI for developing the pages that these components would serve on. Or, is code based solution typically preferred here (Django, fastapi, \u2026)?", "author_fullname": "t2_83g1niecs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tool for form building and application triggering from web?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e23b6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698005517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any good tools for displaying and refreshing tables from a database, showing forms, and creating buttons that can pass form data to a python program on the server?&lt;/p&gt;\n\n&lt;p&gt;Maybe something with a web-based GUI for developing the pages that these components would serve on. Or, is code based solution typically preferred here (Django, fastapi, \u2026)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17e23b6", "is_robot_indexable": true, "report_reasons": null, "author": "_unbanned_datum", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17e23b6/tool_for_form_building_and_application_triggering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17e23b6/tool_for_form_building_and_application_triggering/", "subreddit_subscribers": 135524, "created_utc": 1698005517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Enterprise data lakehouses are a necessity for any company competing in today's data-driven world. For a useful centralized data lake to exist, data must be ingested from disparate sources. Since this generated data comes from multiple sources, each with its own format and protocol, getting this data in and out of the data lake in a consumable form is a basic requirement. If data is a competitive advantage, then the timeliness of it flowing into the data lakehouse from all over the organization and partners is paramount for success.\n\n[https://blog.min.io/simplify-data-pipelines/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=simplify\\_data\\_pipelines+](https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=simplify_data_pipelines+)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplify Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eoegc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698078136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Enterprise data lakehouses are a necessity for any company competing in today&amp;#39;s data-driven world. For a useful centralized data lake to exist, data must be ingested from disparate sources. Since this generated data comes from multiple sources, each with its own format and protocol, getting this data in and out of the data lake in a consumable form is a basic requirement. If data is a competitive advantage, then the timeliness of it flowing into the data lakehouse from all over the organization and partners is paramount for success.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=simplify_data_pipelines+\"&gt;https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=simplify_data_pipelines+&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?auto=webp&amp;s=7a6d3e750a2198d04daec527d19817baf8a96b58", "width": 2000, "height": 3000}, "resolutions": [{"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2fd809163488839b5b561948270b4d17758476f", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0821888c1edaed067738a126039a03a3aeb692e6", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9bcf7bb6093c1432295963b2c104f7ab05c4883", "width": 320, "height": 480}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ce2fd640d5686428f23ce0ab18b3af4ab668b31", "width": 640, "height": 960}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37146372724b1350eed041b92837d5b16989d2c7", "width": 960, "height": 1440}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac0e79ec6c659aebfc5aaa35de9f544c0bd2119a", "width": 1080, "height": 1620}], "variants": {}, "id": "spKdhQnj8fC4Mxkw4OJSOuMPnts3sOPQU4tNyW5TKag"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eoegc", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eoegc/simplify_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eoegc/simplify_data_pipelines/", "subreddit_subscribers": 135524, "created_utc": 1698078136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context - \n\nCurrently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.\n\nThe DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don't trigger during development work, because something about their structure. \n\nSo far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. \n\n&amp;#x200B;", "author_fullname": "t2_vk8zz17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you structure DEV/UAT/PROD with DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enlrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context - &lt;/p&gt;\n\n&lt;p&gt;Currently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.&lt;/p&gt;\n\n&lt;p&gt;The DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don&amp;#39;t trigger during development work, because something about their structure. &lt;/p&gt;\n\n&lt;p&gt;So far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17enlrh", "is_robot_indexable": true, "report_reasons": null, "author": "KrixMercades", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "subreddit_subscribers": 135524, "created_utc": 1698076161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mcyts2q16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hightouch is Just Blindly Copying Rudderstack. Wdyt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17ei53w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sDwL2apsQ5IPJlE_ZltY0TEZ5AGhlDymEC9Dai72kSc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698060607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@laurengreerbalik/hightouch-is-just-blindly-copying-rudderstack-2e80dba56b27", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?auto=webp&amp;s=237924c2b793490cb791291230a57ac09aa42321", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cbfed6233884a1313c908e72e343e658147f3e8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c35419d3ecd5b060ae46f7aea29dbd81797b0113", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88fcb11dcac0936899ef71273fcc8256ede57d18", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea56d650df632e6c7afb83bc70b27f726fcbf0f6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf869bcf795aa702d607012d02c22de4a4244e65", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f853e455af82e53d80eb0e9d5858ef8f27310b43", "width": 1080, "height": 607}], "variants": {}, "id": "uu9S5DZa4nCucZGVTepV-dgBiy0gM3a95Z7UB-V6MDM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ei53w", "is_robot_indexable": true, "report_reasons": null, "author": "aditichauhanofficial", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ei53w/hightouch_is_just_blindly_copying_rudderstack_wdyt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@laurengreerbalik/hightouch-is-just-blindly-copying-rudderstack-2e80dba56b27", "subreddit_subscribers": 135524, "created_utc": 1698060607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m reading an Python ETL book (Pandey, Schoof) and they briefly discuss volatile staging areas to have in addition to staging areas. I can\u2019t seem to find anything on this topic, can someone elaborate on their purpose? They mention they should be used in conjunction with persistent staging areas.", "author_fullname": "t2_14s794", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Volatile Staging Area?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17e2q5l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698007213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m reading an Python ETL book (Pandey, Schoof) and they briefly discuss volatile staging areas to have in addition to staging areas. I can\u2019t seem to find anything on this topic, can someone elaborate on their purpose? They mention they should be used in conjunction with persistent staging areas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17e2q5l", "is_robot_indexable": true, "report_reasons": null, "author": "Lacayo44", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17e2q5l/volatile_staging_area/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17e2q5l/volatile_staging_area/", "subreddit_subscribers": 135524, "created_utc": 1698007213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The company I work for (Tinybird) has recently released some new features to try to more tightly integrate data workflows with software best practices (i.e. version control, testing, and CI/CD). We've seen that many data engineers still don't systematically use these best practices. We wrote a blog outlining the problems we've seen that cause and how we've tried to address them in our product workflows. Curious to hear your thoughts generally about some of these best practices and if you think DEs could do more to adopt them.\n\n[https://www.tinybird.co/blog-posts/automating-data-workflows-with-git](https://www.tinybird.co/blog-posts/automating-data-workflows-with-git)", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating data workflows with plaintext files and Git", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17emraz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698074051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The company I work for (Tinybird) has recently released some new features to try to more tightly integrate data workflows with software best practices (i.e. version control, testing, and CI/CD). We&amp;#39;ve seen that many data engineers still don&amp;#39;t systematically use these best practices. We wrote a blog outlining the problems we&amp;#39;ve seen that cause and how we&amp;#39;ve tried to address them in our product workflows. Curious to hear your thoughts generally about some of these best practices and if you think DEs could do more to adopt them.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.tinybird.co/blog-posts/automating-data-workflows-with-git\"&gt;https://www.tinybird.co/blog-posts/automating-data-workflows-with-git&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?auto=webp&amp;s=90e98a3ff7efd712376046ba35509c84bfe96dde", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9857b9ef8604e91fb85837f2b4fbb108294b81d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=96942b3ad841c04ae478b85b4b99f73c7f21a02d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=14b426e588724f71eeec80f9a51d10b637865dcd", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9958c96ad5db11c9ddb2bb3146ba5d7a15b8f3e9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ef655f6669ec10f17cd90e5b8853164265d000d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/3zemeTtkYCP2BUB4L5-2emZf_j6qjPgbLWvEL8OX_Uk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d100d6ba10b8624c786fac5e42748db1c0e5719f", "width": 1080, "height": 567}], "variants": {}, "id": "9U4HiuvxyHuUJRPt7puBe2DXxNQOcTdZsFcQ61_eBnw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17emraz", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17emraz/automating_data_workflows_with_plaintext_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17emraz/automating_data_workflows_with_plaintext_files/", "subreddit_subscribers": 135524, "created_utc": 1698074051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any good open source dbt alternatives out there? I know about Airflow and Luigi, but are there any others? And which one is the best in your opinion?", "author_fullname": "t2_w1968sk2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best open source dbt alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ecs59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698037683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any good open source dbt alternatives out there? I know about Airflow and Luigi, but are there any others? And which one is the best in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ecs59", "is_robot_indexable": true, "report_reasons": null, "author": "Buxert", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ecs59/best_open_source_dbt_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ecs59/best_open_source_dbt_alternative/", "subreddit_subscribers": 135524, "created_utc": 1698037683.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}