{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're using AWS Managed Airflow and experiencing high resource utilization issues but have only 50 DAGs on a medium instance. DAGs are not dynamically created and they're all of the same type/using similar code to run. There's no clear culprit because nothing we're doing seems particularly resource-intensive. It doesn't seem to be related to DAG parse time based on relevant metrics. It might be related to memory usage but I haven't been able to find out what's taking up a lot of memory. What steps can we take to narrow down possible causes? Nothing stands out apart from the two areas I've just mentioned (DAG parsing or memory usage) so I'm looking for general guidance on areas to investigate further and how.", "author_fullname": "t2_jx5l6r0ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow performance issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wl0dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696115602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re using AWS Managed Airflow and experiencing high resource utilization issues but have only 50 DAGs on a medium instance. DAGs are not dynamically created and they&amp;#39;re all of the same type/using similar code to run. There&amp;#39;s no clear culprit because nothing we&amp;#39;re doing seems particularly resource-intensive. It doesn&amp;#39;t seem to be related to DAG parse time based on relevant metrics. It might be related to memory usage but I haven&amp;#39;t been able to find out what&amp;#39;s taking up a lot of memory. What steps can we take to narrow down possible causes? Nothing stands out apart from the two areas I&amp;#39;ve just mentioned (DAG parsing or memory usage) so I&amp;#39;m looking for general guidance on areas to investigate further and how.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wl0dz", "is_robot_indexable": true, "report_reasons": null, "author": "IllRepresentative858", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wl0dz/airflow_performance_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wl0dz/airflow_performance_issues/", "subreddit_subscribers": 131490, "created_utc": 1696115602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don't want a complex solution which involves many components and there's no need to scale computing beyond a single node.\n\nit seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.\n\nour preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn't require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.\n\nanyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?", "author_fullname": "t2_mojp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to query JSON and parquet files on S3 for data verification and pipeline building?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wysjd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696159977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don&amp;#39;t want a complex solution which involves many components and there&amp;#39;s no need to scale computing beyond a single node.&lt;/p&gt;\n\n&lt;p&gt;it seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.&lt;/p&gt;\n\n&lt;p&gt;our preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn&amp;#39;t require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.&lt;/p&gt;\n\n&lt;p&gt;anyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wysjd", "is_robot_indexable": true, "report_reasons": null, "author": "OneCyrus", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "subreddit_subscribers": 131490, "created_utc": 1696159977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Context**\n\nWe have data pipelines that are a mixture of SQL / python as they are a combo of:\n\n \\- more 'data engineering-y' transformations at the start (SQL)\n\n \\- more 'data science-y' operations further down (python)\n\nWe run on GCP so given we aren't at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.\n\nOrganising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.\n\nHowever, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I'd like to bring it in to the pipelines.\n\n**Question**\n\nHow do you introduce dbt to this infrastructure?\n\nBy having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?\n\nOr by starting with dbt's recommended monorepo, but then meaning a pipeline's logic is now split between:\n\n \\- pipeline repo\n\n \\- that pipeline's section in the dbt mono-repo\n\nI haven't been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?", "author_fullname": "t2_576k5sn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt mono-repo vs individual pipeline repos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wy2vv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696157594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We have data pipelines that are a mixture of SQL / python as they are a combo of:&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data engineering-y&amp;#39; transformations at the start (SQL)&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data science-y&amp;#39; operations further down (python)&lt;/p&gt;\n\n&lt;p&gt;We run on GCP so given we aren&amp;#39;t at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.&lt;/p&gt;\n\n&lt;p&gt;Organising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.&lt;/p&gt;\n\n&lt;p&gt;However, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I&amp;#39;d like to bring it in to the pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you introduce dbt to this infrastructure?&lt;/p&gt;\n\n&lt;p&gt;By having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?&lt;/p&gt;\n\n&lt;p&gt;Or by starting with dbt&amp;#39;s recommended monorepo, but then meaning a pipeline&amp;#39;s logic is now split between:&lt;/p&gt;\n\n&lt;p&gt;- pipeline repo&lt;/p&gt;\n\n&lt;p&gt;- that pipeline&amp;#39;s section in the dbt mono-repo&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wy2vv", "is_robot_indexable": true, "report_reasons": null, "author": "mjam03", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "subreddit_subscribers": 131490, "created_utc": 1696157594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been using snowflake-pulumi to define database objects like storage integrations, users, roles, schemas and so on. It works well for connecting AWS to Snowflake but is very painful to use for Snowflake objects. \n\nSo we are looking into dedicated tools for this. This tool is mostly to define the Snowflake infrastructure  (schemas, RBAC, external tables etc) as we use DBT for tables/views.\n\nI am currently evaluating [schemachange by Snowflake](https://github.com/Snowflake-Labs/schemachange). What tools are you using for database change management?", "author_fullname": "t2_pbcof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommendations for database change management for Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wgdrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696104264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been using snowflake-pulumi to define database objects like storage integrations, users, roles, schemas and so on. It works well for connecting AWS to Snowflake but is very painful to use for Snowflake objects. &lt;/p&gt;\n\n&lt;p&gt;So we are looking into dedicated tools for this. This tool is mostly to define the Snowflake infrastructure  (schemas, RBAC, external tables etc) as we use DBT for tables/views.&lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating &lt;a href=\"https://github.com/Snowflake-Labs/schemachange\"&gt;schemachange by Snowflake&lt;/a&gt;. What tools are you using for database change management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?auto=webp&amp;s=2bedc3b4e62956c2fc668b3d9c76cfa446780aa0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc4f9e13ee6cdbe7b9c1e3085b2baffef2c16ba4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3ef7f0f338c6d2194d6062539bede95f87f397", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d35415883c213b6580d54f1966b78559be0a5cb5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51aa4e03cf7358a2b214f58477e649a6b35606cc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b86e3f7ae5b2bcb1f416de2a981556a3cef68115", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e1abd5d48ca5a2c595f3793d7192f6d353b174f", "width": 1080, "height": 540}], "variants": {}, "id": "yNb2wBkvBc5pn2HPMQUhawwmqIhcCNf5oUQ5id_qneA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16wgdrw", "is_robot_indexable": true, "report_reasons": null, "author": "vish4life", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wgdrw/any_recommendations_for_database_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wgdrw/any_recommendations_for_database_change/", "subreddit_subscribers": 131490, "created_utc": 1696104264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was looking through old posts on this subreddit about system design and came across a comment a couple years ago that discussed a useful scaling exercise to practice for DE interviews: creating a pipeline that ingests 1MB at first, then 1GB, then 10GB, 100GB, 1TB, etc. and then talking about challenges along the way.\n\nI was wondering if this community had some ideas about things to consider as you get further and further up the throughput ladder. Here's a few I've compiled (I assumed the volume at an hourly rate):\n\n&amp;#x200B;\n\n* **@ 1MB / hour**\n   * ingestion: either batch or streaming is possible depending on the nature of the data and our business requirements. Orchestration and processing can live on same machine comfortably.\n   * Throughput is relatively small and should not require distributed processing. Libraries like pandas or numpy would be sufficient for most operations\n   * loading into a relational store or data warehouse should be trivial, though we still need to adopt best practices for designing our schema, managing indexes, etc.\n* **@ 1 GB / hour**\n   * Batch and streaming are both possible, but examine the data to find the most efficient approach. If the data is a single 1GB-sized file arriving hourly, it could be processed in batch, but it wouldn't be ideal to read the whole thing into memory on a lone machine. If the data is from an external source, we also have to pay attention to network I/O. Better to partition the data and have multiple machines read it in parallel. If instead the data is comprised of several small log files or messages in the KB-level, try consuming from an event broker.\n   * Processing data with Pandas on a single machine is possible if scaling vertically, but not ideal. Should switch to a small Spark cluster, or something like Dask. Again, depends on the transformations.\n   * Tools for logging, monitoring pipeline health, and analyzing resource utilization are recommended. (Should be recommended at all levels, but becomes more and more necessary as data scales)\n   * Using an optimized storage format is recommended for large data files (e.g. parquet, avro)\n   * If writing to a relational db, need to be mindful of our transactions/sec and not create strain on the server. (use load balancer and connection pooling)\n* **@ 10 GB / hour**\n   * Horizontal scaling preferred over vertical scaling. Should use a distributed cluster regardless of batch or streaming requirements.\n   * During processing, make sure our joins/transformations aren't creating uneven shards and resulting in bottlenecks on our nodes.\n   * Have strong data governance policies in place for data quality checks, data observability, data lineage, etc.\n   * Continuous monitoring of resource and CPU utilization of the cluster, notifications when thresholds are breached (again, useful at all levels). Also create pipelines for centralized log analysis (with ElasticSearch perhaps?)\n   * Properly partition data in data lake or relational store, with strategies for rolling off data as costs build up.\n   * Optimize compression and indexing wherever possible.\n* **@ 100 GB / hour**\n   * Proper configuration, load balancing, and partitioning of the event broker is essential\n   * Critical to have a properly tuned cluster that can auto-scale to accommodate job size as costs increase.\n   * Watch for bottlenecks in processing, OutOfMemory exceptions are likely if improper join strategies are used.\n   * Clean data, especially data deduplication, is critical for reducing redundant processing.\n   * Writing to traditional relational dbs may struggle to keep up with volume of writes. Distributed databases may be preferred (e.g. Cassandra).\n   * Employ caching liberally, both in serving queries and in processing data\n   * Optimizing queries is crucial, as poorly written SQL can result in long execution and resource contention.\n* **@ 1 TB / hour**\n   * Efficiency in configuring compute and storage is a must. Improperly tuned cloud services can be hugely expensive.\n   * Distributed databases/DWH typically required.\n   * Use an appropriate partitioning strategy in data lake\n   * Avoid processing data that is not necessary for the business, and move data that isn't used to cheaper, long-term storage.\n   * Optimize data model and indexing strategy for efficient queries.\n   * Good data retention policies prevent expensive, unmanageable database growth.\n   * Monitoring and alerting systems should be sophisticated and battle-tested to track overall resource utilization.\n\n# Above all, know how the business plans to use the data, as that will have the biggest influence on design!\n\n**Considerations at all levels:**\n\n* caching\n* security and privacy\n* metadata management\n* CI/CD, testing\n* redundancy and fault-tolerance\n* labor and maintenance overhead\n* cost-complexity ratio\n\n&amp;#x200B;\n\nAnyone have anything else to add? In an interview, I would obviously flesh out a lot of these bullet points.", "author_fullname": "t2_5zyhb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scaling exercise for DE interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16x7m4d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696182404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking through old posts on this subreddit about system design and came across a comment a couple years ago that discussed a useful scaling exercise to practice for DE interviews: creating a pipeline that ingests 1MB at first, then 1GB, then 10GB, 100GB, 1TB, etc. and then talking about challenges along the way.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if this community had some ideas about things to consider as you get further and further up the throughput ladder. Here&amp;#39;s a few I&amp;#39;ve compiled (I assumed the volume at an hourly rate):&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;@ 1MB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ingestion: either batch or streaming is possible depending on the nature of the data and our business requirements. Orchestration and processing can live on same machine comfortably.&lt;/li&gt;\n&lt;li&gt;Throughput is relatively small and should not require distributed processing. Libraries like pandas or numpy would be sufficient for most operations&lt;/li&gt;\n&lt;li&gt;loading into a relational store or data warehouse should be trivial, though we still need to adopt best practices for designing our schema, managing indexes, etc.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 1 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Batch and streaming are both possible, but examine the data to find the most efficient approach. If the data is a single 1GB-sized file arriving hourly, it could be processed in batch, but it wouldn&amp;#39;t be ideal to read the whole thing into memory on a lone machine. If the data is from an external source, we also have to pay attention to network I/O. Better to partition the data and have multiple machines read it in parallel. If instead the data is comprised of several small log files or messages in the KB-level, try consuming from an event broker.&lt;/li&gt;\n&lt;li&gt;Processing data with Pandas on a single machine is possible if scaling vertically, but not ideal. Should switch to a small Spark cluster, or something like Dask. Again, depends on the transformations.&lt;/li&gt;\n&lt;li&gt;Tools for logging, monitoring pipeline health, and analyzing resource utilization are recommended. (Should be recommended at all levels, but becomes more and more necessary as data scales)&lt;/li&gt;\n&lt;li&gt;Using an optimized storage format is recommended for large data files (e.g. parquet, avro)&lt;/li&gt;\n&lt;li&gt;If writing to a relational db, need to be mindful of our transactions/sec and not create strain on the server. (use load balancer and connection pooling)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 10 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Horizontal scaling preferred over vertical scaling. Should use a distributed cluster regardless of batch or streaming requirements.&lt;/li&gt;\n&lt;li&gt;During processing, make sure our joins/transformations aren&amp;#39;t creating uneven shards and resulting in bottlenecks on our nodes.&lt;/li&gt;\n&lt;li&gt;Have strong data governance policies in place for data quality checks, data observability, data lineage, etc.&lt;/li&gt;\n&lt;li&gt;Continuous monitoring of resource and CPU utilization of the cluster, notifications when thresholds are breached (again, useful at all levels). Also create pipelines for centralized log analysis (with ElasticSearch perhaps?)&lt;/li&gt;\n&lt;li&gt;Properly partition data in data lake or relational store, with strategies for rolling off data as costs build up.&lt;/li&gt;\n&lt;li&gt;Optimize compression and indexing wherever possible.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 100 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Proper configuration, load balancing, and partitioning of the event broker is essential&lt;/li&gt;\n&lt;li&gt;Critical to have a properly tuned cluster that can auto-scale to accommodate job size as costs increase.&lt;/li&gt;\n&lt;li&gt;Watch for bottlenecks in processing, OutOfMemory exceptions are likely if improper join strategies are used.&lt;/li&gt;\n&lt;li&gt;Clean data, especially data deduplication, is critical for reducing redundant processing.&lt;/li&gt;\n&lt;li&gt;Writing to traditional relational dbs may struggle to keep up with volume of writes. Distributed databases may be preferred (e.g. Cassandra).&lt;/li&gt;\n&lt;li&gt;Employ caching liberally, both in serving queries and in processing data&lt;/li&gt;\n&lt;li&gt;Optimizing queries is crucial, as poorly written SQL can result in long execution and resource contention.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 1 TB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Efficiency in configuring compute and storage is a must. Improperly tuned cloud services can be hugely expensive.&lt;/li&gt;\n&lt;li&gt;Distributed databases/DWH typically required.&lt;/li&gt;\n&lt;li&gt;Use an appropriate partitioning strategy in data lake&lt;/li&gt;\n&lt;li&gt;Avoid processing data that is not necessary for the business, and move data that isn&amp;#39;t used to cheaper, long-term storage.&lt;/li&gt;\n&lt;li&gt;Optimize data model and indexing strategy for efficient queries.&lt;/li&gt;\n&lt;li&gt;Good data retention policies prevent expensive, unmanageable database growth.&lt;/li&gt;\n&lt;li&gt;Monitoring and alerting systems should be sophisticated and battle-tested to track overall resource utilization.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Above all, know how the business plans to use the data, as that will have the biggest influence on design!&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Considerations at all levels:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;caching&lt;/li&gt;\n&lt;li&gt;security and privacy&lt;/li&gt;\n&lt;li&gt;metadata management&lt;/li&gt;\n&lt;li&gt;CI/CD, testing&lt;/li&gt;\n&lt;li&gt;redundancy and fault-tolerance&lt;/li&gt;\n&lt;li&gt;labor and maintenance overhead&lt;/li&gt;\n&lt;li&gt;cost-complexity ratio&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone have anything else to add? In an interview, I would obviously flesh out a lot of these bullet points.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16x7m4d", "is_robot_indexable": true, "report_reasons": null, "author": "LurkLurkington", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x7m4d/scaling_exercise_for_de_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x7m4d/scaling_exercise_for_de_interviews/", "subreddit_subscribers": 131490, "created_utc": 1696182404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We just inherited a large LookML project with ~1000 .lkml files (and no tests). It has view / explore / model definitions that support internal dashboards and a metrics API hit by our web app backend for customer facing reports.\n\nWe\u2019re seeing avg API response times from Looker around 1+ seconds even when the Snowflake DB it\u2019s hitting is returning from the result cache with quick response. Some requests can be much, much slower too. This is causing downstream problems in our web app.\n\nMost of the team has used Looker before but none of us have had to build out a LookML layer like this before. It feels overly complex and harder to follow than the equivalent SQL / SQLAlechemy would be.\n\nOur initial instinct is to refactor this logic out of Looker and into our modeling layer (DBT), but the performance with the Looker API seems so bad that we\u2019re considering just rolling our own a python API with SQLAlchemy.\n\nHas anyone here been in a similar predicament with Looker / LookML? Any advice on how we can improve performance / maintainability?", "author_fullname": "t2_axl2rt05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have advice for optimizing a large LookML project that serves customer facing metics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wjyrx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696112995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just inherited a large LookML project with ~1000 .lkml files (and no tests). It has view / explore / model definitions that support internal dashboards and a metrics API hit by our web app backend for customer facing reports.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re seeing avg API response times from Looker around 1+ seconds even when the Snowflake DB it\u2019s hitting is returning from the result cache with quick response. Some requests can be much, much slower too. This is causing downstream problems in our web app.&lt;/p&gt;\n\n&lt;p&gt;Most of the team has used Looker before but none of us have had to build out a LookML layer like this before. It feels overly complex and harder to follow than the equivalent SQL / SQLAlechemy would be.&lt;/p&gt;\n\n&lt;p&gt;Our initial instinct is to refactor this logic out of Looker and into our modeling layer (DBT), but the performance with the Looker API seems so bad that we\u2019re considering just rolling our own a python API with SQLAlchemy.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here been in a similar predicament with Looker / LookML? Any advice on how we can improve performance / maintainability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16wjyrx", "is_robot_indexable": true, "report_reasons": null, "author": "deepfuckingbass", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wjyrx/does_anyone_have_advice_for_optimizing_a_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wjyrx/does_anyone_have_advice_for_optimizing_a_large/", "subreddit_subscribers": 131490, "created_utc": 1696112995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Tips. Optimizing JDBC data source reads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_16x7kis", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p1GjGScxcKmQ-FPaqLmcZc3MHdyx7rPDYi802AF_oDE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696182306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "luminousmen.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://luminousmen.com/post/spark-tips-optimizing-jdbc-data-source-reads", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?auto=webp&amp;s=6feb316599e137f192368d3c5f8a09812a2fb9b3", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb47d235eb134e4426e30eb7cc852b9c1767bd28", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=042a2aae04d7b9c5e900bd441411b71f0d3fe891", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6a7cc51735acd26f5ae97f3b0fbcac4f6ca11e4", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b490b48832c9be9449eb2a5452e626717f703562", "width": 640, "height": 480}], "variants": {}, "id": "FAw-sDyD8fCBR3nlCjPbH2rgCvhf8lX7QDTmW7bd0yU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16x7kis", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x7kis/spark_tips_optimizing_jdbc_data_source_reads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://luminousmen.com/post/spark-tips-optimizing-jdbc-data-source-reads", "subreddit_subscribers": 131490, "created_utc": 1696182306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't see any pricing details for glue workflows but I would assume there's an extra cost since it's providing additional functionality. I only see pricing based on DPUs for Glue jobs. Does this mean that if I use Glue Workflows to orchestrate glue jobs, I will only pay for the cost of the glue jobs and the orchestration will just be free? If that's the case, is there any benefit to using Airflow for orchestration of glue jobs if glue workflows can be used?", "author_fullname": "t2_jx5l6r0ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any added cost to using Glue workflows with Glue jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x6tkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696180572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t see any pricing details for glue workflows but I would assume there&amp;#39;s an extra cost since it&amp;#39;s providing additional functionality. I only see pricing based on DPUs for Glue jobs. Does this mean that if I use Glue Workflows to orchestrate glue jobs, I will only pay for the cost of the glue jobs and the orchestration will just be free? If that&amp;#39;s the case, is there any benefit to using Airflow for orchestration of glue jobs if glue workflows can be used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x6tkl", "is_robot_indexable": true, "report_reasons": null, "author": "IllRepresentative858", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x6tkl/is_there_any_added_cost_to_using_glue_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x6tkl/is_there_any_added_cost_to_using_glue_workflows/", "subreddit_subscribers": 131490, "created_utc": 1696180572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Planning to buy a Nuc 11 Enthusiast instead of a Steam Deck to upskill myself as well as not screwing my Gaming PC . Are there any ideas where i can start this journey . My aim to setup homelab is :  \n\n1. Networking side with OpenWrt , pfSense \n2. Containers/Virtualization  with Dockers and Vmware \n3. Server Side with Proxmox\n\nShare if its sufficient to run multiple VMs and Containers with this thing as well as share how do you use homelab for upskilling your DE skills .", "author_fullname": "t2_cl5vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any DEs here have homelabs to upskill , make a dedicated dev env, and do other server side stuff ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x6mf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696180108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Planning to buy a Nuc 11 Enthusiast instead of a Steam Deck to upskill myself as well as not screwing my Gaming PC . Are there any ideas where i can start this journey . My aim to setup homelab is :  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Networking side with OpenWrt , pfSense &lt;/li&gt;\n&lt;li&gt;Containers/Virtualization  with Dockers and Vmware &lt;/li&gt;\n&lt;li&gt;Server Side with Proxmox&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Share if its sufficient to run multiple VMs and Containers with this thing as well as share how do you use homelab for upskilling your DE skills .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x6mf6", "is_robot_indexable": true, "report_reasons": null, "author": "Redxer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x6mf6/any_des_here_have_homelabs_to_upskill_make_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x6mf6/any_des_here_have_homelabs_to_upskill_make_a/", "subreddit_subscribers": 131490, "created_utc": 1696180108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Oct 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1696176058.352, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x4y7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696176058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?auto=webp&amp;s=02e6018b7f945f491d0b3a2effc39732c734f1e1", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dccc2af8931f0a3ac9ada660b34e9cba537b2fd1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24a390861290f321df46393893e52524fe7623f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53c673cdf215f673c496a58a993c4eac464fc2bb", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db34a5d7735459993d15b53c8a1b7aff4d3b2ec4", "width": 640, "height": 333}], "variants": {}, "id": "FmCaWz0_zzuMIYjO5Y9qwrC5XQ9HEDt0Z1CdPOgLQk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x4y7c", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/", "subreddit_subscribers": 131490, "created_utc": 1696176058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.\n\nAfter some time, I have come up with three solutions that I would like to share:\n\n* When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.\n* Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.\n* Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.\n\nBasically, I explained all three solutions in greater details in my newsletter [here](https://dbtips.substack.com/p/get-the-ultimate-developer-experience).\n\nDo you have any other tips of improving DX you can share?", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to improve your developer experience when working with dbt (and not only)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16x9maj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696186924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.&lt;/p&gt;\n\n&lt;p&gt;After some time, I have come up with three solutions that I would like to share:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.&lt;/li&gt;\n&lt;li&gt;Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.&lt;/li&gt;\n&lt;li&gt;Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, I explained all three solutions in greater details in my newsletter &lt;a href=\"https://dbtips.substack.com/p/get-the-ultimate-developer-experience\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Do you have any other tips of improving DX you can share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?auto=webp&amp;s=d3bb661896828bac4429562f6ce7fff4ef505422", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5af75ad43a854078e3cea834f3ef698663e88cdc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37ae9fc5f3b20c68d9199ed55882ebca83ef2855", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5685fef764bd1fa720328ebd972cc947f012bd86", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c958b3293f06f32b4616718b83399f29154edee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89fca96637cd1490f814b4b891f7bdc567c62f1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f972567e2eba31f2c54f818bdce4b34235f400e", "width": 1080, "height": 540}], "variants": {}, "id": "ntcEH-D8yVlbOqdDYiN0TVlVBkXGXM6b3WJWBcIF5sw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16x9maj", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "subreddit_subscribers": 131490, "created_utc": 1696186924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, ive been a data analyst in a FMCG corpo in a smaller BI team, working with microstrategy dwh, azure snowflake, databricks, and SAp, with strong python and sql(downstream analytical) skills. Ive worked in pretty mature data environment, where i had all the raw and unaggregated data \"served\" and could choose using DWH program for ad hoc small extractions, creating dashboards in it aswell, or doing longer/repeating processes with python/sql and extraction to excel.\nI mostly had self made python  libraries and programs which automated SAP and azure connections/sql script running and all the data transformation, excel storing and mail sending. Before i came in the office, processes were very slow and manually made. This is my background in short.\n\nOn new job ill be working in production and sales company, as DE, on AWS, and first time working on  \"other side\" of sql. Ill be working mostly on extraction of data from excel from various departments, some scraping, some data from softwares/web site. I believe its not as data mature company as one where i was before. As i understood, python is 90% of work. \n\nIll also have a smaller part of the job using tableu and creating dashboards. \n\nCurrently reading fundamentals of data engineering. Will focus on sources, how data is used and pipelines when i start working. Ill try to learn AWS and more about cloud. \n\nWhat good info, sources, suggestions and tips do you have for me? Thank you in advance", "author_fullname": "t2_nj1ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Landed new job, from DA to DE would like to hear your advice for first job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16x84y6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696183611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, ive been a data analyst in a FMCG corpo in a smaller BI team, working with microstrategy dwh, azure snowflake, databricks, and SAp, with strong python and sql(downstream analytical) skills. Ive worked in pretty mature data environment, where i had all the raw and unaggregated data &amp;quot;served&amp;quot; and could choose using DWH program for ad hoc small extractions, creating dashboards in it aswell, or doing longer/repeating processes with python/sql and extraction to excel.\nI mostly had self made python  libraries and programs which automated SAP and azure connections/sql script running and all the data transformation, excel storing and mail sending. Before i came in the office, processes were very slow and manually made. This is my background in short.&lt;/p&gt;\n\n&lt;p&gt;On new job ill be working in production and sales company, as DE, on AWS, and first time working on  &amp;quot;other side&amp;quot; of sql. Ill be working mostly on extraction of data from excel from various departments, some scraping, some data from softwares/web site. I believe its not as data mature company as one where i was before. As i understood, python is 90% of work. &lt;/p&gt;\n\n&lt;p&gt;Ill also have a smaller part of the job using tableu and creating dashboards. &lt;/p&gt;\n\n&lt;p&gt;Currently reading fundamentals of data engineering. Will focus on sources, how data is used and pipelines when i start working. Ill try to learn AWS and more about cloud. &lt;/p&gt;\n\n&lt;p&gt;What good info, sources, suggestions and tips do you have for me? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16x84y6", "is_robot_indexable": true, "report_reasons": null, "author": "Kichmad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x84y6/landed_new_job_from_da_to_de_would_like_to_hear/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x84y6/landed_new_job_from_da_to_de_would_like_to_hear/", "subreddit_subscribers": 131490, "created_utc": 1696183611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_94pk0qq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is my query running slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16wqq39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NsPslzl4Q5I5XShWyGQI5NvoYxxJfjkW3zbcfBjxLOQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696131748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "gizmodo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://gizmodo.com/startup-moves-closer-building-data-centers-moon-1850192177", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?auto=webp&amp;s=11ec4788f1d5867990e5c0c95bcf681df5bcaf68", "width": 970, "height": 546}, "resolutions": [{"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9022cd4abb16e345d50ec9be4692bdd89b4e6e9", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df4f067f183dfc2d16ce8da0fa64968850789cb", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ea3a5a9799e1204278fcc11650de99051fc858e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da2874826788e12f20abac009f7964547a8c0b88", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0e25d14c52b9d81a28a41c0ceeecb0fc199e4cc", "width": 960, "height": 540}], "variants": {}, "id": "2ahjXaciyCGSN3b6tmMi8DxjraHqmH4Cqpnx2GFHQrA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16wqq39", "is_robot_indexable": true, "report_reasons": null, "author": "Badger-Flaky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wqq39/why_is_my_query_running_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://gizmodo.com/startup-moves-closer-building-data-centers-moon-1850192177", "subreddit_subscribers": 131490, "created_utc": 1696131748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the strengths and weaknesses of AWS Glue vs ADF?", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Glue vs Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wghdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696104510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the strengths and weaknesses of AWS Glue vs ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wghdl", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wghdl/aws_glue_vs_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wghdl/aws_glue_vs_azure_data_factory/", "subreddit_subscribers": 131490, "created_utc": 1696104510.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}