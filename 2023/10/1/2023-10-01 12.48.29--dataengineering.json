{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're using AWS Managed Airflow and experiencing high resource utilization issues but have only 50 DAGs on a medium instance. DAGs are not dynamically created and they're all of the same type/using similar code to run. There's no clear culprit because nothing we're doing seems particularly resource-intensive. It doesn't seem to be related to DAG parse time based on relevant metrics. It might be related to memory usage but I haven't been able to find out what's taking up a lot of memory. What steps can we take to narrow down possible causes? Nothing stands out apart from the two areas I've just mentioned (DAG parsing or memory usage) so I'm looking for general guidance on areas to investigate further and how.", "author_fullname": "t2_jx5l6r0ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow performance issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wl0dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696115602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re using AWS Managed Airflow and experiencing high resource utilization issues but have only 50 DAGs on a medium instance. DAGs are not dynamically created and they&amp;#39;re all of the same type/using similar code to run. There&amp;#39;s no clear culprit because nothing we&amp;#39;re doing seems particularly resource-intensive. It doesn&amp;#39;t seem to be related to DAG parse time based on relevant metrics. It might be related to memory usage but I haven&amp;#39;t been able to find out what&amp;#39;s taking up a lot of memory. What steps can we take to narrow down possible causes? Nothing stands out apart from the two areas I&amp;#39;ve just mentioned (DAG parsing or memory usage) so I&amp;#39;m looking for general guidance on areas to investigate further and how.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wl0dz", "is_robot_indexable": true, "report_reasons": null, "author": "IllRepresentative858", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wl0dz/airflow_performance_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wl0dz/airflow_performance_issues/", "subreddit_subscribers": 131435, "created_utc": 1696115602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to know according to the data governance roles, what should the team who is  responsible for managing, analyzing and  providing the data to the customers be called? \nThey\u2019re not the data owners. They\u2019re moving and managing it.", "author_fullname": "t2_k27v1kru", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Governance Roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16w8wzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696085700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to know according to the data governance roles, what should the team who is  responsible for managing, analyzing and  providing the data to the customers be called? \nThey\u2019re not the data owners. They\u2019re moving and managing it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16w8wzu", "is_robot_indexable": true, "report_reasons": null, "author": "Outside-Seaweed8464", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16w8wzu/data_governance_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16w8wzu/data_governance_roles/", "subreddit_subscribers": 131435, "created_utc": 1696085700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been using snowflake-pulumi to define database objects like storage integrations, users, roles, schemas and so on. It works well for connecting AWS to Snowflake but is very painful to use for Snowflake objects. \n\nSo we are looking into dedicated tools for this. This tool is mostly to define the Snowflake infrastructure  (schemas, RBAC, external tables etc) as we use DBT for tables/views.\n\nI am currently evaluating [schemachange by Snowflake](https://github.com/Snowflake-Labs/schemachange). What tools are you using for database change management?", "author_fullname": "t2_pbcof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommendations for database change management for Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wgdrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696104264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been using snowflake-pulumi to define database objects like storage integrations, users, roles, schemas and so on. It works well for connecting AWS to Snowflake but is very painful to use for Snowflake objects. &lt;/p&gt;\n\n&lt;p&gt;So we are looking into dedicated tools for this. This tool is mostly to define the Snowflake infrastructure  (schemas, RBAC, external tables etc) as we use DBT for tables/views.&lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating &lt;a href=\"https://github.com/Snowflake-Labs/schemachange\"&gt;schemachange by Snowflake&lt;/a&gt;. What tools are you using for database change management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?auto=webp&amp;s=2bedc3b4e62956c2fc668b3d9c76cfa446780aa0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc4f9e13ee6cdbe7b9c1e3085b2baffef2c16ba4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb3ef7f0f338c6d2194d6062539bede95f87f397", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d35415883c213b6580d54f1966b78559be0a5cb5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51aa4e03cf7358a2b214f58477e649a6b35606cc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b86e3f7ae5b2bcb1f416de2a981556a3cef68115", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3hJfbrka6rAp8ffiR7y5l0vTl93-Z5hxKvrWWypdKJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e1abd5d48ca5a2c595f3793d7192f6d353b174f", "width": 1080, "height": 540}], "variants": {}, "id": "yNb2wBkvBc5pn2HPMQUhawwmqIhcCNf5oUQ5id_qneA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16wgdrw", "is_robot_indexable": true, "report_reasons": null, "author": "vish4life", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wgdrw/any_recommendations_for_database_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wgdrw/any_recommendations_for_database_change/", "subreddit_subscribers": 131435, "created_utc": 1696104264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We just inherited a large LookML project with ~1000 .lkml files (and no tests). It has view / explore / model definitions that support internal dashboards and a metrics API hit by our web app backend for customer facing reports.\n\nWe\u2019re seeing avg API response times from Looker around 1+ seconds even when the Snowflake DB it\u2019s hitting is returning from the result cache with quick response. Some requests can be much, much slower too. This is causing downstream problems in our web app.\n\nMost of the team has used Looker before but none of us have had to build out a LookML layer like this before. It feels overly complex and harder to follow than the equivalent SQL / SQLAlechemy would be.\n\nOur initial instinct is to refactor this logic out of Looker and into our modeling layer (DBT), but the performance with the Looker API seems so bad that we\u2019re considering just rolling our own a python API with SQLAlchemy.\n\nHas anyone here been in a similar predicament with Looker / LookML? Any advice on how we can improve performance / maintainability?", "author_fullname": "t2_axl2rt05", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have advice for optimizing a large LookML project that serves customer facing metics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wjyrx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696112995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just inherited a large LookML project with ~1000 .lkml files (and no tests). It has view / explore / model definitions that support internal dashboards and a metrics API hit by our web app backend for customer facing reports.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re seeing avg API response times from Looker around 1+ seconds even when the Snowflake DB it\u2019s hitting is returning from the result cache with quick response. Some requests can be much, much slower too. This is causing downstream problems in our web app.&lt;/p&gt;\n\n&lt;p&gt;Most of the team has used Looker before but none of us have had to build out a LookML layer like this before. It feels overly complex and harder to follow than the equivalent SQL / SQLAlechemy would be.&lt;/p&gt;\n\n&lt;p&gt;Our initial instinct is to refactor this logic out of Looker and into our modeling layer (DBT), but the performance with the Looker API seems so bad that we\u2019re considering just rolling our own a python API with SQLAlchemy.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here been in a similar predicament with Looker / LookML? Any advice on how we can improve performance / maintainability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16wjyrx", "is_robot_indexable": true, "report_reasons": null, "author": "deepfuckingbass", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wjyrx/does_anyone_have_advice_for_optimizing_a_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wjyrx/does_anyone_have_advice_for_optimizing_a_large/", "subreddit_subscribers": 131435, "created_utc": 1696112995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the strengths and weaknesses of AWS Glue vs ADF?", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Glue vs Azure Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wghdl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696104510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the strengths and weaknesses of AWS Glue vs ADF?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wghdl", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wghdl/aws_glue_vs_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wghdl/aws_glue_vs_azure_data_factory/", "subreddit_subscribers": 131435, "created_utc": 1696104510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a way to link Github to Airbyte UI. From the documentation it seems like the only way is to have a repo with yaml or json config file that call the Airbyte API", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte Git Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16weia5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696099563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to link Github to Airbyte UI. From the documentation it seems like the only way is to have a repo with yaml or json config file that call the Airbyte API&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16weia5", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16weia5/airbyte_git_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16weia5/airbyte_git_integration/", "subreddit_subscribers": 131435, "created_utc": 1696099563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Context**\n\nWe have data pipelines that are a mixture of SQL / python as they are a combo of:\n\n \\- more 'data engineering-y' transformations at the start (SQL)\n\n \\- more 'data science-y' operations further down (python)\n\nWe run on GCP so given we aren't at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.\n\nOrganising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.\n\nHowever, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I'd like to bring it in to the pipelines.\n\n**Question**\n\nHow do you introduce dbt to this infrastructure?\n\nBy having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?\n\nOr by starting with dbt's recommended monorepo, but then meaning a pipeline's logic is now split between:\n\n \\- pipeline repo\n\n \\- that pipeline's section in the dbt mono-repo\n\nI haven't been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?", "author_fullname": "t2_576k5sn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt mono-repo vs individual pipeline repos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16wy2vv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696157594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We have data pipelines that are a mixture of SQL / python as they are a combo of:&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data engineering-y&amp;#39; transformations at the start (SQL)&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data science-y&amp;#39; operations further down (python)&lt;/p&gt;\n\n&lt;p&gt;We run on GCP so given we aren&amp;#39;t at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.&lt;/p&gt;\n\n&lt;p&gt;Organising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.&lt;/p&gt;\n\n&lt;p&gt;However, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I&amp;#39;d like to bring it in to the pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you introduce dbt to this infrastructure?&lt;/p&gt;\n\n&lt;p&gt;By having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?&lt;/p&gt;\n\n&lt;p&gt;Or by starting with dbt&amp;#39;s recommended monorepo, but then meaning a pipeline&amp;#39;s logic is now split between:&lt;/p&gt;\n\n&lt;p&gt;- pipeline repo&lt;/p&gt;\n\n&lt;p&gt;- that pipeline&amp;#39;s section in the dbt mono-repo&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wy2vv", "is_robot_indexable": true, "report_reasons": null, "author": "mjam03", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "subreddit_subscribers": 131435, "created_utc": 1696157594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don't want a complex solution which involves many components and there's no need to scale computing beyond a single node.\n\nit seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.\n\nour preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn't require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.\n\nanyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?", "author_fullname": "t2_mojp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to query JSON and parquet files on S3 for data verification and pipeline building?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16wysjd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696159977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don&amp;#39;t want a complex solution which involves many components and there&amp;#39;s no need to scale computing beyond a single node.&lt;/p&gt;\n\n&lt;p&gt;it seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.&lt;/p&gt;\n\n&lt;p&gt;our preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn&amp;#39;t require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.&lt;/p&gt;\n\n&lt;p&gt;anyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wysjd", "is_robot_indexable": true, "report_reasons": null, "author": "OneCyrus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "subreddit_subscribers": 131435, "created_utc": 1696159977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to build a daily pipeline for Times and Sales derivatives data from this website name [https://www.sgx.com/research-education/derivatives](https://www.sgx.com/research-education/derivatives). Now I have a few question about exception handling:  \n1. If the downloading failed on some days how do you re-download the missed file(s)? What is the best strategy for it ?\n\n2. Some error can appear in the scenario of downloading file from here ?   \n   \n", "author_fullname": "t2_9axjxbxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wdfm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696096914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to build a daily pipeline for Times and Sales derivatives data from this website name &lt;a href=\"https://www.sgx.com/research-education/derivatives\"&gt;https://www.sgx.com/research-education/derivatives&lt;/a&gt;. Now I have a few question about exception handling:&lt;br/&gt;\n1. If the downloading failed on some days how do you re-download the missed file(s)? What is the best strategy for it ?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some error can appear in the scenario of downloading file from here ?&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?auto=webp&amp;s=a8c317969beed8dd1702fa11050007b40fdcf368", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dc8bbbf9c6482d8cfe893ec398c5d11dbc5e096", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2d047300d275d67178b959b644f27f23f995d21", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b6d994229174fb32061a3b10acece70d8fabcfc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0dd21e739827ab60ea488b67d69f6d6479f41b0b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ecd8ce5bb6303c6281d269eebd64f8083a5c5b29", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0iF_CKP4g0IdRh-4dXTLlPQhDzp2twy5Kuo-6lfDuF0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b1c732883a59218acfb8ff548a89d7ec4c1d8b1", "width": 1080, "height": 540}], "variants": {}, "id": "Z8FLd_bT6olULrx3zQDUG3CyuiZaCgt6N0QznAXq14s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wdfm7", "is_robot_indexable": true, "report_reasons": null, "author": "Previous_Aside_8863", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wdfm7/data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wdfm7/data_engineering_project/", "subreddit_subscribers": 131435, "created_utc": 1696096914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi,  \nI am a recent CS grad, currently working as an Associate Data Engineer, most of my work involves stuff like creating pipelines for moving data from prod to S3, refactoring old code. I am somewhat good at Python, Docker and SQL, but only the basics (for example, if you'd ask me how a query planner works or when is a UNION faster than an OR, I would not be able to answer). I have worked with Airflow a while ago but that was mostly looking up stuff from the documentation and calling functions and lining up operators(I don't remember much). I think that I don't know enough when I see my colleagues who understand distributed systems, cryptography and have a solid understanding of OS fundamentals. Like them, I would like to develop a broader skillset, better understand database internals, concurrency, networking and some Machine learning, to keep multiple options available, in case I decide to make a switch.  \nSo, my question to you all is, is doing something like this advisable or should I just focus on my current job?  \nAccording to me a good way to start learning would be to find an intersection of topics across these fields and start from there, but I would like to know from people who have achieved expertise in similar fields what their journey was like, if someone has switched from one to another did you have to start from an entry level role?  \nAlso, let me know what books and topics I should study. ", "author_fullname": "t2_2dgy01ix", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering + ML Engineering + Backend Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16w8gdc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696084613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI am a recent CS grad, currently working as an Associate Data Engineer, most of my work involves stuff like creating pipelines for moving data from prod to S3, refactoring old code. I am somewhat good at Python, Docker and SQL, but only the basics (for example, if you&amp;#39;d ask me how a query planner works or when is a UNION faster than an OR, I would not be able to answer). I have worked with Airflow a while ago but that was mostly looking up stuff from the documentation and calling functions and lining up operators(I don&amp;#39;t remember much). I think that I don&amp;#39;t know enough when I see my colleagues who understand distributed systems, cryptography and have a solid understanding of OS fundamentals. Like them, I would like to develop a broader skillset, better understand database internals, concurrency, networking and some Machine learning, to keep multiple options available, in case I decide to make a switch.&lt;br/&gt;\nSo, my question to you all is, is doing something like this advisable or should I just focus on my current job?&lt;br/&gt;\nAccording to me a good way to start learning would be to find an intersection of topics across these fields and start from there, but I would like to know from people who have achieved expertise in similar fields what their journey was like, if someone has switched from one to another did you have to start from an entry level role?&lt;br/&gt;\nAlso, let me know what books and topics I should study. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16w8gdc", "is_robot_indexable": true, "report_reasons": null, "author": "b_chaitanya", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16w8gdc/data_engineering_ml_engineering_backend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16w8gdc/data_engineering_ml_engineering_backend/", "subreddit_subscribers": 131435, "created_utc": 1696084613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_94pk0qq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is my query running slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16wqq39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NsPslzl4Q5I5XShWyGQI5NvoYxxJfjkW3zbcfBjxLOQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696131748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "gizmodo.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://gizmodo.com/startup-moves-closer-building-data-centers-moon-1850192177", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?auto=webp&amp;s=11ec4788f1d5867990e5c0c95bcf681df5bcaf68", "width": 970, "height": 546}, "resolutions": [{"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9022cd4abb16e345d50ec9be4692bdd89b4e6e9", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df4f067f183dfc2d16ce8da0fa64968850789cb", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4ea3a5a9799e1204278fcc11650de99051fc858e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da2874826788e12f20abac009f7964547a8c0b88", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/6ervbluLbV1xIJKhIF37R5xD9f847pCV3YkiGQcd2YM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0e25d14c52b9d81a28a41c0ceeecb0fc199e4cc", "width": 960, "height": 540}], "variants": {}, "id": "2ahjXaciyCGSN3b6tmMi8DxjraHqmH4Cqpnx2GFHQrA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16wqq39", "is_robot_indexable": true, "report_reasons": null, "author": "Badger-Flaky", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wqq39/why_is_my_query_running_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://gizmodo.com/startup-moves-closer-building-data-centers-moon-1850192177", "subreddit_subscribers": 131435, "created_utc": 1696131748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am a senior data engineer with 7 YOE. I got an offer for a staff data engineer. My question is what skills I need to upgrade to be staff data engineer?", "author_fullname": "t2_nytktitx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staff Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wagnh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.31, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696089444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am a senior data engineer with 7 YOE. I got an offer for a staff data engineer. My question is what skills I need to upgrade to be staff data engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wagnh", "is_robot_indexable": true, "report_reasons": null, "author": "Narrow_Primary223", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wagnh/staff_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wagnh/staff_data_engineer/", "subreddit_subscribers": 131435, "created_utc": 1696089444.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}