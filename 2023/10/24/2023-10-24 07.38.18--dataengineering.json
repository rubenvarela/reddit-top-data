{"kind": "Listing", "data": {"after": "t3_17eoegc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we're dealing with at work:\n\nMy goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.\n\nOther nuances is that the JSON nesting isnt always consistent-- right now, we see for example that 'dob' has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. \n\nTechnically, I can unnest without going through this complexity, but just wanted to be 100% sure I've explored all options\n\nThanks in advance for the help", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to unnest a json recursively", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_17enoxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cgWsf_KzQhOGBEpI2QRqwchmDAwtxl-R2Qu9XppzbOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698076385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we&amp;#39;re dealing with at work:&lt;/p&gt;\n\n&lt;p&gt;My goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.&lt;/p&gt;\n\n&lt;p&gt;Other nuances is that the JSON nesting isnt always consistent-- right now, we see for example that &amp;#39;dob&amp;#39; has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. &lt;/p&gt;\n\n&lt;p&gt;Technically, I can unnest without going through this complexity, but just wanted to be 100% sure I&amp;#39;ve explored all options&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f88mmvrw4zvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?auto=webp&amp;s=8328474a62e520df61153d0f2e28a86f65788cc1", "width": 792, "height": 515}, "resolutions": [{"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=984478729cb739bd5b66b9f6e7d9900fe45ff363", "width": 108, "height": 70}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed30fe70230d04b71b7dcb44525b676c51f9faa2", "width": 216, "height": 140}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d85c033e0523b07fb5ef8cb9f43619bf58a400cb", "width": 320, "height": 208}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b18022056383e51d51926fd2951f0bdc4e23635c", "width": 640, "height": 416}], "variants": {}, "id": "7B8x_aRgiSwSrrL4ftXRJ3VQggLC6QwoppjPOvJrBZQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17enoxk", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enoxk/how_to_unnest_a_json_recursively/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f88mmvrw4zvb1.jpg", "subreddit_subscribers": 135601, "created_utc": 1698076385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is for US roles.\n\nIt\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. \n\nI\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). \n\nI wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.", "author_fullname": "t2_fhmml14j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your opinion on working for low paying F500 companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17evkku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698096241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is for US roles.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). &lt;/p&gt;\n\n&lt;p&gt;I wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17evkku", "is_robot_indexable": true, "report_reasons": null, "author": "Capable-Jicama2155", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "subreddit_subscribers": 135601, "created_utc": 1698096241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you feel rushed with your work?\n\nSomething I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.\n\nI work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.\n\nI could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.\n\nI get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.\n\nEven before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  \n\nBut still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.\n\nI\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.", "author_fullname": "t2_c9apwds3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you always feel rushed with your deliverables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17etfeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you feel rushed with your work?&lt;/p&gt;\n\n&lt;p&gt;Something I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.&lt;/p&gt;\n\n&lt;p&gt;I work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.&lt;/p&gt;\n\n&lt;p&gt;I could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.&lt;/p&gt;\n\n&lt;p&gt;I get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.&lt;/p&gt;\n\n&lt;p&gt;Even before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  &lt;/p&gt;\n\n&lt;p&gt;But still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17etfeh", "is_robot_indexable": true, "report_reasons": null, "author": "american-roast", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "subreddit_subscribers": 135601, "created_utc": 1698090954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " While observing an educational presentation, the speaker displayed a slide emphasizing the complexity before the advent of Databricks Delta. This prompted me to consider the approach to resolving the validation aspect. So, How do you do it?\n\nhttps://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;format=png&amp;auto=webp&amp;s=780a38c414c64843f79b605fe8f32e8c50b25541", "author_fullname": "t2_84ztczxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenge : How you people do the validation part below?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yz9jwi5cxxvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7b6a97e8aa5113418d2fbe26fb030b102bf922c"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=18c50ca1158fe005de1412893e482ac4cb1448ff"}, {"y": 182, "x": 320, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e398a0e1d97013a7ff2964a80377d6629c151f1"}, {"y": 365, "x": 640, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e879c6836e8891df77e2e0da37a8a6ada848ea1"}, {"y": 548, "x": 960, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494be702a880da93cfbf93598ddf0f6ffada72a0"}, {"y": 617, "x": 1080, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1bab5c744e5d620d9be2a9c9e29a32fe76ffe4f"}], "s": {"y": 635, "x": 1111, "u": "https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;format=png&amp;auto=webp&amp;s=780a38c414c64843f79b605fe8f32e8c50b25541"}, "id": "yz9jwi5cxxvb1"}}, "name": "t3_17eijyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w8WCr6QrUV7_7kUCInkEyqC2ntRjW7rWZxAiCF1mKnE.jpg", "edited": 1698067340.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While observing an educational presentation, the speaker displayed a slide emphasizing the complexity before the advent of Databricks Delta. This prompted me to consider the approach to resolving the validation aspect. So, How do you do it?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=780a38c414c64843f79b605fe8f32e8c50b25541\"&gt;https://preview.redd.it/yz9jwi5cxxvb1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=780a38c414c64843f79b605fe8f32e8c50b25541&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17eijyc", "is_robot_indexable": true, "report_reasons": null, "author": "chaachans", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eijyc/challenge_how_you_people_do_the_validation_part/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eijyc/challenge_how_you_people_do_the_validation_part/", "subreddit_subscribers": 135601, "created_utc": 1698062054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.  \nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  \n\n\nA little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  \n\n\nWould be great if someone could help with good reading material, or videos  \n", "author_fullname": "t2_be03utda", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to data engineering, need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ep162", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.&lt;br/&gt;\nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  &lt;/p&gt;\n\n&lt;p&gt;A little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  &lt;/p&gt;\n\n&lt;p&gt;Would be great if someone could help with good reading material, or videos  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ep162", "is_robot_indexable": true, "report_reasons": null, "author": "Full-Natural5932", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "subreddit_subscribers": 135601, "created_utc": 1698079729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the main differences between the different databricks tiers (standard and premium) ?\n\nI\u2019ve been looking for a capabilities comparison between different tiers but not able to find anything beside costs.\n\nIs it worth to have premium instead of standard?\n\nFor instance, I know data lineage it\u2019s only available for premium tier. Is there anything else?", "author_fullname": "t2_9l5ldlre", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks standard vs premium", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17effag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698049271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the main differences between the different databricks tiers (standard and premium) ?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been looking for a capabilities comparison between different tiers but not able to find anything beside costs.&lt;/p&gt;\n\n&lt;p&gt;Is it worth to have premium instead of standard?&lt;/p&gt;\n\n&lt;p&gt;For instance, I know data lineage it\u2019s only available for premium tier. Is there anything else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17effag", "is_robot_indexable": true, "report_reasons": null, "author": "RoundReveal", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17effag/databricks_standard_vs_premium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17effag/databricks_standard_vs_premium/", "subreddit_subscribers": 135601, "created_utc": 1698049271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently engaged in a data management project and I'm seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.\n\nOur data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:\n\n&amp;#x200B;\n\n1. Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?\n2. How does DuckDB handle data deltas?\n3. Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?\n\n&amp;#x200B;\n\nI would greatly appreciate any insights, tips, and recommendations, thank you! \n\n&amp;#x200B;", "author_fullname": "t2_nz9uo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can DuckDB/MotherDuck Replace BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17et5a4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently engaged in a data management project and I&amp;#39;m seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.&lt;/p&gt;\n\n&lt;p&gt;Our data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?&lt;/li&gt;\n&lt;li&gt;How does DuckDB handle data deltas?&lt;/li&gt;\n&lt;li&gt;Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, tips, and recommendations, thank you! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17et5a4", "is_robot_indexable": true, "report_reasons": null, "author": "RX-Vortex", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "subreddit_subscribers": 135601, "created_utc": 1698090202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It's worked fairly well up until now, but there are several problems I'm hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.\n\nHere's what I'm envisioning:\n\n\\- custom tool (there's no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.\n\n\\- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  \n\n\\- \\[this is the part I'm most unclear on and SQL is failing the most\\].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central \"bronze\" table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  \n\n\\- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it's prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source's data, that describes more or less what I'm looking for.  So I don't know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I've got some ML in there as well, so moving the data offline is doable.\n\n\\- Using MERGE will make this more efficient without the overhead of SQL's locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)\n\n\\- Build gold/aggregate tables in data lake with \\[databricks?\\]\n\nPush the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.\n\nI have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We're not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I've got to have something in between for caching and refreshes.\n\n&amp;#x200B;", "author_fullname": "t2_gilvsz7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check architecture please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq1yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It&amp;#39;s worked fairly well up until now, but there are several problems I&amp;#39;m hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m envisioning:&lt;/p&gt;\n\n&lt;p&gt;- custom tool (there&amp;#39;s no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.&lt;/p&gt;\n\n&lt;p&gt;- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  &lt;/p&gt;\n\n&lt;p&gt;- [this is the part I&amp;#39;m most unclear on and SQL is failing the most].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central &amp;quot;bronze&amp;quot; table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  &lt;/p&gt;\n\n&lt;p&gt;- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it&amp;#39;s prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source&amp;#39;s data, that describes more or less what I&amp;#39;m looking for.  So I don&amp;#39;t know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I&amp;#39;ve got some ML in there as well, so moving the data offline is doable.&lt;/p&gt;\n\n&lt;p&gt;- Using MERGE will make this more efficient without the overhead of SQL&amp;#39;s locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)&lt;/p&gt;\n\n&lt;p&gt;- Build gold/aggregate tables in data lake with [databricks?]&lt;/p&gt;\n\n&lt;p&gt;Push the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.&lt;/p&gt;\n\n&lt;p&gt;I have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We&amp;#39;re not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I&amp;#39;ve got to have something in between for caching and refreshes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq1yd", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Log_8482", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "subreddit_subscribers": 135601, "created_utc": 1698082305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i have a lakehouse built on delta tables which are being fed with multilple Spark pipelines. I recently got asked to build the streaming pipeline with a strict performance requirements that will feed one of already existing tables. The load of new pipeline is super small (just few rows) therefore i wonder if this is possibile to use some more lightweight framework than Spark. I've looked on few do far but they suport the standard append/overwrite methods only. Is there any framework that supports delta lake merge like Spark does?", "author_fullname": "t2_bf5ei6em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Spark for lakehouse workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eemod", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698045537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i have a lakehouse built on delta tables which are being fed with multilple Spark pipelines. I recently got asked to build the streaming pipeline with a strict performance requirements that will feed one of already existing tables. The load of new pipeline is super small (just few rows) therefore i wonder if this is possibile to use some more lightweight framework than Spark. I&amp;#39;ve looked on few do far but they suport the standard append/overwrite methods only. Is there any framework that supports delta lake merge like Spark does?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eemod", "is_robot_indexable": true, "report_reasons": null, "author": "Beginning_Hurry2611", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eemod/alternative_to_spark_for_lakehouse_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eemod/alternative_to_spark_for_lakehouse_workflows/", "subreddit_subscribers": 135601, "created_utc": 1698045537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?\n\nMy idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.", "author_fullname": "t2_huqm5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MDM for contacts and addresses? Examples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f2av8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698114834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?&lt;/p&gt;\n\n&lt;p&gt;My idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f2av8", "is_robot_indexable": true, "report_reasons": null, "author": "tbarg91", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "subreddit_subscribers": 135601, "created_utc": 1698114834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. \n\nI am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. \n\nI have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. \n\nMy question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.\n\nThanks in advance!!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling - Modern Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eodmb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698078084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. &lt;/p&gt;\n\n&lt;p&gt;I am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. &lt;/p&gt;\n\n&lt;p&gt;I have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. &lt;/p&gt;\n\n&lt;p&gt;My question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eodmb", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "subreddit_subscribers": 135601, "created_utc": 1698078084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. \n\nJob 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there\n\nJob 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. \n\nJob 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here \n\nI have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. \n\nDo I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?", "author_fullname": "t2_b7eqz4bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On My Third Bad Job This Year\u2026How do I Salvage my Career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eud9r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698093278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. &lt;/p&gt;\n\n&lt;p&gt;Job 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there&lt;/p&gt;\n\n&lt;p&gt;Job 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. &lt;/p&gt;\n\n&lt;p&gt;Job 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here &lt;/p&gt;\n\n&lt;p&gt;I have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. &lt;/p&gt;\n\n&lt;p&gt;Do I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17eud9r", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Box228", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "subreddit_subscribers": 135601, "created_utc": 1698093278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.\n\nThe data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.\n\nGenerally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.\n\nI am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don't want to use a paginated report.\n\nDoes anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much detail is required for a Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq7fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.&lt;/p&gt;\n\n&lt;p&gt;The data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.&lt;/p&gt;\n\n&lt;p&gt;Generally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.&lt;/p&gt;\n\n&lt;p&gt;I am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don&amp;#39;t want to use a paginated report.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq7fw", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "subreddit_subscribers": 135601, "created_utc": 1698082725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context - \n\nCurrently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.\n\nThe DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don't trigger during development work, because something about their structure. \n\nSo far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. \n\n&amp;#x200B;", "author_fullname": "t2_vk8zz17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you structure DEV/UAT/PROD with DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enlrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context - &lt;/p&gt;\n\n&lt;p&gt;Currently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.&lt;/p&gt;\n\n&lt;p&gt;The DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don&amp;#39;t trigger during development work, because something about their structure. &lt;/p&gt;\n\n&lt;p&gt;So far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17enlrh", "is_robot_indexable": true, "report_reasons": null, "author": "KrixMercades", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "subreddit_subscribers": 135601, "created_utc": 1698076161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. \n\nMy job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. \n\nIs there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark sliding window within a window", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17emrm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698074072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. &lt;/p&gt;\n\n&lt;p&gt;My job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17emrm7", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "subreddit_subscribers": 135601, "created_utc": 1698074072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Configure VS Code to Develop Airflow DAGs in Docker at ease!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17elttv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17elttv", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yhN84w9nIeUQQpFrguMmWJid7Gh_zCqk3cid0v4svck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/fsMKV9A1B-I", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?auto=webp&amp;s=c578ca44010c7790f0bc08cf9ba73e7c60be5627", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d443dc814c8a8befeffcb24924974b7df32bdce2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d401374ee81fe607289fb56d722bfa0990f7f72", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4b864db7e7561a16ae0822bb500e174a66c4380", "width": 320, "height": 240}], "variants": {}, "id": "EjbV75W_SKL66g68oZtVMqNqSHRNtBErf-59fQyDhOA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17elttv", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17elttv/new_video_configure_vs_code_to_develop_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/fsMKV9A1B-I", "subreddit_subscribers": 135601, "created_utc": 1698071614.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a postgres database with a materialized view that has some complex logic. We would like to \"stream\" changes/data from this materialized view every 10mins or so to other system. What would be the best approach? We firstly thought of doing backups every 10mins but backups apparently cannot capture materialized view state/data and capturing only MV source data does not make sense as we would need to reimplement the MV logic.", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream changes from materialized view", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eires", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a postgres database with a materialized view that has some complex logic. We would like to &amp;quot;stream&amp;quot; changes/data from this materialized view every 10mins or so to other system. What would be the best approach? We firstly thought of doing backups every 10mins but backups apparently cannot capture materialized view state/data and capturing only MV source data does not make sense as we would need to reimplement the MV logic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17eires", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eires/stream_changes_from_materialized_view/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eires/stream_changes_from_materialized_view/", "subreddit_subscribers": 135601, "created_utc": 1698062717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,  \nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.  \nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job's dependencies on workers only - that way I can keep different environments for different jobs/DAGs.  \nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is \"just install everything in one image\". This greatly concerns me, as we are trying to run away from a *one venv to rule them all* solution.  \nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.", "author_fullname": "t2_11671y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow and python dependencies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17f5o2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698126313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.&lt;br/&gt;\nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job&amp;#39;s dependencies on workers only - that way I can keep different environments for different jobs/DAGs.&lt;br/&gt;\nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is &amp;quot;just install everything in one image&amp;quot;. This greatly concerns me, as we are trying to run away from a &lt;em&gt;one venv to rule them all&lt;/em&gt; solution.&lt;br/&gt;\nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f5o2k", "is_robot_indexable": true, "report_reasons": null, "author": "RealBrofessor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "subreddit_subscribers": 135601, "created_utc": 1698126313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently became interested in doing data extraction using MarketScan and have been watching some YouTube videos. The resource I utilize to learn this is called \"Redivis.\" Are there other resources I can utilize to learn this skill?", "author_fullname": "t2_2zwndxlf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I learn data extraction from MarketScan?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f4r66", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698122869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently became interested in doing data extraction using MarketScan and have been watching some YouTube videos. The resource I utilize to learn this is called &amp;quot;Redivis.&amp;quot; Are there other resources I can utilize to learn this skill?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17f4r66", "is_robot_indexable": true, "report_reasons": null, "author": "phymathnerd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f4r66/how_do_i_learn_data_extraction_from_marketscan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f4r66/how_do_i_learn_data_extraction_from_marketscan/", "subreddit_subscribers": 135601, "created_utc": 1698122869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).\n\nOne of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.\n\nI was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.", "author_fullname": "t2_6qoeic3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bigquery Omni Vs. Transfer for AWS, thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f0heb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698109554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).&lt;/p&gt;\n\n&lt;p&gt;One of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.&lt;/p&gt;\n\n&lt;p&gt;I was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f0heb", "is_robot_indexable": true, "report_reasons": null, "author": "Dumac89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "subreddit_subscribers": 135601, "created_utc": 1698109554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.\n\nMy question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.\n\nAny ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...", "author_fullname": "t2_31exp9bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Midware solution for data analytics tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17euvsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698094583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.&lt;/p&gt;\n\n&lt;p&gt;My question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.&lt;/p&gt;\n\n&lt;p&gt;Any ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17euvsp", "is_robot_indexable": true, "report_reasons": null, "author": "Sonny-Orkidea", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "subreddit_subscribers": 135601, "created_utc": 1698094583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can't find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.", "author_fullname": "t2_n52imw1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "export WhatsApp business group chat data in the structure format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eo0tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698077435.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698077193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can&amp;#39;t find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eo0tu", "is_robot_indexable": true, "report_reasons": null, "author": "SurpriseLopsided5536", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "subreddit_subscribers": 135601, "created_utc": 1698077193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Sai, CEO of PeerDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17ekedw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iZVPEagtKd4g-yZ1SN-JXfC_3KqR03WLkeqHwug84FI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698067631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-sai-ceo-of-peerdb?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/N_ro3wuF5xRpqIcQRIVm72fejyoOccTTLtmJEbOXvYQ.jpg?auto=webp&amp;s=932168d967dfe8ac8fd2930ad813a8f8b39a9c5c", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/N_ro3wuF5xRpqIcQRIVm72fejyoOccTTLtmJEbOXvYQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d97700e259c2beb2704b5939561eff65eae5015", "width": 108, "height": 108}], "variants": {}, "id": "-AnNYmf6lpdn2cWORPDWCAHghdmrGM0zk9Wk0x7XetI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ekedw", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ekedw/interview_with_sai_ceo_of_peerdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-sai-ceo-of-peerdb?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 135601, "created_utc": 1698067631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mcyts2q16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hightouch is Just Blindly Copying Rudderstack. Wdyt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17ei53w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sDwL2apsQ5IPJlE_ZltY0TEZ5AGhlDymEC9Dai72kSc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698060607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@laurengreerbalik/hightouch-is-just-blindly-copying-rudderstack-2e80dba56b27", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?auto=webp&amp;s=237924c2b793490cb791291230a57ac09aa42321", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cbfed6233884a1313c908e72e343e658147f3e8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c35419d3ecd5b060ae46f7aea29dbd81797b0113", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88fcb11dcac0936899ef71273fcc8256ede57d18", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea56d650df632e6c7afb83bc70b27f726fcbf0f6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf869bcf795aa702d607012d02c22de4a4244e65", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/f4irtg7XBL5nhKRR8eC3SOLaJT9iWcrlhQiThZC5KH4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f853e455af82e53d80eb0e9d5858ef8f27310b43", "width": 1080, "height": 607}], "variants": {}, "id": "uu9S5DZa4nCucZGVTepV-dgBiy0gM3a95Z7UB-V6MDM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ei53w", "is_robot_indexable": true, "report_reasons": null, "author": "aditichauhanofficial", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ei53w/hightouch_is_just_blindly_copying_rudderstack_wdyt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@laurengreerbalik/hightouch-is-just-blindly-copying-rudderstack-2e80dba56b27", "subreddit_subscribers": 135601, "created_utc": 1698060607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Enterprise data lakehouses are a necessity for any company competing in today's data-driven world. For a useful centralized data lake to exist, data must be ingested from disparate sources. Since this generated data comes from multiple sources, each with its own format and protocol, getting this data in and out of the data lake in a consumable form is a basic requirement. If data is a competitive advantage, then the timeliness of it flowing into the data lakehouse from all over the organization and partners is paramount for success.\n\n[https://blog.min.io/simplify-data-pipelines/?utm\\_source=reddit&amp;utm\\_medium=organic-social+&amp;utm\\_campaign=simplify\\_data\\_pipelines+](https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;utm_medium=organic-social+&amp;utm_campaign=simplify_data_pipelines+)", "author_fullname": "t2_csphaytka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplify Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eoegc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698078136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Enterprise data lakehouses are a necessity for any company competing in today&amp;#39;s data-driven world. For a useful centralized data lake to exist, data must be ingested from disparate sources. Since this generated data comes from multiple sources, each with its own format and protocol, getting this data in and out of the data lake in a consumable form is a basic requirement. If data is a competitive advantage, then the timeliness of it flowing into the data lakehouse from all over the organization and partners is paramount for success.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=simplify_data_pipelines+\"&gt;https://blog.min.io/simplify-data-pipelines/?utm_source=reddit&amp;amp;utm_medium=organic-social+&amp;amp;utm_campaign=simplify_data_pipelines+&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?auto=webp&amp;s=7a6d3e750a2198d04daec527d19817baf8a96b58", "width": 2000, "height": 3000}, "resolutions": [{"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2fd809163488839b5b561948270b4d17758476f", "width": 108, "height": 162}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0821888c1edaed067738a126039a03a3aeb692e6", "width": 216, "height": 324}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9bcf7bb6093c1432295963b2c104f7ab05c4883", "width": 320, "height": 480}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ce2fd640d5686428f23ce0ab18b3af4ab668b31", "width": 640, "height": 960}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37146372724b1350eed041b92837d5b16989d2c7", "width": 960, "height": 1440}, {"url": "https://external-preview.redd.it/Li0PZ8q6h6Ia50-PSt9xvKpMKGf0LxE_PMp9udlQQ-Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac0e79ec6c659aebfc5aaa35de9f544c0bd2119a", "width": 1080, "height": 1620}], "variants": {}, "id": "spKdhQnj8fC4Mxkw4OJSOuMPnts3sOPQU4tNyW5TKag"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eoegc", "is_robot_indexable": true, "report_reasons": null, "author": "swodtke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eoegc/simplify_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eoegc/simplify_data_pipelines/", "subreddit_subscribers": 135601, "created_utc": 1698078136.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}