{"kind": "Listing", "data": {"after": "t3_17eo0tu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we're dealing with at work:\n\nMy goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.\n\nOther nuances is that the JSON nesting isnt always consistent-- right now, we see for example that 'dob' has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. \n\nTechnically, I can unnest without going through this complexity, but just wanted to be 100% sure I've explored all options\n\nThanks in advance for the help", "author_fullname": "t2_7ki1otgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to unnest a json recursively", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "name": "t3_17enoxk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/cgWsf_KzQhOGBEpI2QRqwchmDAwtxl-R2Qu9XppzbOE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698076385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I have a JSON structure like this-- and honestly, this is an oversimplified version of what we&amp;#39;re dealing with at work:&lt;/p&gt;\n\n&lt;p&gt;My goal is to create a Python script that can recursively unnest this JSON, such that each array in the JSON is flattened/normalized into its own dataframes. We want it in separate tables because the granularities arent the same across arrays (not shown in this sample)-- some are on order level, some on order-item level (i.e. one row per item purchase,), some on customer level, all within the same object.&lt;/p&gt;\n\n&lt;p&gt;Other nuances is that the JSON nesting isnt always consistent-- right now, we see for example that &amp;#39;dob&amp;#39; has entries for month-year-date, sometimes that can be empty or saved as a list with an array inside. &lt;/p&gt;\n\n&lt;p&gt;Technically, I can unnest without going through this complexity, but just wanted to be 100% sure I&amp;#39;ve explored all options&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f88mmvrw4zvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?auto=webp&amp;s=8328474a62e520df61153d0f2e28a86f65788cc1", "width": 792, "height": 515}, "resolutions": [{"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=984478729cb739bd5b66b9f6e7d9900fe45ff363", "width": 108, "height": 70}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed30fe70230d04b71b7dcb44525b676c51f9faa2", "width": 216, "height": 140}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d85c033e0523b07fb5ef8cb9f43619bf58a400cb", "width": 320, "height": 208}, {"url": "https://preview.redd.it/f88mmvrw4zvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b18022056383e51d51926fd2951f0bdc4e23635c", "width": 640, "height": 416}], "variants": {}, "id": "7B8x_aRgiSwSrrL4ftXRJ3VQggLC6QwoppjPOvJrBZQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17enoxk", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Soup4733", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enoxk/how_to_unnest_a_json_recursively/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f88mmvrw4zvb1.jpg", "subreddit_subscribers": 135627, "created_utc": 1698076385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is for US roles.\n\nIt\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. \n\nI\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). \n\nI wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.", "author_fullname": "t2_fhmml14j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your opinion on working for low paying F500 companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17evkku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698096241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is for US roles.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). &lt;/p&gt;\n\n&lt;p&gt;I wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17evkku", "is_robot_indexable": true, "report_reasons": null, "author": "Capable-Jicama2155", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "subreddit_subscribers": 135627, "created_utc": 1698096241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you feel rushed with your work?\n\nSomething I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.\n\nI work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.\n\nI could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.\n\nI get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.\n\nEven before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  \n\nBut still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.\n\nI\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.", "author_fullname": "t2_c9apwds3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you always feel rushed with your deliverables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17etfeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you feel rushed with your work?&lt;/p&gt;\n\n&lt;p&gt;Something I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.&lt;/p&gt;\n\n&lt;p&gt;I work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.&lt;/p&gt;\n\n&lt;p&gt;I could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.&lt;/p&gt;\n\n&lt;p&gt;I get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.&lt;/p&gt;\n\n&lt;p&gt;Even before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  &lt;/p&gt;\n\n&lt;p&gt;But still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17etfeh", "is_robot_indexable": true, "report_reasons": null, "author": "american-roast", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "subreddit_subscribers": 135627, "created_utc": 1698090954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.  \nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  \n\n\nA little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  \n\n\nWould be great if someone could help with good reading material, or videos  \n", "author_fullname": "t2_be03utda", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to data engineering, need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ep162", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have recently joined as a Software Engineer in a mid-sized startup, my title is Software Engineer.&lt;br/&gt;\nPrimary tech stack is Spark,Python(Pyspark),Azure cloud, Databricks. So far I have got basic cookie-cutter tasks, which mainly involve writing or modifying existing spark SQL queries. I have a basic understanding of Spark, which has helped me till now. But I want to explore the field more, how can I go about learning stuff which will help me not just in my role, but also enhance my overall understanding.  &lt;/p&gt;\n\n&lt;p&gt;A little background, I have previously worked as a Backend engineer(Java,Spring Boot,Microservices,Docker,basic AWS). What all things should I learn?  &lt;/p&gt;\n\n&lt;p&gt;Would be great if someone could help with good reading material, or videos  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ep162", "is_robot_indexable": true, "report_reasons": null, "author": "Full-Natural5932", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ep162/new_to_data_engineering_need_advice/", "subreddit_subscribers": 135627, "created_utc": 1698079729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience working with Informatica as an ETL tool but want to switch to big data technologies. I have started learning different tools in hadoop ecosystem but there are so many of them for different purposes. I am confused on which one's to learn in depth.\nLooking at job openings I have seen Spark, Hive, Sqoop,   Kafka, Spark streaming, etc. being used more.\nBut what about other tools like Flume, Storm, Hue, Hbase, and many more. Should I completely skip them?\n\nOr\n\nPlease give me a list of big data tools to learn that will help me switch to big data roles. I dont want to waste time learning tools like Dril, phoenix which no one uses as far as I have seen.", "author_fullname": "t2_ht9x5dmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So many tools to learn!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f6vkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698131406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience working with Informatica as an ETL tool but want to switch to big data technologies. I have started learning different tools in hadoop ecosystem but there are so many of them for different purposes. I am confused on which one&amp;#39;s to learn in depth.\nLooking at job openings I have seen Spark, Hive, Sqoop,   Kafka, Spark streaming, etc. being used more.\nBut what about other tools like Flume, Storm, Hue, Hbase, and many more. Should I completely skip them?&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Please give me a list of big data tools to learn that will help me switch to big data roles. I dont want to waste time learning tools like Dril, phoenix which no one uses as far as I have seen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f6vkz", "is_robot_indexable": true, "report_reasons": null, "author": "kaachejl", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f6vkz/so_many_tools_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f6vkz/so_many_tools_to_learn/", "subreddit_subscribers": 135627, "created_utc": 1698131406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently engaged in a data management project and I'm seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.\n\nOur data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:\n\n&amp;#x200B;\n\n1. Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?\n2. How does DuckDB handle data deltas?\n3. Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?\n\n&amp;#x200B;\n\nI would greatly appreciate any insights, tips, and recommendations, thank you! \n\n&amp;#x200B;", "author_fullname": "t2_nz9uo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can DuckDB/MotherDuck Replace BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17et5a4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently engaged in a data management project and I&amp;#39;m seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.&lt;/p&gt;\n\n&lt;p&gt;Our data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?&lt;/li&gt;\n&lt;li&gt;How does DuckDB handle data deltas?&lt;/li&gt;\n&lt;li&gt;Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, tips, and recommendations, thank you! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17et5a4", "is_robot_indexable": true, "report_reasons": null, "author": "RX-Vortex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "subreddit_subscribers": 135627, "created_utc": 1698090202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It's worked fairly well up until now, but there are several problems I'm hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.\n\nHere's what I'm envisioning:\n\n\\- custom tool (there's no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.\n\n\\- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  \n\n\\- \\[this is the part I'm most unclear on and SQL is failing the most\\].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central \"bronze\" table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  \n\n\\- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it's prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source's data, that describes more or less what I'm looking for.  So I don't know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I've got some ML in there as well, so moving the data offline is doable.\n\n\\- Using MERGE will make this more efficient without the overhead of SQL's locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)\n\n\\- Build gold/aggregate tables in data lake with \\[databricks?\\]\n\nPush the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.\n\nI have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We're not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I've got to have something in between for caching and refreshes.\n\n&amp;#x200B;", "author_fullname": "t2_gilvsz7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check architecture please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq1yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It&amp;#39;s worked fairly well up until now, but there are several problems I&amp;#39;m hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m envisioning:&lt;/p&gt;\n\n&lt;p&gt;- custom tool (there&amp;#39;s no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.&lt;/p&gt;\n\n&lt;p&gt;- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  &lt;/p&gt;\n\n&lt;p&gt;- [this is the part I&amp;#39;m most unclear on and SQL is failing the most].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central &amp;quot;bronze&amp;quot; table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  &lt;/p&gt;\n\n&lt;p&gt;- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it&amp;#39;s prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source&amp;#39;s data, that describes more or less what I&amp;#39;m looking for.  So I don&amp;#39;t know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I&amp;#39;ve got some ML in there as well, so moving the data offline is doable.&lt;/p&gt;\n\n&lt;p&gt;- Using MERGE will make this more efficient without the overhead of SQL&amp;#39;s locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)&lt;/p&gt;\n\n&lt;p&gt;- Build gold/aggregate tables in data lake with [databricks?]&lt;/p&gt;\n\n&lt;p&gt;Push the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.&lt;/p&gt;\n\n&lt;p&gt;I have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We&amp;#39;re not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I&amp;#39;ve got to have something in between for caching and refreshes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq1yd", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Log_8482", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "subreddit_subscribers": 135627, "created_utc": 1698082305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. \n\nJob 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there\n\nJob 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. \n\nJob 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here \n\nI have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. \n\nDo I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?", "author_fullname": "t2_b7eqz4bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On My Third Bad Job This Year\u2026How do I Salvage my Career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eud9r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698093278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. &lt;/p&gt;\n\n&lt;p&gt;Job 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there&lt;/p&gt;\n\n&lt;p&gt;Job 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. &lt;/p&gt;\n\n&lt;p&gt;Job 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here &lt;/p&gt;\n\n&lt;p&gt;I have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. &lt;/p&gt;\n\n&lt;p&gt;Do I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17eud9r", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Box228", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "subreddit_subscribers": 135627, "created_utc": 1698093278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?\n\nMy idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.", "author_fullname": "t2_huqm5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MDM for contacts and addresses? Examples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f2av8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698114834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?&lt;/p&gt;\n\n&lt;p&gt;My idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f2av8", "is_robot_indexable": true, "report_reasons": null, "author": "tbarg91", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "subreddit_subscribers": 135627, "created_utc": 1698114834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. \n\nI am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. \n\nI have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. \n\nMy question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.\n\nThanks in advance!!", "author_fullname": "t2_9rbxc91v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling - Modern Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eodmb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698078084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I\u2019m in Snowflake operating on some large datasets. 10BN row fact tables, across multiple client environments. &lt;/p&gt;\n\n&lt;p&gt;I am POCing other tools out there now. Currently we are in Snowflake with db replication from our source environments, we have a transformation tool as well. &lt;/p&gt;\n\n&lt;p&gt;I have a small team and I\u2019m looking to move from our current transformation tool (qlik compose) to something like Coalesce.io, dbt or anything else. &lt;/p&gt;\n\n&lt;p&gt;My question is\u2026 What are the best tools to manage large data models with smaller teams  and specifically I want to manage our data infrastructure as a \u201cproduct\u201d not so much by client environment.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eodmb", "is_robot_indexable": true, "report_reasons": null, "author": "AnnualDepth8843", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eodmb/data_modeling_modern_stack/", "subreddit_subscribers": 135627, "created_utc": 1698078084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.\n\nThe data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.\n\nGenerally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.\n\nI am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don't want to use a paginated report.\n\nDoes anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much detail is required for a Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq7fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.&lt;/p&gt;\n\n&lt;p&gt;The data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.&lt;/p&gt;\n\n&lt;p&gt;Generally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.&lt;/p&gt;\n\n&lt;p&gt;I am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don&amp;#39;t want to use a paginated report.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq7fw", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "subreddit_subscribers": 135627, "created_utc": 1698082725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Announcing v0.15: Interactive Declarative Migrations, Functions, Procedures and Domains", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17fakcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nhBgVCSkn43mEZZuegXT044GK2frFJgTtRt1LS1YMJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698147009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/10/19/atlas-v-0-15", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?auto=webp&amp;s=1414e9580f0d5089f4769ab2f2dcf09a48ef0eab", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f75cc7278b4601814b4fe2ac8c6d401e3d76a85", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a195df6f7b63ccc0ed1fc2acd6ff54e5892a64e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a24f0728b182efec43a4f4bcce3bba439970957b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b72d2ebcc090ddfdb55d337b468a87ea00c6d0ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a78605ab50cea5dc4a89f243953163ce50015b2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7452ef2bb59607883b5a2f7c165be3a43579b90", "width": 1080, "height": 567}], "variants": {}, "id": "YeWm-cV5fn_dffXIIXchwIx5O5_ErL1r2-7yIlehqe8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17fakcd", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fakcd/announcing_v015_interactive_declarative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/10/19/atlas-v-0-15", "subreddit_subscribers": 135627, "created_utc": 1698147009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks\n\n  \nwe at dlthub added a very cool feature for copying production databases. By using ConnectorX and arrow, the sql -&gt; analytics copying can go up to 30x faster over the classic sqlite connector.\n\nRead about the benchmark comparison and the underlying technology here: [https://dlthub.com/docs/blog/dlt-arrow-loading](https://dlthub.com/docs/blog/dlt-arrow-loading) \n\nOne disclaimer is that since this method does not do row by row processing, we cannot microbatch the data through small buffers - so pay attention to the memory size on your extraction machine.  \nCode example how to use: [https://dlthub.com/docs/examples/connector\\_x\\_arrow/](https://dlthub.com/docs/examples/connector_x_arrow/)\n\nBy adding this support, we also enable these sources:  \n[https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas](https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas)  \n\n\nIf you need help, don't miss the gpt helper link at the bottom of our docs or the slack link at the top.\n\nFeedback is very welcome!\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get 30x speedups when reading databases with ConnectorX + Arrow + dlt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f9arc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698142214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks&lt;/p&gt;\n\n&lt;p&gt;we at dlthub added a very cool feature for copying production databases. By using ConnectorX and arrow, the sql -&amp;gt; analytics copying can go up to 30x faster over the classic sqlite connector.&lt;/p&gt;\n\n&lt;p&gt;Read about the benchmark comparison and the underlying technology here: &lt;a href=\"https://dlthub.com/docs/blog/dlt-arrow-loading\"&gt;https://dlthub.com/docs/blog/dlt-arrow-loading&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;One disclaimer is that since this method does not do row by row processing, we cannot microbatch the data through small buffers - so pay attention to the memory size on your extraction machine.&lt;br/&gt;\nCode example how to use: &lt;a href=\"https://dlthub.com/docs/examples/connector_x_arrow/\"&gt;https://dlthub.com/docs/examples/connector_x_arrow/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;By adding this support, we also enable these sources:&lt;br/&gt;\n&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas\"&gt;https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;If you need help, don&amp;#39;t miss the gpt helper link at the bottom of our docs or the slack link at the top.&lt;/p&gt;\n\n&lt;p&gt;Feedback is very welcome!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?auto=webp&amp;s=84b3e18f109f221a97f70a438603578a4cc30a30", "width": 1309, "height": 517}, "resolutions": [{"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07f5c414d1f377a0af62579d1879c251208a002d", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d366d379f340396f585011f76db6b5167876c8a", "width": 216, "height": 85}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=612a47aa4d36d7cbabd424857fa08ac068b5b251", "width": 320, "height": 126}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f9200099e81460a0137696884981d75dbcc7128", "width": 640, "height": 252}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f143bf5100e86da625d4b5c285d27a2019c5b024", "width": 960, "height": 379}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7786c8602fd15012d8b83199f5529b88efbaee6", "width": 1080, "height": 426}], "variants": {}, "id": "RpQ31g0wFMUU1Y9Q5xC9JZd4FCRb2QH7tpQk_FkDslM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17f9arc", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f9arc/get_30x_speedups_when_reading_databases_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f9arc/get_30x_speedups_when_reading_databases_with/", "subreddit_subscribers": 135627, "created_utc": 1698142214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pandas is a fantastic library for reading datasets on the go and performing daily data analysis tasks. However, is it advisable to use it in our Python production code?", "author_fullname": "t2_thjew3z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should, a data engineer, uses Pandas in his production code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_17f8xjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.59, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhFEG8gMqYsGt5ayuiDID0lG830Z_9wJOWor8VEfAbo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698140631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pandas is a fantastic library for reading datasets on the go and performing daily data analysis tasks. However, is it advisable to use it in our Python production code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ka02hx4yf4wb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?auto=webp&amp;s=ee31c2efe3f8142ee0424e99c32174815efd2afb", "width": 352, "height": 143}, "resolutions": [{"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17fbc3bc4ca58a1c4e0408104719c801a4d0833b", "width": 108, "height": 43}, {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11d535ed2c854ab9399bdc659fd600380dfc994e", "width": 216, "height": 87}, {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86e812dd9c71607f6a50a68ea8be9e616d9139f5", "width": 320, "height": 130}], "variants": {}, "id": "VEho78jGdiwibgJDgFfs2YXZdAGd0CAh4SxYqT363h4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f8xjx", "is_robot_indexable": true, "report_reasons": null, "author": "Lower_Platform_4190", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f8xjx/should_a_data_engineer_uses_pandas_in_his/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ka02hx4yf4wb1.jpg", "subreddit_subscribers": 135627, "created_utc": 1698140631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learning DBT (with Oracle\u2026 and maybe that\u2019s the root of my troubles) \u2026 am I missing something\u2026 I have a model materializing as a view, run it, created a view in the DB\u2026 and if I add a column to it, I get an error and have to log in to the DB and manually drop the view so DBT can materialize it again with my added column. Makes me wonder if it\u2019s a weakness of the Oracle adapter. Coming from a place of doing this type of stuff in SQL Developer where the VIEW code is \u201cCreate or Replace\u201d, and any change can just be recompiled, DBT View materialization seems like a one-time DBT-only functionality. What am I missing here. Full refresh doesn\u2019t work. Why the heck does this work better with materialized tables and not views?", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT manually drop VIEWs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f7dsu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698134091.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698133620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learning DBT (with Oracle\u2026 and maybe that\u2019s the root of my troubles) \u2026 am I missing something\u2026 I have a model materializing as a view, run it, created a view in the DB\u2026 and if I add a column to it, I get an error and have to log in to the DB and manually drop the view so DBT can materialize it again with my added column. Makes me wonder if it\u2019s a weakness of the Oracle adapter. Coming from a place of doing this type of stuff in SQL Developer where the VIEW code is \u201cCreate or Replace\u201d, and any change can just be recompiled, DBT View materialization seems like a one-time DBT-only functionality. What am I missing here. Full refresh doesn\u2019t work. Why the heck does this work better with materialized tables and not views?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f7dsu", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f7dsu/dbt_manually_drop_views/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f7dsu/dbt_manually_drop_views/", "subreddit_subscribers": 135627, "created_utc": 1698133620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context - \n\nCurrently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.\n\nThe DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don't trigger during development work, because something about their structure. \n\nSo far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. \n\n&amp;#x200B;", "author_fullname": "t2_vk8zz17", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you structure DEV/UAT/PROD with DBT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enlrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context - &lt;/p&gt;\n\n&lt;p&gt;Currently running a five person team utilizing a DBT Cloud, Fivetran, Snowflake stack. I came into the situation relatively late as far as setting up the stack and the environments. They were about 5 months in converting from running raw sql in snowflake + stitch to this new stack.&lt;/p&gt;\n\n&lt;p&gt;The DBT Stack is brittle at best - snapshots have been moderately used, but the points where they are being used are critical and they keep breaking. They were also setup so they don&amp;#39;t trigger during development work, because something about their structure. &lt;/p&gt;\n\n&lt;p&gt;So far the best that I have come up with for having a UAT environment is running a special branch of Git that is not the production branch, that we work against and test against. However this has limits because of how those snapshots behave. Looking for insight or advice on how to go about getting a more robust testing process going with this current setup. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17enlrh", "is_robot_indexable": true, "report_reasons": null, "author": "KrixMercades", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17enlrh/how_do_you_structure_devuatprod_with_dbt/", "subreddit_subscribers": 135627, "created_utc": 1698076161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. \n\nMy job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. \n\nIs there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark sliding window within a window", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17emrm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698074072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that gets submitted by sensors and contains a reading of 0/1. My sensors submit data approximately every 2-3 minutes depending on server time and preprocessing. There are multiple sensors inside a room. &lt;/p&gt;\n\n&lt;p&gt;My job is to aggregate the sensor readings up to a room level. That\u2019s a simple task using windows. However, i need to guarantee that only 1 sensor reading is used per window, and every window has 1 reading from each sensor. To guarantee each window has a sensor reading, I\u2019ve been doing a window of 5 minutes. However, sometimes I will have windows that have 2 of the same sensor included. In this case, I need to use the latest reading. &lt;/p&gt;\n\n&lt;p&gt;Is there a way to essentially do a window inside a window? I could window the 5 minutes, and then window the data within that to group by sensor ID and take the first reading ordered by time desc. Right now I do a 5 minute window and gather all the readings in a struct, explode that,and do another window on that to get the 1 reading using most recent per sensor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17emrm7", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17emrm7/spark_sliding_window_within_a_window/", "subreddit_subscribers": 135627, "created_utc": 1698074072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Video \ud83e\udd73 Configure VS Code to Develop Airflow DAGs in Docker at ease!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17elttv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17elttv", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yhN84w9nIeUQQpFrguMmWJid7Gh_zCqk3cid0v4svck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698071614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/fsMKV9A1B-I", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?auto=webp&amp;s=c578ca44010c7790f0bc08cf9ba73e7c60be5627", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d443dc814c8a8befeffcb24924974b7df32bdce2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d401374ee81fe607289fb56d722bfa0990f7f72", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/yG0aznOHX8BG-2qe0SUnvbbSY61Ab_8-VhTbWrkwMWk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4b864db7e7561a16ae0822bb500e174a66c4380", "width": 320, "height": 240}], "variants": {}, "id": "EjbV75W_SKL66g68oZtVMqNqSHRNtBErf-59fQyDhOA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17elttv", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17elttv/new_video_configure_vs_code_to_develop_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/fsMKV9A1B-I", "subreddit_subscribers": 135627, "created_utc": 1698071614.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Configure VS Code to Develop Airflow DAGs in Docker at ease!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fsMKV9A1B-I?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Configure VS Code to Develop Airflow DAGs in Docker at ease!\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fsMKV9A1B-I/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, I am currently working as a Database Administrator primarily working on Oracle and exposure to Postgres/MySQL. I am having a working experience of DBA near to 7 months and also this is my first job out of college. I want to switch into Data Engineering, what skills or tools would you advise me that can land a position in this field. Also I know that it is kind of hard to get into Data engineering without experience but what will you be learning if you were at my place having knowledge and experience e of the industry?", "author_fullname": "t2_8mo944ce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBA looking to switch into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fbtht", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698151184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I am currently working as a Database Administrator primarily working on Oracle and exposure to Postgres/MySQL. I am having a working experience of DBA near to 7 months and also this is my first job out of college. I want to switch into Data Engineering, what skills or tools would you advise me that can land a position in this field. Also I know that it is kind of hard to get into Data engineering without experience but what will you be learning if you were at my place having knowledge and experience e of the industry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17fbtht", "is_robot_indexable": true, "report_reasons": null, "author": "datastoner", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fbtht/dba_looking_to_switch_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fbtht/dba_looking_to_switch_into_data_engineering/", "subreddit_subscribers": 135627, "created_utc": 1698151184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI'm am working on a little project that would allow people to apply custom functions to create features for a feature store. The project will be running on top of kubernetes. What it would look like is the user would write a script (python/java/sql?) or spin up a container and then the data would be extracted from the feature store transformed according to the script and then be put back into the feature store. I'm looking at stuff like Elyra and apache beam to accomplish this. But i would also need a way where the user would spin up a container and not do it via the script something like tekton i think. So basically a serverless way and a way with containers. Also it will have to be able to handle both batch and stream processing. Can you guys recommend me any other frameworks that would help with this? thank you in advance", "author_fullname": "t2_fcsujw3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with data pipelines.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fbe1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698149827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m am working on a little project that would allow people to apply custom functions to create features for a feature store. The project will be running on top of kubernetes. What it would look like is the user would write a script (python/java/sql?) or spin up a container and then the data would be extracted from the feature store transformed according to the script and then be put back into the feature store. I&amp;#39;m looking at stuff like Elyra and apache beam to accomplish this. But i would also need a way where the user would spin up a container and not do it via the script something like tekton i think. So basically a serverless way and a way with containers. Also it will have to be able to handle both batch and stream processing. Can you guys recommend me any other frameworks that would help with this? thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fbe1x", "is_robot_indexable": true, "report_reasons": null, "author": "Abject-Battle1215", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fbe1x/help_with_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fbe1x/help_with_data_pipelines/", "subreddit_subscribers": 135627, "created_utc": 1698149827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data Engineering Community,\n\nI am new to data engineering and I need your advice/help/directions - anything at all. And forgive my English.\n\nI am currently working on a research project that involves package status prediction. To do so, time-series data related to the package is required. The monitored data can be temperature, humidity, or anything that affects the quality of the package and can be picked up by a sensor (which is attached to the package).\n\nI have been searching for the past two weeks nonstop. I visited most of the dataset websites, searched for websites that I may be able to scrape data from, and even played with VAERS reports to extract data but just noticed the data is not helpful in my case. I have emailed countless authors who have used similar data in their research papers but none of them replied.\n\nIs there any way at all where I can get or gather such data? I just need temperature readings over time and how the package status is at the given time.\n\nThank you in advance for your assistance and for being part of this amazing community!\n\n&amp;#x200B;", "author_fullname": "t2_7kezs9t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Monitored Time-Series Data in Package Delivery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f7gwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698138742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698134006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Engineering Community,&lt;/p&gt;\n\n&lt;p&gt;I am new to data engineering and I need your advice/help/directions - anything at all. And forgive my English.&lt;/p&gt;\n\n&lt;p&gt;I am currently working on a research project that involves package status prediction. To do so, time-series data related to the package is required. The monitored data can be temperature, humidity, or anything that affects the quality of the package and can be picked up by a sensor (which is attached to the package).&lt;/p&gt;\n\n&lt;p&gt;I have been searching for the past two weeks nonstop. I visited most of the dataset websites, searched for websites that I may be able to scrape data from, and even played with VAERS reports to extract data but just noticed the data is not helpful in my case. I have emailed countless authors who have used similar data in their research papers but none of them replied.&lt;/p&gt;\n\n&lt;p&gt;Is there any way at all where I can get or gather such data? I just need temperature readings over time and how the package status is at the given time.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your assistance and for being part of this amazing community!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f7gwr", "is_robot_indexable": true, "report_reasons": null, "author": "pheonix_Mai", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f7gwr/looking_for_monitored_timeseries_data_in_package/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f7gwr/looking_for_monitored_timeseries_data_in_package/", "subreddit_subscribers": 135627, "created_utc": 1698134006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,  \nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.  \nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job's dependencies on workers only - that way I can keep different environments for different jobs/DAGs.  \nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is \"just install everything in one image\". This greatly concerns me, as we are trying to run away from a *one venv to rule them all* solution.  \nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.", "author_fullname": "t2_11671y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow and python dependencies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f5o2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698126313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.&lt;br/&gt;\nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job&amp;#39;s dependencies on workers only - that way I can keep different environments for different jobs/DAGs.&lt;br/&gt;\nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is &amp;quot;just install everything in one image&amp;quot;. This greatly concerns me, as we are trying to run away from a &lt;em&gt;one venv to rule them all&lt;/em&gt; solution.&lt;br/&gt;\nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f5o2k", "is_robot_indexable": true, "report_reasons": null, "author": "RealBrofessor", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "subreddit_subscribers": 135627, "created_utc": 1698126313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).\n\nOne of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.\n\nI was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.", "author_fullname": "t2_6qoeic3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bigquery Omni Vs. Transfer for AWS, thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f0heb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698109554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).&lt;/p&gt;\n\n&lt;p&gt;One of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.&lt;/p&gt;\n\n&lt;p&gt;I was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f0heb", "is_robot_indexable": true, "report_reasons": null, "author": "Dumac89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "subreddit_subscribers": 135627, "created_utc": 1698109554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.\n\nMy question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.\n\nAny ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...", "author_fullname": "t2_31exp9bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Midware solution for data analytics tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17euvsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698094583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.&lt;/p&gt;\n\n&lt;p&gt;My question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.&lt;/p&gt;\n\n&lt;p&gt;Any ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17euvsp", "is_robot_indexable": true, "report_reasons": null, "author": "Sonny-Orkidea", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "subreddit_subscribers": 135627, "created_utc": 1698094583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can't find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.", "author_fullname": "t2_n52imw1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "export WhatsApp business group chat data in the structure format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eo0tu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698077435.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698077193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;one question, are there any ways to export WhatsApp business group chat data in structure format I have searched a lot but can&amp;#39;t find a solution. when I export it, it creates a text file of messages how do I extract it in columns and rows? like extracting business data for analysis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eo0tu", "is_robot_indexable": true, "report_reasons": null, "author": "SurpriseLopsided5536", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eo0tu/export_whatsapp_business_group_chat_data_in_the/", "subreddit_subscribers": 135627, "created_utc": 1698077193.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}