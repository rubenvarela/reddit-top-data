{"kind": "Listing", "data": {"after": "t3_17euvsp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is for US roles.\n\nIt\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. \n\nI\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). \n\nI wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.", "author_fullname": "t2_fhmml14j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your opinion on working for low paying F500 companies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17evkku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698096241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is for US roles.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s becoming apparent to me that there are a ton of F500 companies in banking, retail, construction, etc. that pay 130k for \u201clead data engineer\u201d positions that require +7 years of experience and a bunch of different things. Not to mention the jobs are on-site, and expected raises are 3% yearly. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a mid level data engineer at a late stage startup. I make 130k with great work life balance, and work remotely from a MCOL area. My last job was at 115k remote as well with decent work life balance(I left to work with a cooler company/tech stack). &lt;/p&gt;\n\n&lt;p&gt;I wouldn\u2019t consider myself a top candidate either. I don\u2019t have a quant background or crazy credentials. So I\u2019m just curious, do you all consider these positions? Or are they mostly for people who need sponsorship, were laid off, trying to escape a terrible job? Etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17evkku", "is_robot_indexable": true, "report_reasons": null, "author": "Capable-Jicama2155", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17evkku/whats_your_opinion_on_working_for_low_paying_f500/", "subreddit_subscribers": 135666, "created_utc": 1698096241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you feel rushed with your work?\n\nSomething I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.\n\nI work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.\n\nI could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.\n\nI get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.\n\nEven before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  \n\nBut still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.\n\nI\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.", "author_fullname": "t2_c9apwds3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you always feel rushed with your deliverables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17etfeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you feel rushed with your work?&lt;/p&gt;\n\n&lt;p&gt;Something I\u2019m pretty terrible at is consistently keeping a clean commit history and maintaining good code quality.&lt;/p&gt;\n\n&lt;p&gt;I work for a consulting firms, and this is especially difficult for me since I jump across clients, projects, and technologies.&lt;/p&gt;\n\n&lt;p&gt;I could be in a dbt repo one hour, another client\u2019s dbt repo in the next hour (with totally different styling and organization), a custom ingestion repo in Python the hour after, and then wrap up my day pouring through the last client\u2019s airflow repo.  And then 2-4 weeks later the clients cycle and I start all over.&lt;/p&gt;\n\n&lt;p&gt;I get given a ticket to jump into a monster, super messy repo that I\u2019m not familiar with and am expected to have a PR up by EOD.&lt;/p&gt;\n\n&lt;p&gt;Even before consulting, the one year I did in industry was the same with the only difference being I got know (and contribute to) the 1-3 monster repo\u2019s instead of learning a new on each week.  &lt;/p&gt;\n\n&lt;p&gt;But still, it was always just push stuff out as fast as you can since the backlog is so big and the team so small.  And I see this mentality with the data teams I work with at the clients I\u2019m assigned.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not saying that we should drag our feet to ensure absolute perfection, but I feel like this is harming my growth as I learn poor practices with disincentive to take time to learn good practices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17etfeh", "is_robot_indexable": true, "report_reasons": null, "author": "american-roast", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17etfeh/do_you_always_feel_rushed_with_your_deliverables/", "subreddit_subscribers": 135666, "created_utc": 1698090954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have experience working with Informatica as an ETL tool but want to switch to big data technologies. I have started learning different tools in hadoop ecosystem but there are so many of them for different purposes. I am confused on which one's to learn in depth.\nLooking at job openings I have seen Spark, Hive, Sqoop,   Kafka, Spark streaming, etc. being used more.\nBut what about other tools like Flume, Storm, Hue, Hbase, and many more. Should I completely skip them?\n\nOr\n\nPlease give me a list of big data tools to learn that will help me switch to big data roles. I dont want to waste time learning tools like Dril, phoenix which no one uses as far as I have seen.", "author_fullname": "t2_ht9x5dmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So many tools to learn!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f6vkz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698131406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have experience working with Informatica as an ETL tool but want to switch to big data technologies. I have started learning different tools in hadoop ecosystem but there are so many of them for different purposes. I am confused on which one&amp;#39;s to learn in depth.\nLooking at job openings I have seen Spark, Hive, Sqoop,   Kafka, Spark streaming, etc. being used more.\nBut what about other tools like Flume, Storm, Hue, Hbase, and many more. Should I completely skip them?&lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Please give me a list of big data tools to learn that will help me switch to big data roles. I dont want to waste time learning tools like Dril, phoenix which no one uses as far as I have seen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f6vkz", "is_robot_indexable": true, "report_reasons": null, "author": "kaachejl", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f6vkz/so_many_tools_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f6vkz/so_many_tools_to_learn/", "subreddit_subscribers": 135666, "created_utc": 1698131406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pulling back the charade for a moment here... as a vendor, I really empathize with all the DEs in the thread trying to sort through the noise.\n\nI'm not sure if it is the vast amount of VC funding that came into the space, the potential level of nuance to every pipeline, or something else (act of god?)... but it is borderline unimaginable how many new/different vendors exist for creating data pipelines.\n\nJust taking a basic example, googling say Postgres to Snowflake will quite literally yield hundreds of distinct possible vendors. I scrolled so long waiting for repeat vendor domains that I actually got bored and stopped. And this is just revealing all the companies that have put in the effort to try and hack Google SEO results (stitch even bought hundreds of domains of [https://postgres.tosnowflake.com/](https://postgres.tosnowflake.com/) \\-- IMHO: Google Search for B2B is increasingly a failed product for surfacing quality products/content... but I digress.)\n\nAdd in all the open-source, native tooling, or code-based ways to create a pipeline... and I think there might literally be 200+ legit ways to ETL from Postgres to Snowflake.\n\nHow is there possibly \\*SO\\* many solutions? Is massive consolidation of point-to-point ETL tools coming immediately?\n\nWhile I think what we do is somewhat unique and cool (estuary.dev), to my partially trained eye.... it feels like Rivery/Fivetran/Hevo/DMS/Stitch/Talend --- just all basically the exact same 'sometimes good enough' solution w/ up to 99% the same features\n\nEND RANT\n\n&amp;#x200B;", "author_fullname": "t2_sa3mbz4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "vendor confession: there's just too many ETL/ELT tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fgte6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698165518.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698164775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pulling back the charade for a moment here... as a vendor, I really empathize with all the DEs in the thread trying to sort through the noise.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if it is the vast amount of VC funding that came into the space, the potential level of nuance to every pipeline, or something else (act of god?)... but it is borderline unimaginable how many new/different vendors exist for creating data pipelines.&lt;/p&gt;\n\n&lt;p&gt;Just taking a basic example, googling say Postgres to Snowflake will quite literally yield hundreds of distinct possible vendors. I scrolled so long waiting for repeat vendor domains that I actually got bored and stopped. And this is just revealing all the companies that have put in the effort to try and hack Google SEO results (stitch even bought hundreds of domains of &lt;a href=\"https://postgres.tosnowflake.com/\"&gt;https://postgres.tosnowflake.com/&lt;/a&gt; -- IMHO: Google Search for B2B is increasingly a failed product for surfacing quality products/content... but I digress.)&lt;/p&gt;\n\n&lt;p&gt;Add in all the open-source, native tooling, or code-based ways to create a pipeline... and I think there might literally be 200+ legit ways to ETL from Postgres to Snowflake.&lt;/p&gt;\n\n&lt;p&gt;How is there possibly *SO* many solutions? Is massive consolidation of point-to-point ETL tools coming immediately?&lt;/p&gt;\n\n&lt;p&gt;While I think what we do is somewhat unique and cool (estuary.dev), to my partially trained eye.... it feels like Rivery/Fivetran/Hevo/DMS/Stitch/Talend --- just all basically the exact same &amp;#39;sometimes good enough&amp;#39; solution w/ up to 99% the same features&lt;/p&gt;\n\n&lt;p&gt;END RANT&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "17fgte6", "is_robot_indexable": true, "report_reasons": null, "author": "MooJerseyCreamery", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fgte6/vendor_confession_theres_just_too_many_etlelt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fgte6/vendor_confession_theres_just_too_many_etlelt/", "subreddit_subscribers": 135666, "created_utc": 1698164775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pandas is a fantastic library for reading datasets on the go and performing daily data analysis tasks. However, is it advisable to use it in our Python production code?", "author_fullname": "t2_thjew3z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should, a data engineer, uses Pandas in his production code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_17f8xjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zhFEG8gMqYsGt5ayuiDID0lG830Z_9wJOWor8VEfAbo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698140631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pandas is a fantastic library for reading datasets on the go and performing daily data analysis tasks. However, is it advisable to use it in our Python production code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ka02hx4yf4wb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?auto=webp&amp;s=ee31c2efe3f8142ee0424e99c32174815efd2afb", "width": 352, "height": 143}, "resolutions": [{"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17fbc3bc4ca58a1c4e0408104719c801a4d0833b", "width": 108, "height": 43}, {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11d535ed2c854ab9399bdc659fd600380dfc994e", "width": 216, "height": 87}, {"url": "https://preview.redd.it/ka02hx4yf4wb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86e812dd9c71607f6a50a68ea8be9e616d9139f5", "width": 320, "height": 130}], "variants": {}, "id": "VEho78jGdiwibgJDgFfs2YXZdAGd0CAh4SxYqT363h4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f8xjx", "is_robot_indexable": true, "report_reasons": null, "author": "Lower_Platform_4190", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f8xjx/should_a_data_engineer_uses_pandas_in_his/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ka02hx4yf4wb1.jpg", "subreddit_subscribers": 135666, "created_utc": 1698140631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently engaged in a data management project and I'm seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.\n\nOur data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:\n\n&amp;#x200B;\n\n1. Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?\n2. How does DuckDB handle data deltas?\n3. Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?\n\n&amp;#x200B;\n\nI would greatly appreciate any insights, tips, and recommendations, thank you! \n\n&amp;#x200B;", "author_fullname": "t2_nz9uo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can DuckDB/MotherDuck Replace BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17et5a4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698090202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently engaged in a data management project and I&amp;#39;m seeking guidance and insights on the implementation of a data lake using DuckDB for sourcing data from a Blob Storage account. Our current workflow involves working with BigQuery and ingesting data through Stitch from CSV files.&lt;/p&gt;\n\n&lt;p&gt;Our data consists of a historical database file in CSV format, complemented by daily delta updates stored in Blob Storage. The objective is to centralize all this data within a data lake, facilitating storage and analytical processes. To that end, I have several specific questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is DuckDB or MotherDuck a suitable data warehousing solution for our specific use case?&lt;/li&gt;\n&lt;li&gt;How does DuckDB handle data deltas?&lt;/li&gt;\n&lt;li&gt;Can DuckDB or MotherDuck seamlessly integrate with a BI tool such as PowerBI?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, tips, and recommendations, thank you! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17et5a4", "is_robot_indexable": true, "report_reasons": null, "author": "RX-Vortex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17et5a4/can_duckdbmotherduck_replace_bigquery/", "subreddit_subscribers": 135666, "created_utc": 1698090202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It's worked fairly well up until now, but there are several problems I'm hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.\n\nHere's what I'm envisioning:\n\n\\- custom tool (there's no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.\n\n\\- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  \n\n\\- \\[this is the part I'm most unclear on and SQL is failing the most\\].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central \"bronze\" table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  \n\n\\- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it's prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source's data, that describes more or less what I'm looking for.  So I don't know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I've got some ML in there as well, so moving the data offline is doable.\n\n\\- Using MERGE will make this more efficient without the overhead of SQL's locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)\n\n\\- Build gold/aggregate tables in data lake with \\[databricks?\\]\n\nPush the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.\n\nI have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We're not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I've got to have something in between for caching and refreshes.\n\n&amp;#x200B;", "author_fullname": "t2_gilvsz7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sanity check architecture please?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq1yd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working with a data set several terabytes in size, that comes from over 1000 sources in about 20 different data structures.  This data is ingested in a semi-raw format into SQL, and then there are delta updates via views that merge the data into a star-schema warehouse, and then PBI runs out of this SQL warehouse.  It&amp;#39;s worked fairly well up until now, but there are several problems I&amp;#39;m hitting that I think can be greatly improved with a MDS(TM).  Namely these are statistics hell, parallel processing (time to analytics), determining which rows have changed instead of doing a full reprocess, and query performance (hitting covered indexes and filtering appropriate are essential, and not done).  The star schema itself has worked well.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;m envisioning:&lt;/p&gt;\n\n&lt;p&gt;- custom tool (there&amp;#39;s no getting around this) continues to ingest via API on a daily basis (business EOD).  API is there already and gives us a good layer of decoupling.&lt;/p&gt;\n\n&lt;p&gt;- API puts raw data into data lake.  Each type of data, the actual source its coming from, and the source table is the hierarchy.  Does it make sense here to further split the hierarchy by month updated, or some other ID column?  &lt;/p&gt;\n\n&lt;p&gt;- [this is the part I&amp;#39;m most unclear on and SQL is failing the most].  I now need to get this data into the central Dim/Fact tables.  Use databricks to pull updated + deleted data per source since the last sync, and transform (possibly offline?) into a central &amp;quot;bronze&amp;quot; table representing our current star schema that holds all sources, essentially what we have in SQL now.  I can track deletions separately unless this is built into the data lake.  &lt;/p&gt;\n\n&lt;p&gt;- Where I have the least confidence is what kind of change detection is available in data lakes.  A single source may have anywhere between 1-20 million rows of history, but only a very small subset is updated regularly.  Today, I am managing updates by hashes and checking updates on related tables in the raw data, but it&amp;#39;s prone to error.  If I could have something like a materialized view, with hashes or a date stamp, and then do a compare between that and the slice of the fact table that holds that source&amp;#39;s data, that describes more or less what I&amp;#39;m looking for.  So I don&amp;#39;t know if a) that is possible, b) it makes sense to create bronze tables instead for each slice that represents our star schema and combine later, or c) some other solution.  I&amp;#39;ve got some ML in there as well, so moving the data offline is doable.&lt;/p&gt;\n\n&lt;p&gt;- Using MERGE will make this more efficient without the overhead of SQL&amp;#39;s locking (SQL Merge falls on its face now, and I have to run three statements for INSERT/UPDATE/DELETE)&lt;/p&gt;\n\n&lt;p&gt;- Build gold/aggregate tables in data lake with [databricks?]&lt;/p&gt;\n\n&lt;p&gt;Push the central bronze/gold tables to SQL (for now, compatibility) and a better reporting tool (SSAS/Snowflake).  Not sure if there is a way to filter the number of rows sent over as well based off date/hashes, there would be about a billion rows here.&lt;/p&gt;\n\n&lt;p&gt;I have read that ADF is the pits, but any thoughts on SSAS tabular vs Snowflake vs Redshift appreciated as well.  We&amp;#39;re not going to be moving off PBI for the foreseeable future, but there are enough issues with PBIRS that I&amp;#39;ve got to have something in between for caching and refreshes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq1yd", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Log_8482", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq1yd/sanity_check_architecture_please/", "subreddit_subscribers": 135666, "created_utc": 1698082305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Announcing v0.15: Interactive Declarative Migrations, Functions, Procedures and Domains", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17fakcd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nhBgVCSkn43mEZZuegXT044GK2frFJgTtRt1LS1YMJ4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698147009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/10/19/atlas-v-0-15", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?auto=webp&amp;s=1414e9580f0d5089f4769ab2f2dcf09a48ef0eab", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f75cc7278b4601814b4fe2ac8c6d401e3d76a85", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a195df6f7b63ccc0ed1fc2acd6ff54e5892a64e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a24f0728b182efec43a4f4bcce3bba439970957b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b72d2ebcc090ddfdb55d337b468a87ea00c6d0ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a78605ab50cea5dc4a89f243953163ce50015b2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/DDCXetSpUIFJ4t6xJYeeQ91De5Yug6Hw5xzosDHz7zU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7452ef2bb59607883b5a2f7c165be3a43579b90", "width": 1080, "height": 567}], "variants": {}, "id": "YeWm-cV5fn_dffXIIXchwIx5O5_ErL1r2-7yIlehqe8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17fakcd", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fakcd/announcing_v015_interactive_declarative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/10/19/atlas-v-0-15", "subreddit_subscribers": 135666, "created_utc": 1698147009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. \n\nJob 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there\n\nJob 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. \n\nJob 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here \n\nI have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. \n\nDo I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?", "author_fullname": "t2_b7eqz4bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On My Third Bad Job This Year\u2026How do I Salvage my Career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eud9r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698093278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO and about 6 months from finishing MSDS. &lt;/p&gt;\n\n&lt;p&gt;Job 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there&lt;/p&gt;\n\n&lt;p&gt;Job 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. &lt;/p&gt;\n\n&lt;p&gt;Job 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here &lt;/p&gt;\n\n&lt;p&gt;I have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. &lt;/p&gt;\n\n&lt;p&gt;Do I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17eud9r", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Box228", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eud9r/on_my_third_bad_job_this_yearhow_do_i_salvage_my/", "subreddit_subscribers": 135666, "created_utc": 1698093278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?\n\nMy idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.", "author_fullname": "t2_huqm5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MDM for contacts and addresses? Examples?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f2av8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698114834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project that might require MDM to check duplicate addresses not sure yet joe to solve it but was thinking in hashing the address to check if its unique or not inside the db for quicker ingestion . Are there any books or tutorials / guides on how to implement a MDM inside a DB? Or any tips on hoe to approach this?&lt;/p&gt;\n\n&lt;p&gt;My idea is to hash the full address and on insert compare the new hash to existing hash if exist to not insert and just use as fk the existing hash id.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f2av8", "is_robot_indexable": true, "report_reasons": null, "author": "tbarg91", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f2av8/mdm_for_contacts_and_addresses_examples/", "subreddit_subscribers": 135666, "created_utc": 1698114834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learning DBT (with Oracle\u2026 and maybe that\u2019s the root of my troubles) \u2026 am I missing something\u2026 I have a model materializing as a view, run it, created a view in the DB\u2026 and if I add a column to it, I get an error and have to log in to the DB and manually drop the view so DBT can materialize it again with my added column. Makes me wonder if it\u2019s a weakness of the Oracle adapter. Coming from a place of doing this type of stuff in SQL Developer where the VIEW code is \u201cCreate or Replace\u201d, and any change can just be recompiled, DBT View materialization seems like a one-time DBT-only functionality. What am I missing here. Full refresh doesn\u2019t work. Why the heck does this work better with materialized tables and not views?", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT manually drop VIEWs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f7dsu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698134091.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698133620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learning DBT (with Oracle\u2026 and maybe that\u2019s the root of my troubles) \u2026 am I missing something\u2026 I have a model materializing as a view, run it, created a view in the DB\u2026 and if I add a column to it, I get an error and have to log in to the DB and manually drop the view so DBT can materialize it again with my added column. Makes me wonder if it\u2019s a weakness of the Oracle adapter. Coming from a place of doing this type of stuff in SQL Developer where the VIEW code is \u201cCreate or Replace\u201d, and any change can just be recompiled, DBT View materialization seems like a one-time DBT-only functionality. What am I missing here. Full refresh doesn\u2019t work. Why the heck does this work better with materialized tables and not views?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f7dsu", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f7dsu/dbt_manually_drop_views/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f7dsu/dbt_manually_drop_views/", "subreddit_subscribers": 135666, "created_utc": 1698133620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.\n\nThe data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.\n\nGenerally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.\n\nI am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don't want to use a paginated report.\n\nDoes anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much detail is required for a Data Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eq7fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a Big Data Model which I have inherited.  This data is stored as a basic Star schema.  The central Fact table contains 15 Billion Rows and we receive approximately 30 million rows of data daily, which is growing.&lt;/p&gt;\n\n&lt;p&gt;The data is stored in Snowflake using SnowPipe from Kafka and processed fine, however, this is then pushed out to Analysis Services where we start to pay a lot of money.&lt;/p&gt;\n\n&lt;p&gt;Generally, when we are dealing with this amount of data, we will try to group it up into Daily, Weekly, and Monthly Fact Tables with the various dimension mappings and that has given the business enough context.  If they want more granular data, we would provide them with Paginated Reports which query Snowflake directly to pull the more granular data.&lt;/p&gt;\n\n&lt;p&gt;I am getting a lot of pushback from the current BI Analyst and their Finance Director, explaining that finance uses the tabular model for granular reporting and they like being able to connect in Excel directly to design their own reports.  They don&amp;#39;t want to use a paginated report.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advice on how to proceed, especially how you have dealt with larger datasets like this in the past?  Also, is a Star Schema even the right approach for this size of data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17eq7fw", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17eq7fw/how_much_detail_is_required_for_a_data_model/", "subreddit_subscribers": 135666, "created_utc": 1698082725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks\n\n  \nwe at dlthub added a very cool feature for copying production databases. By using ConnectorX and arrow, the sql -&gt; analytics copying can go up to 30x faster over the classic sqlite connector.\n\nRead about the benchmark comparison and the underlying technology here: [https://dlthub.com/docs/blog/dlt-arrow-loading](https://dlthub.com/docs/blog/dlt-arrow-loading) \n\nOne disclaimer is that since this method does not do row by row processing, we cannot microbatch the data through small buffers - so pay attention to the memory size on your extraction machine.  \nCode example how to use: [https://dlthub.com/docs/examples/connector\\_x\\_arrow/](https://dlthub.com/docs/examples/connector_x_arrow/)\n\nBy adding this support, we also enable these sources:  \n[https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas](https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas)  \n\n\nIf you need help, don't miss the gpt helper link at the bottom of our docs or the slack link at the top.\n\nFeedback is very welcome!\n\n&amp;#x200B;", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get 30x speedups when reading databases with ConnectorX + Arrow + dlt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f9arc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698142214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks&lt;/p&gt;\n\n&lt;p&gt;we at dlthub added a very cool feature for copying production databases. By using ConnectorX and arrow, the sql -&amp;gt; analytics copying can go up to 30x faster over the classic sqlite connector.&lt;/p&gt;\n\n&lt;p&gt;Read about the benchmark comparison and the underlying technology here: &lt;a href=\"https://dlthub.com/docs/blog/dlt-arrow-loading\"&gt;https://dlthub.com/docs/blog/dlt-arrow-loading&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;One disclaimer is that since this method does not do row by row processing, we cannot microbatch the data through small buffers - so pay attention to the memory size on your extraction machine.&lt;br/&gt;\nCode example how to use: &lt;a href=\"https://dlthub.com/docs/examples/connector_x_arrow/\"&gt;https://dlthub.com/docs/examples/connector_x_arrow/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;By adding this support, we also enable these sources:&lt;br/&gt;\n&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas\"&gt;https://dlthub.com/docs/dlt-ecosystem/verified-sources/arrow-pandas&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;If you need help, don&amp;#39;t miss the gpt helper link at the bottom of our docs or the slack link at the top.&lt;/p&gt;\n\n&lt;p&gt;Feedback is very welcome!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?auto=webp&amp;s=84b3e18f109f221a97f70a438603578a4cc30a30", "width": 1309, "height": 517}, "resolutions": [{"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07f5c414d1f377a0af62579d1879c251208a002d", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d366d379f340396f585011f76db6b5167876c8a", "width": 216, "height": 85}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=612a47aa4d36d7cbabd424857fa08ac068b5b251", "width": 320, "height": 126}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f9200099e81460a0137696884981d75dbcc7128", "width": 640, "height": 252}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f143bf5100e86da625d4b5c285d27a2019c5b024", "width": 960, "height": 379}, {"url": "https://external-preview.redd.it/xDEk8_PXnpPC-6ge-45ltq0lViLX2u1w65DKMVA5FfA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7786c8602fd15012d8b83199f5529b88efbaee6", "width": 1080, "height": 426}], "variants": {}, "id": "RpQ31g0wFMUU1Y9Q5xC9JZd4FCRb2QH7tpQk_FkDslM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17f9arc", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f9arc/get_30x_speedups_when_reading_databases_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f9arc/get_30x_speedups_when_reading_databases_with/", "subreddit_subscribers": 135666, "created_utc": 1698142214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I\u2019m currently an ETL developer and looking to prep myself up so that I can get into FAANG as a DE. Tools I currently use are SQL and Azure services. Can someone help me visualize the necessary tools I should have under my belt to get into FAANG ?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL developer to FAANG DE: map?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ffhfw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698161297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I\u2019m currently an ETL developer and looking to prep myself up so that I can get into FAANG as a DE. Tools I currently use are SQL and Azure services. Can someone help me visualize the necessary tools I should have under my belt to get into FAANG ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ffhfw", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ffhfw/etl_developer_to_faang_de_map/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ffhfw/etl_developer_to_faang_de_map/", "subreddit_subscribers": 135666, "created_utc": 1698161297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).\n\nOne of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.\n\nI was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.", "author_fullname": "t2_6qoeic3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bigquery Omni Vs. Transfer for AWS, thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f0heb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698109554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I\u2019ve been charged with reducing my company\u2019s data costs significantly, especially for ETL (we use a third party service currently).&lt;/p&gt;\n\n&lt;p&gt;One of our data sources is a CSV (typically 3 GB) hat gets dropped into an S3 bucket on a near daily basis. It requires minimal cleanup and typically it\u2019s only incorporated into a larger table.&lt;/p&gt;\n\n&lt;p&gt;I was wondering what experience people have with BigQuery Omni vs. the data transfer tool and some of the pros and cons. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f0heb", "is_robot_indexable": true, "report_reasons": null, "author": "Dumac89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f0heb/bigquery_omni_vs_transfer_for_aws_thoughts/", "subreddit_subscribers": 135666, "created_utc": 1698109554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you track what data products your users actually use and which you can deprecate/stop supporting?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data products usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fhhw3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698166601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you track what data products your users actually use and which you can deprecate/stop supporting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fhhw3", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fhhw3/data_products_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fhhw3/data_products_usage/", "subreddit_subscribers": 135666, "created_utc": 1698166601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am interested in the pro's and cons of clevertap vs a warehouse (tables from separate databases put together in the same place, star sch\u00e9me, for example redshift, snowflake, bigquery or postgresql). Also interested to know the technical limitations of each if any. For example, *is there something we can't do in clevertap that we can do in a warehouse* ?\nIf you have a strong opinion, please share !\nMany thanks in advance!", "author_fullname": "t2_8kenyeuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clevertap vs a data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fgqnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "620fb7b8-ac9d-11eb-a99a-0ed5d8300de1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698164807.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698164571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am interested in the pro&amp;#39;s and cons of clevertap vs a warehouse (tables from separate databases put together in the same place, star sch\u00e9me, for example redshift, snowflake, bigquery or postgresql). Also interested to know the technical limitations of each if any. For example, &lt;em&gt;is there something we can&amp;#39;t do in clevertap that we can do in a warehouse&lt;/em&gt; ?\nIf you have a strong opinion, please share !\nMany thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Tech Lead", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fgqnf", "is_robot_indexable": true, "report_reasons": null, "author": "btenami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17fgqnf/clevertap_vs_a_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fgqnf/clevertap_vs_a_data_warehouse/", "subreddit_subscribers": 135666, "created_utc": 1698164571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to know what are the scenario based interview questions based on data engineering role on cloud premises especially on AWS that the interviewers might ask.\n\nOne example is: Designing URL shortening cloud infrastructure based on AWS. \n\nIf you know the best answer, the please include the answer with questions as well.\n\nThank you", "author_fullname": "t2_4aji175n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scenario based interview questions that you guys faced during interview? Example: URL shortening", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17fg0p8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698162693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to know what are the scenario based interview questions based on data engineering role on cloud premises especially on AWS that the interviewers might ask.&lt;/p&gt;\n\n&lt;p&gt;One example is: Designing URL shortening cloud infrastructure based on AWS. &lt;/p&gt;\n\n&lt;p&gt;If you know the best answer, the please include the answer with questions as well.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fg0p8", "is_robot_indexable": true, "report_reasons": null, "author": "seeker112", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fg0p8/scenario_based_interview_questions_that_you_guys/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fg0p8/scenario_based_interview_questions_that_you_guys/", "subreddit_subscribers": 135666, "created_utc": 1698162693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mxj2oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Backups and Disaster Recovery in PostgreSQL: Your Questions, Answered", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17ffcnu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/G_qJt4NDflLNlCGHHewhB1ViJl99-2oWBrx8eQK4iKU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698160953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "timescale.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.timescale.com/blog/database-backups-and-disaster-recovery-in-postgresql-your-questions-answered/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?auto=webp&amp;s=e0884e335526c884d524e7361055c600d01d7697", "width": 1099, "height": 615}, "resolutions": [{"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0ca673870c0e201d58002cbfb96dd7da30a86a4", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6771ed607ac2448d4949f0bff69f8d6740beccfc", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fcd9cb51df0bfebe036aeb76af7227a3417a7a58", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cdf55919640c238209b9657c66a9fee86be286a", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2afc0d1180fb440817b20bd6957cae9a1264dc2a", "width": 960, "height": 537}, {"url": "https://external-preview.redd.it/qeM5n8uwqOjvAI8twpncx7A0fSnVAUpze9uHtpfAPWs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7718034ac5a9bf775fa0f63e25b4469349e62c19", "width": 1080, "height": 604}], "variants": {}, "id": "cxqY9Qyo2mXSm20uQIaBfbnuBosNZob1dhXp9w_HwgA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ffcnu", "is_robot_indexable": true, "report_reasons": null, "author": "carlotasoto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ffcnu/database_backups_and_disaster_recovery_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.timescale.com/blog/database-backups-and-disaster-recovery-in-postgresql-your-questions-answered/", "subreddit_subscribers": 135666, "created_utc": 1698160953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\nI have a list of integer ids. I am calling a method getMeDf(ids),  which need this id and returns a dataframe. This method query some table and do some other processing before returning a dataframe. Now list of ids is huge so I want to execute as much parallel calls to getMeDf(ids) as possible. I wrote code like this:\n\n```\nval pool= Executors.newFixedThreadPool(5)\n\nval ec = ExecutorContext.fromExecutor(pool)\n\nval df = ids.map(x =&gt; { \n    Future{ \n        getMeDf(x) \n     }(ec)\n})\n\nval allDone = Future.sequence(results)\n\nval dfList = Await.result(allDone, Duration.Inf)\n\ndfList.reduce(_ union _).count()\n```\n\nThe result is 0 records.\n\nMy understanding is dataframe being lazy eval is not getting evaluated.\n\nCan anyone help and let me know what I am doing wrong?", "author_fullname": "t2_gzyg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scala Futures and Spark dataframe evaluation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fetk1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698160173.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698159592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I have a list of integer ids. I am calling a method getMeDf(ids),  which need this id and returns a dataframe. This method query some table and do some other processing before returning a dataframe. Now list of ids is huge so I want to execute as much parallel calls to getMeDf(ids) as possible. I wrote code like this:&lt;/p&gt;\n\n&lt;p&gt;```\nval pool= Executors.newFixedThreadPool(5)&lt;/p&gt;\n\n&lt;p&gt;val ec = ExecutorContext.fromExecutor(pool)&lt;/p&gt;\n\n&lt;p&gt;val df = ids.map(x =&amp;gt; { \n    Future{ \n        getMeDf(x) \n     }(ec)\n})&lt;/p&gt;\n\n&lt;p&gt;val allDone = Future.sequence(results)&lt;/p&gt;\n\n&lt;p&gt;val dfList = Await.result(allDone, Duration.Inf)&lt;/p&gt;\n\n&lt;p&gt;dfList.reduce(_ union _).count()\n```&lt;/p&gt;\n\n&lt;p&gt;The result is 0 records.&lt;/p&gt;\n\n&lt;p&gt;My understanding is dataframe being lazy eval is not getting evaluated.&lt;/p&gt;\n\n&lt;p&gt;Can anyone help and let me know what I am doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fetk1", "is_robot_indexable": true, "report_reasons": null, "author": "ps2931", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fetk1/scala_futures_and_spark_dataframe_evaluation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fetk1/scala_futures_and_spark_dataframe_evaluation/", "subreddit_subscribers": 135666, "created_utc": 1698159592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you use the payment date instead of the booking date. It will likely impact all the reports.\n\nThis kind of update is not that frequent, but how do you share a definition update ? A slack message ? An internal data newsletter ?\n\nI've talked with team that have a Notion page dedicated to this, with basic info like when it happened, who is the owner, what's the impact on the metric. And I have started [implementing it for my users](https://app.data-drift.io/drift-overview) mostly as an exploration (because in the end users want to see it in notion, not a new tool).\n\nWhat would you add to the update report ?\n\nOr, on the other way, do you never update a metric because of all the question it will raise ? ", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Remodeling: when you update a metric definition, how do you communicate to your data consumers ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fd05d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698154590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you use the payment date instead of the booking date. It will likely impact all the reports.&lt;/p&gt;\n\n&lt;p&gt;This kind of update is not that frequent, but how do you share a definition update ? A slack message ? An internal data newsletter ?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve talked with team that have a Notion page dedicated to this, with basic info like when it happened, who is the owner, what&amp;#39;s the impact on the metric. And I have started &lt;a href=\"https://app.data-drift.io/drift-overview\"&gt;implementing it for my users&lt;/a&gt; mostly as an exploration (because in the end users want to see it in notion, not a new tool).&lt;/p&gt;\n\n&lt;p&gt;What would you add to the update report ?&lt;/p&gt;\n\n&lt;p&gt;Or, on the other way, do you never update a metric because of all the question it will raise ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17fd05d", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fd05d/remodeling_when_you_update_a_metric_definition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fd05d/remodeling_when_you_update_a_metric_definition/", "subreddit_subscribers": 135666, "created_utc": 1698154590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI'm am working on a little project that would allow people to apply custom functions to create features for a feature store. The project will be running on top of kubernetes. What it would look like is the user would write a script (python/java/sql?) or spin up a container and then the data would be extracted from the feature store transformed according to the script and then be put back into the feature store. I'm looking at stuff like Elyra and apache beam to accomplish this. But i would also need a way where the user would spin up a container and not do it via the script something like tekton i think. So basically a serverless way and a way with containers. Also it will have to be able to handle both batch and stream processing. Can you guys recommend me any other frameworks that would help with this? thank you in advance", "author_fullname": "t2_fcsujw3l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with data pipelines.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17fbe1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698149827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m am working on a little project that would allow people to apply custom functions to create features for a feature store. The project will be running on top of kubernetes. What it would look like is the user would write a script (python/java/sql?) or spin up a container and then the data would be extracted from the feature store transformed according to the script and then be put back into the feature store. I&amp;#39;m looking at stuff like Elyra and apache beam to accomplish this. But i would also need a way where the user would spin up a container and not do it via the script something like tekton i think. So basically a serverless way and a way with containers. Also it will have to be able to handle both batch and stream processing. Can you guys recommend me any other frameworks that would help with this? thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17fbe1x", "is_robot_indexable": true, "report_reasons": null, "author": "Abject-Battle1215", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17fbe1x/help_with_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17fbe1x/help_with_data_pipelines/", "subreddit_subscribers": 135666, "created_utc": 1698149827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Data Engineering Community,\n\nI am new to data engineering and I need your advice/help/directions - anything at all. And forgive my English.\n\nI am currently working on a research project that involves package status prediction. To do so, time-series data related to the package is required. The monitored data can be temperature, humidity, or anything that affects the quality of the package and can be picked up by a sensor (which is attached to the package).\n\nI have been searching for the past two weeks nonstop. I visited most of the dataset websites, searched for websites that I may be able to scrape data from, and even played with VAERS reports to extract data but just noticed the data is not helpful in my case. I have emailed countless authors who have used similar data in their research papers but none of them replied.\n\nIs there any way at all where I can get or gather such data? I just need temperature readings over time and how the package status is at the given time.\n\nThank you in advance for your assistance and for being part of this amazing community!\n\n&amp;#x200B;", "author_fullname": "t2_7kezs9t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Monitored Time-Series Data in Package Delivery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f7gwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698138742.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698134006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Engineering Community,&lt;/p&gt;\n\n&lt;p&gt;I am new to data engineering and I need your advice/help/directions - anything at all. And forgive my English.&lt;/p&gt;\n\n&lt;p&gt;I am currently working on a research project that involves package status prediction. To do so, time-series data related to the package is required. The monitored data can be temperature, humidity, or anything that affects the quality of the package and can be picked up by a sensor (which is attached to the package).&lt;/p&gt;\n\n&lt;p&gt;I have been searching for the past two weeks nonstop. I visited most of the dataset websites, searched for websites that I may be able to scrape data from, and even played with VAERS reports to extract data but just noticed the data is not helpful in my case. I have emailed countless authors who have used similar data in their research papers but none of them replied.&lt;/p&gt;\n\n&lt;p&gt;Is there any way at all where I can get or gather such data? I just need temperature readings over time and how the package status is at the given time.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your assistance and for being part of this amazing community!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17f7gwr", "is_robot_indexable": true, "report_reasons": null, "author": "pheonix_Mai", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f7gwr/looking_for_monitored_timeseries_data_in_package/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f7gwr/looking_for_monitored_timeseries_data_in_package/", "subreddit_subscribers": 135666, "created_utc": 1698134006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,  \nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.  \nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job's dependencies on workers only - that way I can keep different environments for different jobs/DAGs.  \nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is \"just install everything in one image\". This greatly concerns me, as we are trying to run away from a *one venv to rule them all* solution.  \nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.", "author_fullname": "t2_11671y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow and python dependencies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f5o2k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698126313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nso we recently deployed airflow using the official helm chart on AWS EKS, we use KubernetesExecutor and git-sync.&lt;br/&gt;\nI started playing around with it and first thing that I came across is the dependency handling. Right now our deployment has 4 different containers - webserver, triggerer, scheduler and worker. I was always under the impression that you will install your job&amp;#39;s dependencies on workers only - that way I can keep different environments for different jobs/DAGs.&lt;br/&gt;\nHowever I can not get it work without installing the dependencies on scheduler as well - getting module import errors since they are not installed. Furthermore when I looked this up online, the major opinion is &amp;quot;just install everything in one image&amp;quot;. This greatly concerns me, as we are trying to run away from a &lt;em&gt;one venv to rule them all&lt;/em&gt; solution.&lt;br/&gt;\nIs this even possible in airflow? We have tons of python jobs with various dependencies and having one big environment with all dependencies is straight up nightmare fuel for me. Any tips on this topic are hugely appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17f5o2k", "is_robot_indexable": true, "report_reasons": null, "author": "RealBrofessor", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17f5o2k/airflow_and_python_dependencies/", "subreddit_subscribers": 135666, "created_utc": 1698126313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.\n\nMy question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.\n\nAny ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...", "author_fullname": "t2_31exp9bt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Midware solution for data analytics tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17euvsp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698094583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. My job is to integrate bloomreach engagement platform effectively (former Exponea). There is API tracking and front-end tracking options to send event data directly to the platform. But many programmers and engineers make a lot of mistakes and incoming data are bad - formats, types, values, logic .... anything can be wrong.&lt;/p&gt;\n\n&lt;p&gt;My question is. If I wanted to make a tracking solution (middleware), where engineers will send all their data to my system, i will process it and send it do the platform filtered. clean, correct. Where i have to start? Cloud computing? I code daily in python (flask, pandas, numpy) or JS, but i dont know how to make an app which will work as a filter or ETL solution.&lt;/p&gt;\n\n&lt;p&gt;Any ideas where to start? I dont want to store the data in any DB, just filter and send to the platform (or where i choose to)...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17euvsp", "is_robot_indexable": true, "report_reasons": null, "author": "Sonny-Orkidea", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17euvsp/midware_solution_for_data_analytics_tool/", "subreddit_subscribers": 135666, "created_utc": 1698094583.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}