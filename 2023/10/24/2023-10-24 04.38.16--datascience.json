{"kind": "Listing", "data": {"after": "t3_17epzut", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Ok so, I was hired as a senior member of a pre-existing data science team. I now manage a few other team members (who were there before me). They are all contractors and their day rate is HIGH. They are all 'Data Scientists' and graduates.\n\nI'm older. I've done lots of technical roles and I'm not really sure what my official title is. I can do data science but I really just build stuff. I've done Data Engineering in the past, MLOps, DevOps, Cloud etc. I'm a jack of all trades, master of none.\n\nNow, I know what ***I think*** a 'Data Scientist' should be able to do:\n\n1. Pandas, Numpy, Scikit learn, matplotlib blah blah blah\n2. Version control (Git)\n3. Managing virtual environments\n4. Debugging within an IDE\n5. Scoping out a project, ideation, exploration\n6. Report writing skills/communication skills\n7. Some exposure to clean code conventions (PEP-8)\n8. Some exposure to SQL like syntax\n9. bit of linux would be cool (I can teach them)\n10. bit of cloud would be cool (I can teach them)\n\nI've had to mentor the team HARD. Most of the team did not know what Git was, most of the team had never debugged their code, never made a venv. In fact I have had to teach them steps 1-5. That would be fine if they were now hitting the ground running, but the moment I stop mentoring them, the productivity stops. No initiative.\n\nAnd yet, I want to hire externally. I want to give them the opportunity to apply but I just know they won't measure up against the talent pool out there. I've hired Data Scientists before and I know how good people are out there.\n\nAm I totally wrong? Do I need to cut them some slack? Anyone got any comments?\n\nedit: spelling", "author_fullname": "t2_315ygt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Contractors who are called Data Scientists but can't do what I'd expect. What to do next.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eu3rm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 92, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 92, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698095922.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698092618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok so, I was hired as a senior member of a pre-existing data science team. I now manage a few other team members (who were there before me). They are all contractors and their day rate is HIGH. They are all &amp;#39;Data Scientists&amp;#39; and graduates.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m older. I&amp;#39;ve done lots of technical roles and I&amp;#39;m not really sure what my official title is. I can do data science but I really just build stuff. I&amp;#39;ve done Data Engineering in the past, MLOps, DevOps, Cloud etc. I&amp;#39;m a jack of all trades, master of none.&lt;/p&gt;\n\n&lt;p&gt;Now, I know what &lt;strong&gt;&lt;em&gt;I think&lt;/em&gt;&lt;/strong&gt; a &amp;#39;Data Scientist&amp;#39; should be able to do:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Pandas, Numpy, Scikit learn, matplotlib blah blah blah&lt;/li&gt;\n&lt;li&gt;Version control (Git)&lt;/li&gt;\n&lt;li&gt;Managing virtual environments&lt;/li&gt;\n&lt;li&gt;Debugging within an IDE&lt;/li&gt;\n&lt;li&gt;Scoping out a project, ideation, exploration&lt;/li&gt;\n&lt;li&gt;Report writing skills/communication skills&lt;/li&gt;\n&lt;li&gt;Some exposure to clean code conventions (PEP-8)&lt;/li&gt;\n&lt;li&gt;Some exposure to SQL like syntax&lt;/li&gt;\n&lt;li&gt;bit of linux would be cool (I can teach them)&lt;/li&gt;\n&lt;li&gt;bit of cloud would be cool (I can teach them)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve had to mentor the team HARD. Most of the team did not know what Git was, most of the team had never debugged their code, never made a venv. In fact I have had to teach them steps 1-5. That would be fine if they were now hitting the ground running, but the moment I stop mentoring them, the productivity stops. No initiative.&lt;/p&gt;\n\n&lt;p&gt;And yet, I want to hire externally. I want to give them the opportunity to apply but I just know they won&amp;#39;t measure up against the talent pool out there. I&amp;#39;ve hired Data Scientists before and I know how good people are out there.&lt;/p&gt;\n\n&lt;p&gt;Am I totally wrong? Do I need to cut them some slack? Anyone got any comments?&lt;/p&gt;\n\n&lt;p&gt;edit: spelling&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eu3rm", "is_robot_indexable": true, "report_reasons": null, "author": "wagwagtail", "discussion_type": null, "num_comments": 101, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eu3rm/contractors_who_are_called_data_scientists_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eu3rm/contractors_who_are_called_data_scientists_but/", "subreddit_subscribers": 1096907, "created_utc": 1698092618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I see a lot of People job hopping these days And I just don't know how they do it. I really don't. Because the last few jobs that I interviewed for, it was exhausting. Truly exhausted. The first few interviews are easy, but then you do it 20 times, and having to answer behavioral interview questions becomes just old. Completely exhausting. Tiring beyond belief. Every job that's worth working for, at least 5 to 6 interviews, and small chance of getting the job, so you have to do 5 to 6 interviews at 20 different jobs in order to have a chance.  \n\n\nLike, how do people have the energy to do all the interviews while working a full-time job? Because at my current job I have to program a lot and I'm exhausted", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People aren't lazy, they are suffering from interview fatigue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ezjmw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 104, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 104, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698106874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of People job hopping these days And I just don&amp;#39;t know how they do it. I really don&amp;#39;t. Because the last few jobs that I interviewed for, it was exhausting. Truly exhausted. The first few interviews are easy, but then you do it 20 times, and having to answer behavioral interview questions becomes just old. Completely exhausting. Tiring beyond belief. Every job that&amp;#39;s worth working for, at least 5 to 6 interviews, and small chance of getting the job, so you have to do 5 to 6 interviews at 20 different jobs in order to have a chance.  &lt;/p&gt;\n\n&lt;p&gt;Like, how do people have the energy to do all the interviews while working a full-time job? Because at my current job I have to program a lot and I&amp;#39;m exhausted&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17ezjmw", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ezjmw/people_arent_lazy_they_are_suffering_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ezjmw/people_arent_lazy_they_are_suffering_from/", "subreddit_subscribers": 1096907, "created_utc": 1698106874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a director and I feel like I barely do \"Data Science\" any more. My job is mostly about working with engineers and architects to facilitate data collection and data tools (python, spark) for my team. Is this relevant for career advancement or do I need to refocus more on hard skills and learning new stuff.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do fellow Data Science Directors do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17el93s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698070046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a director and I feel like I barely do &amp;quot;Data Science&amp;quot; any more. My job is mostly about working with engineers and architects to facilitate data collection and data tools (python, spark) for my team. Is this relevant for career advancement or do I need to refocus more on hard skills and learning new stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17el93s", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17el93s/what_do_fellow_data_science_directors_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17el93s/what_do_fellow_data_science_directors_do/", "subreddit_subscribers": 1096907, "created_utc": 1698070046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the non-data scientist tasks that you still do in your data scientist role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17efkcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698049934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17efkcz", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17efkcz/what_are_the_nondata_scientist_tasks_that_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17efkcz/what_are_the_nondata_scientist_tasks_that_you/", "subreddit_subscribers": 1096907, "created_utc": 1698049934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_1yjldyeq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas-based library for graphing emotion events with LMs for in-depth sentiment analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17eiqbl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BU0fAANwIMR57EGmmleNI78KoM_wCoeHt1otjTk7UFY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698062621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/hdls4kgrzxvb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?auto=webp&amp;s=d3222101a81f485e597fb2eb79cd097d49297aeb", "width": 1150, "height": 768}, "resolutions": [{"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dff2d78d3fd6edaaf19223d754d9fd559bacedc", "width": 108, "height": 72}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83dea6f55fa8676296c8ae66c64117cfd02ae04d", "width": 216, "height": 144}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e37743083741785d5ae0554750102754d503d6a9", "width": 320, "height": 213}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aaae51500e62cefb4fe77530e21e7b847fe75f90", "width": 640, "height": 427}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=de8137f2681ebbfae39897b3a8028ab208fa78b5", "width": 960, "height": 641}, {"url": "https://preview.redd.it/hdls4kgrzxvb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da785ad2dbe896d0589134bd0bbb6fc8373ae552", "width": 1080, "height": 721}], "variants": {}, "id": "OFW8OTEOtv794Im1BEzy9osCxAL3qmYG3gzh1yd1b3o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17eiqbl", "is_robot_indexable": true, "report_reasons": null, "author": "helliun", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eiqbl/pandasbased_library_for_graphing_emotion_events/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/hdls4kgrzxvb1.jpg", "subreddit_subscribers": 1096907, "created_utc": 1698062621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My work primarily stores data in a full databases. Pandas has a lot of similar functionality to SQL in regards to the ability to group data and preform calculations, even being able to take full on SQL queries to import data. Do you guys do all your calculations in the query itself, or in python after the data has been imported? What about with grouping data?", "author_fullname": "t2_2o12zv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you do in SQL vs Pandas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eot80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698079172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My work primarily stores data in a full databases. Pandas has a lot of similar functionality to SQL in regards to the ability to group data and preform calculations, even being able to take full on SQL queries to import data. Do you guys do all your calculations in the query itself, or in python after the data has been imported? What about with grouping data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17eot80", "is_robot_indexable": true, "report_reasons": null, "author": "Alucard2051", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eot80/what_do_you_do_in_sql_vs_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eot80/what_do_you_do_in_sql_vs_pandas/", "subreddit_subscribers": 1096907, "created_utc": 1698079172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\nI've heard of a lot of people using chat GPT in their jobs. What I don't understand is how? How is everyone doing this? I've been told not to upload proprietary company data to chat GPT. I'm not sure if a SQL query with column names counts as proprietary data but I'm assuming that it does? What I don't understand is how there are so many thousands of people out there using chat GPT when they are stuck on a problem, but I am expected to never ever consult it or use it in my job ever, because it's not allowed...\n\n\nHow do people get away with it?", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People who upload proprietary data to GPT, how do you get away with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17evy53", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698097182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard of a lot of people using chat GPT in their jobs. What I don&amp;#39;t understand is how? How is everyone doing this? I&amp;#39;ve been told not to upload proprietary company data to chat GPT. I&amp;#39;m not sure if a SQL query with column names counts as proprietary data but I&amp;#39;m assuming that it does? What I don&amp;#39;t understand is how there are so many thousands of people out there using chat GPT when they are stuck on a problem, but I am expected to never ever consult it or use it in my job ever, because it&amp;#39;s not allowed...&lt;/p&gt;\n\n&lt;p&gt;How do people get away with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17evy53", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17evy53/people_who_upload_proprietary_data_to_gpt_how_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17evy53/people_who_upload_proprietary_data_to_gpt_how_do/", "subreddit_subscribers": 1096907, "created_utc": 1698097182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's been a lot of chatter about AI, specifically things like LLAMA 2, GPT-4, etc. But, what have been some recent advancements not in the AI sphere that are important in Data Science?", "author_fullname": "t2_53gvtiyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Outside of Generative AI, what are the big advances currently happening in Data Science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17esy03", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698089666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s been a lot of chatter about AI, specifically things like LLAMA 2, GPT-4, etc. But, what have been some recent advancements not in the AI sphere that are important in Data Science?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17esy03", "is_robot_indexable": true, "report_reasons": null, "author": "htii_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17esy03/outside_of_generative_ai_what_are_the_big/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17esy03/outside_of_generative_ai_what_are_the_big/", "subreddit_subscribers": 1096907, "created_utc": 1698089666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my present company we are just chasing ad hoc analytical  work - these never gets into production. The processes are very ad hoc, not streamlined, no structure to it, running from personal notebooks. It\u2019s very demoralizing to see models developed from 2017 that are in production and have not been refreshed thought the data it used for inference is constantly changing as my company looks at market finance data. \n\nI\u2019m wondering what are other good companies to look out for that are either applying best practices in DS/ML and not just the talk or building product/services. \n\nI understand recent news in GenAI is sparking lot of conversations but which companies out there are grabbing it by the horns and taking the lead? Perhaps if you are fortunate to work for one such company you may want to share your story. Appreciate your insights very much!", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Besides FAANG, what other companies out there are doing actual DS or MLE work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17er8bt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698085365.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my present company we are just chasing ad hoc analytical  work - these never gets into production. The processes are very ad hoc, not streamlined, no structure to it, running from personal notebooks. It\u2019s very demoralizing to see models developed from 2017 that are in production and have not been refreshed thought the data it used for inference is constantly changing as my company looks at market finance data. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m wondering what are other good companies to look out for that are either applying best practices in DS/ML and not just the talk or building product/services. &lt;/p&gt;\n\n&lt;p&gt;I understand recent news in GenAI is sparking lot of conversations but which companies out there are grabbing it by the horns and taking the lead? Perhaps if you are fortunate to work for one such company you may want to share your story. Appreciate your insights very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17er8bt", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17er8bt/besides_faang_what_other_companies_out_there_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17er8bt/besides_faang_what_other_companies_out_there_are/", "subreddit_subscribers": 1096907, "created_utc": 1698085365.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_dif6b393", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would anyone start to use Hex? What\u2019s the need or situation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eu7oy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698092900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17eu7oy", "is_robot_indexable": true, "report_reasons": null, "author": "ExpressOcelot8977", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eu7oy/why_would_anyone_start_to_use_hex_whats_the_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eu7oy/why_would_anyone_start_to_use_hex_whats_the_need/", "subreddit_subscribers": 1096907, "created_utc": 1698092900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO. \n\nJob 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there\n\nJob 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. \n\nJob 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here \n\nI have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. \n\nDo I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?", "author_fullname": "t2_b7eqz4bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Hop for 4th Time This Year? How Do I Save my Career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ezosy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698107282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a turbulent year career and mental health wise and need some advice. I\u2019m 27YO. &lt;/p&gt;\n\n&lt;p&gt;Job 1: govt contracting employer making $90K in HCOL, was there for 3 years. Salary was low and put onto a multi year project I didn\u2019t want to do, so I left in March. Was high performer there&lt;/p&gt;\n\n&lt;p&gt;Job 2: another govt contractor for ~130K and was hired as a DE. Put on a project with no DE work, so my performance was low doing unrelated tasks. Stayed for 5ish months, then had to relocate back home across country due to severe depression. &lt;/p&gt;\n\n&lt;p&gt;Job 3: ~110K and full on-site in automotive space (think gas stations, car washes, etc). Took this job to keep money coming in. Been here for about 6 weeks. Bad/nearly non existent tech stack and lots of tech debt. Lots of stress due to organizational dysfunction. In my hometown, but don\u2019t really want to live here &lt;/p&gt;\n\n&lt;p&gt;I have a possible lead to go to another govt contractor, remote and for more money. Although I do want to work in private industry long term. &lt;/p&gt;\n\n&lt;p&gt;Do I need to stick it out at my current job for a year+ to repair my now trashed resume? WWYD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ezosy", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Box228", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ezosy/job_hop_for_4th_time_this_year_how_do_i_save_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ezosy/job_hop_for_4th_time_this_year_how_do_i_save_my/", "subreddit_subscribers": 1096907, "created_utc": 1698107282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I need to explore an odd problem. We have an old dataset of interview sessions (its not our dataset). It works as follows.\n\nThe candidate comes in, goes through several rounds of interviews (from 1 - 5) each with its own interviewer. (We know the number of interviewers)\n\nAfter each round, the candidate rates the interviewer (score from 0 to 5). (We do not have this data)\n\nFinally, an overall score is calculated for the entire interview session based on the ratings for each round. (We know the overall score but we do not know how it was calculated)\n\nSo essentially, the dataset is roughly off the form:\n\nsession_id, score, [interviewer_id1, interviewer_id2, interviewer_id3 ...] (This list is unordered)\n\nThe question is: given a particular interviewer_id, is it possible to determine whether he generally got positive or negative ratings?\n\nFor context, I write software and don't know much beyond stats 101 so I would appreciate any and all pointers. I would ordinarily say no to the above question but I have met people who've been able to pull signals out of noise so it behoves me to ask.\n\nThanks.", "author_fullname": "t2_rz22aza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wondering whether the following problem is workable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eirhz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698062724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I need to explore an odd problem. We have an old dataset of interview sessions (its not our dataset). It works as follows.&lt;/p&gt;\n\n&lt;p&gt;The candidate comes in, goes through several rounds of interviews (from 1 - 5) each with its own interviewer. (We know the number of interviewers)&lt;/p&gt;\n\n&lt;p&gt;After each round, the candidate rates the interviewer (score from 0 to 5). (We do not have this data)&lt;/p&gt;\n\n&lt;p&gt;Finally, an overall score is calculated for the entire interview session based on the ratings for each round. (We know the overall score but we do not know how it was calculated)&lt;/p&gt;\n\n&lt;p&gt;So essentially, the dataset is roughly off the form:&lt;/p&gt;\n\n&lt;p&gt;session_id, score, [interviewer_id1, interviewer_id2, interviewer_id3 ...] (This list is unordered)&lt;/p&gt;\n\n&lt;p&gt;The question is: given a particular interviewer_id, is it possible to determine whether he generally got positive or negative ratings?&lt;/p&gt;\n\n&lt;p&gt;For context, I write software and don&amp;#39;t know much beyond stats 101 so I would appreciate any and all pointers. I would ordinarily say no to the above question but I have met people who&amp;#39;ve been able to pull signals out of noise so it behoves me to ask.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17eirhz", "is_robot_indexable": true, "report_reasons": null, "author": "grchelp2018", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eirhz/wondering_whether_the_following_problem_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eirhz/wondering_whether_the_following_problem_is/", "subreddit_subscribers": 1096907, "created_utc": 1698062724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Happy monday guys! \n\nQuick question \u2013 what do you do on light days where you don\u2019t have much(or any) work and want to maintain your productivity, especially when working from home? \n\nI would love to increase my theory/stress on learning new skills! So if you\u2019re one who reads books/blogs would love to know what you guys read or any book recommendations\n\nCheers guys, have a great week!", "author_fullname": "t2_7xxtza3n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Productivity help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eh6kb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698056978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Happy monday guys! &lt;/p&gt;\n\n&lt;p&gt;Quick question \u2013 what do you do on light days where you don\u2019t have much(or any) work and want to maintain your productivity, especially when working from home? &lt;/p&gt;\n\n&lt;p&gt;I would love to increase my theory/stress on learning new skills! So if you\u2019re one who reads books/blogs would love to know what you guys read or any book recommendations&lt;/p&gt;\n\n&lt;p&gt;Cheers guys, have a great week!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17eh6kb", "is_robot_indexable": true, "report_reasons": null, "author": "Asleep-Fun-6508", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eh6kb/productivity_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eh6kb/productivity_help/", "subreddit_subscribers": 1096907, "created_utc": 1698056978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi Everyone,\n\nRecently, I have been doing a task related to paraphrasing in writing tones. Specifically, I'm trying to fine-tune the pre-trained model (text generation model) to create a model capable of rewriting according to the transmitted tone.\n\nCurrently, I am trying to crawl data (about 1500 samples) for training. However, the results were not as good as I thought. I'm currently quite stuck, can you guys suggest to me some research or open-source or pre-trained models that you've tried?\n\nThank you\n\nP/s: model I have tried\n\n[https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic](https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic)\n\n[https://huggingface.co/Vamsi/T5\\_Paraphrase\\_Paws](https://huggingface.co/Vamsi/T5_Paraphrase_Paws)\n\n[https://huggingface.co/humarin/chatgpt\\_paraphraser\\_on\\_T5\\_base](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base)", "author_fullname": "t2_6cetklvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Discussion] Paraphrase for Writing Tone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17f3whi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "AI", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698119886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;Recently, I have been doing a task related to paraphrasing in writing tones. Specifically, I&amp;#39;m trying to fine-tune the pre-trained model (text generation model) to create a model capable of rewriting according to the transmitted tone.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am trying to crawl data (about 1500 samples) for training. However, the results were not as good as I thought. I&amp;#39;m currently quite stuck, can you guys suggest to me some research or open-source or pre-trained models that you&amp;#39;ve tried?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n\n&lt;p&gt;P/s: model I have tried&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic\"&gt;https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Vamsi/T5_Paraphrase_Paws\"&gt;https://huggingface.co/Vamsi/T5_Paraphrase_Paws&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base\"&gt;https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?auto=webp&amp;s=d0be1127ce5f79f82e948ab8cf96e7210cb6be1a", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=426608452fb9896e13faf3927088eff2f627b403", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b087c0785d84c60389bb1655f2ae98cb6f248ffd", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=302eaab41bd4a8b53bfa9061b3ac9140b505d2da", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f1dc3a4e793990bd41aa7d500a77bda02a577a8", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d9232b9dbd57fad170f7fe8ef7a8a9dd15ac2bd6", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/Ki-ySKqPYoVB810RDNEcpSDJ5eaLWtKb9nFryfAioJE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46b0b248ceceee525161b9c7014ca6179262b962", "width": 1080, "height": 583}], "variants": {}, "id": "8Fr77lFd7Gnv94R4vPukgxG6yPtiiDq5OFz_y3a-gpQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2f731e52-70eb-11ee-bec5-5a5142e6a4d2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "17f3whi", "is_robot_indexable": true, "report_reasons": null, "author": "unknow_from_vietnam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17f3whi/discussion_paraphrase_for_writing_tone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17f3whi/discussion_paraphrase_for_writing_tone/", "subreddit_subscribers": 1096907, "created_utc": 1698119886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Would be nice to understand frameworks , experiment types, how to determine what experiment to use , and where and when to apply them to a saas company and help them prioritize a roadmap against it. \n", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have a good blog or resource on Product-led experimentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17f02jx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698109573.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698108373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would be nice to understand frameworks , experiment types, how to determine what experiment to use , and where and when to apply them to a saas company and help them prioritize a roadmap against it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17f02jx", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17f02jx/anyone_have_a_good_blog_or_resource_on_productled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17f02jx/anyone_have_a_good_blog_or_resource_on_productled/", "subreddit_subscribers": 1096907, "created_utc": 1698108373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've got the task to estimate the sales level of a store in a place near a mall and a office area. Would like to know if somebody here has made a similar task reacently or has any idea of how can i get an estimation.\n\nI have data of 6 more stores of the same company (sales, transactions, area fo the store, #people near a 15 minute isochrone, if the stores are near offices, colleges, residential areas, etc).\n\nI've been planning to run a regression model or a decision tree and later use trained model to estimate the sales level of the new position, but just having 6 stores makes it hard to have a consistent estimation.\n\nWhat other options could i do to have a good estimation of this new position? what other things i have to consider o look for to have as data in my model? is there any framework for this kind of task?\n\nThanks!", "author_fullname": "t2_6ihag2sc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Estimating sales of a new store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ew2w4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Challenges", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698097522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got the task to estimate the sales level of a store in a place near a mall and a office area. Would like to know if somebody here has made a similar task reacently or has any idea of how can i get an estimation.&lt;/p&gt;\n\n&lt;p&gt;I have data of 6 more stores of the same company (sales, transactions, area fo the store, #people near a 15 minute isochrone, if the stores are near offices, colleges, residential areas, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been planning to run a regression model or a decision tree and later use trained model to estimate the sales level of the new position, but just having 6 stores makes it hard to have a consistent estimation.&lt;/p&gt;\n\n&lt;p&gt;What other options could i do to have a good estimation of this new position? what other things i have to consider o look for to have as data in my model? is there any framework for this kind of task?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "417296a0-70eb-11ee-8c58-122e95e91c4c", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffd635", "id": "17ew2w4", "is_robot_indexable": true, "report_reasons": null, "author": "bbmr__95", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ew2w4/estimating_sales_of_a_new_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ew2w4/estimating_sales_of_a_new_store/", "subreddit_subscribers": 1096907, "created_utc": 1698097522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello there,\n\nI'm an undergrad student that is currently working on a Kaggle dataset and I want to document my progression and be able to share it as I go. In addition, I'd really want to get involved with the DS community. Now, I do have deficiency in certain tools like GitHub which is a place I could post my work. However, I do also want to be able to include it in my resume as I think it would make it more appealing for recruiters in the future. What is the best way to go about this? Just create a reddit or LinkedIn Post (like a progress post) or simply just have it up on GitHub and learn how to use the tool ? Thank you in advance for your suggestions.", "author_fullname": "t2_8dsh3icl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to go about showing progress as work is done to a dataset", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17euytw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698094791.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an undergrad student that is currently working on a Kaggle dataset and I want to document my progression and be able to share it as I go. In addition, I&amp;#39;d really want to get involved with the DS community. Now, I do have deficiency in certain tools like GitHub which is a place I could post my work. However, I do also want to be able to include it in my resume as I think it would make it more appealing for recruiters in the future. What is the best way to go about this? Just create a reddit or LinkedIn Post (like a progress post) or simply just have it up on GitHub and learn how to use the tool ? Thank you in advance for your suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17euytw", "is_robot_indexable": true, "report_reasons": null, "author": "Notgen3ric", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17euytw/best_way_to_go_about_showing_progress_as_work_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17euytw/best_way_to_go_about_showing_progress_as_work_is/", "subreddit_subscribers": 1096907, "created_utc": 1698094791.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently undergoing Apprenticeships programme for ML, and looking for projects in our organization.\n\n\"Demand Transference and Substititabilty\" in retail food stores is one of the ideas that came up. So i am trying to find on how to implement it and if we have all the required data before finalising the project selection. \n\nAny resources or information would be great :)", "author_fullname": "t2_4pgpyc7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any pointers / resources on how one would implement a ML model for product demand transference and substititabilty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eug5a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698093478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently undergoing Apprenticeships programme for ML, and looking for projects in our organization.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Demand Transference and Substititabilty&amp;quot; in retail food stores is one of the ideas that came up. So i am trying to find on how to implement it and if we have all the required data before finalising the project selection. &lt;/p&gt;\n\n&lt;p&gt;Any resources or information would be great :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17eug5a", "is_robot_indexable": true, "report_reasons": null, "author": "FrozenSoul90", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eug5a/any_pointers_resources_on_how_one_would_implement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eug5a/any_pointers_resources_on_how_one_would_implement/", "subreddit_subscribers": 1096907, "created_utc": 1698093478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have talked this previously, that like, I am working as a data analyst but is it worth to learn graph database. I got some comments that saying master SQL first, then learn other tools. For me, learning a new fun tool is for my free time so I thought, OK, I will just try it. It is been a month almost and came back to think like,,, I don't feel the graph database is that much worth to learn especially if I consider the size of the market.\n\nHowever, maybe, if there's a PG extension that adds graph analytics to PG database, which I use everyday, it would be fun because I can actually utilize it with my PG data. Apache AGE is an open-source PG extension that really solves the problem that I'm having right now. I will leave the [github link](https://github.com/apache/age) and a [webinar link](https://us06web.zoom.us/webinar/register/2516980853755/WN_mzhlCggCQ_ytIxiGb9ioTg) that they (I guess Apache Foundation?) organize like bi-weekly. For those who are having same thought process with me, I think you guys also can just try? What do you think?", "author_fullname": "t2_59z60tud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PG extension (Apache AGE) for adding graph analytics functionality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eriso", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698086112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have talked this previously, that like, I am working as a data analyst but is it worth to learn graph database. I got some comments that saying master SQL first, then learn other tools. For me, learning a new fun tool is for my free time so I thought, OK, I will just try it. It is been a month almost and came back to think like,,, I don&amp;#39;t feel the graph database is that much worth to learn especially if I consider the size of the market.&lt;/p&gt;\n\n&lt;p&gt;However, maybe, if there&amp;#39;s a PG extension that adds graph analytics to PG database, which I use everyday, it would be fun because I can actually utilize it with my PG data. Apache AGE is an open-source PG extension that really solves the problem that I&amp;#39;m having right now. I will leave the &lt;a href=\"https://github.com/apache/age\"&gt;github link&lt;/a&gt; and a &lt;a href=\"https://us06web.zoom.us/webinar/register/2516980853755/WN_mzhlCggCQ_ytIxiGb9ioTg\"&gt;webinar link&lt;/a&gt; that they (I guess Apache Foundation?) organize like bi-weekly. For those who are having same thought process with me, I think you guys also can just try? What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?auto=webp&amp;s=a4a9678ee6d4fee3ad114c4e3f4c60bd3a9878b9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c61ccf87e96ae7f1e62a23e0d3e0edaf1e9231ad", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c3d8c63135c41a9e9b540db64704378fc6d327a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc77ff3491686199af775da523847897c7960e45", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ced5be128d162387dc23847553096ba7a82d98e4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=280b511ee72232f7f9af49618992ae1faf180cb0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/3qLWaHB72-aNQu6FqcxITz9lByzkf8IppAI_7rEEX-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=796899fab0653df124ea60ce9b1a3e08173916ad", "width": 1080, "height": 540}], "variants": {}, "id": "SJvZ9bFaaSdT-WOtIPVpyQVNRTxT1PIoiuA3-IrbPYc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17eriso", "is_robot_indexable": true, "report_reasons": null, "author": "oh5oh5", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eriso/pg_extension_apache_age_for_adding_graph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eriso/pg_extension_apache_age_for_adding_graph/", "subreddit_subscribers": 1096907, "created_utc": 1698086112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m looking to discover new relationships that exist in the relational database and then generate ingestion script to populate a graph database. Are there tools already exhausting for this and what are their limitations? Can we he new LLMs come to rescue?", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational database to graph database using NLP/LLM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17erhca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698086005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to discover new relationships that exist in the relational database and then generate ingestion script to populate a graph database. Are there tools already exhausting for this and what are their limitations? Can we he new LLMs come to rescue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17erhca", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17erhca/relational_database_to_graph_database_using_nlpllm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17erhca/relational_database_to_graph_database_using_nlpllm/", "subreddit_subscribers": 1096907, "created_utc": 1698086005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a text corpus of tweets written by users in a four month period, and I computed the sentiment of all words written by the same user in my data. I then calculated the share of negative words written by individual A across across all words written by individual A. Ex: individual A wrote 3000 words, and 1000 of which were classified as negative, then individual A's negative share would be \\~ 33%.\n\n I am then comparing how the share of negative words for each username before and after a certain event occurs, but I am not sure how to besd visualize this type of data using R.\n\nHere is a data example:\n\n\\`\\`\\`\n\ndput(sentiment\\_twitter\\[1186:1194,c(1,2,3)\\])\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nLooking at the output below, we can learn that the \"negative\\_word\\_share\" for id 1194 has increased after the \"treatment\\_implementation\", which refers to the event of interest.\n\nThe challenge here is that I have \\~3K observations, and I am not sure what's the best way to visualize this type of granular data?\n\n\\`\\`\\`\n\nstructure(list(id = c(912L, 912L, 913L, 914L, 915L, 916L, 917L, \n\n918L, 919L), treatment\\_implementation = c(0, 1, 1, 0, 0, 1, 1, \n\n1, 0), negative\\_word\\_share = c(20, 49.4252873563218, 0, 60, 50, \n\n0, 100, 88.8888888888889, 0)), class = c(\"grouped\\_df\", \"tbl\\_df\", \n\n\"tbl\", \"data.frame\"), row.names = c(NA, -9L), groups = structure(list(\n\nid = 912:919, .rows = structure(list(1:2, 3L, 4L, 5L, 6L, \n\n7L, 8L, 9L), ptype = integer(0), class = c(\"vctrs\\_list\\_of\", \n\n\"vctrs\\_vctr\", \"list\"))), class = c(\"tbl\\_df\", \"tbl\", \"data.frame\"\n\n), row.names = c(NA, -8L), .drop = TRUE))\n\n\\`\\`\\`", "author_fullname": "t2_hz4cyqnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for visualizing user-level sentiment before and after an event", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17enpwe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698076456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a text corpus of tweets written by users in a four month period, and I computed the sentiment of all words written by the same user in my data. I then calculated the share of negative words written by individual A across across all words written by individual A. Ex: individual A wrote 3000 words, and 1000 of which were classified as negative, then individual A&amp;#39;s negative share would be ~ 33%.&lt;/p&gt;\n\n&lt;p&gt;I am then comparing how the share of negative words for each username before and after a certain event occurs, but I am not sure how to besd visualize this type of data using R.&lt;/p&gt;\n\n&lt;p&gt;Here is a data example:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;dput(sentiment_twitter[1186:1194,c(1,2,3)])&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Looking at the output below, we can learn that the &amp;quot;negative_word_share&amp;quot; for id 1194 has increased after the &amp;quot;treatment_implementation&amp;quot;, which refers to the event of interest.&lt;/p&gt;\n\n&lt;p&gt;The challenge here is that I have ~3K observations, and I am not sure what&amp;#39;s the best way to visualize this type of granular data?&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;structure(list(id = c(912L, 912L, 913L, 914L, 915L, 916L, 917L, &lt;/p&gt;\n\n&lt;p&gt;918L, 919L), treatment_implementation = c(0, 1, 1, 0, 0, 1, 1, &lt;/p&gt;\n\n&lt;p&gt;1, 0), negative_word_share = c(20, 49.4252873563218, 0, 60, 50, &lt;/p&gt;\n\n&lt;p&gt;0, 100, 88.8888888888889, 0)), class = c(&amp;quot;grouped_df&amp;quot;, &amp;quot;tbl_df&amp;quot;, &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;tbl&amp;quot;, &amp;quot;data.frame&amp;quot;), row.names = c(NA, -9L), groups = structure(list(&lt;/p&gt;\n\n&lt;p&gt;id = 912:919, .rows = structure(list(1:2, 3L, 4L, 5L, 6L, &lt;/p&gt;\n\n&lt;p&gt;7L, 8L, 9L), ptype = integer(0), class = c(&amp;quot;vctrs_list_of&amp;quot;, &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;vctrs_vctr&amp;quot;, &amp;quot;list&amp;quot;))), class = c(&amp;quot;tbl_df&amp;quot;, &amp;quot;tbl&amp;quot;, &amp;quot;data.frame&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;), row.names = c(NA, -8L), .drop = TRUE))&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17enpwe", "is_robot_indexable": true, "report_reasons": null, "author": "nesta1970", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17enpwe/ideas_for_visualizing_userlevel_sentiment_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17enpwe/ideas_for_visualizing_userlevel_sentiment_before/", "subreddit_subscribers": 1096907, "created_utc": 1698076456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI'm updating my Portfolio to get back to DS. Working on a project I'd like to put the algorithm into an interface. Is it better to try and do it using other programming languages like JavaScript or Python is sufficient using Flask or Streamlit ? ", "author_fullname": "t2_fd92duw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Interface.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17en64n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698075041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m updating my Portfolio to get back to DS. Working on a project I&amp;#39;d like to put the algorithm into an interface. Is it better to try and do it using other programming languages like JavaScript or Python is sufficient using Flask or Streamlit ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "17en64n", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-School-07", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17en64n/project_interface/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17en64n/project_interface/", "subreddit_subscribers": 1096907, "created_utc": 1698075041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "# A real-world case study of performance optimization in Numpy\n\nThis article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/).\n\n&amp;#x200B;\n\n[ How to Optimize Multidimensional Numpy Array Operations with Numexpr. Photo Credit: Created by Author, Canva. ](https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;format=png&amp;auto=webp&amp;s=ab8950800797f55f538fdb1343df6d275bd07152)\n\nThis is a relatively brief article. In it, I will use a real-world scenario as an example to explain how to use [Numexpr expressions](https://numexpr.readthedocs.io/en/latest/user_guide.html?ref=dataleadsfuture.com#supported-functions) in multidimensional Numpy arrays to achieve substantial performance improvements.\n\nThere aren't many articles explaining how to use Numexpr in multidimensional Numpy arrays and how to use Numexpr expressions, so I hope this one will help you.\n\n# Introduction\n\nRecently, while reviewing some of my old work, I stumbled upon this piece of code:\n\n    def predict(X, w, b):\n        z = np.dot(X, w)\n        y_hat = sigmoid(z)\n        y_pred = np.zeros((y_hat.shape[0], 1))\n    \n        for i in range(y_hat.shape[0]):\n            if y_hat[i, 0] &lt; 0.5:\n                y_pred[i, 0] = 0\n            else:\n                y_pred[i, 0] = 1\n        return y_pred\n\nThis code transforms prediction results from probabilities to classification results of 0 or 1 in the logistic regression model of machine learning.\n\nBut heavens, who would use a `for loop` to iterate over Numpy ndarray?\n\nYou can foresee that when the data reaches a certain amount, it will not only occupy a lot of memory, but the performance will also be inferior.\n\nThat's right, the person who wrote this code was me when I was younger.\n\nWith a sense of responsibility, I plan to rewrite this code with the Numexpr library today.\n\nAlong the way, I will show you how to use Numexpr and Numexpr's `where` expression in multidimensional Numpy arrays to achieve significant performance improvements.\n\n## Code Implementation\n\nIf you are not familiar with the basic usage of Numexpr, you can refer to this article:\n\n[https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/](https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/)\n\nThis article uses a real-world example to demonstrate the specific usage of Numexpr's API and expressions in Numpy and Pandas.\n\n*where(bool, number1, number2): number* *- number1 if the bool condition is true, number2 otherwise.*\n\nThe above is the usage of the where expression in Numpy.\n\nWhen dealing with matrix data, you may used to using Pandas `DataFrame`. But since the `eval` method of Pandas does not support the `where` expression, you can only choose to use Numexpr in multidimensional Numpy ndarray.\n\nDon't worry, I'll explain it to you right away.\n\nBefore starting, we need to import the necessary packages and implement a `generate_ndarray` method to generate a specific size ndarray for testing:\n\n    from typing import Callable\n    import time\n    \n    import numpy as np\n    import numexpr as ne\n    import matplotlib.pyplot as plt\n    \n    rng = np.random.default_rng(seed=4000)\n    \n    def generate_ndarray(rows: int) -&gt; np.ndarray:\n        result_array = rng.random((rows, 1))\n        return result_array\n\nFirst, we generate a matrix of 200 rows to see if it is the test data we want:\n\n    In:  arr = generate_ndarray(200)\n         print(f\"The dimension of this array: {arr.ndim}\")\n         print(f\"The shape of this array: {arr.shape}\")\n    \n    \n    Out: The dimension of this array: 2\n         The shape of this array: (200, 1)\n\nTo be close to the actual situation of the logistic regression model, we generate an ndarray of the shape `(200, 1)` Of course, you can also test other shapes of ndarray according to your needs.\n\nThen, we start writing the specific use of Numexpr in the `numexpr_to_binary` method:\n\n* First, we use the index to separate the columns that need to be processed.\n* Then, use the where expression of Numexpr to process the values.\n* Finally, merge the processed columns with other columns to generate the required results.\n\nSince the ndarray's shape here is `(200, 1)`, there is only one column, so I add a new dimension.\n\nThe code is as follows:\n\n    def numexpr_to_binary(np_array: np.ndarray) -&gt; np.ndarray:\n        temp = np_array[:, 0]\n        temp = ne.evaluate(\"where(temp&lt;0.5, 0, 1)\")\n        return temp[:, np.newaxis]\n\nWe can test the result with an array of 10 rows to see if it is what I want:\n\n    arr = generate_ndarray(10)\n    result = numexpr_to_binary(arr)\n    \n    mapping = np.column_stack((arr, result))\n    mapping\n\n[ I test an array of 10 rows and the result is what I want. Image by Author ](https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;format=png&amp;auto=webp&amp;s=d6977cc422be66a8c37980554c1478e76d2d326c)\n\nLook, the match is correct. Our task is completed.\n\nThe entire process can be demonstrated with the following figure:\n\n&amp;#x200B;\n\n[ The entire process of how Numexpr transforms the multidimensional ndarray. Image by Author ](https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;format=png&amp;auto=webp&amp;s=df44481e44b2dc48b3acd522b51327b9030e2335)\n\n## Performance Comparison\n\nAfter the code implementation, we need to compare the Numexpr implementation version with the previous `for each` implementation version to confirm that there has been a performance improvement.\n\nFirst, we implement a `numexpr_example` method. This method is based on the implementation of Numexpr:\n\n    def numexpr_example(rows: int) -&gt; np.ndarray:\n        orig_arr = generate_ndarray(rows)\n        the_result = numexpr_to_binary(orig_arr)\n        return the_result\n\nThen, we need to supplement a `for_loop_example` method. This method refers to the original code I need to rewrite and is used as a performance benchmark:\n\n    def for_loop_example(rows: int) -&gt; np.ndarray:\n        the_arr = generate_ndarray(rows)\n        for i in range(the_arr.shape[0]):\n            if the_arr[i][0] &lt; 0.5:\n                the_arr[i][0] = 0\n            else:\n                the_arr[i][0] = 1\n        return the_arr\n\nThen, I wrote a test method `time_method`. This method will generate data from 10 to 10 to the 9th power rows separately, call the corresponding method, and finally save the time required for different data amounts:\n\n    def time_method(method: Callable):\n        time_dict = dict()\n        for i in range(9):\n            begin = time.perf_counter()\n            rows = 10 ** i\n            method(rows)\n            end = time.perf_counter()\n            time_dict[i] = end - begin\n        return time_dict\n\nWe test the numexpr version and the `for_loop` version separately, and use `matplotlib` to draw the time required for different amounts of data:\n\n    t_m = time_method(for_loop_example)\n    t_m_2 = time_method(numexpr_example)\n    plt.plot(t_m.keys(), t_m.values(), c=\"red\", linestyle=\"solid\")\n    plt.plot(t_m_2.keys(), t_m_2.values(), c=\"green\", linestyle=\"dashed\")\n    plt.legend([\"for loop\", \"numexpr\"])\n    plt.xlabel(\"exponent\")\n    plt.ylabel(\"time\")\n    plt.show()\n\n[ The Numexpr version of the implementation has a huge performance improvement. Image by Author ](https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;format=png&amp;auto=webp&amp;s=d508bddb500f8065c75921c1905f14e414ccf932)\n\nIt can be seen that when the number of rows of data is greater than 10 to the 6th power, the Numexpr version of the implementation has a huge performance improvement.\n\n## Conclusion\n\nAfter explaining the basic usage of Numexpr in the previous article, this article uses a specific example in actual work to explain how to use Numexpr to rewrite existing code to obtain performance improvement.\n\nThis article mainly uses two features of Numexpr:\n\n1. Numexpr allows calculations to be performed in a vectorized manner.\n2. During the calculation of Numexpr, no new arrays will be generated, thereby significantly reducing memory usage.\n\nThank you for reading. If you have other solutions, please feel free to leave a message and discuss them with me.\n\nThis article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/).", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Optimize Multidimensional Numpy Array Operations with Numexpr", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"i5trs6h79xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 78, "x": 108, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c265d077c76d74f45c5cb125f9355561a7f5e03b"}, {"y": 156, "x": 216, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=543d74bdf357b325082cbe4cb60187df5cd783c0"}, {"y": 231, "x": 320, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed7c629584bc9b778eb0e7d678bb40a828d31a2f"}], "s": {"y": 430, "x": 595, "u": "https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;format=png&amp;auto=webp&amp;s=d508bddb500f8065c75921c1905f14e414ccf932"}, "id": "i5trs6h79xvb1"}, "aw26lp8q8xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82706ba0390facc4e733375417b45445f7c662a1"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22228f5dad76a4b48ca4e453156a2901bb54b88e"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e786249aa06a8e8f13a6a851ec5fe5987fe8eac"}, {"y": 490, "x": 640, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8af89b6cbada268340bfd9195964ebdbf23bd67"}], "s": {"y": 701, "x": 915, "u": "https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;format=png&amp;auto=webp&amp;s=df44481e44b2dc48b3acd522b51327b9030e2335"}, "id": "aw26lp8q8xvb1"}, "r24q1n674yvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aecb3555e64f8a91d18098d7a590df334e914973"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4887aa200906b3ad3351f555ed8ffb7cfebb0d3"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=633e830499b83a2b99e2d148d3b7d85b117e41af"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60662b1da6eee8d758e63f4038d3f808ff3fac73"}, {"y": 639, "x": 960, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55e3bee81e42e0c9aa309b45c5b16ba342f13c06"}, {"y": 719, "x": 1080, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b865d7cab7c9cf76cb713eef7c45433ff49d312"}], "s": {"y": 924, "x": 1387, "u": "https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;format=png&amp;auto=webp&amp;s=ab8950800797f55f538fdb1343df6d275bd07152"}, "id": "r24q1n674yvb1"}, "o1h5bwdn8xvb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 73, "x": 108, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4122e67f9a4134bc9caf07a853c9778f36b525c"}, {"y": 146, "x": 216, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d2086eaad1fc13fb23f5e326b05ba6d0b3a44da"}, {"y": 216, "x": 320, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb3d5913b3730778257fc3e0713c166604658b00"}], "s": {"y": 238, "x": 351, "u": "https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;format=png&amp;auto=webp&amp;s=d6977cc422be66a8c37980554c1478e76d2d326c"}, "id": "o1h5bwdn8xvb1"}}, "name": "t3_17egeux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Coding", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bVSQy67lM6KFb-o8nRKeJvIBP8TdwnEgDfNjXQbap2s.jpg", "edited": 1698064078.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1698053752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;A real-world case study of performance optimization in Numpy&lt;/h1&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/\"&gt;Data Leads Future&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r24q1n674yvb1.png?width=1387&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab8950800797f55f538fdb1343df6d275bd07152\"&gt; How to Optimize Multidimensional Numpy Array Operations with Numexpr. Photo Credit: Created by Author, Canva. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a relatively brief article. In it, I will use a real-world scenario as an example to explain how to use &lt;a href=\"https://numexpr.readthedocs.io/en/latest/user_guide.html?ref=dataleadsfuture.com#supported-functions\"&gt;Numexpr expressions&lt;/a&gt; in multidimensional Numpy arrays to achieve substantial performance improvements.&lt;/p&gt;\n\n&lt;p&gt;There aren&amp;#39;t many articles explaining how to use Numexpr in multidimensional Numpy arrays and how to use Numexpr expressions, so I hope this one will help you.&lt;/p&gt;\n\n&lt;h1&gt;Introduction&lt;/h1&gt;\n\n&lt;p&gt;Recently, while reviewing some of my old work, I stumbled upon this piece of code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def predict(X, w, b):\n    z = np.dot(X, w)\n    y_hat = sigmoid(z)\n    y_pred = np.zeros((y_hat.shape[0], 1))\n\n    for i in range(y_hat.shape[0]):\n        if y_hat[i, 0] &amp;lt; 0.5:\n            y_pred[i, 0] = 0\n        else:\n            y_pred[i, 0] = 1\n    return y_pred\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This code transforms prediction results from probabilities to classification results of 0 or 1 in the logistic regression model of machine learning.&lt;/p&gt;\n\n&lt;p&gt;But heavens, who would use a &lt;code&gt;for loop&lt;/code&gt; to iterate over Numpy ndarray?&lt;/p&gt;\n\n&lt;p&gt;You can foresee that when the data reaches a certain amount, it will not only occupy a lot of memory, but the performance will also be inferior.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s right, the person who wrote this code was me when I was younger.&lt;/p&gt;\n\n&lt;p&gt;With a sense of responsibility, I plan to rewrite this code with the Numexpr library today.&lt;/p&gt;\n\n&lt;p&gt;Along the way, I will show you how to use Numexpr and Numexpr&amp;#39;s &lt;code&gt;where&lt;/code&gt; expression in multidimensional Numpy arrays to achieve significant performance improvements.&lt;/p&gt;\n\n&lt;h2&gt;Code Implementation&lt;/h2&gt;\n\n&lt;p&gt;If you are not familiar with the basic usage of Numexpr, you can refer to this article:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/\"&gt;https://www.dataleadsfuture.com/exploring-numexpr-a-powerful-engine-behind-pandas/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article uses a real-world example to demonstrate the specific usage of Numexpr&amp;#39;s API and expressions in Numpy and Pandas.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;where(bool, number1, number2): number&lt;/em&gt; &lt;em&gt;- number1 if the bool condition is true, number2 otherwise.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;The above is the usage of the where expression in Numpy.&lt;/p&gt;\n\n&lt;p&gt;When dealing with matrix data, you may used to using Pandas &lt;code&gt;DataFrame&lt;/code&gt;. But since the &lt;code&gt;eval&lt;/code&gt; method of Pandas does not support the &lt;code&gt;where&lt;/code&gt; expression, you can only choose to use Numexpr in multidimensional Numpy ndarray.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t worry, I&amp;#39;ll explain it to you right away.&lt;/p&gt;\n\n&lt;p&gt;Before starting, we need to import the necessary packages and implement a &lt;code&gt;generate_ndarray&lt;/code&gt; method to generate a specific size ndarray for testing:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from typing import Callable\nimport time\n\nimport numpy as np\nimport numexpr as ne\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(seed=4000)\n\ndef generate_ndarray(rows: int) -&amp;gt; np.ndarray:\n    result_array = rng.random((rows, 1))\n    return result_array\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;First, we generate a matrix of 200 rows to see if it is the test data we want:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;In:  arr = generate_ndarray(200)\n     print(f&amp;quot;The dimension of this array: {arr.ndim}&amp;quot;)\n     print(f&amp;quot;The shape of this array: {arr.shape}&amp;quot;)\n\n\nOut: The dimension of this array: 2\n     The shape of this array: (200, 1)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be close to the actual situation of the logistic regression model, we generate an ndarray of the shape &lt;code&gt;(200, 1)&lt;/code&gt; Of course, you can also test other shapes of ndarray according to your needs.&lt;/p&gt;\n\n&lt;p&gt;Then, we start writing the specific use of Numexpr in the &lt;code&gt;numexpr_to_binary&lt;/code&gt; method:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;First, we use the index to separate the columns that need to be processed.&lt;/li&gt;\n&lt;li&gt;Then, use the where expression of Numexpr to process the values.&lt;/li&gt;\n&lt;li&gt;Finally, merge the processed columns with other columns to generate the required results.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Since the ndarray&amp;#39;s shape here is &lt;code&gt;(200, 1)&lt;/code&gt;, there is only one column, so I add a new dimension.&lt;/p&gt;\n\n&lt;p&gt;The code is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def numexpr_to_binary(np_array: np.ndarray) -&amp;gt; np.ndarray:\n    temp = np_array[:, 0]\n    temp = ne.evaluate(&amp;quot;where(temp&amp;lt;0.5, 0, 1)&amp;quot;)\n    return temp[:, np.newaxis]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We can test the result with an array of 10 rows to see if it is what I want:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;arr = generate_ndarray(10)\nresult = numexpr_to_binary(arr)\n\nmapping = np.column_stack((arr, result))\nmapping\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/o1h5bwdn8xvb1.png?width=351&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6977cc422be66a8c37980554c1478e76d2d326c\"&gt; I test an array of 10 rows and the result is what I want. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Look, the match is correct. Our task is completed.&lt;/p&gt;\n\n&lt;p&gt;The entire process can be demonstrated with the following figure:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/aw26lp8q8xvb1.png?width=915&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df44481e44b2dc48b3acd522b51327b9030e2335\"&gt; The entire process of how Numexpr transforms the multidimensional ndarray. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Performance Comparison&lt;/h2&gt;\n\n&lt;p&gt;After the code implementation, we need to compare the Numexpr implementation version with the previous &lt;code&gt;for each&lt;/code&gt; implementation version to confirm that there has been a performance improvement.&lt;/p&gt;\n\n&lt;p&gt;First, we implement a &lt;code&gt;numexpr_example&lt;/code&gt; method. This method is based on the implementation of Numexpr:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def numexpr_example(rows: int) -&amp;gt; np.ndarray:\n    orig_arr = generate_ndarray(rows)\n    the_result = numexpr_to_binary(orig_arr)\n    return the_result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, we need to supplement a &lt;code&gt;for_loop_example&lt;/code&gt; method. This method refers to the original code I need to rewrite and is used as a performance benchmark:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def for_loop_example(rows: int) -&amp;gt; np.ndarray:\n    the_arr = generate_ndarray(rows)\n    for i in range(the_arr.shape[0]):\n        if the_arr[i][0] &amp;lt; 0.5:\n            the_arr[i][0] = 0\n        else:\n            the_arr[i][0] = 1\n    return the_arr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, I wrote a test method &lt;code&gt;time_method&lt;/code&gt;. This method will generate data from 10 to 10 to the 9th power rows separately, call the corresponding method, and finally save the time required for different data amounts:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def time_method(method: Callable):\n    time_dict = dict()\n    for i in range(9):\n        begin = time.perf_counter()\n        rows = 10 ** i\n        method(rows)\n        end = time.perf_counter()\n        time_dict[i] = end - begin\n    return time_dict\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We test the numexpr version and the &lt;code&gt;for_loop&lt;/code&gt; version separately, and use &lt;code&gt;matplotlib&lt;/code&gt; to draw the time required for different amounts of data:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;t_m = time_method(for_loop_example)\nt_m_2 = time_method(numexpr_example)\nplt.plot(t_m.keys(), t_m.values(), c=&amp;quot;red&amp;quot;, linestyle=&amp;quot;solid&amp;quot;)\nplt.plot(t_m_2.keys(), t_m_2.values(), c=&amp;quot;green&amp;quot;, linestyle=&amp;quot;dashed&amp;quot;)\nplt.legend([&amp;quot;for loop&amp;quot;, &amp;quot;numexpr&amp;quot;])\nplt.xlabel(&amp;quot;exponent&amp;quot;)\nplt.ylabel(&amp;quot;time&amp;quot;)\nplt.show()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/i5trs6h79xvb1.png?width=595&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d508bddb500f8065c75921c1905f14e414ccf932\"&gt; The Numexpr version of the implementation has a huge performance improvement. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It can be seen that when the number of rows of data is greater than 10 to the 6th power, the Numexpr version of the implementation has a huge performance improvement.&lt;/p&gt;\n\n&lt;h2&gt;Conclusion&lt;/h2&gt;\n\n&lt;p&gt;After explaining the basic usage of Numexpr in the previous article, this article uses a specific example in actual work to explain how to use Numexpr to rewrite existing code to obtain performance improvement.&lt;/p&gt;\n\n&lt;p&gt;This article mainly uses two features of Numexpr:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Numexpr allows calculations to be performed in a vectorized manner.&lt;/li&gt;\n&lt;li&gt;During the calculation of Numexpr, no new arrays will be generated, thereby significantly reducing memory usage.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you for reading. If you have other solutions, please feel free to leave a message and discuss them with me.&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/how-to-optimize-multidimensional-numpy-array-operations-with-numexpr/\"&gt;Data Leads Future&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?auto=webp&amp;s=366216577b9b38ba41452286b2fcf4f2c68c9636", "width": 1387, "height": 924}, "resolutions": [{"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0120bd33f146716d9a9571e26276d1f7f86fb93", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=939e7da9c16e695695b1ad59dec1d4e6e8d683d6", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a4456f1d0041d99b553a235dbd71d4165addac3", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b2b091715dcd26bde14ed92aaf499316d099b9", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57b2c9250bad68102634450d43d77c65d882e11e", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/uiwdTJerETCwCpUIgHQSBF4mQJT7rPyLZF2NSGKldx4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2393289abf0d2b8b91de65ffb3765a8ecc5bb9e", "width": 1080, "height": 719}], "variants": {}, "id": "i2JEOSQeihCf3kK3-z6ZS0cLg5Reifzq6bHfNWMCWZA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4ab9c418-70eb-11ee-8a37-4a495429ae82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17egeux", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17egeux/how_to_optimize_multidimensional_numpy_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17egeux/how_to_optimize_multidimensional_numpy_array/", "subreddit_subscribers": 1096907, "created_utc": 1698053752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am studying a master\u2019s in data science and working as a \u201cjunior data scientist\u201d as my first ever job at a start up. Problem is, even though I have ended the more \u201cdata science\u201d part of my degree (ML, advanced math/statistics etc.), at work, I\u2019m working more on reporting (power bi, excel, sql). I have never built or implemented any model, except for the finals I passed like 5 months ago. Sadly, I don\u2019t remember anything from them. \n\nI\u2019m approaching 1 year in experience, and my goal is to apply for junior/entry level jobs preferably in the UK or Netherlands. However, I fear that even if I land an interview, there\u2019s no way I can make it past any of them because of the discrepency between my title and actual experience.", "author_fullname": "t2_m826ekr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Title not matching tasks, am I making it a big deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17eeucx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698046517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am studying a master\u2019s in data science and working as a \u201cjunior data scientist\u201d as my first ever job at a start up. Problem is, even though I have ended the more \u201cdata science\u201d part of my degree (ML, advanced math/statistics etc.), at work, I\u2019m working more on reporting (power bi, excel, sql). I have never built or implemented any model, except for the finals I passed like 5 months ago. Sadly, I don\u2019t remember anything from them. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m approaching 1 year in experience, and my goal is to apply for junior/entry level jobs preferably in the UK or Netherlands. However, I fear that even if I land an interview, there\u2019s no way I can make it past any of them because of the discrepency between my title and actual experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17eeucx", "is_robot_indexable": true, "report_reasons": null, "author": "Utterizi", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17eeucx/title_not_matching_tasks_am_i_making_it_a_big_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17eeucx/title_not_matching_tasks_am_i_making_it_a_big_deal/", "subreddit_subscribers": 1096907, "created_utc": 1698046517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Currently on a job search, and of course many DS roles are seeking prediction/forecasting skills. Can anyone recommend an overview of different predictive techniques? It could be an article, video, book, or even your own explanation.\n\nThere are so many things one could learn about regression, machine learning, etc. and I would find it useful to have some sort of organizing framework for various methods of prediction.\n\nThanks!", "author_fullname": "t2_9111ukf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good survey of predictive techniques?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17epzut", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698088665.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698082160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently on a job search, and of course many DS roles are seeking prediction/forecasting skills. Can anyone recommend an overview of different predictive techniques? It could be an article, video, book, or even your own explanation.&lt;/p&gt;\n\n&lt;p&gt;There are so many things one could learn about regression, machine learning, etc. and I would find it useful to have some sort of organizing framework for various methods of prediction.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17epzut", "is_robot_indexable": true, "report_reasons": null, "author": "ConsiderationRoyal87", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17epzut/good_survey_of_predictive_techniques/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17epzut/good_survey_of_predictive_techniques/", "subreddit_subscribers": 1096907, "created_utc": 1698082160.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}