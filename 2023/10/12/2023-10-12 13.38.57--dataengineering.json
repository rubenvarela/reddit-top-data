{"kind": "Listing", "data": {"after": "t3_175e7ao", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't mean the definition of \"Data Mart\", that we all know. Subset of data... aimed at specific business teams... pre-aggregated... etc...\n\nWhat I don't understand, and can't seem to find actual examples online that draw from realistic datasets, is what it is supposed to look like **in practice.**\n\nIs it just a bunch of views that expose only certain tables to certain users?\n\nAre there any aggregations and if yes, how does one decide which ones to build?\n\nIf we're building aggregations, thus losing in flexibility, does that mean that one has to aggregate for each necessary KPI the business users demand? Sounds like a massive PITA.\n\n--- \n\nI often wonder, isn't it much easier to expose a single common layer (idk like OBT, or even a star schema if your analysts are good) to whatever BI software(s) one's using, and let the analysts build whatever aggregation/metric comes to mind over there, rather than at the end of ELT? \n\nMany thanks in advance for your help.", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What the fuck *actually is* a Data Mart and why should I build one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175hslp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697040519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t mean the definition of &amp;quot;Data Mart&amp;quot;, that we all know. Subset of data... aimed at specific business teams... pre-aggregated... etc...&lt;/p&gt;\n\n&lt;p&gt;What I don&amp;#39;t understand, and can&amp;#39;t seem to find actual examples online that draw from realistic datasets, is what it is supposed to look like &lt;strong&gt;in practice.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is it just a bunch of views that expose only certain tables to certain users?&lt;/p&gt;\n\n&lt;p&gt;Are there any aggregations and if yes, how does one decide which ones to build?&lt;/p&gt;\n\n&lt;p&gt;If we&amp;#39;re building aggregations, thus losing in flexibility, does that mean that one has to aggregate for each necessary KPI the business users demand? Sounds like a massive PITA.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I often wonder, isn&amp;#39;t it much easier to expose a single common layer (idk like OBT, or even a star schema if your analysts are good) to whatever BI software(s) one&amp;#39;s using, and let the analysts build whatever aggregation/metric comes to mind over there, rather than at the end of ELT? &lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175hslp", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 89, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175hslp/what_the_fuck_actually_is_a_data_mart_and_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175hslp/what_the_fuck_actually_is_a_data_mart_and_why/", "subreddit_subscribers": 133424, "created_utc": 1697040519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please no roast. Just curious on your thoughts. I read some previous posts, and people mentioned that it's riskier to let juniors access your data, because they could be prone to mistakes. And that transitioning into DE is much more doable. But that still doesn't answer my question", "author_fullname": "t2_e58lby3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are there data engineering internships if there is such a lackluster amount of data engineer junior roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175p2bq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697058691.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please no roast. Just curious on your thoughts. I read some previous posts, and people mentioned that it&amp;#39;s riskier to let juniors access your data, because they could be prone to mistakes. And that transitioning into DE is much more doable. But that still doesn&amp;#39;t answer my question&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "175p2bq", "is_robot_indexable": true, "report_reasons": null, "author": "kid2002", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175p2bq/why_are_there_data_engineering_internships_if/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175p2bq/why_are_there_data_engineering_internships_if/", "subreddit_subscribers": 133424, "created_utc": 1697058691.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yo, a recent [post](https://www.reddit.com/r/dataengineering/comments/15mhunt/is_our_dbt_project_as_bad_as_i_think/) here made me want to start a thread on dbt model explosion. Repeating my comment there, 5 engineers and a thousand models doesn't shock anybody anymore. What are your numbers, or worst numbers you've seen so far? When did you realise it's too much? Was it mostly the cost that made you realise, organisation inside the team, any impact on business? How did you go about improving it?\n\nFound into two articles on Medium, funny when you read them in order: [1](https://medium.com/@imweijian/lessons-learned-after-1-year-with-dbt-a7f0ccf85b12)  [2](https://medium.com/@imweijian/lessons-learned-when-scaling-dbt-models-quickly-c3fcd1551663). Hilarious summary in the  search, too.\n\nhttps://preview.redd.it/0xkehgr5dmtb1.png?width=1284&amp;format=png&amp;auto=webp&amp;s=439fae017967f1a4a4566ea038483e4be62345b4\n\nDisclaimer: I work for a data governance company circling around this topic. Will probably write an article about this soon, but obv won't use anything from the comments without reaching out. Genuinely curious what's the industry standard + I feel nobody really talks about this??", "author_fullname": "t2_9wfvtd4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many models is too many models? DBT horror stories thread?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": false, "media_metadata": {"0xkehgr5dmtb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 25, "x": 108, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=af083f822308075eb004d3827be68c5feb3d2150"}, {"y": 51, "x": 216, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daab666af58641e11916869fa490234d22e35367"}, {"y": 75, "x": 320, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a031c40ad1340ab93595ea249abc12007b4b9a05"}, {"y": 151, "x": 640, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=316657186c1fbd98a143246c04f0a45c46fe5518"}, {"y": 227, "x": 960, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac34c02d88cc245d074fa0cbdcab1793df866d25"}, {"y": 255, "x": 1080, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31c2d8176d82af828cd2021efd7690bf5a48ebb1"}], "s": {"y": 304, "x": 1284, "u": "https://preview.redd.it/0xkehgr5dmtb1.png?width=1284&amp;format=png&amp;auto=webp&amp;s=439fae017967f1a4a4566ea038483e4be62345b4"}, "id": "0xkehgr5dmtb1"}}, "name": "t3_175me07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ck9bMyO_eFCmURFU17rIsZERtaEhO3ueRYoqVeTH30w.jpg", "edited": 1697114592.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1697052014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yo, a recent &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/15mhunt/is_our_dbt_project_as_bad_as_i_think/\"&gt;post&lt;/a&gt; here made me want to start a thread on dbt model explosion. Repeating my comment there, 5 engineers and a thousand models doesn&amp;#39;t shock anybody anymore. What are your numbers, or worst numbers you&amp;#39;ve seen so far? When did you realise it&amp;#39;s too much? Was it mostly the cost that made you realise, organisation inside the team, any impact on business? How did you go about improving it?&lt;/p&gt;\n\n&lt;p&gt;Found into two articles on Medium, funny when you read them in order: &lt;a href=\"https://medium.com/@imweijian/lessons-learned-after-1-year-with-dbt-a7f0ccf85b12\"&gt;1&lt;/a&gt;  &lt;a href=\"https://medium.com/@imweijian/lessons-learned-when-scaling-dbt-models-quickly-c3fcd1551663\"&gt;2&lt;/a&gt;. Hilarious summary in the  search, too.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0xkehgr5dmtb1.png?width=1284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=439fae017967f1a4a4566ea038483e4be62345b4\"&gt;https://preview.redd.it/0xkehgr5dmtb1.png?width=1284&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=439fae017967f1a4a4566ea038483e4be62345b4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I work for a data governance company circling around this topic. Will probably write an article about this soon, but obv won&amp;#39;t use anything from the comments without reaching out. Genuinely curious what&amp;#39;s the industry standard + I feel nobody really talks about this??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nDEm8n2_3UDnhnp_h-2UxtpsdFmyKtCw9o41Bjj4q0E.jpg?auto=webp&amp;s=069b2930a996fc502f9c1a9e77cff14c02471d09", "width": 498, "height": 325}, "resolutions": [{"url": "https://external-preview.redd.it/nDEm8n2_3UDnhnp_h-2UxtpsdFmyKtCw9o41Bjj4q0E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d092de33c1ab0397b27185b86e91d8c8200f6b6", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/nDEm8n2_3UDnhnp_h-2UxtpsdFmyKtCw9o41Bjj4q0E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb711131867a14479b516a01c0854eb1330244ff", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/nDEm8n2_3UDnhnp_h-2UxtpsdFmyKtCw9o41Bjj4q0E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2954149f72d3ffbe86df2842454988cca1a675b", "width": 320, "height": 208}], "variants": {}, "id": "J17xFutWhTnnV7t2i9OmckmAz-6Y6l-FaLCNibXJdtU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175me07", "is_robot_indexable": true, "report_reasons": null, "author": "prsrboi", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175me07/how_many_models_is_too_many_models_dbt_horror/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175me07/how_many_models_is_too_many_models_dbt_horror/", "subreddit_subscribers": 133424, "created_utc": 1697052014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was surprised by recent computations when, after a bunch of optimization, I was able to get costs down to about $0.10 per terabyte processed (this is cheaper than I expected).  I ran into this result oddly consistently.  It turns out this is roughly the cost of S3 bandwidth to EC2 machines if you do everything correctly.\n\nThis blogpost goes through that back-of-the-envelope calculation:\n\n[https://medium.com/coiled-hq/ten-cents-per-terabyte-91ff24363612](https://medium.com/coiled-hq/ten-cents-per-terabyte-91ff24363612)\n\nInterestingly, it's 1000x cheaper than egress charges.  It really makes it clear how some parts of the cloud are really really cheap, while other parts are really really expensive.  Cloud pricing confuses me \ud83d\ude42", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ten Cents Per Terabyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175f5h5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697034028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was surprised by recent computations when, after a bunch of optimization, I was able to get costs down to about $0.10 per terabyte processed (this is cheaper than I expected).  I ran into this result oddly consistently.  It turns out this is roughly the cost of S3 bandwidth to EC2 machines if you do everything correctly.&lt;/p&gt;\n\n&lt;p&gt;This blogpost goes through that back-of-the-envelope calculation:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/coiled-hq/ten-cents-per-terabyte-91ff24363612\"&gt;https://medium.com/coiled-hq/ten-cents-per-terabyte-91ff24363612&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Interestingly, it&amp;#39;s 1000x cheaper than egress charges.  It really makes it clear how some parts of the cloud are really really cheap, while other parts are really really expensive.  Cloud pricing confuses me \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?auto=webp&amp;s=23e98c5db27ef0bb50185489af551b0e6f0cf919", "width": 1200, "height": 794}, "resolutions": [{"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14f58fb5b58d3519bc37919fceda865fb79cdd65", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6c2e545dd03edb9063521f5c17ed98928c8b0a89", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc27adadd598b90f3a6b9ed7870cc021ec169d0a", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c0e79474df83979b9df193de4f4ae6432d921e0", "width": 640, "height": 423}, {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c09fc46c2e892ddb45fed5b5ef6c77f66df0fbce", "width": 960, "height": 635}, {"url": "https://external-preview.redd.it/1TrbF8IjE116XWSO8LbYmit0IxGt6MFPl9Pp1ipJF4w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cec916b1399cba6db0af64843ed7779f01df94f7", "width": 1080, "height": 714}], "variants": {}, "id": "wvundtH4fVcA5Dbp3mtdFbWYKmV96Yddn2MWEfaXip8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "175f5h5", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175f5h5/ten_cents_per_terabyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175f5h5/ten_cents_per_terabyte/", "subreddit_subscribers": 133424, "created_utc": 1697034028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context:I work at a big-ish company that is exclusively on-prem, and am working as a Lead Data Engineer of sorts. We're not all that advanced in maturity: we just recently deployed airflow on openshift(before that we used it on docker, and before that... well it was really bad), and we use airflow+ apache hop on kubernetes as our ETL toolset, all of which I am the one who deployed+learned and then teached how to use.\n\nNow we want to build a data-lake(which we kinda reallllllllyy need.) , and I figured out that something like CEPH+ apache Hive+ Trino/PrestoSQL ought to be enough for now, while I think that eventually we'll go after delta-lake probably.\n\nIn terms of BI Team, my current team is made of not very competent people who use only PowerBI.\n\nIn terms of AI team.... we kinda have one but they're less programmers and more like an statistics team.\n\n&amp;#x200B;\n\nThe question I want to ask is:What would I use spark for? is it really something that I need to have considering I kinda don't have an AI team? I know it can be used as an ETLish tool, but I don't really think I need it for that.\n\n&amp;#x200B;\n\nEdit:  \nI work as lead data engineer in this company mostly because where I live data engineers are rarer than diamonds. On a good company I'd probably be at Entry/Intermediate level.", "author_fullname": "t2_5vq0n4ol", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do I really need spark ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175it77", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697043311.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697042999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context:I work at a big-ish company that is exclusively on-prem, and am working as a Lead Data Engineer of sorts. We&amp;#39;re not all that advanced in maturity: we just recently deployed airflow on openshift(before that we used it on docker, and before that... well it was really bad), and we use airflow+ apache hop on kubernetes as our ETL toolset, all of which I am the one who deployed+learned and then teached how to use.&lt;/p&gt;\n\n&lt;p&gt;Now we want to build a data-lake(which we kinda reallllllllyy need.) , and I figured out that something like CEPH+ apache Hive+ Trino/PrestoSQL ought to be enough for now, while I think that eventually we&amp;#39;ll go after delta-lake probably.&lt;/p&gt;\n\n&lt;p&gt;In terms of BI Team, my current team is made of not very competent people who use only PowerBI.&lt;/p&gt;\n\n&lt;p&gt;In terms of AI team.... we kinda have one but they&amp;#39;re less programmers and more like an statistics team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The question I want to ask is:What would I use spark for? is it really something that I need to have considering I kinda don&amp;#39;t have an AI team? I know it can be used as an ETLish tool, but I don&amp;#39;t really think I need it for that.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;br/&gt;\nI work as lead data engineer in this company mostly because where I live data engineers are rarer than diamonds. On a good company I&amp;#39;d probably be at Entry/Intermediate level.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175it77", "is_robot_indexable": true, "report_reasons": null, "author": "ImmortalLotusFlower", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175it77/do_i_really_need_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175it77/do_i_really_need_spark/", "subreddit_subscribers": 133424, "created_utc": 1697042999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There seems to be lot of dislike in software engineering in general for consultants specially they talk about big things and complicated processes with very less actual work etc. What is your experience in general working with consultants in data engineering world?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why you don't like about data engineering consultants in your company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175zng5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697090011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There seems to be lot of dislike in software engineering in general for consultants specially they talk about big things and complicated processes with very less actual work etc. What is your experience in general working with consultants in data engineering world?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175zng5", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175zng5/why_you_dont_like_about_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175zng5/why_you_dont_like_about_data_engineering/", "subreddit_subscribers": 133424, "created_utc": 1697090011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've noticed that many remote job opportunities are available for backend and frontend positions within small and medium-sized companies. Data engineering roles, on the other hand, are typically associated with larger organizations. \nWhat are the possibilities of finding remote data engineering jobs in this context?", "author_fullname": "t2_6oud3gpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current possibilities Data engineering remote jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175ksaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697047904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve noticed that many remote job opportunities are available for backend and frontend positions within small and medium-sized companies. Data engineering roles, on the other hand, are typically associated with larger organizations. \nWhat are the possibilities of finding remote data engineering jobs in this context?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "175ksaj", "is_robot_indexable": true, "report_reasons": null, "author": "Consistent-Artist735", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175ksaj/current_possibilities_data_engineering_remote_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175ksaj/current_possibilities_data_engineering_remote_jobs/", "subreddit_subscribers": 133424, "created_utc": 1697047904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an academic scientist. I am trying to build a GUI tool to allow other scientists to easily get access to scientific data not stored on databases and enable them to perform meta analysis (a process of aggregating results from many studies and finding the \u201cconsensus\u201d result). Traditionally in my field, research papers are posted on a central repository (Pubmed Central), and they often have associated supplementary data files that have raw or semi-processed experimental results. Usually these are xlsx or csvs.\n\nThe tool I\u2019m building is a PyQt based GUI app that aims to walk users through the process of searching for papers, previewing data files, extracting more or less regularised tabular data from those datasets, choosing tables/features they want, and dumping it all into a SQLite db. I have opted for a desktop GUI because I don\u2019t know if I\u2019ll have the resources for web hosting yet, and PyQt seemed a good choice for integrating with some of the more data manipulation intensive parts of the code.\n\nI have built a scraper to get the research papers + data, some (IMO) cool code to extract discrete tables from the weird and wacky world of spreadsheets, to filter those tables using an intuitive search syntax (rather than SQL), and to quickly select/unselect columns of data considered relevant/irrelevant. At the end of it, the data will of course still need some cleaning, but the idea is this whole process is a lot easier than manually crawling through the scientific literature and having a million excel files open to look through (or, frankly, in your IDE if you\u2019re coding; the assumption is you still need to look at the data visually at some point!).\n\nThe tool is very janky at the moment and I think whether it *really* provides a value add depends if I can get the UI/UX right, but I guess in the ideal case, I\u2019m curious if it counts as DE, and what professional DEs would think of a tool like this. You know, maybe there\u2019s off the shelf stuff and I\u2019m wasting my time (but I think not - I did look!).", "author_fullname": "t2_cfrbonu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175g154", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697036309.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an academic scientist. I am trying to build a GUI tool to allow other scientists to easily get access to scientific data not stored on databases and enable them to perform meta analysis (a process of aggregating results from many studies and finding the \u201cconsensus\u201d result). Traditionally in my field, research papers are posted on a central repository (Pubmed Central), and they often have associated supplementary data files that have raw or semi-processed experimental results. Usually these are xlsx or csvs.&lt;/p&gt;\n\n&lt;p&gt;The tool I\u2019m building is a PyQt based GUI app that aims to walk users through the process of searching for papers, previewing data files, extracting more or less regularised tabular data from those datasets, choosing tables/features they want, and dumping it all into a SQLite db. I have opted for a desktop GUI because I don\u2019t know if I\u2019ll have the resources for web hosting yet, and PyQt seemed a good choice for integrating with some of the more data manipulation intensive parts of the code.&lt;/p&gt;\n\n&lt;p&gt;I have built a scraper to get the research papers + data, some (IMO) cool code to extract discrete tables from the weird and wacky world of spreadsheets, to filter those tables using an intuitive search syntax (rather than SQL), and to quickly select/unselect columns of data considered relevant/irrelevant. At the end of it, the data will of course still need some cleaning, but the idea is this whole process is a lot easier than manually crawling through the scientific literature and having a million excel files open to look through (or, frankly, in your IDE if you\u2019re coding; the assumption is you still need to look at the data visually at some point!).&lt;/p&gt;\n\n&lt;p&gt;The tool is very janky at the moment and I think whether it &lt;em&gt;really&lt;/em&gt; provides a value add depends if I can get the UI/UX right, but I guess in the ideal case, I\u2019m curious if it counts as DE, and what professional DEs would think of a tool like this. You know, maybe there\u2019s off the shelf stuff and I\u2019m wasting my time (but I think not - I did look!).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175g154", "is_robot_indexable": true, "report_reasons": null, "author": "Honest_Shopping_2053", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175g154/is_this_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175g154/is_this_data_engineering/", "subreddit_subscribers": 133424, "created_utc": 1697036309.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks, I've been working on Kenobi ([github link](https://github.com/DrDroidLab/kenobi/)) since a while now -- we made the repo open source finally (when we felt the project can have out of the box value \ud83d\ude05), so sharing about it.\n\n**What is it:** A data platform for event analytics and correlation.\n\n**Where could it be more useful:** Kenobi is a solution for companies that want to monitor operational metrics &amp; funnels with a capability to get alerts in realtime.\n\n**How is it different from:**\n\n* **Postgres/MySQL + Metabase/Superset:** Kenobi is for companies who want to move out of their OLTP for analytics due to scaling challenges.\n* **ELK:** Kenobi is optimised for aggregations on complex event joins (examples in github link); has a one-click deployment; it is easier to learn/use with no-code interface; also it has a buffer layer for data filtering and transformation on the UI.\n* **DWH:** Kenobi is designed for short-term, low-latency use-cases.\n\n**Here are some of the capabilities:**\n\n* Ingest events through multiple sources, including logs &amp; event streams (Kafka / Kinesis)\n* Create complex join queries on multiple event-types\n* Aggregate data with slicing and dicing basis the entire event payload\n* Single click deployment with horizontal scaling (tested upto 130M events/hour)\n* Near real-time alerting infrastructure baked into the platform\n\nIf you have any questions, feedback or queries about the project, please comment -- I'm happy to answer! \n\nIf you've built something like this internally, would love to hear how you did it.", "author_fullname": "t2_15bgjr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kenobi: Open Source analytics tool for funnel analysis, optimised for near real-time performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175t57y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697069409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, I&amp;#39;ve been working on Kenobi (&lt;a href=\"https://github.com/DrDroidLab/kenobi/\"&gt;github link&lt;/a&gt;) since a while now -- we made the repo open source finally (when we felt the project can have out of the box value \ud83d\ude05), so sharing about it.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What is it:&lt;/strong&gt; A data platform for event analytics and correlation.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Where could it be more useful:&lt;/strong&gt; Kenobi is a solution for companies that want to monitor operational metrics &amp;amp; funnels with a capability to get alerts in realtime.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How is it different from:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Postgres/MySQL + Metabase/Superset:&lt;/strong&gt; Kenobi is for companies who want to move out of their OLTP for analytics due to scaling challenges.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ELK:&lt;/strong&gt; Kenobi is optimised for aggregations on complex event joins (examples in github link); has a one-click deployment; it is easier to learn/use with no-code interface; also it has a buffer layer for data filtering and transformation on the UI.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DWH:&lt;/strong&gt; Kenobi is designed for short-term, low-latency use-cases.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Here are some of the capabilities:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ingest events through multiple sources, including logs &amp;amp; event streams (Kafka / Kinesis)&lt;/li&gt;\n&lt;li&gt;Create complex join queries on multiple event-types&lt;/li&gt;\n&lt;li&gt;Aggregate data with slicing and dicing basis the entire event payload&lt;/li&gt;\n&lt;li&gt;Single click deployment with horizontal scaling (tested upto 130M events/hour)&lt;/li&gt;\n&lt;li&gt;Near real-time alerting infrastructure baked into the platform&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you have any questions, feedback or queries about the project, please comment -- I&amp;#39;m happy to answer! &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve built something like this internally, would love to hear how you did it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?auto=webp&amp;s=ea2a9756f52902c9bc0a75eea0189097e340b1bf", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59b13f88947c7af7ef42d6e1400a448a95d19762", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=98e023efe8b1144d6e0790166ff23c4dca49a874", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c199595f8850ce41ce6a872200d19e7a8dd388c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d741f65f3d95fc7c237e23b01c63b65cd096503", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f36df2c7c31f0cc2569b61160917cb469f3863", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/G_Nmio5Wu0zp02x-TrS0VZtc3sbPojamX2Ld0cNsxGk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f67c922080b59c606fcdfd16a4c11d24ed30d924", "width": 1080, "height": 540}], "variants": {}, "id": "0ZCNUyfMsKpp5Vwml3FxO427XhPYYQEM4nvcBYQMIMY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "175t57y", "is_robot_indexable": true, "report_reasons": null, "author": "siddharthnibjiya", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175t57y/kenobi_open_source_analytics_tool_for_funnel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175t57y/kenobi_open_source_analytics_tool_for_funnel/", "subreddit_subscribers": 133424, "created_utc": 1697069409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started at my current position about a year ago while the team was in very early stages of doing a large scale cloud migration with the idea that I'd be able to play a somewhat significant role in the formation and development of the new architecture. After a year, though, we've managed to accrue a significant amount of tech debt, and many of our data consumption and usage patterns are just head-scratchers to me. Along with that, I feel as if I've been pigeon-holed into doing more manual and tedious tasks, and a lot of the feedback I've brought up about the current state of our architecture has been either thrown by the wayside, challenged with poor arguments (which makes me wonder if some of the DE's even know what they're talking about), or deemed a non-problem. For reference, I've been working as a DE for 3+ years which I know isn't very senior, but having gone through another full scale migration with another company and considering that my coworkers and boss don't have that much more experience, I would have hoped to at least feel like my concerns weren't being voiced into a void. I don't feel comfortable divulging more details in case any of my coworkers peruse the sub, but if interested, feel free to PM me.\n\nIt's really been a test of humility for me, so I'm curious if there are other people who either feel like they're in a similar scenario or who have previously been in this situation, especially with the prevalence of ever-changing data requirements, architectures, and processes. For many other reasons, jumping ship isn't what I'm considering at the moment, so if anyone has advice on how to manage team dynamics and voicing ideas/concerns, I'd greatly appreciate it. Even if you don't have advice, I'd love to hear about your experience.\n\nTeam dynamics and managing personalities and interpersonal skills seem to get neglected a lot of the time since we work in such a high demand field, but they're often just as important as knowing how to code in whatever language, especially because of the prevalence of imposter syndrome and inflated egos.", "author_fullname": "t2_ahnkvspqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Navigating team dynamics w/ new (and flawed) architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175ivss", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697043165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started at my current position about a year ago while the team was in very early stages of doing a large scale cloud migration with the idea that I&amp;#39;d be able to play a somewhat significant role in the formation and development of the new architecture. After a year, though, we&amp;#39;ve managed to accrue a significant amount of tech debt, and many of our data consumption and usage patterns are just head-scratchers to me. Along with that, I feel as if I&amp;#39;ve been pigeon-holed into doing more manual and tedious tasks, and a lot of the feedback I&amp;#39;ve brought up about the current state of our architecture has been either thrown by the wayside, challenged with poor arguments (which makes me wonder if some of the DE&amp;#39;s even know what they&amp;#39;re talking about), or deemed a non-problem. For reference, I&amp;#39;ve been working as a DE for 3+ years which I know isn&amp;#39;t very senior, but having gone through another full scale migration with another company and considering that my coworkers and boss don&amp;#39;t have that much more experience, I would have hoped to at least feel like my concerns weren&amp;#39;t being voiced into a void. I don&amp;#39;t feel comfortable divulging more details in case any of my coworkers peruse the sub, but if interested, feel free to PM me.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s really been a test of humility for me, so I&amp;#39;m curious if there are other people who either feel like they&amp;#39;re in a similar scenario or who have previously been in this situation, especially with the prevalence of ever-changing data requirements, architectures, and processes. For many other reasons, jumping ship isn&amp;#39;t what I&amp;#39;m considering at the moment, so if anyone has advice on how to manage team dynamics and voicing ideas/concerns, I&amp;#39;d greatly appreciate it. Even if you don&amp;#39;t have advice, I&amp;#39;d love to hear about your experience.&lt;/p&gt;\n\n&lt;p&gt;Team dynamics and managing personalities and interpersonal skills seem to get neglected a lot of the time since we work in such a high demand field, but they&amp;#39;re often just as important as knowing how to code in whatever language, especially because of the prevalence of imposter syndrome and inflated egos.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175ivss", "is_robot_indexable": true, "report_reasons": null, "author": "null_user_617", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175ivss/navigating_team_dynamics_w_new_and_flawed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175ivss/navigating_team_dynamics_w_new_and_flawed/", "subreddit_subscribers": 133424, "created_utc": 1697043165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \nI'm trying to figure out what are some of the best practices for building data documentation. I came across tools like DataHub and DBT documentation. Has anyone tried using LLM's in building a data documentation? like for example \"how to find active customers\" and it provides the relevant tables that are to be used. \n\nThanks,", "author_fullname": "t2_gl7de", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for data documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175zjdw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697089567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI&amp;#39;m trying to figure out what are some of the best practices for building data documentation. I came across tools like DataHub and DBT documentation. Has anyone tried using LLM&amp;#39;s in building a data documentation? like for example &amp;quot;how to find active customers&amp;quot; and it provides the relevant tables that are to be used. &lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175zjdw", "is_robot_indexable": true, "report_reasons": null, "author": "dna_o_O", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175zjdw/best_practices_for_data_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175zjdw/best_practices_for_data_documentation/", "subreddit_subscribers": 133424, "created_utc": 1697089567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5paor1zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accelerating Spark: Databricks Photon Runtime", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_175uk3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/J3uny-MxlfRS-MJI1XvXm3vqnlH1KqaQAc7or3o4JuA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697073535.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@philipdakin/accelerating-spark-databricks-photon-runtime-9a7a53824d1b", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iiNdop72CHvNr4BX_bhsAYLweJLIjntWHo5xd_fOfUo.jpg?auto=webp&amp;s=a0e4c7bd816fcbb020cb8ca369a6210bfb613e2b", "width": 482, "height": 544}, "resolutions": [{"url": "https://external-preview.redd.it/iiNdop72CHvNr4BX_bhsAYLweJLIjntWHo5xd_fOfUo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4e6951bd885c65b233618c7569774ba568e929f", "width": 108, "height": 121}, {"url": "https://external-preview.redd.it/iiNdop72CHvNr4BX_bhsAYLweJLIjntWHo5xd_fOfUo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a281a7b9ff4b59423fea9d6c0a028fece4aab802", "width": 216, "height": 243}, {"url": "https://external-preview.redd.it/iiNdop72CHvNr4BX_bhsAYLweJLIjntWHo5xd_fOfUo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2c932de2dbdf2a3a0724a3183a9a34e69011e4b", "width": 320, "height": 361}], "variants": {}, "id": "-0-L7CTEvHKfAYbxp9aB6JixOk_WCEouHBv-j6sJ2pc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "175uk3s", "is_robot_indexable": true, "report_reasons": null, "author": "phildakin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175uk3s/accelerating_spark_databricks_photon_runtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@philipdakin/accelerating-spark-databricks-photon-runtime-9a7a53824d1b", "subreddit_subscribers": 133424, "created_utc": 1697073535.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say we have a 100 source tables from an OLTP system.\n\nWhen we bring the data into Data Warehouse:\n1. Should we have a SCD2 version of the source / raw tables as a best practice? I can see how knowing what changed when can be useful.\n\n\n2. If we now want to use 30 of the source tables to build a downstream table C - which we do want to track changes on. \n\nShould we join all the raw tables (if raw is ace then read scd active row flag =1) , apply transformations,then build a wide and large table C_stg. Finally a SCD2 based on what changed with respect to current table C (active row flag=1).\n\nIs this the right way for downstream SCD tables, this seems like can get very resource intensive as source tables get bigger and bigger.", "author_fullname": "t2_tov4xq2h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCD2 - what is the right way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175rkdy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697065085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a 100 source tables from an OLTP system.&lt;/p&gt;\n\n&lt;p&gt;When we bring the data into Data Warehouse:\n1. Should we have a SCD2 version of the source / raw tables as a best practice? I can see how knowing what changed when can be useful.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If we now want to use 30 of the source tables to build a downstream table C - which we do want to track changes on. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Should we join all the raw tables (if raw is ace then read scd active row flag =1) , apply transformations,then build a wide and large table C_stg. Finally a SCD2 based on what changed with respect to current table C (active row flag=1).&lt;/p&gt;\n\n&lt;p&gt;Is this the right way for downstream SCD tables, this seems like can get very resource intensive as source tables get bigger and bigger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175rkdy", "is_robot_indexable": true, "report_reasons": null, "author": "nanksk", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175rkdy/scd2_what_is_the_right_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175rkdy/scd2_what_is_the_right_way/", "subreddit_subscribers": 133424, "created_utc": 1697065085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you deal with using data from Mongo?  IME the data is always nested and needs to be flattened to use.  So do you flatten it in python, or in sql? or do you use something that skips the flat data requirements?  \n\n\nWe are currently playing with the idea of automatic unnesting and semantic modelling on top to enable analysing the data without writing code. This technical article describes it: [https://dlthub.com/docs/blog/MongoDB-dlt-Holistics](https://dlthub.com/docs/blog/MongoDB-dlt-Holistics)\n\n  \nWhat are your approaches to this problem?", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy way to model data from Mongo?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17659m7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697112029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you deal with using data from Mongo?  IME the data is always nested and needs to be flattened to use.  So do you flatten it in python, or in sql? or do you use something that skips the flat data requirements?  &lt;/p&gt;\n\n&lt;p&gt;We are currently playing with the idea of automatic unnesting and semantic modelling on top to enable analysing the data without writing code. This technical article describes it: &lt;a href=\"https://dlthub.com/docs/blog/MongoDB-dlt-Holistics\"&gt;https://dlthub.com/docs/blog/MongoDB-dlt-Holistics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What are your approaches to this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?auto=webp&amp;s=f3270f1d29472d455066155750c702cd23b02a29", "width": 2693, "height": 1485}, "resolutions": [{"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=124e5689c7e17fffd0ec326f9c69e86cf0119568", "width": 108, "height": 59}, {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c115a2cf76cd8cb7008c1f7f04558f706001bfb", "width": 216, "height": 119}, {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30864ec2f18c41e3d49bc38aeda7693d72f5b20b", "width": 320, "height": 176}, {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccdc7ed9dc8f87d67b86e1768bde51460e98b3be", "width": 640, "height": 352}, {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c88cbcb9f20ad3223c021f61e0d0203cfc67337b", "width": 960, "height": 529}, {"url": "https://external-preview.redd.it/jGEN9OFm6IxpaJTDe_7kViVHyYAO4wkq3xYbeKVqSHc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ef8cda761549d8cc64b02b1af700157b9b4dce4", "width": 1080, "height": 595}], "variants": {}, "id": "dY1Tyj6CFUjtPbkf11Mdzugrhaa5hN60x9WCS8uUcYQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17659m7", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17659m7/easy_way_to_model_data_from_mongo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17659m7/easy_way_to_model_data_from_mongo/", "subreddit_subscribers": 133424, "created_utc": 1697112029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\n&amp;#x200B;\n\nI'm looking for some guidance on how to extract data from a WordPress MySQL database that's hosted on a shared server. I use Python for data extraction and pipeline but I am not able to connect to the database.\n\nSuggest to me if there is another way to do the same task .\n\n&amp;#x200B;\n\nimport mysql.connector\n\ndb = mysql.connector.connect(\n\nhost='',\n\nport=,\n\nuser='',\n\npassword='',\n\ndatabase=''\n\n)\n\nif db.is\\_connected():\n\nprint('Connected to the MySQL database successfully!')\n\nelse:\n\nprint('Failed to connect to the MySQL database!')\n\nits not connecting to database\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHow to Extract Data from a WordPress MySQL Shared Server Database Using Python?", "author_fullname": "t2_a4mv1fuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Extract Data from a WordPress MySQL Shared Server Database Using Python and store in BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1760tby", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697094567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for some guidance on how to extract data from a WordPress MySQL database that&amp;#39;s hosted on a shared server. I use Python for data extraction and pipeline but I am not able to connect to the database.&lt;/p&gt;\n\n&lt;p&gt;Suggest to me if there is another way to do the same task .&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;import mysql.connector&lt;/p&gt;\n\n&lt;p&gt;db = mysql.connector.connect(&lt;/p&gt;\n\n&lt;p&gt;host=&amp;#39;&amp;#39;,&lt;/p&gt;\n\n&lt;p&gt;port=,&lt;/p&gt;\n\n&lt;p&gt;user=&amp;#39;&amp;#39;,&lt;/p&gt;\n\n&lt;p&gt;password=&amp;#39;&amp;#39;,&lt;/p&gt;\n\n&lt;p&gt;database=&amp;#39;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;if db.is_connected():&lt;/p&gt;\n\n&lt;p&gt;print(&amp;#39;Connected to the MySQL database successfully!&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;else:&lt;/p&gt;\n\n&lt;p&gt;print(&amp;#39;Failed to connect to the MySQL database!&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;its not connecting to database&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How to Extract Data from a WordPress MySQL Shared Server Database Using Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1760tby", "is_robot_indexable": true, "report_reasons": null, "author": "Galaxy_Pegasus_777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1760tby/how_to_extract_data_from_a_wordpress_mysql_shared/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1760tby/how_to_extract_data_from_a_wordpress_mysql_shared/", "subreddit_subscribers": 133424, "created_utc": 1697094567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am trying to get an understanding from technical capabilities standpoint on what are some of the strengths and weaknesses of Azure Data Factory when compared to Informatica\u2019s Data Integration service on IDMC. Does ADF support both ETL and ELT like Informatica\u2019s Cloud Data Integration?", "author_fullname": "t2_4mdsqonb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strengths and weaknesses of ADF compared to Cloud Data Integration on Informatica Data Management Cloud (IDMC)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175svid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697068650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am trying to get an understanding from technical capabilities standpoint on what are some of the strengths and weaknesses of Azure Data Factory when compared to Informatica\u2019s Data Integration service on IDMC. Does ADF support both ETL and ELT like Informatica\u2019s Cloud Data Integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "175svid", "is_robot_indexable": true, "report_reasons": null, "author": "vrakshith28", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175svid/strengths_and_weaknesses_of_adf_compared_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175svid/strengths_and_weaknesses_of_adf_compared_to_cloud/", "subreddit_subscribers": 133424, "created_utc": 1697068650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I spent the whole evening preparing a presentation for an Interview that I embedded in a pbix file. With data modeling, strategy and data visualizations. Not the first time doing of course these kind of things. But you guys put it in your portfolio? It is legal? It has no sensitive data, all the data is sample data I generated to create metrics in dax. It has only the name of the company.", "author_fullname": "t2_bsuu4apm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you put the assessment done for an interview in you portfolios?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175k0jm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697045971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spent the whole evening preparing a presentation for an Interview that I embedded in a pbix file. With data modeling, strategy and data visualizations. Not the first time doing of course these kind of things. But you guys put it in your portfolio? It is legal? It has no sensitive data, all the data is sample data I generated to create metrics in dax. It has only the name of the company.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "175k0jm", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonMentone30", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175k0jm/do_you_put_the_assessment_done_for_an_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175k0jm/do_you_put_the_assessment_done_for_an_interview/", "subreddit_subscribers": 133424, "created_utc": 1697045971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks, looking for advice on best practices on what I think is a pretty common setup, yet I couldn't find any good examples on this. \n\nI have multiple APIs as sources, each with their own script. I want to insert the data into a MySQL database. The aim is to have these tasks running separately using the k8sPodOperator. I'm using poetry as my dependency manager. GitHub Actions for CICD. \n\nMy questions are twofold:\n\n1. Is it better practice to have one large image that contains all the code for every API, and pass different commands to the image in different k8sPodOperators? Or would it be better to have multiple images, one per API pipeline? \n\n2. How should I organize my code, considering we want to keep a monorepo structure, where multiple projects within that repo will be using Airflow?\n\nCurrently the monorepo looks like:\n\n    monorepo/\n\n    --my-project/\n\n    ----dags/\n\n    ------dag_1.py\n\n    ------dag_2.py\n\n    ----api/\n\n    ------base_api_class.py\n\n    ------api1/\n\n    --------api1.py\n\n    --------api1_execute.py\n\n    ------api2/\n\n    --------api2.py\n\n    --------api2_execute.py\n\n    ----migrations/\n\n    ----pyproject.toml\n\n    ----Dockerfile (Open to change based on answer to q1)\n\n    --other_project/\n\n\nI think I'll be moving the dags/ folder to the top level in the monorepo, since other projects will be eventually using dags as well. \n\nBut I'm stuck on the others, considering that I need to take into account that we'll want the image building/pushing in CICD as well. \n\nAny examples or advice would be greatly appreciated.", "author_fullname": "t2_blmi7z1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for organizing a project using Airflow + multiple pipelines + poetry + CICD WITHIN a monorepo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17666db", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697114866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, looking for advice on best practices on what I think is a pretty common setup, yet I couldn&amp;#39;t find any good examples on this. &lt;/p&gt;\n\n&lt;p&gt;I have multiple APIs as sources, each with their own script. I want to insert the data into a MySQL database. The aim is to have these tasks running separately using the k8sPodOperator. I&amp;#39;m using poetry as my dependency manager. GitHub Actions for CICD. &lt;/p&gt;\n\n&lt;p&gt;My questions are twofold:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is it better practice to have one large image that contains all the code for every API, and pass different commands to the image in different k8sPodOperators? Or would it be better to have multiple images, one per API pipeline? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How should I organize my code, considering we want to keep a monorepo structure, where multiple projects within that repo will be using Airflow?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Currently the monorepo looks like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;monorepo/\n\n--my-project/\n\n----dags/\n\n------dag_1.py\n\n------dag_2.py\n\n----api/\n\n------base_api_class.py\n\n------api1/\n\n--------api1.py\n\n--------api1_execute.py\n\n------api2/\n\n--------api2.py\n\n--------api2_execute.py\n\n----migrations/\n\n----pyproject.toml\n\n----Dockerfile (Open to change based on answer to q1)\n\n--other_project/\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I think I&amp;#39;ll be moving the dags/ folder to the top level in the monorepo, since other projects will be eventually using dags as well. &lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m stuck on the others, considering that I need to take into account that we&amp;#39;ll want the image building/pushing in CICD as well. &lt;/p&gt;\n\n&lt;p&gt;Any examples or advice would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17666db", "is_robot_indexable": true, "report_reasons": null, "author": "lingorioriorio", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17666db/best_practices_for_organizing_a_project_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17666db/best_practices_for_organizing_a_project_using/", "subreddit_subscribers": 133424, "created_utc": 1697114866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, so I have a website that was created in WordPress, and as we know, the backend or database is in PHPMyAdmin. We want to extract that data and store it in BigQuery. My website is hosted using HostPapa. I am using python code to do it. Can anyone guide me on what to do and the step to do it?", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create a pipeline to store data from phpmyadmin to BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1761x4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697099189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, so I have a website that was created in WordPress, and as we know, the backend or database is in PHPMyAdmin. We want to extract that data and store it in BigQuery. My website is hosted using HostPapa. I am using python code to do it. Can anyone guide me on what to do and the step to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1761x4f", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1761x4f/create_a_pipeline_to_store_data_from_phpmyadmin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1761x4f/create_a_pipeline_to_store_data_from_phpmyadmin/", "subreddit_subscribers": 133424, "created_utc": 1697099189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi !   \nI am working on data ingestion and schema comparison tool. One of initial assumption is to use S3 as a lake for incoming files. I'd like to create Kafka stream to connect it with final destination (to simplify let's assume it is PostresSql DB).  \nI'd like to perform validation of the streamed messages based on precreated schema.   \nCould you please share your thoughts on potential solution for this problem ? I've read about Custom Single Message Transformations, but without wider experience with Kafka, it is hard for me to choose the best practice way.   \nThanks ", "author_fullname": "t2_9ndqvodu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17605w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697091949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi !&lt;br/&gt;\nI am working on data ingestion and schema comparison tool. One of initial assumption is to use S3 as a lake for incoming files. I&amp;#39;d like to create Kafka stream to connect it with final destination (to simplify let&amp;#39;s assume it is PostresSql DB).&lt;br/&gt;\nI&amp;#39;d like to perform validation of the streamed messages based on precreated schema.&lt;br/&gt;\nCould you please share your thoughts on potential solution for this problem ? I&amp;#39;ve read about Custom Single Message Transformations, but without wider experience with Kafka, it is hard for me to choose the best practice way.&lt;br/&gt;\nThanks &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17605w3", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessionalPin7939", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17605w3/kafka_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17605w3/kafka_best_practices/", "subreddit_subscribers": 133424, "created_utc": 1697091949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have bee through many blogs, posts and videos regarding which one to choose but still confused. I am currently an ETL Developer having experience in Traditional ETL tools and SQL. I want to move to Big data and cloud related roles where I will be working on Spark, AWS Glue, S3, etc.\n\nAmong Solutions Architect Associate vs Developer Associate, studying for which cert will help me more in my journey to transition to the above mentioned roles?", "author_fullname": "t2_ht9x5dmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Solutions Architect or Developer associate for Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175zm51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697089864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have bee through many blogs, posts and videos regarding which one to choose but still confused. I am currently an ETL Developer having experience in Traditional ETL tools and SQL. I want to move to Big data and cloud related roles where I will be working on Spark, AWS Glue, S3, etc.&lt;/p&gt;\n\n&lt;p&gt;Among Solutions Architect Associate vs Developer Associate, studying for which cert will help me more in my journey to transition to the above mentioned roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "175zm51", "is_robot_indexable": true, "report_reasons": null, "author": "kaachejl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175zm51/aws_solutions_architect_or_developer_associate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175zm51/aws_solutions_architect_or_developer_associate/", "subreddit_subscribers": 133424, "created_utc": 1697089864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have several config tables that represent jobs to be run. We have a dev, test, stage and prod environment and have multiple teams working in these environments at different paces. For example, one team may start in dev before another team but may deploy to test after.\n\nTeams create entries in the config table in dev with incrementing ids and the the ids are used as foreign keys as well to connect the config tables to fully represent each job. The issue is that the INSERT statements into the config tables are added to code and then deployed to other environments but the ids may have to be different in different environments if another team has already used the same ID in another environment. This results in the foreign keys in the INSERT statement not being correct. How is this situation typically handled?", "author_fullname": "t2_la443gmcs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Consistency of surrogate key for config tables in different environments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175xecm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697082025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have several config tables that represent jobs to be run. We have a dev, test, stage and prod environment and have multiple teams working in these environments at different paces. For example, one team may start in dev before another team but may deploy to test after.&lt;/p&gt;\n\n&lt;p&gt;Teams create entries in the config table in dev with incrementing ids and the the ids are used as foreign keys as well to connect the config tables to fully represent each job. The issue is that the INSERT statements into the config tables are added to code and then deployed to other environments but the ids may have to be different in different environments if another team has already used the same ID in another environment. This results in the foreign keys in the INSERT statement not being correct. How is this situation typically handled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "175xecm", "is_robot_indexable": true, "report_reasons": null, "author": "No_Willingness2818", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175xecm/consistency_of_surrogate_key_for_config_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175xecm/consistency_of_surrogate_key_for_config_tables_in/", "subreddit_subscribers": 133424, "created_utc": 1697082025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for a book or blog or resource website to learn about Google big query \n\nSpecifically I want to learn about its data architecture. I know databricks has a lot of resources for medellion architecture in databricks , delta lake houses , etc. looking for similar resources for big query\n\nThanks!", "author_fullname": "t2_lkm6psdhm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Bigquery resource to learn data engineering/data warehouse architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175pp6l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697060255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a book or blog or resource website to learn about Google big query &lt;/p&gt;\n\n&lt;p&gt;Specifically I want to learn about its data architecture. I know databricks has a lot of resources for medellion architecture in databricks , delta lake houses , etc. looking for similar resources for big query&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "175pp6l", "is_robot_indexable": true, "report_reasons": null, "author": "BigFix7421", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175pp6l/google_bigquery_resource_to_learn_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175pp6l/google_bigquery_resource_to_learn_data/", "subreddit_subscribers": 133424, "created_utc": 1697060255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have collected a [multilingual corpus](https://huggingface.co/datasets/yachay/text_coordinates_regions) with text data and coordinates. The dataset is divided into the 123 most populated regions of the world: \\~500,000 messages from social media + their coordinates, each in a separate json file according to the region. The dataset is suitable for tasks such *as geotagging text data*. Pls, share your opinion if you're interested and so we keep collecting them  \nPS we also have a similar dataset with timestamps, let me know if you need it ", "author_fullname": "t2_tn9uzy0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does someone need text + coordinates datasets for the projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175ka0p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697046628.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have collected a &lt;a href=\"https://huggingface.co/datasets/yachay/text_coordinates_regions\"&gt;multilingual corpus&lt;/a&gt; with text data and coordinates. The dataset is divided into the 123 most populated regions of the world: ~500,000 messages from social media + their coordinates, each in a separate json file according to the region. The dataset is suitable for tasks such &lt;em&gt;as geotagging text data&lt;/em&gt;. Pls, share your opinion if you&amp;#39;re interested and so we keep collecting them&lt;br/&gt;\nPS we also have a similar dataset with timestamps, let me know if you need it &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?auto=webp&amp;s=aefb7c60f2588961073e7a4bd5a5b3a0ac2e709c", "width": 1200, "height": 648}, "resolutions": [{"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11c62108f2b4c2dbb99dc26f91f05760e80705b4", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=18a62a1c06b8cd26dc72a24da6473503754c41d7", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a79f253ae36909b761122272136cbbfa4a31822", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=54bf22a45de90b5d20158b4332f4007b4faa8e5e", "width": 640, "height": 345}, {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcc78f456db1f55d98113b1170fb9adeb6e1c01c", "width": 960, "height": 518}, {"url": "https://external-preview.redd.it/EuBx6pxNL2HaedDhyu39H9Tww3LIqizD5KLHfdlik8U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d623c6624869e45629200e1e74f5847eadc4452", "width": 1080, "height": 583}], "variants": {}, "id": "SiILfaUuXW0H0FnQwaI1nDqfQNgZ2T2Bl9GKvbFr11Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "175ka0p", "is_robot_indexable": true, "report_reasons": null, "author": "yachay_ai", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175ka0p/does_someone_need_text_coordinates_datasets_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175ka0p/does_someone_need_text_coordinates_datasets_for/", "subreddit_subscribers": 133424, "created_utc": 1697046628.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone.\n\nI need help making a data model (I don't have much experience in this area). I'm going to have a file of orders that I'm going to receive (XML, CSV, etc.) and I want to create the data model for it.\n\nThe source file has:- Orders- Order History- Orders Details- Order Details History- Order Comments\n\nI have a lot of attributes in orders table and orders details but I don't want to expose them.\n\nShould I have multiple link tables or have them integrated into the main order table? We have done this separated because in our ETL this is populated after the insertion in Order table because we do not have this external IDs in the source file.\n\nRegarding the history tables, they have the same columns that orders table have, what do you think of inserting all in the same table and use a bit \"active\" do identify the active ones?\n\nMy idea is something like this: [https://dbdiagram.io/d/65269781ffbf5169f07bbd5a](https://dbdiagram.io/d/65269781ffbf5169f07bbd5a)\n\nAny tips or improvements I appreciate. This is only a logical model and and It's just a very general sketch.\n\nTIA", "author_fullname": "t2_yvtk2r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Model - Tips/Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_175e7ao", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697031398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone.&lt;/p&gt;\n\n&lt;p&gt;I need help making a data model (I don&amp;#39;t have much experience in this area). I&amp;#39;m going to have a file of orders that I&amp;#39;m going to receive (XML, CSV, etc.) and I want to create the data model for it.&lt;/p&gt;\n\n&lt;p&gt;The source file has:- Orders- Order History- Orders Details- Order Details History- Order Comments&lt;/p&gt;\n\n&lt;p&gt;I have a lot of attributes in orders table and orders details but I don&amp;#39;t want to expose them.&lt;/p&gt;\n\n&lt;p&gt;Should I have multiple link tables or have them integrated into the main order table? We have done this separated because in our ETL this is populated after the insertion in Order table because we do not have this external IDs in the source file.&lt;/p&gt;\n\n&lt;p&gt;Regarding the history tables, they have the same columns that orders table have, what do you think of inserting all in the same table and use a bit &amp;quot;active&amp;quot; do identify the active ones?&lt;/p&gt;\n\n&lt;p&gt;My idea is something like this: &lt;a href=\"https://dbdiagram.io/d/65269781ffbf5169f07bbd5a\"&gt;https://dbdiagram.io/d/65269781ffbf5169f07bbd5a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any tips or improvements I appreciate. This is only a logical model and and It&amp;#39;s just a very general sketch.&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?auto=webp&amp;s=e6f39f4cc506644b3dd96b5ab364bc1ebaa29e7c", "width": 2732, "height": 1535}, "resolutions": [{"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35423c8bf55e2cb4b8ed590b41c60c5f1d7240b7", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a67ac04e59f63a79e1318db1da67be631f57ec42", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=79d930054804e7cab1c197bc5f6f4bb8909e37a8", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=96eabecda93b9c2e9ca270cf80091dcaad9594f6", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d1380a4036b2c828338681a3401f5cc2fc8c430", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/-2PDTDve5kzBex-XzB_ooF3fpP6uWAQq9UAGlOoRjFw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69a4ea5ce93cc9d1d613f7ff4690f76259ea59e5", "width": 1080, "height": 606}], "variants": {}, "id": "Z4L3UKl_FCXbak-THS-ZYw-t0pLR1gdITw7RDufiwiA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "175e7ao", "is_robot_indexable": true, "report_reasons": null, "author": "peixinho3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/175e7ao/data_model_tipssuggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/175e7ao/data_model_tipssuggestions/", "subreddit_subscribers": 133424, "created_utc": 1697031398.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}