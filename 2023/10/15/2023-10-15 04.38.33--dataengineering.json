{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of the popular ways to do \"delta computation\" on top of the data warehouse?\n\nI'm talking about fetching incremental changes from your Snowflake/BigQuery. The only way I can think of is via the query.\n\n    select field1, field2, field3,..fieldn from view except select field1, field2, field3,..fieldn from previous_query_snapshot\n\nBut running such queries on a column storage can get super expensive for larger datasets.\n\nI am asking this question, as this is the base use case for Reverse ETL. How has your experience been with such queries on large datasets? What are other ways of optimising it?\u00a0  \n\n\nNote that the delta computation needs to be performed on a query - maybe a join of 3 tables and not on a table as such.", "author_fullname": "t2_h4q7gewg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost effective way to fetch incremental changes from your data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177qecg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697300234.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697292514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of the popular ways to do &amp;quot;delta computation&amp;quot; on top of the data warehouse?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m talking about fetching incremental changes from your Snowflake/BigQuery. The only way I can think of is via the query.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;select field1, field2, field3,..fieldn from view except select field1, field2, field3,..fieldn from previous_query_snapshot\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But running such queries on a column storage can get super expensive for larger datasets.&lt;/p&gt;\n\n&lt;p&gt;I am asking this question, as this is the base use case for Reverse ETL. How has your experience been with such queries on large datasets? What are other ways of optimising it?\u00a0  &lt;/p&gt;\n\n&lt;p&gt;Note that the delta computation needs to be performed on a query - maybe a join of 3 tables and not on a table as such.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177qecg", "is_robot_indexable": true, "report_reasons": null, "author": "aruntdharan", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177qecg/cost_effective_way_to_fetch_incremental_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177qecg/cost_effective_way_to_fetch_incremental_changes/", "subreddit_subscribers": 134079, "created_utc": 1697292514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends, a total data engineer n00b here. So i got something im trying to sort out in my head for my own personal project, which I want to scrape and merge data from my social media profiles.\n\nI have a Postgres DB that im hosting my application datas. Pretty typical software dev stuff. But I do need to essentially enrich the said data with other sources (from other PG dbs + social media + web scrapped things)  to make things more useful.\n\nI was thinking to dump things into my applications PG, then use SQL to do data processing to clean up and merge data w/ my application data, but i felt this is not the right way to do it and that this should be done in a data warehouse. \n\nAt my previous companies we spent $$$ on redshift + teams of data engineers to do this. I personally can't spend that so looking for a cheap/free solution to do all this, so I've been running airbyte locally to do the syncing. \n\nWhat should I use as a cheap/free data engineering pipeline + data warehouse? Should i stick with throwing things at PG? \n\nI guess at this point, I'm really looking for an easy to use + cheap solution for all this, running this locally via docker takes a lot of mental overhead...\n\n&amp;#x200B;\n\nThanks!\n\n&amp;#x200B;", "author_fullname": "t2_s0001jox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's an easy/cheap data engineering pipeline+ warehouse stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177yh79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697316301.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697315830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends, a total data engineer n00b here. So i got something im trying to sort out in my head for my own personal project, which I want to scrape and merge data from my social media profiles.&lt;/p&gt;\n\n&lt;p&gt;I have a Postgres DB that im hosting my application datas. Pretty typical software dev stuff. But I do need to essentially enrich the said data with other sources (from other PG dbs + social media + web scrapped things)  to make things more useful.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to dump things into my applications PG, then use SQL to do data processing to clean up and merge data w/ my application data, but i felt this is not the right way to do it and that this should be done in a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;At my previous companies we spent $$$ on redshift + teams of data engineers to do this. I personally can&amp;#39;t spend that so looking for a cheap/free solution to do all this, so I&amp;#39;ve been running airbyte locally to do the syncing. &lt;/p&gt;\n\n&lt;p&gt;What should I use as a cheap/free data engineering pipeline + data warehouse? Should i stick with throwing things at PG? &lt;/p&gt;\n\n&lt;p&gt;I guess at this point, I&amp;#39;m really looking for an easy to use + cheap solution for all this, running this locally via docker takes a lot of mental overhead...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177yh79", "is_robot_indexable": true, "report_reasons": null, "author": "life_on_my_terms", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177yh79/whats_an_easycheap_data_engineering_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177yh79/whats_an_easycheap_data_engineering_pipeline/", "subreddit_subscribers": 134079, "created_utc": 1697315830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI'm quite new to this field and I would like to see what are the options for my use case :\n\nImagine you need to calculate a value in real-time (less than 100ms for the whole process) for each json file you receive, and you receive millions of files each day. How would you go about it?", "author_fullname": "t2_4ff39qhj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie question: High performance and big data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177mrh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697280377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite new to this field and I would like to see what are the options for my use case :&lt;/p&gt;\n\n&lt;p&gt;Imagine you need to calculate a value in real-time (less than 100ms for the whole process) for each json file you receive, and you receive millions of files each day. How would you go about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177mrh8", "is_robot_indexable": true, "report_reasons": null, "author": "elmoptimistic", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177mrh8/newbie_question_high_performance_and_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177mrh8/newbie_question_high_performance_and_big_data/", "subreddit_subscribers": 134079, "created_utc": 1697280377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking to see what approaches others use. Do you use Airflow or equivalent? Use some cloud event triggers? Run bash scripts in cron jobs from a Hadoop cluster? Let\u2019s discuss what DE\u2019s have tried. Also indicate if it was for batch vs streaming.", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you schedule Spark jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177xz4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697314371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to see what approaches others use. Do you use Airflow or equivalent? Use some cloud event triggers? Run bash scripts in cron jobs from a Hadoop cluster? Let\u2019s discuss what DE\u2019s have tried. Also indicate if it was for batch vs streaming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177xz4u", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177xz4u/how_do_you_schedule_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177xz4u/how_do_you_schedule_spark_jobs/", "subreddit_subscribers": 134079, "created_utc": 1697314371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was just trying to learn kakfa , i know python and have been working with it for a while but i wanted to try something with kafka and my existing skillset. Have a look and give me some feedbacks.\n\nGithub:\nhttps://github.com/kanchansapkota27/Youtube-LiveChat-Analysis\n\nDemo:\nhttps://youtu.be/RPR3K9yUDVM?si=RFiK__28yvslYSba", "author_fullname": "t2_61oxwl70", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First Project With Kafka - Youtube Live Chat Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177h56q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697256897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was just trying to learn kakfa , i know python and have been working with it for a while but i wanted to try something with kafka and my existing skillset. Have a look and give me some feedbacks.&lt;/p&gt;\n\n&lt;p&gt;Github:\n&lt;a href=\"https://github.com/kanchansapkota27/Youtube-LiveChat-Analysis\"&gt;https://github.com/kanchansapkota27/Youtube-LiveChat-Analysis&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Demo:\n&lt;a href=\"https://youtu.be/RPR3K9yUDVM?si=RFiK__28yvslYSba\"&gt;https://youtu.be/RPR3K9yUDVM?si=RFiK__28yvslYSba&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?auto=webp&amp;s=958aff6974296dca0057f3f74c73395a1905dfb5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=84301e7dfe7c9eada3fcfb55a744df6bd85dc46f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5370400a44e535f4f00a4029fad432339a84f568", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4748ff88db2cd3c734822ab7793a49f32ab6a93", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b97d034ce58974b8a4cc67b6eeb276e5d811cd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=688dd1d313d3f8bbec14e3b113d49896ef257b0e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XX53ppuRWWh8-LszXb5Nw3IWLFa7arKXrqwwOapwPew.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1529ad431ee8e45e9859a343dd05c79a3f4e3ebb", "width": 1080, "height": 540}], "variants": {}, "id": "59wvsjoWNHprIB_-VEkPE5SkOqxz-SsWi7bCL42S9-c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "177h56q", "is_robot_indexable": true, "report_reasons": null, "author": "EonWolf27", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177h56q/first_project_with_kafka_youtube_live_chat/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177h56q/first_project_with_kafka_youtube_live_chat/", "subreddit_subscribers": 134079, "created_utc": 1697256897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_tjka5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From MLOps to ML Systems with Feature/Training/Inference Pipelines - Hopsworks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_177yxwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HREOqn0jR8eyDnB9G_xWuL0ZYIVBtxvVczijdNoxjig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697317185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hopsworks.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?auto=webp&amp;s=e584672affb31f55a28cb295b6ce0d3741ca9a27", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=012fa05405997f1278ea1a29e6d9cb532605f60c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1689dbb06e54895401d3994a6b5a63a5b499c26f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=221bc774a997ed2693b683b00cb9e9db7f9e4618", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6748d204d3b9c43ff65ba132adf78e0036e43d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e2993db7fd351f84200fd864973de1109720b40", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb1676fce96da8ed128777fa31987308befd068d", "width": 1080, "height": 567}], "variants": {}, "id": "mWT0XPUROLLzz8SuoQWoDanA_b48E0ui60D5VzxrCwM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "177yxwu", "is_robot_indexable": true, "report_reasons": null, "author": "jpdowlin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177yxwu/from_mlops_to_ml_systems_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines", "subreddit_subscribers": 134079, "created_utc": 1697317185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\ud83d\udc4b looking for thoughts on some dagster design with respect to IO managers.\n\nI have an API that doesn\u2019t have timestamps (either update or create) and I\u2019m looking to load new and updated date (and mark deleted).\n\nAsset oriented approach obviously won\u2019t work since I can\u2019t just wipe it all and materialize (since I lose the snapshot); equally can\u2019t partition against anything either.\n\nWhat I settled on was to request all the data and then compare to my current table to perform the insert/update/delete (flag).\n\nQuestion is where would you bundle the logic for actually updating it? Right now I have an IO manager that does this; for a given input table, go compare to a PK field to get the required changes. In my case the asset function returns the full data, then the IO manager controls finding the changes before actually saving the changes.\n\nHow do people feel about that? Would you rather bundle the change logic in the asset function and return nothing from the function, or this method (returns everything, IO figure out the delta).\n\nThis method works (and has the benefit of downstream assets being able to load using the IO manager like anything else. Downside is the function actually returns everything, which might be a bit misleading.\n\nLast option would be a hybrid approach where I just return the changes I determine.\n\nCurious on thoughts!\n\nAlso sorry on formatting, Reddit mobile\u2026\n\nOption 1\n\n    @asset(io_manager_key=\u201cdelta_io\u201d) \n    def new_asset(): \n        api_data = get_data() \n        return api_data\n    \n    class delta_io(\u2026): \n        def handle_output(self, data): \n            current_data = \u2026 \n            compare data = \u2026 \n            save data = \u2026 \n        def load_input(self): \n            return get_current_data() \n\nOption 2\n\n    @asset() \n    def new_asset(): \n        api_data = get_data() \n        current_data = get_current() \n        changes = compare(api_data, current_data) \n        save_data(changes) \n        return None\n\n&amp;#x200B;", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster - Updates/insert change data on existing table - IO design question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177thde", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697301985.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697301412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\udc4b looking for thoughts on some dagster design with respect to IO managers.&lt;/p&gt;\n\n&lt;p&gt;I have an API that doesn\u2019t have timestamps (either update or create) and I\u2019m looking to load new and updated date (and mark deleted).&lt;/p&gt;\n\n&lt;p&gt;Asset oriented approach obviously won\u2019t work since I can\u2019t just wipe it all and materialize (since I lose the snapshot); equally can\u2019t partition against anything either.&lt;/p&gt;\n\n&lt;p&gt;What I settled on was to request all the data and then compare to my current table to perform the insert/update/delete (flag).&lt;/p&gt;\n\n&lt;p&gt;Question is where would you bundle the logic for actually updating it? Right now I have an IO manager that does this; for a given input table, go compare to a PK field to get the required changes. In my case the asset function returns the full data, then the IO manager controls finding the changes before actually saving the changes.&lt;/p&gt;\n\n&lt;p&gt;How do people feel about that? Would you rather bundle the change logic in the asset function and return nothing from the function, or this method (returns everything, IO figure out the delta).&lt;/p&gt;\n\n&lt;p&gt;This method works (and has the benefit of downstream assets being able to load using the IO manager like anything else. Downside is the function actually returns everything, which might be a bit misleading.&lt;/p&gt;\n\n&lt;p&gt;Last option would be a hybrid approach where I just return the changes I determine.&lt;/p&gt;\n\n&lt;p&gt;Curious on thoughts!&lt;/p&gt;\n\n&lt;p&gt;Also sorry on formatting, Reddit mobile\u2026&lt;/p&gt;\n\n&lt;p&gt;Option 1&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@asset(io_manager_key=\u201cdelta_io\u201d) \ndef new_asset(): \n    api_data = get_data() \n    return api_data\n\nclass delta_io(\u2026): \n    def handle_output(self, data): \n        current_data = \u2026 \n        compare data = \u2026 \n        save data = \u2026 \n    def load_input(self): \n        return get_current_data() \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Option 2&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@asset() \ndef new_asset(): \n    api_data = get_data() \n    current_data = get_current() \n    changes = compare(api_data, current_data) \n    save_data(changes) \n    return None\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177thde", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177thde/dagster_updatesinsert_change_data_on_existing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177thde/dagster_updatesinsert_change_data_on_existing/", "subreddit_subscribers": 134079, "created_utc": 1697301412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a Software Engineer for more than 10 years with some experience with small scale Data Engineering projects. I got an opportunity to move to a full-time Data Engineer role for a good company which is investing heavily in a Data platform. I think it would be a great chance to learn and grow as an Engineer.  What are your thought?\n\n[View Poll](https://www.reddit.com/poll/177sfe2)", "author_fullname": "t2_8mj6c2k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software Engineer to Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177sfe2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697298368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a Software Engineer for more than 10 years with some experience with small scale Data Engineering projects. I got an opportunity to move to a full-time Data Engineer role for a good company which is investing heavily in a Data platform. I think it would be a great chance to learn and grow as an Engineer.  What are your thought?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/177sfe2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "177sfe2", "is_robot_indexable": true, "report_reasons": null, "author": "SignalFocus3819", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1697557568863, "options": [{"text": "Its a career ending decision", "id": "25273532"}, {"text": "Challenging but worth it", "id": "25273533"}, {"text": "Great oppotunity", "id": "25273534"}, {"text": "Neutral", "id": "25273535"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 865, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177sfe2/software_engineer_to_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/177sfe2/software_engineer_to_data_engineer/", "subreddit_subscribers": 134079, "created_utc": 1697298368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "PowerBI is my preferred software for charting, however, constructing a motion chart can be cumbersome. Therefore, I am experimenting with websocket and chart.js. Any better suggestion?\n\nThe following use case involves executing a filter-innerjoin-groupby operation 100 times. The dataset used ranges from 1 million to 100 million rows, with each run increasing the row count by 1 million.\n\nThe video:  [https://youtu.be/YL4TeYP-GNA](https://youtu.be/YL4TeYP-GNA)\n\nThe full script for Polars, DuckDB and Peaks:   [https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range](https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range)\n\nThis is the query I use for the motion chart.js. \n\nquery1 := peakgo.**Query**(  \n \"filter\", \"Style(=F)\",  \n \"build\\_key\\_value\", \"Product, Style =&gt; Table(key\\_value)\")\n\n  \n master\\_df := peakgo.**RunBatch**(df, \"Inbox/Master.csv\", query1)\n\n  \n query2 := peakgo.**Query**(  \n \"filter\", \"Shop(S77..S78)\",  \n \"join\\_key\\_value\", \"Product, Style =&gt; Inner(key\\_value)\",  \n \"add\\_column\", \"Quantity, Unit\\_Price =&gt; Multiply(Amount)\",  \n \"filter\", \"Amount:Float(&gt;100000)\",  \n \"group\\_by\", \"Shop, Product =&gt; Count() Sum(Quantity) Sum(Amount)\")  \n source\\_file := peakgo.**GetCliFilePath**(\"10-MillionRows.csv\")  \n result\\_file := \\[\\]string{\"Outbox/PeakGo-Detail-Result-\" + filepath.**Base**(source\\_file),  \n \"Outbox/Peakgo-Summary-Result-\" + filepath.**Base**(source\\_file)}\n\n  \n peakgo.**RunStream**(df, &amp;master\\_df, source\\_file, query2, result\\_file)\n\n&amp;#x200B;", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demonstrate the Elapsed Time of DataFrame\u2019s InnerJoin-GroupBy Operation using Python Websocket and Chart.js", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1785650", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697336303.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697335975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PowerBI is my preferred software for charting, however, constructing a motion chart can be cumbersome. Therefore, I am experimenting with websocket and chart.js. Any better suggestion?&lt;/p&gt;\n\n&lt;p&gt;The following use case involves executing a filter-innerjoin-groupby operation 100 times. The dataset used ranges from 1 million to 100 million rows, with each run increasing the row count by 1 million.&lt;/p&gt;\n\n&lt;p&gt;The video:  &lt;a href=\"https://youtu.be/YL4TeYP-GNA\"&gt;https://youtu.be/YL4TeYP-GNA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The full script for Polars, DuckDB and Peaks:   &lt;a href=\"https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range\"&gt;https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the query I use for the motion chart.js. &lt;/p&gt;\n\n&lt;p&gt;query1 := peakgo.&lt;strong&gt;Query&lt;/strong&gt;(&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Style(=F)&amp;quot;,&lt;br/&gt;\n &amp;quot;build_key_value&amp;quot;, &amp;quot;Product, Style =&amp;gt; Table(key_value)&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;master_df := peakgo.&lt;strong&gt;RunBatch&lt;/strong&gt;(df, &amp;quot;Inbox/Master.csv&amp;quot;, query1)&lt;/p&gt;\n\n&lt;p&gt;query2 := peakgo.&lt;strong&gt;Query&lt;/strong&gt;(&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Shop(S77..S78)&amp;quot;,&lt;br/&gt;\n &amp;quot;join_key_value&amp;quot;, &amp;quot;Product, Style =&amp;gt; Inner(key_value)&amp;quot;,&lt;br/&gt;\n &amp;quot;add_column&amp;quot;, &amp;quot;Quantity, Unit_Price =&amp;gt; Multiply(Amount)&amp;quot;,&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Amount:Float(&amp;gt;100000)&amp;quot;,&lt;br/&gt;\n &amp;quot;group_by&amp;quot;, &amp;quot;Shop, Product =&amp;gt; Count() Sum(Quantity) Sum(Amount)&amp;quot;)&lt;br/&gt;\n source_file := peakgo.&lt;strong&gt;GetCliFilePath&lt;/strong&gt;(&amp;quot;10-MillionRows.csv&amp;quot;)&lt;br/&gt;\n result_file := []string{&amp;quot;Outbox/PeakGo-Detail-Result-&amp;quot; + filepath.&lt;strong&gt;Base&lt;/strong&gt;(source_file),&lt;br/&gt;\n &amp;quot;Outbox/Peakgo-Summary-Result-&amp;quot; + filepath.&lt;strong&gt;Base&lt;/strong&gt;(source_file)}&lt;/p&gt;\n\n&lt;p&gt;peakgo.&lt;strong&gt;RunStream&lt;/strong&gt;(df, &amp;amp;master_df, source_file, query2, result_file)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?auto=webp&amp;s=2aff142f52e27be6874fcd2644a50cbde0ae911b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23eeb53d06fc23558196c5fdc0cb8aec4e8a3f58", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a41bdeb1305e044556b4685a1f9f03e994485d4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ca2d8c2a234a3acb3aab693da69d36b68e38bc2", "width": 320, "height": 240}], "variants": {}, "id": "k4zGogCh-hB3FShJVR_vC1Mzoxuv_VBM7oVVkSZrIsE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1785650", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1785650/demonstrate_the_elapsed_time_of_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1785650/demonstrate_the_elapsed_time_of_dataframes/", "subreddit_subscribers": 134079, "created_utc": 1697335975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been managing a team package using a Git equivalent and an internal tool to execute cron jobs for scheduled script execution. I've also developed a script to parse logs to get a report on job failures to troubleshoot if it's a script side issue or  some configuration issue.\n\nMy main confusion is that the approach I've been doing is quite simplistic compared to the articles I see online and tools used definitely are not industry standard, or tools I just cobbled together on my own.", "author_fullname": "t2_14fnbt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have I been doing CI/CD in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177uos5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697304907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been managing a team package using a Git equivalent and an internal tool to execute cron jobs for scheduled script execution. I&amp;#39;ve also developed a script to parse logs to get a report on job failures to troubleshoot if it&amp;#39;s a script side issue or  some configuration issue.&lt;/p&gt;\n\n&lt;p&gt;My main confusion is that the approach I&amp;#39;ve been doing is quite simplistic compared to the articles I see online and tools used definitely are not industry standard, or tools I just cobbled together on my own.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177uos5", "is_robot_indexable": true, "report_reasons": null, "author": "git0ffmylawnm8", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177uos5/have_i_been_doing_cicd_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177uos5/have_i_been_doing_cicd_in_data_engineering/", "subreddit_subscribers": 134079, "created_utc": 1697304907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h557nj7lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "12 Best Data Engineering Online Courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_177orgc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DLwIlPvZnGOErVuJUNezhVxa-Bvw2QGveGtzggSg40g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697287566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/best-data-engineering-courses-online/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?auto=webp&amp;s=fdbb5490d03d4a12d9929b68aa41da4fd7289284", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8a5bff3d2dde793a9af3e8f06d0b7b589e05e835", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c33b5a634b3e15805e097ce9d00ca38bfe4bde9", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=81210ad03f0f4a5ecf1ccf2df0c9617a1273cd9f", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=748aef7ad01063aad7c71e3fc7680041bd7dcb8f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdb6217bd9a0dafc35b5d1a64f9b1e785c07308e", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/fnL7w7uzJDtGjYWyM_QG7sY4vm042-z6Iy9gyNUiKOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0bb1bc1227b0a45a17b17536e3adb2ac5e6da9af", "width": 1080, "height": 607}], "variants": {}, "id": "BQXgWkRDAqSE-J5LYR1c1O0AFpuNT_1sZanW8n326h0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "177orgc", "is_robot_indexable": true, "report_reasons": null, "author": "Aqsa81", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177orgc/12_best_data_engineering_online_courses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/best-data-engineering-courses-online/", "subreddit_subscribers": 134079, "created_utc": 1697287566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I work on a project with an outside consultancy. The project is to do with building a cloud data platform. We are moving from a on-prem dwh to the cloud, so a big learning curve to everyone.\n\nOne project resource that we  have requested is DataOps engineer. And consulting company keeps sending us profiles of IaC DevOps engineers or Software DevOps engineers. None of them build any devops for Data Platforms / Data Pipelines. Consultancy keep saying that any DevOps engineer will be able to do the job. But I am not so sure. Mind you - our on-prem set-up had little devops, so we are all learning.\n\nSo question: **is DevOps Engineering vs DataOps Engineering very same or very different?** What things might be missed if software DevOps builds things for data pipelines? One things I am concerned is testing as I think automating tests for software and automating tests for data pipelines is somewhat different. Like in  data pipelines omme has to think about the code and about the data.\n\nAs I don't have hands one of neither devops or dataops I might be seeing a problem where actually there is none. \n\nThank you!\n\n&amp;#x200B;", "author_fullname": "t2_49dbxejy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DevOps Engineer vs DataOps Engineer - same skill set? Can software DevOps be hired for DataOps project role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177o38a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697285322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I work on a project with an outside consultancy. The project is to do with building a cloud data platform. We are moving from a on-prem dwh to the cloud, so a big learning curve to everyone.&lt;/p&gt;\n\n&lt;p&gt;One project resource that we  have requested is DataOps engineer. And consulting company keeps sending us profiles of IaC DevOps engineers or Software DevOps engineers. None of them build any devops for Data Platforms / Data Pipelines. Consultancy keep saying that any DevOps engineer will be able to do the job. But I am not so sure. Mind you - our on-prem set-up had little devops, so we are all learning.&lt;/p&gt;\n\n&lt;p&gt;So question: &lt;strong&gt;is DevOps Engineering vs DataOps Engineering very same or very different?&lt;/strong&gt; What things might be missed if software DevOps builds things for data pipelines? One things I am concerned is testing as I think automating tests for software and automating tests for data pipelines is somewhat different. Like in  data pipelines omme has to think about the code and about the data.&lt;/p&gt;\n\n&lt;p&gt;As I don&amp;#39;t have hands one of neither devops or dataops I might be seeing a problem where actually there is none. &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177o38a", "is_robot_indexable": true, "report_reasons": null, "author": "HereJustForAnswers", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177o38a/devops_engineer_vs_dataops_engineer_same_skill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177o38a/devops_engineer_vs_dataops_engineer_same_skill/", "subreddit_subscribers": 134079, "created_utc": 1697285322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cuasmkxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello, I'm keenly interested in Analytics and Data Engineering roles and am actively seeking to transition into these fields. I'm open to any suggestions, advice, and feedback on my current resume. Thank you in advance!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_1786ije", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/NFlKQt_fVyxGKFq2m--FNEN6WcwsqltM2i8GLzUUuM4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697340480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9h0jxm55aaub1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9h0jxm55aaub1.png?auto=webp&amp;s=f44f126d4485b3d60bdf82301573ea7709bb7490", "width": 658, "height": 825}, "resolutions": [{"url": "https://preview.redd.it/9h0jxm55aaub1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c552b198106444f38e742b2529c20a3d20900dc", "width": 108, "height": 135}, {"url": "https://preview.redd.it/9h0jxm55aaub1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=511d2b9faa20425781b41e534467e6cdf59502f1", "width": 216, "height": 270}, {"url": "https://preview.redd.it/9h0jxm55aaub1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e5b0870174307fc1dae02554940d75666e98da1", "width": 320, "height": 401}, {"url": "https://preview.redd.it/9h0jxm55aaub1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34de2d343e791000f05912dcf9dac9289e31e5cb", "width": 640, "height": 802}], "variants": {}, "id": "mjL0G4-KJ-deUo83MY9qMqFolix4irygY6lW6Fw8qJc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1786ije", "is_robot_indexable": true, "report_reasons": null, "author": "Massive-Middle6043", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1786ije/hello_im_keenly_interested_in_analytics_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9h0jxm55aaub1.png", "subreddit_subscribers": 134079, "created_utc": 1697340480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an  developer, and I'll be presenting our new Azure Big Data stack architecture, including Databricks, devops,(medallion )Delta to a group of business analysts and SAP professionals who have limited knowledge of these tools. \n\nWhat are some of the fascinating facts, trivia, or impressive use cases about spark, big data stack that I can include in my keynote to wow the audience or show them the scale of these technologies?\n\nI was thinking of mentioning about Netflix and their used cases of handling the data volume using spark/big data techs.", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help: Fascinating Spark/Big Data, Devops facts for my presentation!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17866lx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697340206.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697339357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an  developer, and I&amp;#39;ll be presenting our new Azure Big Data stack architecture, including Databricks, devops,(medallion )Delta to a group of business analysts and SAP professionals who have limited knowledge of these tools. &lt;/p&gt;\n\n&lt;p&gt;What are some of the fascinating facts, trivia, or impressive use cases about spark, big data stack that I can include in my keynote to wow the audience or show them the scale of these technologies?&lt;/p&gt;\n\n&lt;p&gt;I was thinking of mentioning about Netflix and their used cases of handling the data volume using spark/big data techs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17866lx", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17866lx/help_fascinating_sparkbig_data_devops_facts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17866lx/help_fascinating_sparkbig_data_devops_facts_for/", "subreddit_subscribers": 134079, "created_utc": 1697339357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Giving access in Redshift to different users or groups becomes really cumbersome. We have been providing access manually everytime somebody asks for it. Is there a way to automate this process. Any tool you guys using for access management?", "author_fullname": "t2_96l36oyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage access control in Redshift or any other data warehouse. Is there a way to automate this process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177zkic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697318942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Giving access in Redshift to different users or groups becomes really cumbersome. We have been providing access manually everytime somebody asks for it. Is there a way to automate this process. Any tool you guys using for access management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177zkic", "is_robot_indexable": true, "report_reasons": null, "author": "veryuncanny_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177zkic/how_do_you_manage_access_control_in_redshift_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177zkic/how_do_you_manage_access_control_in_redshift_or/", "subreddit_subscribers": 134079, "created_utc": 1697318942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating a medallion based data pipeline and am unsure at what state I should be storing data in the bronze layer.\n\nAs an example, I have json files being streamed into a data lake on s3, should I then read that data in a scheduled stream from the bucket and write it to delta with a defined schema into my bronze layer or should I keep it in the original json format (This seems basically like a pointless copy).\n\nTo me it seems better to do basic schema definition at the early pipeline stage where minimal transformation and enrichment is done as it makes it simpler to handle changes in the data that will inevitably occur.", "author_fullname": "t2_4e7vcqz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Medallion - Databricks Bronze Layer Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177x70q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697312149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating a medallion based data pipeline and am unsure at what state I should be storing data in the bronze layer.&lt;/p&gt;\n\n&lt;p&gt;As an example, I have json files being streamed into a data lake on s3, should I then read that data in a scheduled stream from the bucket and write it to delta with a defined schema into my bronze layer or should I keep it in the original json format (This seems basically like a pointless copy).&lt;/p&gt;\n\n&lt;p&gt;To me it seems better to do basic schema definition at the early pipeline stage where minimal transformation and enrichment is done as it makes it simpler to handle changes in the data that will inevitably occur.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177x70q", "is_robot_indexable": true, "report_reasons": null, "author": "noodlehead13", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177x70q/medallion_databricks_bronze_layer_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177x70q/medallion_databricks_bronze_layer_best_practice/", "subreddit_subscribers": 134079, "created_utc": 1697312149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After a successful career in tech consulting (did hands on BI then DE and ML before running consulting practices) I pivoted to a role outside of professional services to pursue a goal of becoming a Chief Data Officer. I landed at a start up as the first real Data SME at a great salary and nice title with the expectation that I\u2019ll start building an org and delivering data and analytics to the business. \n\nUnfortunately right after I started (2022) the market did what it did and the company\u2019s growth stalled. And with it all the promises of a budget, headcount, etc. went out the window. \n\nA year into it I am an IC doing what I can with no line of sight into building what I thought I would be building. I\u2019m not getting any traction applying to roles via LinkedIn at companies that seem more established and likely to be a better fit. I\u2019m starting to think staying at this job is doing a real disservice to my career. \n\nLooking for perspectives on staying at this job that is really unfulfilling (but provides a lot of financial stability) versus jumping ship when/if the opportunity arises. Am I missing some untapped potential in my current position?", "author_fullname": "t2_6yf372g9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice after Consulting Pivot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177suyo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697299598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a successful career in tech consulting (did hands on BI then DE and ML before running consulting practices) I pivoted to a role outside of professional services to pursue a goal of becoming a Chief Data Officer. I landed at a start up as the first real Data SME at a great salary and nice title with the expectation that I\u2019ll start building an org and delivering data and analytics to the business. &lt;/p&gt;\n\n&lt;p&gt;Unfortunately right after I started (2022) the market did what it did and the company\u2019s growth stalled. And with it all the promises of a budget, headcount, etc. went out the window. &lt;/p&gt;\n\n&lt;p&gt;A year into it I am an IC doing what I can with no line of sight into building what I thought I would be building. I\u2019m not getting any traction applying to roles via LinkedIn at companies that seem more established and likely to be a better fit. I\u2019m starting to think staying at this job is doing a real disservice to my career. &lt;/p&gt;\n\n&lt;p&gt;Looking for perspectives on staying at this job that is really unfulfilling (but provides a lot of financial stability) versus jumping ship when/if the opportunity arises. Am I missing some untapped potential in my current position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "177suyo", "is_robot_indexable": true, "report_reasons": null, "author": "No-Regret-3024", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177suyo/advice_after_consulting_pivot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177suyo/advice_after_consulting_pivot/", "subreddit_subscribers": 134079, "created_utc": 1697299598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "am building a etl project with apache airflow which need to extracts data from dynamic api (not streaming, new data is added weekly), and each page contain about 50 items, now if i insert that data page by page in sql database this might slow down db. what to do in this scenario?", "author_fullname": "t2_ibbr02dcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Advise needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177rrws", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697296475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;am building a etl project with apache airflow which need to extracts data from dynamic api (not streaming, new data is added weekly), and each page contain about 50 items, now if i insert that data page by page in sql database this might slow down db. what to do in this scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177rrws", "is_robot_indexable": true, "report_reasons": null, "author": "incoming_sign", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177rrws/project_advise_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177rrws/project_advise_needed/", "subreddit_subscribers": 134079, "created_utc": 1697296475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have 10+ experience in Data Engineering but only in inhouse Hadoop Clusters mostly in operations however most openings need cloud experince of a couple of years for next level.\n\nI am scaling up in AWS and going deep into it.  \nGiving AWS Practisioner next week. \nMy concern is how to scale up in cloud for experience level i.e. learning nuances associated with my experience level. \nIf any one is available please can we connect.", "author_fullname": "t2_35v94", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 + Experienced Data Engineer looking to for guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177okev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697286938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have 10+ experience in Data Engineering but only in inhouse Hadoop Clusters mostly in operations however most openings need cloud experince of a couple of years for next level.&lt;/p&gt;\n\n&lt;p&gt;I am scaling up in AWS and going deep into it.&lt;br/&gt;\nGiving AWS Practisioner next week. \nMy concern is how to scale up in cloud for experience level i.e. learning nuances associated with my experience level. \nIf any one is available please can we connect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "177okev", "is_robot_indexable": true, "report_reasons": null, "author": "rishiarora", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177okev/10_experienced_data_engineer_looking_to_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177okev/10_experienced_data_engineer_looking_to_for/", "subreddit_subscribers": 134079, "created_utc": 1697286938.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}