{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends, a total data engineer n00b here. So i got something im trying to sort out in my head for my own personal project, which I want to scrape and merge data from my social media profiles.\n\nI have a Postgres DB that im hosting my application datas. Pretty typical software dev stuff. But I do need to essentially enrich the said data with other sources (from other PG dbs + social media + web scrapped things)  to make things more useful.\n\nI was thinking to dump things into my applications PG, then use SQL to do data processing to clean up and merge data w/ my application data, but i felt this is not the right way to do it and that this should be done in a data warehouse. \n\nAt my previous companies we spent $$$ on redshift + teams of data engineers to do this. I personally can't spend that so looking for a cheap/free solution to do all this, so I've been running airbyte locally to do the syncing. \n\nWhat should I use as a cheap/free data engineering pipeline + data warehouse? Should i stick with throwing things at PG? \n\nI guess at this point, I'm really looking for an easy to use + cheap solution for all this, running this locally via docker takes a lot of mental overhead...\n\n&amp;#x200B;\n\nThanks!\n\n&amp;#x200B;", "author_fullname": "t2_s0001jox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's an easy/cheap data engineering pipeline+ warehouse stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177yh79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697316301.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697315830.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends, a total data engineer n00b here. So i got something im trying to sort out in my head for my own personal project, which I want to scrape and merge data from my social media profiles.&lt;/p&gt;\n\n&lt;p&gt;I have a Postgres DB that im hosting my application datas. Pretty typical software dev stuff. But I do need to essentially enrich the said data with other sources (from other PG dbs + social media + web scrapped things)  to make things more useful.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to dump things into my applications PG, then use SQL to do data processing to clean up and merge data w/ my application data, but i felt this is not the right way to do it and that this should be done in a data warehouse. &lt;/p&gt;\n\n&lt;p&gt;At my previous companies we spent $$$ on redshift + teams of data engineers to do this. I personally can&amp;#39;t spend that so looking for a cheap/free solution to do all this, so I&amp;#39;ve been running airbyte locally to do the syncing. &lt;/p&gt;\n\n&lt;p&gt;What should I use as a cheap/free data engineering pipeline + data warehouse? Should i stick with throwing things at PG? &lt;/p&gt;\n\n&lt;p&gt;I guess at this point, I&amp;#39;m really looking for an easy to use + cheap solution for all this, running this locally via docker takes a lot of mental overhead...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177yh79", "is_robot_indexable": true, "report_reasons": null, "author": "life_on_my_terms", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177yh79/whats_an_easycheap_data_engineering_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177yh79/whats_an_easycheap_data_engineering_pipeline/", "subreddit_subscribers": 134173, "created_utc": 1697315830.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking to see what approaches others use. Do you use Airflow or equivalent? Use some cloud event triggers? Run bash scripts in cron jobs from a Hadoop cluster? Let\u2019s discuss what DE\u2019s have tried. Also indicate if it was for batch vs streaming.", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you schedule Spark jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177xz4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697314371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to see what approaches others use. Do you use Airflow or equivalent? Use some cloud event triggers? Run bash scripts in cron jobs from a Hadoop cluster? Let\u2019s discuss what DE\u2019s have tried. Also indicate if it was for batch vs streaming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177xz4u", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177xz4u/how_do_you_schedule_spark_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177xz4u/how_do_you_schedule_spark_jobs/", "subreddit_subscribers": 134173, "created_utc": 1697314371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_rw355scd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Unity Catalog: Guide to Streamline Your Data Assets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 120, "top_awarded_type": null, "hide_score": false, "name": "t3_178be0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/KIu4Ih-rTVysMqeg_yMdNSuv4kSnP2cZ-BvcbyHfIq8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697360541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "lakefs.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://lakefs.io/blog/unity-catalog-guide-streamline-data-assets/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fA1qIP_XtI43-KBUYT98KPtpRcCDvMOJFJ7DYVkzIgw.jpg?auto=webp&amp;s=9458e915e9dd1f4ab13fadc59535a0c65fac8b0e", "width": 350, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/fA1qIP_XtI43-KBUYT98KPtpRcCDvMOJFJ7DYVkzIgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=256e684f7534e8f29a0281ed7df436084d2538e4", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/fA1qIP_XtI43-KBUYT98KPtpRcCDvMOJFJ7DYVkzIgw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=206cc51f034151c6ecdedd754567eb117b61122c", "width": 216, "height": 185}, {"url": "https://external-preview.redd.it/fA1qIP_XtI43-KBUYT98KPtpRcCDvMOJFJ7DYVkzIgw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cbef2474a7d55294e60b8529d7c8e3bbe3de98c", "width": 320, "height": 274}], "variants": {}, "id": "CRQEZJT75CK-lmF177mkcpqX8ry3JIX7rvkT9XblRjQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "178be0n", "is_robot_indexable": true, "report_reasons": null, "author": "henits784", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178be0n/databricks_unity_catalog_guide_to_streamline_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://lakefs.io/blog/unity-catalog-guide-streamline-data-assets/", "subreddit_subscribers": 134173, "created_utc": 1697360541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_tjka5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From MLOps to ML Systems with Feature/Training/Inference Pipelines - Hopsworks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_177yxwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HREOqn0jR8eyDnB9G_xWuL0ZYIVBtxvVczijdNoxjig.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1697317185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hopsworks.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?auto=webp&amp;s=e584672affb31f55a28cb295b6ce0d3741ca9a27", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=012fa05405997f1278ea1a29e6d9cb532605f60c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1689dbb06e54895401d3994a6b5a63a5b499c26f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=221bc774a997ed2693b683b00cb9e9db7f9e4618", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3d6748d204d3b9c43ff65ba132adf78e0036e43d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e2993db7fd351f84200fd864973de1109720b40", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ZAMiYSs8p0i8kv9zisx4jofC3oZ9WBIs3Xg1plq7oE4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb1676fce96da8ed128777fa31987308befd068d", "width": 1080, "height": 567}], "variants": {}, "id": "mWT0XPUROLLzz8SuoQWoDanA_b48E0ui60D5VzxrCwM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "177yxwu", "is_robot_indexable": true, "report_reasons": null, "author": "jpdowlin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177yxwu/from_mlops_to_ml_systems_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines", "subreddit_subscribers": 134173, "created_utc": 1697317185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been managing a team package using a Git equivalent and an internal tool to execute cron jobs for scheduled script execution. I've also developed a script to parse logs to get a report on job failures to troubleshoot if it's a script side issue or  some configuration issue.\n\nMy main confusion is that the approach I've been doing is quite simplistic compared to the articles I see online and tools used definitely are not industry standard, or tools I just cobbled together on my own.", "author_fullname": "t2_14fnbt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Have I been doing CI/CD in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177uos5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697304907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been managing a team package using a Git equivalent and an internal tool to execute cron jobs for scheduled script execution. I&amp;#39;ve also developed a script to parse logs to get a report on job failures to troubleshoot if it&amp;#39;s a script side issue or  some configuration issue.&lt;/p&gt;\n\n&lt;p&gt;My main confusion is that the approach I&amp;#39;ve been doing is quite simplistic compared to the articles I see online and tools used definitely are not industry standard, or tools I just cobbled together on my own.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177uos5", "is_robot_indexable": true, "report_reasons": null, "author": "git0ffmylawnm8", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177uos5/have_i_been_doing_cicd_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177uos5/have_i_been_doing_cicd_in_data_engineering/", "subreddit_subscribers": 134173, "created_utc": 1697304907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "PowerBI is my preferred software for charting, however, constructing a motion chart can be cumbersome. Therefore, I am experimenting with websocket and chart.js. Any better suggestion?\n\nThe following use case involves executing a filter-innerjoin-groupby operation 100 times. The dataset used ranges from 1 million to 100 million rows, with each run increasing the row count by 1 million.\n\nThe video:  [https://youtu.be/YL4TeYP-GNA](https://youtu.be/YL4TeYP-GNA)\n\nThe full script for Polars, DuckDB and Peaks:   [https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range](https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range)\n\nThis is the query I use for the motion chart.js. \n\nquery1 := peakgo.**Query**(  \n \"filter\", \"Style(=F)\",  \n \"build\\_key\\_value\", \"Product, Style =&gt; Table(key\\_value)\")\n\n  \n master\\_df := peakgo.**RunBatch**(df, \"Inbox/Master.csv\", query1)\n\n  \n query2 := peakgo.**Query**(  \n \"filter\", \"Shop(S77..S78)\",  \n \"join\\_key\\_value\", \"Product, Style =&gt; Inner(key\\_value)\",  \n \"add\\_column\", \"Quantity, Unit\\_Price =&gt; Multiply(Amount)\",  \n \"filter\", \"Amount:Float(&gt;100000)\",  \n \"group\\_by\", \"Shop, Product =&gt; Count() Sum(Quantity) Sum(Amount)\")  \n source\\_file := peakgo.**GetCliFilePath**(\"10-MillionRows.csv\")  \n result\\_file := \\[\\]string{\"Outbox/PeakGo-Detail-Result-\" + filepath.**Base**(source\\_file),  \n \"Outbox/Peakgo-Summary-Result-\" + filepath.**Base**(source\\_file)}\n\n  \n peakgo.**RunStream**(df, &amp;master\\_df, source\\_file, query2, result\\_file)\n\n&amp;#x200B;", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Demonstrate the Elapsed Time of DataFrame\u2019s InnerJoin-GroupBy Operation using Python Websocket and Chart.js", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1785650", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697336303.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1697335975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PowerBI is my preferred software for charting, however, constructing a motion chart can be cumbersome. Therefore, I am experimenting with websocket and chart.js. Any better suggestion?&lt;/p&gt;\n\n&lt;p&gt;The following use case involves executing a filter-innerjoin-groupby operation 100 times. The dataset used ranges from 1 million to 100 million rows, with each run increasing the row count by 1 million.&lt;/p&gt;\n\n&lt;p&gt;The video:  &lt;a href=\"https://youtu.be/YL4TeYP-GNA\"&gt;https://youtu.be/YL4TeYP-GNA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The full script for Polars, DuckDB and Peaks:   &lt;a href=\"https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range\"&gt;https://github.com/hkpeaks/polars-cf-peaks/tree/main/cf-002%20InnerJoin-GroupBy/Narrow-Filter-Range&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the query I use for the motion chart.js. &lt;/p&gt;\n\n&lt;p&gt;query1 := peakgo.&lt;strong&gt;Query&lt;/strong&gt;(&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Style(=F)&amp;quot;,&lt;br/&gt;\n &amp;quot;build_key_value&amp;quot;, &amp;quot;Product, Style =&amp;gt; Table(key_value)&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;master_df := peakgo.&lt;strong&gt;RunBatch&lt;/strong&gt;(df, &amp;quot;Inbox/Master.csv&amp;quot;, query1)&lt;/p&gt;\n\n&lt;p&gt;query2 := peakgo.&lt;strong&gt;Query&lt;/strong&gt;(&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Shop(S77..S78)&amp;quot;,&lt;br/&gt;\n &amp;quot;join_key_value&amp;quot;, &amp;quot;Product, Style =&amp;gt; Inner(key_value)&amp;quot;,&lt;br/&gt;\n &amp;quot;add_column&amp;quot;, &amp;quot;Quantity, Unit_Price =&amp;gt; Multiply(Amount)&amp;quot;,&lt;br/&gt;\n &amp;quot;filter&amp;quot;, &amp;quot;Amount:Float(&amp;gt;100000)&amp;quot;,&lt;br/&gt;\n &amp;quot;group_by&amp;quot;, &amp;quot;Shop, Product =&amp;gt; Count() Sum(Quantity) Sum(Amount)&amp;quot;)&lt;br/&gt;\n source_file := peakgo.&lt;strong&gt;GetCliFilePath&lt;/strong&gt;(&amp;quot;10-MillionRows.csv&amp;quot;)&lt;br/&gt;\n result_file := []string{&amp;quot;Outbox/PeakGo-Detail-Result-&amp;quot; + filepath.&lt;strong&gt;Base&lt;/strong&gt;(source_file),&lt;br/&gt;\n &amp;quot;Outbox/Peakgo-Summary-Result-&amp;quot; + filepath.&lt;strong&gt;Base&lt;/strong&gt;(source_file)}&lt;/p&gt;\n\n&lt;p&gt;peakgo.&lt;strong&gt;RunStream&lt;/strong&gt;(df, &amp;amp;master_df, source_file, query2, result_file)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?auto=webp&amp;s=2aff142f52e27be6874fcd2644a50cbde0ae911b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23eeb53d06fc23558196c5fdc0cb8aec4e8a3f58", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a41bdeb1305e044556b4685a1f9f03e994485d4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/AJabP9VIRjPmGTmcurjW_qzx-0saZQznw-4VPs7cJZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ca2d8c2a234a3acb3aab693da69d36b68e38bc2", "width": 320, "height": 240}], "variants": {}, "id": "k4zGogCh-hB3FShJVR_vC1Mzoxuv_VBM7oVVkSZrIsE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1785650", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1785650/demonstrate_the_elapsed_time_of_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1785650/demonstrate_the_elapsed_time_of_dataframes/", "subreddit_subscribers": 134173, "created_utc": 1697335975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Giving access in Redshift to different users or groups becomes really cumbersome. We have been providing access manually everytime somebody asks for it. Is there a way to automate this process. Any tool you guys using for access management?", "author_fullname": "t2_96l36oyd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage access control in Redshift or any other data warehouse. Is there a way to automate this process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177zkic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697318942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Giving access in Redshift to different users or groups becomes really cumbersome. We have been providing access manually everytime somebody asks for it. Is there a way to automate this process. Any tool you guys using for access management?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "177zkic", "is_robot_indexable": true, "report_reasons": null, "author": "veryuncanny_", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177zkic/how_do_you_manage_access_control_in_redshift_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177zkic/how_do_you_manage_access_control_in_redshift_or/", "subreddit_subscribers": 134173, "created_utc": 1697318942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building an application that accepts exercise data from devices then kicks off a ETL pipeline. This data is then displayed to users. Right now I get really slowed down when I have to test changes and as I add more devices this will only become a bigger problem. Does anyone have experience creating automated tests for something like this.", "author_fullname": "t2_5ntpnb49", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automated testing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_178j1j1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697387181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building an application that accepts exercise data from devices then kicks off a ETL pipeline. This data is then displayed to users. Right now I get really slowed down when I have to test changes and as I add more devices this will only become a bigger problem. Does anyone have experience creating automated tests for something like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178j1j1", "is_robot_indexable": true, "report_reasons": null, "author": "ExtensionResearcher2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178j1j1/automated_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178j1j1/automated_testing/", "subreddit_subscribers": 134173, "created_utc": 1697387181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking for a solution I can integrate into my app that can convert data stored in human readable formats from one schema to another. Ideally it would convert both ways from CSV, JSON, etc, but I need it to also be able to handle relational and nested data. \nSo it would need to be able to nest and un-nest data, flatten and un- flatten data, deal with arrays and hierarchy, etc. convert data from multiple tables into one and back. You get the picture.", "author_fullname": "t2_gjrwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an open source solution that handles complex conversion of data structures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178hvif", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697383892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for a solution I can integrate into my app that can convert data stored in human readable formats from one schema to another. Ideally it would convert both ways from CSV, JSON, etc, but I need it to also be able to handle relational and nested data. \nSo it would need to be able to nest and un-nest data, flatten and un- flatten data, deal with arrays and hierarchy, etc. convert data from multiple tables into one and back. You get the picture.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178hvif", "is_robot_indexable": true, "report_reasons": null, "author": "prutwo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178hvif/is_there_an_open_source_solution_that_handles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178hvif/is_there_an_open_source_solution_that_handles/", "subreddit_subscribers": 134173, "created_utc": 1697383892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are 7 private pyfile modules that have been created.\n\nCurrently there is a main Jupyter notebook which call the private modules and sticks them into an AWS pipeline.\n\nThe run time is 4 minutes for each module.\n\nI\u2019m supposed to cut down the time of the operation. By instead of calling them one by one using a wrapper.\n\nOnly thing I can think of to cut the time down is to generalise the code applicable to each module.", "author_fullname": "t2_bkou9use8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Head is scrambled a little with this current project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178ftgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697386805.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697377934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are 7 private pyfile modules that have been created.&lt;/p&gt;\n\n&lt;p&gt;Currently there is a main Jupyter notebook which call the private modules and sticks them into an AWS pipeline.&lt;/p&gt;\n\n&lt;p&gt;The run time is 4 minutes for each module.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m supposed to cut down the time of the operation. By instead of calling them one by one using a wrapper.&lt;/p&gt;\n\n&lt;p&gt;Only thing I can think of to cut the time down is to generalise the code applicable to each module.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178ftgf", "is_robot_indexable": true, "report_reasons": null, "author": "RagingCharlotte", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178ftgf/head_is_scrambled_a_little_with_this_current/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178ftgf/head_is_scrambled_a_little_with_this_current/", "subreddit_subscribers": 134173, "created_utc": 1697377934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating a medallion based data pipeline and am unsure at what state I should be storing data in the bronze layer.\n\nAs an example, I have json files being streamed into a data lake on s3, should I then read that data in a scheduled stream from the bucket and write it to delta with a defined schema into my bronze layer or should I keep it in the original json format (This seems basically like a pointless copy).\n\nTo me it seems better to do basic schema definition at the early pipeline stage where minimal transformation and enrichment is done as it makes it simpler to handle changes in the data that will inevitably occur.", "author_fullname": "t2_4e7vcqz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Medallion - Databricks Bronze Layer Best Practice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_177x70q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697312149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating a medallion based data pipeline and am unsure at what state I should be storing data in the bronze layer.&lt;/p&gt;\n\n&lt;p&gt;As an example, I have json files being streamed into a data lake on s3, should I then read that data in a scheduled stream from the bucket and write it to delta with a defined schema into my bronze layer or should I keep it in the original json format (This seems basically like a pointless copy).&lt;/p&gt;\n\n&lt;p&gt;To me it seems better to do basic schema definition at the early pipeline stage where minimal transformation and enrichment is done as it makes it simpler to handle changes in the data that will inevitably occur.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "177x70q", "is_robot_indexable": true, "report_reasons": null, "author": "noodlehead13", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/177x70q/medallion_databricks_bronze_layer_best_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/177x70q/medallion_databricks_bronze_layer_best_practice/", "subreddit_subscribers": 134173, "created_utc": 1697312149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Maybe when you meet on a vacation, in a bus. \n(Spare the Tech Stack Question please)", "author_fullname": "t2_t9iw9tl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the first few questions you ask when you come across a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_178jc4k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697387992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe when you meet on a vacation, in a bus. \n(Spare the Tech Stack Question please)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "178jc4k", "is_robot_indexable": true, "report_reasons": null, "author": "Straight-End4310", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178jc4k/whats_the_first_few_questions_you_ask_when_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178jc4k/whats_the_first_few_questions_you_ask_when_you/", "subreddit_subscribers": 134173, "created_utc": 1697387992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hello i am trying to perform some actions using twitter api, for some reason i can't use retweet or like.   \ni authenticate using consumer\\_key , consumer\\_key\\_secret, and user's token , token\\_secret.   \ni can perform other actions like ( reply, tweet,quote ) but only retweet and like get me the following error, i will appreciate it if anyone can help or if anyone is dealing with the same issue.\n\n \"When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.\" ", "author_fullname": "t2_i62lmiqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "twitter api help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_178h6pi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697381954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello i am trying to perform some actions using twitter api, for some reason i can&amp;#39;t use retweet or like.&lt;br/&gt;\ni authenticate using consumer_key , consumer_key_secret, and user&amp;#39;s token , token_secret.&lt;br/&gt;\ni can perform other actions like ( reply, tweet,quote ) but only retweet and like get me the following error, i will appreciate it if anyone can help or if anyone is dealing with the same issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.&amp;quot; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "178h6pi", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Resolution-1025", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/178h6pi/twitter_api_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/178h6pi/twitter_api_help/", "subreddit_subscribers": 134173, "created_utc": 1697381954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm an  developer, and I'll be presenting our new Azure Big Data stack architecture, including Databricks, devops,(medallion )Delta to a group of business analysts and SAP professionals who have limited knowledge of these tools. \n\nWhat are some of the fascinating facts, trivia, or impressive use cases about spark, big data stack that I can include in my keynote to wow the audience or show them the scale of these technologies?\n\nI was thinking of mentioning about Netflix and their used cases of handling the data volume using spark/big data techs.", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help: Fascinating Spark/Big Data, Devops facts for my presentation!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17866lx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1697340206.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1697339357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an  developer, and I&amp;#39;ll be presenting our new Azure Big Data stack architecture, including Databricks, devops,(medallion )Delta to a group of business analysts and SAP professionals who have limited knowledge of these tools. &lt;/p&gt;\n\n&lt;p&gt;What are some of the fascinating facts, trivia, or impressive use cases about spark, big data stack that I can include in my keynote to wow the audience or show them the scale of these technologies?&lt;/p&gt;\n\n&lt;p&gt;I was thinking of mentioning about Netflix and their used cases of handling the data volume using spark/big data techs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17866lx", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17866lx/help_fascinating_sparkbig_data_devops_facts_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17866lx/help_fascinating_sparkbig_data_devops_facts_for/", "subreddit_subscribers": 134173, "created_utc": 1697339357.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}