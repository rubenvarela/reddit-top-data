{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As a data science manager how do you manage your team? Specifically how do you manage your DSs career growth and promotion opportunities? Imagine you have a team of 5 DSs: 2 DS1, 2 DS2, and 1 DS3, where DSX is a Data Scientist 1-4. What is your measure of success - promotions, completed projects, revenue contribution,etc? How do DSX become DSX+1?\n\nSome of my thoughts:\n\n1. As a manager, I can support my DSs by **NOT** micromanaging.  I will track your project and encourage model reviews, code reviews and present final outputs to the team. All necessary skills of a DS.\n\n2. I can ensure my DSs have the skills to mange a project. A DS1 would see many touch points with the manager(me) or a DS3-4 on projects to ensure success, a DS2 less, and DS3 probably none. This in fact is my basis for promotion - shows level of competency on managing projects and deliverables. \n\n3. There can also be project based performance promotion, that is, DS possibly lacking project managing skills but tackles difficult projects and delivers top notch work consistently. \n\n4. The bigger issue is about personal development(PD).  How do managers balance PD against available projects? The DSs may want to gain experience in applying AI or unstructured learning , GPGPU models, specific toolsets like Vertex AI, NLP etc. Your team\u2019s project assignments  may not see this diverse a set of projects. When a project becomes available I balance availability against skill set in order to complete the projects based on delivery times and quarterly goals because these are the measures of success for my team. Typically I fill the void with targeted training courses and allocate time to PD. \n\n5. Some managers think PD is solely the DS\u2019s responsibility. Thoughts?\n\n6. How do you deal with HR when there are no clear DS role descriptions?\n\nNot a simple optimization problem!", "author_fullname": "t2_7ilx2oko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do data scientist managers manage data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_172zdgx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696772308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a data science manager how do you manage your team? Specifically how do you manage your DSs career growth and promotion opportunities? Imagine you have a team of 5 DSs: 2 DS1, 2 DS2, and 1 DS3, where DSX is a Data Scientist 1-4. What is your measure of success - promotions, completed projects, revenue contribution,etc? How do DSX become DSX+1?&lt;/p&gt;\n\n&lt;p&gt;Some of my thoughts:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;As a manager, I can support my DSs by &lt;strong&gt;NOT&lt;/strong&gt; micromanaging.  I will track your project and encourage model reviews, code reviews and present final outputs to the team. All necessary skills of a DS.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can ensure my DSs have the skills to mange a project. A DS1 would see many touch points with the manager(me) or a DS3-4 on projects to ensure success, a DS2 less, and DS3 probably none. This in fact is my basis for promotion - shows level of competency on managing projects and deliverables. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There can also be project based performance promotion, that is, DS possibly lacking project managing skills but tackles difficult projects and delivers top notch work consistently. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The bigger issue is about personal development(PD).  How do managers balance PD against available projects? The DSs may want to gain experience in applying AI or unstructured learning , GPGPU models, specific toolsets like Vertex AI, NLP etc. Your team\u2019s project assignments  may not see this diverse a set of projects. When a project becomes available I balance availability against skill set in order to complete the projects based on delivery times and quarterly goals because these are the measures of success for my team. Typically I fill the void with targeted training courses and allocate time to PD. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Some managers think PD is solely the DS\u2019s responsibility. Thoughts?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How do you deal with HR when there are no clear DS role descriptions?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Not a simple optimization problem!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "172zdgx", "is_robot_indexable": true, "report_reasons": null, "author": "cazzobomba", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/172zdgx/how_do_data_scientist_managers_manage_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/172zdgx/how_do_data_scientist_managers_manage_data/", "subreddit_subscribers": 1077311, "created_utc": 1696772308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm an SWE (**not a data scientist**) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.\n\nI started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don't think they solve the issue of validating *changes in data* (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.\n\nWhat I'm looking for is a tool that validates changes in data by comparing the previous value with the new value.\n\nIn some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there's obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).\n\nThis is just an example, but it would be helpful if we can call an API to do this sort of validation for us.\n\nAnd instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I'm just brainstorming here.\n\nWould highly appreciate some recommendations/tips for tackling this problem. Thank you!", "author_fullname": "t2_mxg44sgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to validate data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173broc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696808736.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696803736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an SWE (&lt;strong&gt;not a data scientist&lt;/strong&gt;) and trying to build a generic data validation tool (or find appropriate tools to adopt) for my company.&lt;/p&gt;\n\n&lt;p&gt;I started looking into libraries such as Great Expectations, Pydantic, etc.. And they all seem promising, but I don&amp;#39;t think they solve the issue of validating &lt;em&gt;changes in data&lt;/em&gt; (as far as I can tell). They seem to be good at validating that data is within an expected range, of an expected type, etc., but I need a little more.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a tool that validates changes in data by comparing the previous value with the new value.&lt;/p&gt;\n\n&lt;p&gt;In some of our applications, new data is first pumped into a staging table. We then calculate relative change % between the staging and target table (for each field), and if the change is higher than some threshold, validation fails. But there&amp;#39;s obviously a lot of issues with this (like in cases where a change from 1 to 18 is normal but produces a percent change of 1700).&lt;/p&gt;\n\n&lt;p&gt;This is just an example, but it would be helpful if we can call an API to do this sort of validation for us.&lt;/p&gt;\n\n&lt;p&gt;And instead of using absolute change, relative change, etc... is there perhaps a tool that can validate based on historical changes? Perhaps by capturing changes for some set time and using that information to validate future changes? I&amp;#39;m just brainstorming here.&lt;/p&gt;\n\n&lt;p&gt;Would highly appreciate some recommendations/tips for tackling this problem. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "173broc", "is_robot_indexable": true, "report_reasons": null, "author": "Sensitive-Main-6700", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/173broc/how_to_validate_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/173broc/how_to_validate_data/", "subreddit_subscribers": 1077311, "created_utc": 1696803736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_p59o0l5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is currently the most in demand Analytics/DS by Healthcare institutions (hospitals, clinics, big pharma, government, etc.)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173cxz4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696806888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "173cxz4", "is_robot_indexable": true, "report_reasons": null, "author": "EcstaticStructure830", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/173cxz4/what_is_currently_the_most_in_demand_analyticsds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/173cxz4/what_is_currently_the_most_in_demand_analyticsds/", "subreddit_subscribers": 1077311, "created_utc": 1696806888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm starting out on a team that is very collaborative and I've realized that while I've worked with other people before, I'm not used to doing it the way they do, where a project could be divided up into lots of smaller parts and it might not be me on every one of those parts. \n\nDoes anyone have advice for dealing with what almost feels like getting territorial over a model? It's nothing against the people on my team - they've all been there for longer than me and are much smarter than me. I just am used to seeing things 100% of the way and I took a lot of pride in being able to look at a finished thing and be like \"I built that.\" It also almost feels like it's my fault for not being able to do all of the work myself, like if I was a better worker I'd be able to get more of the work done and people wouldn't have to pick up my slack.\n\nIs this something that just goes away with time if you continue working on a team that works in this way? I didn't expect there to be an emotional challenge component to this and I'm struggling to know what to do and how to adapt, especially because this doesn't feel like the kind of thing you can really share/get support from coworkers on, because they're the ones working on it with me if that makes sense.", "author_fullname": "t2_m3zm5vg6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What advice would you give someone starting out on learning to collaborate on large projects and not be the sole person responsible for a model build?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173dd6h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696808065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting out on a team that is very collaborative and I&amp;#39;ve realized that while I&amp;#39;ve worked with other people before, I&amp;#39;m not used to doing it the way they do, where a project could be divided up into lots of smaller parts and it might not be me on every one of those parts. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have advice for dealing with what almost feels like getting territorial over a model? It&amp;#39;s nothing against the people on my team - they&amp;#39;ve all been there for longer than me and are much smarter than me. I just am used to seeing things 100% of the way and I took a lot of pride in being able to look at a finished thing and be like &amp;quot;I built that.&amp;quot; It also almost feels like it&amp;#39;s my fault for not being able to do all of the work myself, like if I was a better worker I&amp;#39;d be able to get more of the work done and people wouldn&amp;#39;t have to pick up my slack.&lt;/p&gt;\n\n&lt;p&gt;Is this something that just goes away with time if you continue working on a team that works in this way? I didn&amp;#39;t expect there to be an emotional challenge component to this and I&amp;#39;m struggling to know what to do and how to adapt, especially because this doesn&amp;#39;t feel like the kind of thing you can really share/get support from coworkers on, because they&amp;#39;re the ones working on it with me if that makes sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "173dd6h", "is_robot_indexable": true, "report_reasons": null, "author": "Champaign__Supernova", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/173dd6h/what_advice_would_you_give_someone_starting_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/173dd6h/what_advice_would_you_give_someone_starting_out/", "subreddit_subscribers": 1077311, "created_utc": 1696808065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A friend of mine asked me to see if there was a way to automatically add labels to customer complaints based on the text in the complaint. Presently, on a monthly basis they read every customer complaint and manually apply a label based on their judgement of what it is. There is a specific set of labels they use to classify their complaints.\n\nThis seems like a problem for NLP but I'm unsure of where to start or just not confident. It's been at least 7 years since I've done any real 'data science' stuff. The data is tidy, I can read it into a data frame. I know there are a number of tutorials online that discuss stemming, lemmatization, and other factors so I think I can get some of those basic steps down. But I would be happy if you had a specific guidebook that you've used that you like and could share.\n\nAm I oversimplifying this or overly confident? I should be able to build a model that tries to applies the same labels they previously applied manually but automatically with this program. Am I thinking about this correctly?\n\nI'm really not certain what the best tools in R to use for this are. Back when I did I used caret, keras, SnowballRC and some other things like dplyr. I'm not certain what models or validation approaches to use either. Are there any good guides that a simpleton like me could use to build a relatively confident validation stage?\n\nThanks for your thoughtfulness on this :)", "author_fullname": "t2_viztxm6i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to automate the labeling of strings of text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173cl4s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696805952.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A friend of mine asked me to see if there was a way to automatically add labels to customer complaints based on the text in the complaint. Presently, on a monthly basis they read every customer complaint and manually apply a label based on their judgement of what it is. There is a specific set of labels they use to classify their complaints.&lt;/p&gt;\n\n&lt;p&gt;This seems like a problem for NLP but I&amp;#39;m unsure of where to start or just not confident. It&amp;#39;s been at least 7 years since I&amp;#39;ve done any real &amp;#39;data science&amp;#39; stuff. The data is tidy, I can read it into a data frame. I know there are a number of tutorials online that discuss stemming, lemmatization, and other factors so I think I can get some of those basic steps down. But I would be happy if you had a specific guidebook that you&amp;#39;ve used that you like and could share.&lt;/p&gt;\n\n&lt;p&gt;Am I oversimplifying this or overly confident? I should be able to build a model that tries to applies the same labels they previously applied manually but automatically with this program. Am I thinking about this correctly?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really not certain what the best tools in R to use for this are. Back when I did I used caret, keras, SnowballRC and some other things like dplyr. I&amp;#39;m not certain what models or validation approaches to use either. Are there any good guides that a simpleton like me could use to build a relatively confident validation stage?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your thoughtfulness on this :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "173cl4s", "is_robot_indexable": true, "report_reasons": null, "author": "rationally_speaking", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/173cl4s/is_it_possible_to_automate_the_labeling_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/173cl4s/is_it_possible_to_automate_the_labeling_of/", "subreddit_subscribers": 1077311, "created_utc": 1696805952.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Where is the best place to run an ARIMA model? I have done all the work in python to determine the best parameters but it is so confusing to actually fit the model correctly. Thanks!", "author_fullname": "t2_cjm93tal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running ARIMA Models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17369yn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696789910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where is the best place to run an ARIMA model? I have done all the work in python to determine the best parameters but it is so confusing to actually fit the model correctly. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17369yn", "is_robot_indexable": true, "report_reasons": null, "author": "fhckgkgkgjdh", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17369yn/running_arima_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17369yn/running_arima_models/", "subreddit_subscribers": 1077311, "created_utc": 1696789910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a beginner trying to create a Model with Image detection using Convolutional Neural Network. I have a project in mind where I would detect the type of banknotes. I have already collected some images to be used but as far as i know. I need to annotate it and then train it. \n\nI don't know how will i link the annotated JSON file of the images when training. Does anyone know how?", "author_fullname": "t2_okaww79i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Image Detection with CNN Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17316vc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696777078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a beginner trying to create a Model with Image detection using Convolutional Neural Network. I have a project in mind where I would detect the type of banknotes. I have already collected some images to be used but as far as i know. I need to annotate it and then train it. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know how will i link the annotated JSON file of the images when training. Does anyone know how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17316vc", "is_robot_indexable": true, "report_reasons": null, "author": "AutomaticResearch337", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17316vc/image_detection_with_cnn_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17316vc/image_detection_with_cnn_model/", "subreddit_subscribers": 1077311, "created_utc": 1696777078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I've been given this task to create clustering on users dataset. The model itself performs well but the management wants me to somehow automate the output/insights so it can be translated to other datasets too. I expressed my worries for them as I don't think that it is possible but I was trying my luck here to see maybe there is a method/idea which I am not aware of?\n\nThe only thing I could come up with is looping for each cluster and finding if there is a feature which has a value count of more than 90% (or any threshold) and just saving the cluster-feature-value trio that is answering this condition. I don't know how much I'm up for that method because its very technical and automatic and might miss valuable (for example - If I have a country feature, and let's say if I have 50 countries in a cluster. Maybe the prevelance of all countries is equal to 2% but because 49 of the 50 countries are from Asia so it means 98% of them are from Asia which is a valuable information I am missing).\n\nIs there even any method to do that? Or should I just insist that it is not feasible?  \nThanks", "author_fullname": "t2_3hqmko1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automation of insights extraction from Clustering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1731r1r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696778383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been given this task to create clustering on users dataset. The model itself performs well but the management wants me to somehow automate the output/insights so it can be translated to other datasets too. I expressed my worries for them as I don&amp;#39;t think that it is possible but I was trying my luck here to see maybe there is a method/idea which I am not aware of?&lt;/p&gt;\n\n&lt;p&gt;The only thing I could come up with is looping for each cluster and finding if there is a feature which has a value count of more than 90% (or any threshold) and just saving the cluster-feature-value trio that is answering this condition. I don&amp;#39;t know how much I&amp;#39;m up for that method because its very technical and automatic and might miss valuable (for example - If I have a country feature, and let&amp;#39;s say if I have 50 countries in a cluster. Maybe the prevelance of all countries is equal to 2% but because 49 of the 50 countries are from Asia so it means 98% of them are from Asia which is a valuable information I am missing).&lt;/p&gt;\n\n&lt;p&gt;Is there even any method to do that? Or should I just insist that it is not feasible?&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1731r1r", "is_robot_indexable": true, "report_reasons": null, "author": "nuriel8833", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1731r1r/automation_of_insights_extraction_from_clustering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1731r1r/automation_of_insights_extraction_from_clustering/", "subreddit_subscribers": 1077311, "created_utc": 1696778383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "bit more context\u2014 me and my groupmates are conducting a study in which we would determine a person's MBTI (a personality classification method) based on their posts on twitter using sentiment analysis. \n\nsince our research focuses on personality classification instead of identifying a statement's positive and negative connotations, we decided to exclude sarcasm out of the equation since we treat every user's word as a determining factor of their MBTI. but our thesis moderator asked the concern regarding sarcasm out of curiosity and we still have quite some struggles defending this idea.\n\nany help would be appreciated, thanks!", "author_fullname": "t2_dsy6u36zv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "a controversial request, but please help me out in defending that sarcasm doesn't affect sentiment analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_172subu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696749194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;bit more context\u2014 me and my groupmates are conducting a study in which we would determine a person&amp;#39;s MBTI (a personality classification method) based on their posts on twitter using sentiment analysis. &lt;/p&gt;\n\n&lt;p&gt;since our research focuses on personality classification instead of identifying a statement&amp;#39;s positive and negative connotations, we decided to exclude sarcasm out of the equation since we treat every user&amp;#39;s word as a determining factor of their MBTI. but our thesis moderator asked the concern regarding sarcasm out of curiosity and we still have quite some struggles defending this idea.&lt;/p&gt;\n\n&lt;p&gt;any help would be appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "172subu", "is_robot_indexable": true, "report_reasons": null, "author": "shostakophiles", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/172subu/a_controversial_request_but_please_help_me_out_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/172subu/a_controversial_request_but_please_help_me_out_in/", "subreddit_subscribers": 1077311, "created_utc": 1696749194.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}