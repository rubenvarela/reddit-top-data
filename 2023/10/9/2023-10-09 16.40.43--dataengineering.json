{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had to leav my $130K remote job in August after 6 toxic months.  Had to take a 100% on site gig for $110K last month at a bad company to keep food on the table. Been still interviewing heavily to find something better. \n\nWhen I was casually looking around late last year, jobs were around $140K - $160K at my experience level, but I couldn\u2019t land anything. \n\n Even though I make $110K now, I tell recruiters I make $150K in my new role. Most refute and say that their max budget is $120Kish and full on site. \n\nThis is ~20% lower pay than what was being thrown around late last year, and on site. Anyone experiencing similiar?\n\nI\u2019m also might just be in a bad spot since I had to take this new gig, and people see my 2 short tenures in a row as a red flag. Advice?", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone Else Seeing Salaries Collapse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173fj4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696814480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had to leav my $130K remote job in August after 6 toxic months.  Had to take a 100% on site gig for $110K last month at a bad company to keep food on the table. Been still interviewing heavily to find something better. &lt;/p&gt;\n\n&lt;p&gt;When I was casually looking around late last year, jobs were around $140K - $160K at my experience level, but I couldn\u2019t land anything. &lt;/p&gt;\n\n&lt;p&gt;Even though I make $110K now, I tell recruiters I make $150K in my new role. Most refute and say that their max budget is $120Kish and full on site. &lt;/p&gt;\n\n&lt;p&gt;This is ~20% lower pay than what was being thrown around late last year, and on site. Anyone experiencing similiar?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also might just be in a bad spot since I had to take this new gig, and people see my 2 short tenures in a row as a red flag. Advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173fj4h", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173fj4h/anyone_else_seeing_salaries_collapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173fj4h/anyone_else_seeing_salaries_collapse/", "subreddit_subscribers": 132883, "created_utc": 1696814480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have recently tried snowflake in my new company after using spark for several years.\n\nPersonally, I really like that it delivers what it promises, but I don't like lack of customization and the fact their support/sales have a know-it-all attitude and they ask you to trust everything works automagically.\n\nI am also concerned that in the long term this very strong lock-in in pair with a revamp of billing policies (think about dbt cloud) will cause many troubles to data teams. I already don't like the pricing of warehouses where you can only double the amount of spent credits to improve query performance...\n\nIs this a legitimate concern? With databricks you can always opt out and migrate your spark/delta codebase somewhere else. With snowflake I don't see this option.", "author_fullname": "t2_jni6yu7mo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nkx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696843711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have recently tried snowflake in my new company after using spark for several years.&lt;/p&gt;\n\n&lt;p&gt;Personally, I really like that it delivers what it promises, but I don&amp;#39;t like lack of customization and the fact their support/sales have a know-it-all attitude and they ask you to trust everything works automagically.&lt;/p&gt;\n\n&lt;p&gt;I am also concerned that in the long term this very strong lock-in in pair with a revamp of billing policies (think about dbt cloud) will cause many troubles to data teams. I already don&amp;#39;t like the pricing of warehouses where you can only double the amount of spent credits to improve query performance...&lt;/p&gt;\n\n&lt;p&gt;Is this a legitimate concern? With databricks you can always opt out and migrate your spark/delta codebase somewhere else. With snowflake I don&amp;#39;t see this option.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173nkx2", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Data-810", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nkx2/what_are_your_thoughts_on_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nkx2/what_are_your_thoughts_on_snowflake/", "subreddit_subscribers": 132883, "created_utc": 1696843711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\ud83d\ude80 Exciting News! Just released my latest YouTube video - \"PySpark Tutorial for Beginners: 1-Hour Full Course\" \ud83d\udc0d\ud83d\udca1\n\nAre you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I'll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.\n\n\ud83d\udd25 What You'll Learn:\n\u2705 Spark Introduction\n\u2705 Spark Installation\n\u2705 Setting Up Your PySpark Environment\n\u2705 Spark RDD\n\u2705 DataFrame Operations\n\u2705 Spark SQL\n\u2705 And much more!\n\nWhether you're a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!\n\nWatch it now \ud83d\udc49 https://youtu.be/EB8lfdxpirM\nGitHub Repo \ud83d\udc49 https://github.com/coder2j/pyspark-tutorial\n\nDon't forget to like, subscribe, and share with your network. Let's spread the knowledge together! \ud83d\udcda\ud83d\udcaa\n#PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial", "author_fullname": "t2_h4j43yry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Tutorial for Beginners: 1-Hour Full Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_173lca6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "PySpark Tutorial for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/EB8lfdxpirM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/173lca6", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nyeAsJ_Iyn208srWSK-sRJw_-S14SsgknHSuV_YPKCM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696834356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\ude80 Exciting News! Just released my latest YouTube video - &amp;quot;PySpark Tutorial for Beginners: 1-Hour Full Course&amp;quot; \ud83d\udc0d\ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;Are you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I&amp;#39;ll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd25 What You&amp;#39;ll Learn:\n\u2705 Spark Introduction\n\u2705 Spark Installation\n\u2705 Setting Up Your PySpark Environment\n\u2705 Spark RDD\n\u2705 DataFrame Operations\n\u2705 Spark SQL\n\u2705 And much more!&lt;/p&gt;\n\n&lt;p&gt;Whether you&amp;#39;re a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!&lt;/p&gt;\n\n&lt;p&gt;Watch it now \ud83d\udc49 &lt;a href=\"https://youtu.be/EB8lfdxpirM\"&gt;https://youtu.be/EB8lfdxpirM&lt;/a&gt;\nGitHub Repo \ud83d\udc49 &lt;a href=\"https://github.com/coder2j/pyspark-tutorial\"&gt;https://github.com/coder2j/pyspark-tutorial&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t forget to like, subscribe, and share with your network. Let&amp;#39;s spread the knowledge together! \ud83d\udcda\ud83d\udcaa&lt;/p&gt;\n\n&lt;h1&gt;PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/EB8lfdxpirM", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?auto=webp&amp;s=721ceed222baa9b21597759f40ad168578d0ee74", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9318f73383bcd738f86d14479864d192ae47b270", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6616b558982a89e258beb94901e9d212ce6bf76a", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/82Ln0Z8hZhrRQ6_8TIvFLpE_UPEKESS_TxaZOyjr6Ig.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b465b2e80a183e03df5ef8d4f758d78f53dd3454", "width": 320, "height": 240}], "variants": {}, "id": "wp5OUtzYt-eYRgOkWVPGa-zaQJ8PGPB2r-t4qza7MPs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173lca6", "is_robot_indexable": true, "report_reasons": null, "author": "Coder2j", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173lca6/pyspark_tutorial_for_beginners_1hour_full_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/EB8lfdxpirM", "subreddit_subscribers": 132883, "created_utc": 1696834356.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "PySpark Tutorial for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/EB8lfdxpirM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"PySpark Tutorial for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/EB8lfdxpirM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Umm so I am kind of stuck  at where I am as a Data Engineer or Data Platform Engineer or whether those two should even be seperate roles. \n\nMy current employer is a IT service provider and I joined as a Data Engineer associate straight outta college (not even CS background, got lucky I guess). They trained us in the basics SQL/Python and then I got onboarded on a project focusing on AWS EMR and Databricks and the complete AWS stack. To my luck I was onboarded on the platform team so I learnt quite a bit of DevOps on the fly. \n\n3 years later, I set up the entire Databricks infrastructure here, and created the SOPs for development teams to follow. But I haven't written a single line of code myself. Worked purely on architecture and benchmarkings (like what's better, what's cheaper etc, which has more security protocols) \n\nHere comes the question I realised I am severely underpaid, but no where I go I can say my experience matches apples to apples. \n\nI don't even know what my role classifies as and how should I be preparing for the next role. Any help? any ideas? Kinda fried my brains rn.", "author_fullname": "t2_4c9picnu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's like a realistic amount of time when you are considered as an experienced / senior Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173dp8t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696808994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Umm so I am kind of stuck  at where I am as a Data Engineer or Data Platform Engineer or whether those two should even be seperate roles. &lt;/p&gt;\n\n&lt;p&gt;My current employer is a IT service provider and I joined as a Data Engineer associate straight outta college (not even CS background, got lucky I guess). They trained us in the basics SQL/Python and then I got onboarded on a project focusing on AWS EMR and Databricks and the complete AWS stack. To my luck I was onboarded on the platform team so I learnt quite a bit of DevOps on the fly. &lt;/p&gt;\n\n&lt;p&gt;3 years later, I set up the entire Databricks infrastructure here, and created the SOPs for development teams to follow. But I haven&amp;#39;t written a single line of code myself. Worked purely on architecture and benchmarkings (like what&amp;#39;s better, what&amp;#39;s cheaper etc, which has more security protocols) &lt;/p&gt;\n\n&lt;p&gt;Here comes the question I realised I am severely underpaid, but no where I go I can say my experience matches apples to apples. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t even know what my role classifies as and how should I be preparing for the next role. Any help? any ideas? Kinda fried my brains rn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173dp8t", "is_robot_indexable": true, "report_reasons": null, "author": "Gora_HabshiYoYo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173dp8t/whats_like_a_realistic_amount_of_time_when_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173dp8t/whats_like_a_realistic_amount_of_time_when_you/", "subreddit_subscribers": 132883, "created_utc": 1696808994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you do freelance work or consultancy in data engineering/analytics? I have some questions:\n\n* How do you deliver the work to customers, for example an ETL pipeline? (do they assemble everything on the client including orchestrator, python, database, etc?) - this really worry me because I think that i will need to setup all the environment in their side and this could be really complex to do.\n* What services do you do?\n* What are the most requested services?\n* Which ones would you like to do, but don't have time to learn/pratice?\n* Which ones you don't do because you don't want to and it doesn't pay well?\n* Can you describe an example of a project you have done?\n* Can you share the areas/niches (size of companies) that you usually work with?\n\n&amp;#x200B;\n\nContext: i did some freelancing projects in the past but were more related to data cleaning and data transformations. And now I want to start something from zero and offer more and create internet presence to share my projects. I have a few years of experience too.", "author_fullname": "t2_6ip8cun1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1737dp9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696793122.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696792618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you do freelance work or consultancy in data engineering/analytics? I have some questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do you deliver the work to customers, for example an ETL pipeline? (do they assemble everything on the client including orchestrator, python, database, etc?) - this really worry me because I think that i will need to setup all the environment in their side and this could be really complex to do.&lt;/li&gt;\n&lt;li&gt;What services do you do?&lt;/li&gt;\n&lt;li&gt;What are the most requested services?&lt;/li&gt;\n&lt;li&gt;Which ones would you like to do, but don&amp;#39;t have time to learn/pratice?&lt;/li&gt;\n&lt;li&gt;Which ones you don&amp;#39;t do because you don&amp;#39;t want to and it doesn&amp;#39;t pay well?&lt;/li&gt;\n&lt;li&gt;Can you describe an example of a project you have done?&lt;/li&gt;\n&lt;li&gt;Can you share the areas/niches (size of companies) that you usually work with?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Context: i did some freelancing projects in the past but were more related to data cleaning and data transformations. And now I want to start something from zero and offer more and create internet presence to share my projects. I have a few years of experience too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1737dp9", "is_robot_indexable": true, "report_reasons": null, "author": "23am50", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1737dp9/data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1737dp9/data_engineers/", "subreddit_subscribers": 132883, "created_utc": 1696792618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am defining data validation standards for our data pipelines  on ingestion. I am planning that checks should be done in the pipeline itself. So far I have this list:\n\n1. Check for schema changes\n2. Check if received data matches the agreed schema\n3. Nulls, Unique, where applicable\n4. Check for integrity by comparing to other datasets (like that FK and master tables\n5. Maybe check counts vs some periodic averages\n\n&amp;#x200B;\n\nAny other recommendations? Even if we aim to ingest raw data, we will want to check its validity before storing. These are for structured data from structured sources that will be stored in a columnar database.\n\nAlso, maybe those would be an overkill? I am not a DE, but I am responsible for Data Quality. So looking for some practical suggestions that would not seem too crazy for the team.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_49dbxejy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for data validation on data ingestion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173kp9o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696831831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am defining data validation standards for our data pipelines  on ingestion. I am planning that checks should be done in the pipeline itself. So far I have this list:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Check for schema changes&lt;/li&gt;\n&lt;li&gt;Check if received data matches the agreed schema&lt;/li&gt;\n&lt;li&gt;Nulls, Unique, where applicable&lt;/li&gt;\n&lt;li&gt;Check for integrity by comparing to other datasets (like that FK and master tables&lt;/li&gt;\n&lt;li&gt;Maybe check counts vs some periodic averages&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations? Even if we aim to ingest raw data, we will want to check its validity before storing. These are for structured data from structured sources that will be stored in a columnar database.&lt;/p&gt;\n\n&lt;p&gt;Also, maybe those would be an overkill? I am not a DE, but I am responsible for Data Quality. So looking for some practical suggestions that would not seem too crazy for the team.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173kp9o", "is_robot_indexable": true, "report_reasons": null, "author": "HereJustForAnswers", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173kp9o/ideas_for_data_validation_on_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173kp9o/ideas_for_data_validation_on_data_ingestion/", "subreddit_subscribers": 132883, "created_utc": 1696831831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I once worked in a medium sized company where there's no data or BI platform to store sql related things. And there's only 2 data+BI people so we work pretty freestyle. \n\nI'm in charge of a mix of data &amp; BI role. the challenge I'm facing are primarily related to BI I guess.\n\nI processed about 10-20 queries each day and ended up copy-pasting everything from my sql client to notion ( to store some context about the task and sometimes the screenshot of the visualization)\n\nBut once the modifications of the same requirement comes back I needed to copy paste things back from the notepad into the sql client. Not to mention I need to make visualizations and share them using Google sheet.\n\nIt still kinda bugs me today. I wonder how do you guys manage those workflows when you don't have a good infra?", "author_fullname": "t2_cr04g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "where do you store your sql queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nrq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696848209.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696844494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I once worked in a medium sized company where there&amp;#39;s no data or BI platform to store sql related things. And there&amp;#39;s only 2 data+BI people so we work pretty freestyle. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in charge of a mix of data &amp;amp; BI role. the challenge I&amp;#39;m facing are primarily related to BI I guess.&lt;/p&gt;\n\n&lt;p&gt;I processed about 10-20 queries each day and ended up copy-pasting everything from my sql client to notion ( to store some context about the task and sometimes the screenshot of the visualization)&lt;/p&gt;\n\n&lt;p&gt;But once the modifications of the same requirement comes back I needed to copy paste things back from the notepad into the sql client. Not to mention I need to make visualizations and share them using Google sheet.&lt;/p&gt;\n\n&lt;p&gt;It still kinda bugs me today. I wonder how do you guys manage those workflows when you don&amp;#39;t have a good infra?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173nrq2", "is_robot_indexable": true, "report_reasons": null, "author": "jchnxu", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nrq2/where_do_you_store_your_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nrq2/where_do_you_store_your_sql_queries/", "subreddit_subscribers": 132883, "created_utc": 1696844494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI took a two year break to help my dad with his business. I was working at T-mobile as a data engineer dealing with traditional SQL framework (Cubes, ETL, Warehousing). \n\nNow I feel like the game has changed completely and my skill sets are obsolete. \n\nIf my aim to get a job at FAANG, can you guys point me to some reading materials/certification/technology stack I should upgrade my self with? \n\nI know my way around SQL, Python (scripting).", "author_fullname": "t2_532r23kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prepping for Data Engineering interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173i7qa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696822714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I took a two year break to help my dad with his business. I was working at T-mobile as a data engineer dealing with traditional SQL framework (Cubes, ETL, Warehousing). &lt;/p&gt;\n\n&lt;p&gt;Now I feel like the game has changed completely and my skill sets are obsolete. &lt;/p&gt;\n\n&lt;p&gt;If my aim to get a job at FAANG, can you guys point me to some reading materials/certification/technology stack I should upgrade my self with? &lt;/p&gt;\n\n&lt;p&gt;I know my way around SQL, Python (scripting).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173i7qa", "is_robot_indexable": true, "report_reasons": null, "author": "Dunklerverfaast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173i7qa/prepping_for_data_engineering_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173i7qa/prepping_for_data_engineering_interviews/", "subreddit_subscribers": 132883, "created_utc": 1696822714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.", "author_fullname": "t2_uqyn3qdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an equivalent of \u201cThe Data Warehouse Toolkit\u201d but for Lakehouses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173sl3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696860130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know kimball has been around a lot longer but what I mean is there a bible or good resource of some sort.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173sl3s", "is_robot_indexable": true, "report_reasons": null, "author": "variance-explained", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173sl3s/is_there_an_equivalent_of_the_data_warehouse/", "subreddit_subscribers": 132883, "created_utc": 1696860130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI've been working as an MLE for some time now and realized that there's a lack of a scalable framework that can create API endpoints with dynamic queries. And found that a lot of existing solutions just didn't hit the sweet spot in terms of flexibility and ease of use.\n\nSo... I ended up creating one from scratch! It's called **Squirrels**. I've integrated some neat features like dynamic SQL &amp; Python query generation with Jinja templating, cascading parameters, and in-memory caching.\n\nTogether with my college, we recently published it on PyPI with a corresponding documentation page, and we're aiming to make it into a full-fledged solution so would genuinely love to have some of you try it out. Please let us know if you have any feedback, good or bad. I'm curious to see how it might fit into some of your workflows or where we can improve.\n\n&amp;#x200B;\n\nHere's the link to the GitHub page:\n\n[squirrels-nest/squirrels (github.com)](https://github.com/squirrels-nest/squirrels)\n\nAnd the documentation page:\n\n[Squirrels Documentation (squirrels-nest.github.io)](https://squirrels-nest.github.io/squirrels-docs/)\n\nWe even made a YouTube channel to house some tutorials if y'all are interested:   \n [Squirrels - YouTube](https://www.youtube.com/@Squirrels-hk7rj/featured) \n\n&amp;#x200B;\n\nThanks in advance for giving it a whirl! \ud83c\udf7b\ud83d\udc3f\ufe0f\n\n&amp;#x200B;\n\n(Disclaimer for rule no.8: we're not an entity of any sort, as Squirrels is only a side project that we're working on)", "author_fullname": "t2_kjvfum79o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Made a Python package for creating API endpoints with dynamic queries.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173k91u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696830117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as an MLE for some time now and realized that there&amp;#39;s a lack of a scalable framework that can create API endpoints with dynamic queries. And found that a lot of existing solutions just didn&amp;#39;t hit the sweet spot in terms of flexibility and ease of use.&lt;/p&gt;\n\n&lt;p&gt;So... I ended up creating one from scratch! It&amp;#39;s called &lt;strong&gt;Squirrels&lt;/strong&gt;. I&amp;#39;ve integrated some neat features like dynamic SQL &amp;amp; Python query generation with Jinja templating, cascading parameters, and in-memory caching.&lt;/p&gt;\n\n&lt;p&gt;Together with my college, we recently published it on PyPI with a corresponding documentation page, and we&amp;#39;re aiming to make it into a full-fledged solution so would genuinely love to have some of you try it out. Please let us know if you have any feedback, good or bad. I&amp;#39;m curious to see how it might fit into some of your workflows or where we can improve.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to the GitHub page:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/squirrels-nest/squirrels\"&gt;squirrels-nest/squirrels (github.com)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the documentation page:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://squirrels-nest.github.io/squirrels-docs/\"&gt;Squirrels Documentation (squirrels-nest.github.io)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We even made a YouTube channel to house some tutorials if y&amp;#39;all are interested:&lt;br/&gt;\n &lt;a href=\"https://www.youtube.com/@Squirrels-hk7rj/featured\"&gt;Squirrels - YouTube&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for giving it a whirl! \ud83c\udf7b\ud83d\udc3f\ufe0f&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(Disclaimer for rule no.8: we&amp;#39;re not an entity of any sort, as Squirrels is only a side project that we&amp;#39;re working on)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "173k91u", "is_robot_indexable": true, "report_reasons": null, "author": "squirrels-api", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173k91u/made_a_python_package_for_creating_api_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173k91u/made_a_python_package_for_creating_api_endpoints/", "subreddit_subscribers": 132883, "created_utc": 1696830117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a simple data pipeline that reads data from an API every minute and sends it through Kafka to AWS S3 and DynamoDB for some analytics. The goal is to showcase this project in my resume in a month or two. \n\nThe throughput is quite low (basically 5-6 records per minute), I know that Kafka might be overhead but I just want to showcase my skills with it. Which option would be the most affordable to deploy the Kafka server/cluster (although a single instance would be enough) and run it for 6 months : \n\n\\- Confluent Cloud (they have a free option, but you still pay for the infrastructure which is more than what's required for this simple use case).\n\n\\- A single EC2 instance running Kafka\n\n\\- A Kafka docker container in AWS ECS. \n\n&amp;#x200B;", "author_fullname": "t2_nq65wse7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best affordable way to deploy host a Kafka setup for a toy project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1739kqr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696798220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a simple data pipeline that reads data from an API every minute and sends it through Kafka to AWS S3 and DynamoDB for some analytics. The goal is to showcase this project in my resume in a month or two. &lt;/p&gt;\n\n&lt;p&gt;The throughput is quite low (basically 5-6 records per minute), I know that Kafka might be overhead but I just want to showcase my skills with it. Which option would be the most affordable to deploy the Kafka server/cluster (although a single instance would be enough) and run it for 6 months : &lt;/p&gt;\n\n&lt;p&gt;- Confluent Cloud (they have a free option, but you still pay for the infrastructure which is more than what&amp;#39;s required for this simple use case).&lt;/p&gt;\n\n&lt;p&gt;- A single EC2 instance running Kafka&lt;/p&gt;\n\n&lt;p&gt;- A Kafka docker container in AWS ECS. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1739kqr", "is_robot_indexable": true, "report_reasons": null, "author": "lancelot_of_camelot", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1739kqr/best_affordable_way_to_deploy_host_a_kafka_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1739kqr/best_affordable_way_to_deploy_host_a_kafka_setup/", "subreddit_subscribers": 132883, "created_utc": 1696798220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to document the data flows of a BI product using a data lineage tool. Basically the to-be-documented flow is as follows: copy data from ERP source --&gt; transform data using SQL --&gt; store in data warehouse --&gt; load in Power BI --&gt; do some small calculations in Power BI.\n\nI have looked into tools which can do this, however many consist of large data catalog/data management systems such as Atlan. These large packages however I do not need. I am only looking for a tool in which I can manually or automatically extract and document the lineage of my data flow, that's all. \n\nDoes anyone have any tips or experience with using these specific types of tools? All information is appreciated!", "author_fullname": "t2_3tjzfxzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lineage tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173o66v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696846089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to document the data flows of a BI product using a data lineage tool. Basically the to-be-documented flow is as follows: copy data from ERP source --&amp;gt; transform data using SQL --&amp;gt; store in data warehouse --&amp;gt; load in Power BI --&amp;gt; do some small calculations in Power BI.&lt;/p&gt;\n\n&lt;p&gt;I have looked into tools which can do this, however many consist of large data catalog/data management systems such as Atlan. These large packages however I do not need. I am only looking for a tool in which I can manually or automatically extract and document the lineage of my data flow, that&amp;#39;s all. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any tips or experience with using these specific types of tools? All information is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173o66v", "is_robot_indexable": true, "report_reasons": null, "author": "basr98", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173o66v/data_lineage_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173o66v/data_lineage_tools/", "subreddit_subscribers": 132883, "created_utc": 1696846089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.\n\nHowever, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don't use it much). The only downside is that the bank doesn't work with cloud solutions, and I'm not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.\n\nOn the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don't want to give up that part of my job), and are willing to provide the necessary training.\n\nThe salaries for both positions are the same, as the company will also give me a raise.\n\nHere's my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that's not true and starting with an on prem solution will be better. What do you think about this?\n\nThanks for all the advice!", "author_fullname": "t2_7jc8db23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173r3am", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696856073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI would like to ask for career advice.\nI have been working as a Power Bi programmer in a large international consulting firm for the past year.\nMy goal for the future is to work in the Business Intelligence area on the backend / data engineer side.&lt;/p&gt;\n\n&lt;p&gt;However, I recently received a job offer from a bank for a Business Analyst/Reporting Specialist position. In this role, I would be responsible for creating and enhancing reports using Power BI, delving into Oracle, MSSQL and Hadoop. The offer also includes the opportunity to learn Python for data processing and visualization (but this was an add-on option in the offer, so they probably don&amp;#39;t use it much). The only downside is that the bank doesn&amp;#39;t work with cloud solutions, and I&amp;#39;m not sure how much of my work will be related to data processing, and how much will be related to visualization and working with Power Bi and (probably sometimes) Excel reports.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, my current company has made a counter-proposal, suggesting that I stay and learn new skills. They are proposing that I become a Big Query programmer at GCP (probably 50/50 with my role as a powerbi programmer, which is good because I don&amp;#39;t want to give up that part of my job), and are willing to provide the necessary training.&lt;/p&gt;\n\n&lt;p&gt;The salaries for both positions are the same, as the company will also give me a raise.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my question. Which role is most suitable for a future BI backend dev / data engineer role? I want to learn how to work with advanced sql and etl processes, and in my company everything revolves around the cloud, so in my mind moving to the cloud is the only option, but maybe that&amp;#39;s not true and starting with an on prem solution will be better. What do you think about this?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all the advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "173r3am", "is_robot_indexable": true, "report_reasons": null, "author": "eqwlknam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173r3am/job_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173r3am/job_advice/", "subreddit_subscribers": 132883, "created_utc": 1696856073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Created an account a month or so ago. I get passed the login/creds page and land into an infinite loop of url calls at \n\nhttps://cloud.getdbt.com/api/auth/auth-login/?next=\n\nTheir outage page shows all-clear. Tried multiple browsers/devices/networks.\n\nBummed. I had an afternoon to start learning and looks like they might be ?down?", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "anyone having trouble logging into DBT cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173dcpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696808025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Created an account a month or so ago. I get passed the login/creds page and land into an infinite loop of url calls at &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://cloud.getdbt.com/api/auth/auth-login/?next=\"&gt;https://cloud.getdbt.com/api/auth/auth-login/?next=&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their outage page shows all-clear. Tried multiple browsers/devices/networks.&lt;/p&gt;\n\n&lt;p&gt;Bummed. I had an afternoon to start learning and looks like they might be ?down?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173dcpc", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173dcpc/anyone_having_trouble_logging_into_dbt_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173dcpc/anyone_having_trouble_logging_into_dbt_cloud/", "subreddit_subscribers": 132883, "created_utc": 1696808025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  \n\nI work for a very old company so we have sales/financial/build records going all the way back to the 1970's....but I have never worked on any projects where anyone looked at anything older than 5 years.    \n\nEvery time I build anything, the business always wants \"everything\" and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn't exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.", "author_fullname": "t2_l5hazyvs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prevent data hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173rxlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696858427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the age of cheap storage, how do you stop people from requesting absurd amounts of historical data they are never ever going to use, just because they can?  &lt;/p&gt;\n\n&lt;p&gt;I work for a very old company so we have sales/financial/build records going all the way back to the 1970&amp;#39;s....but I have never worked on any projects where anyone looked at anything older than 5 years.    &lt;/p&gt;\n\n&lt;p&gt;Every time I build anything, the business always wants &amp;quot;everything&amp;quot; and then I am flooded with tickets for data quality issues on a build record from 1983 and I have to explain that the source system that it came from doesn&amp;#39;t exist any more and even if it did, absolutely no one is going to do anything to fix a 40 year old typo.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173rxlz", "is_robot_indexable": true, "report_reasons": null, "author": "One_Ball8812", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173rxlz/prevent_data_hoarding/", "subreddit_subscribers": 132883, "created_utc": 1696858427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a multi-database project, which among other databases, supports both Postgres AND TimescaleDB. For the time being, the queries for timescaledb and postgres are exactly identical. \n\nSo I wonder - when using timescaledb, in contrast with postgres, are there some query optimisations that I could already do to improve performance, or gain some efficiency of some of the \"read\" queries?\n\nDisclaimer before I go further - this is the first time I'm attempting to use TimescaleDB, so this is definitely a skill gap on my end. \n\nConsider this following table structure:\n\n[https://github.com/dotmethodme/storywise/blob/main/api/src/migrations/timescale/001\\_basic\\_schema.ts](https://github.com/dotmethodme/storywise/blob/main/api/src/migrations/timescale/001_basic_schema.ts) (essentially a table containing pageview events for a website)\n\nCan you recommend any query optimisations specific to TimescaleDB that could help with queries such as this for example:\n\n[https://github.com/dotmethodme/storywise/blob/main/api/src/repository/postgres.ts#L71](https://github.com/dotmethodme/storywise/blob/main/api/src/repository/postgres.ts#L71)\n\nThe same question could apply for some of the other read queries in the same file too.\n\nMore generally - do you have pointers for someone trying to \"translate\" from vanilla postgres to timescaledb?", "author_fullname": "t2_tup5ajb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrate from postgres to timescaledb and optimize queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1739v1o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696798956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a multi-database project, which among other databases, supports both Postgres AND TimescaleDB. For the time being, the queries for timescaledb and postgres are exactly identical. &lt;/p&gt;\n\n&lt;p&gt;So I wonder - when using timescaledb, in contrast with postgres, are there some query optimisations that I could already do to improve performance, or gain some efficiency of some of the &amp;quot;read&amp;quot; queries?&lt;/p&gt;\n\n&lt;p&gt;Disclaimer before I go further - this is the first time I&amp;#39;m attempting to use TimescaleDB, so this is definitely a skill gap on my end. &lt;/p&gt;\n\n&lt;p&gt;Consider this following table structure:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dotmethodme/storywise/blob/main/api/src/migrations/timescale/001_basic_schema.ts\"&gt;https://github.com/dotmethodme/storywise/blob/main/api/src/migrations/timescale/001_basic_schema.ts&lt;/a&gt; (essentially a table containing pageview events for a website)&lt;/p&gt;\n\n&lt;p&gt;Can you recommend any query optimisations specific to TimescaleDB that could help with queries such as this for example:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dotmethodme/storywise/blob/main/api/src/repository/postgres.ts#L71\"&gt;https://github.com/dotmethodme/storywise/blob/main/api/src/repository/postgres.ts#L71&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The same question could apply for some of the other read queries in the same file too.&lt;/p&gt;\n\n&lt;p&gt;More generally - do you have pointers for someone trying to &amp;quot;translate&amp;quot; from vanilla postgres to timescaledb?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?auto=webp&amp;s=32330b47ef85935bb7741eab49d9bc253dc582a9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1933c3c7eb6fc14679199ac7f44cba030fbc5f34", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55b20698677b6210d94a4afcf1b97498baa3abef", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e5d0c23d501722a27b25e91228494d1a2211934", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=13d1d5ba52be7df4785e508a54b10295c8dd0a9a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ba2c2e1fc88998100df0af08c92f4ffd1f0f05c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Hz7jfFbQVTpFB0UVG8ao-YzaeUK9BBOKfEibweBkpMw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02e6ec8ae5ef84572b83a9889e9f482be8302c7c", "width": 1080, "height": 540}], "variants": {}, "id": "taBimoUKzm8zcgt4RSSA4pCZWsln9ySPLztN1LqtGcs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1739v1o", "is_robot_indexable": true, "report_reasons": null, "author": "dotmethod_me", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1739v1o/migrate_from_postgres_to_timescaledb_and_optimize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1739v1o/migrate_from_postgres_to_timescaledb_and_optimize/", "subreddit_subscribers": 132883, "created_utc": 1696798956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working towards a job in Data Analysis, my pathway is DA-DE, please rate my github projects so i know how close/far i am from a junior da job, (i have a non cs degree and masters)\n\ngithub:[https://github.com/khorne182](https://github.com/khorne182)\n\nthank you", "author_fullname": "t2_5466c8m7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please Rate My Portfolio Projects (Data Analysis)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1733cnc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696782472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working towards a job in Data Analysis, my pathway is DA-DE, please rate my github projects so i know how close/far i am from a junior da job, (i have a non cs degree and masters)&lt;/p&gt;\n\n&lt;p&gt;github:&lt;a href=\"https://github.com/khorne182\"&gt;https://github.com/khorne182&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_JP0h6CZLbbqD-yhTXY8XLDC70mXB8LGs6ZdD9gfe3Y.jpg?auto=webp&amp;s=2db572d484141700b80685f5fa7fc80a53b860b1", "width": 420, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/_JP0h6CZLbbqD-yhTXY8XLDC70mXB8LGs6ZdD9gfe3Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12f0da57b0bd50320cd50c11efb43ede4d148ab1", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_JP0h6CZLbbqD-yhTXY8XLDC70mXB8LGs6ZdD9gfe3Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=955d74aa2e811c9d5f46c08d145e185b27f4b7c2", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/_JP0h6CZLbbqD-yhTXY8XLDC70mXB8LGs6ZdD9gfe3Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2431a5014ced6ea10078408be4753fc636cefb2", "width": 320, "height": 320}], "variants": {}, "id": "Zdv2L8vDMZBOia6vZR3chtNVQhUeXESnOzRdlMFY3Ig"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1733cnc", "is_robot_indexable": true, "report_reasons": null, "author": "EmotionalResolve9", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1733cnc/please_rate_my_portfolio_projects_data_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1733cnc/please_rate_my_portfolio_projects_data_analysis/", "subreddit_subscribers": 132883, "created_utc": 1696782472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nLet's say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.\n\n&amp;#x200B;\n\nHow do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?\n\n&amp;#x200B;\n\nThank you.", "author_fullname": "t2_a7y0xzcr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you model customer behavior tracking data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_173tosp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696862916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we implement Google Analytics in our application, we collect the user-session information to understand the behavior of the customer, such as button clicking, scrolling etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you model such behavior data? Do you use data modeling like dimensional modeling or how do you handle it to increase the flexibility of different use case?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "173tosp", "is_robot_indexable": true, "report_reasons": null, "author": "masamibb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173tosp/how_do_you_model_customer_behavior_tracking_data/", "subreddit_subscribers": 132883, "created_utc": 1696862916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am facing a challenge related to customer data integration in my product SaaS environment. As soon as customers integrate their systems with my product, I need to create a data pipeline using tools like Airbyte or Fivetran and fully migrate their data into my database.\n\nI have been recommended to use \"Powered by Airbyte,\" but am currently exploring all available options due to budgetary constraints. Hence, I'm turning to this knowledgeable community to seek advice and alternatives.\n\n1. Is there a way I can utilize access tokens from my customers' logins to fetch their data using plugins similar to Airbyte without opting for a premium product?\n   \n2. Considering Airbyte is open-source, what might be the complications or challenges in setting it up independently versus opting for a premium, priced service?\n\n3. If anyone has experience or knowledge regarding \"Powered by Airbyte\" pricing, I would highly appreciate insights into the cost and value derived from it.\n\nThank you in advance for sharing your expertise and helping me navigate through this challenge!", "author_fullname": "t2_4idps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimizing Customer Data Pipeline Integration Using Airbyte in a Production Environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173s6ru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696859111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am facing a challenge related to customer data integration in my product SaaS environment. As soon as customers integrate their systems with my product, I need to create a data pipeline using tools like Airbyte or Fivetran and fully migrate their data into my database.&lt;/p&gt;\n\n&lt;p&gt;I have been recommended to use &amp;quot;Powered by Airbyte,&amp;quot; but am currently exploring all available options due to budgetary constraints. Hence, I&amp;#39;m turning to this knowledgeable community to seek advice and alternatives.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is there a way I can utilize access tokens from my customers&amp;#39; logins to fetch their data using plugins similar to Airbyte without opting for a premium product?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Considering Airbyte is open-source, what might be the complications or challenges in setting it up independently versus opting for a premium, priced service?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If anyone has experience or knowledge regarding &amp;quot;Powered by Airbyte&amp;quot; pricing, I would highly appreciate insights into the cost and value derived from it.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you in advance for sharing your expertise and helping me navigate through this challenge!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173s6ru", "is_robot_indexable": true, "report_reasons": null, "author": "hurryup", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173s6ru/optimizing_customer_data_pipeline_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173s6ru/optimizing_customer_data_pipeline_integration/", "subreddit_subscribers": 132883, "created_utc": 1696859111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am planning to re-factor/re-design the management of database connections of some part of old business logic code.\n\nTo date, the code works as follows: there are multiple databases (e.g. db1, db2, ... dbN) and each has multiple \"tasks\" (i.e. generic business logic work) that reads from the associated database, (e.g. t11, t12, ... , t1N, ..., tM1, tM2, ..., tMN). The queries are written directly in SQL dialect, i.e. no \"ORM\" framework. We are mantaining both posgresql and mssql to date, duplicating the queries when needed. We plan to be non-agnognic and pick only one dialect, I think posgressql being free.\n\nThe logic open all the database connections at the start, then iterate over the tasks and exploits the open connections. If between tasks a timeout is reached, the connection is checked again and re-opened. Sometimes the connections are not closed properly and the connections are managed at low level directly with the available python drivers.\n\nAfter some thinking, I came with the following steps for the re-design:\n\n1. Order the (database, task) pairs in order to group by database and run the associated tasks in order, i.e. sequentially.\n2. Open and closing the database connection inside the \"group by for loop\" so that the logic to manage the connection is somehow limited to the loop iteration, this should help the transition and re-design by having more control.\n3. Switch from using the low-level driver to a production-ready library already optimized for the maganement of pools of connections in a threaded/async way. I was thinking about SQL Alchemy for this task.\n4. Re-designing the writing queries to be indipedent of each other. To date, some queries need to know the ID generated by a previous query, so they are runned in a non-atomic way (i.e. with autocommit set to true). I would like to set autocommit to false and commit only at the end of each task so to avoid corruping the database in the case if the task is stopped while running (to date we do not have control of this and sometimes  we find corruped data). How can I solve this problem?\n\nI would like to have your ideas on this refactoring process, if you need to ask me more questions or have more information, feel free to ask me: I wish to brainstorm here and collect some experience from senior data engineers as I am learning the role and I would like to re-design this in a robust way.\n\n&amp;#x200B;", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refactoring database connection management with SQL Alchemy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nluq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696843819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to re-factor/re-design the management of database connections of some part of old business logic code.&lt;/p&gt;\n\n&lt;p&gt;To date, the code works as follows: there are multiple databases (e.g. db1, db2, ... dbN) and each has multiple &amp;quot;tasks&amp;quot; (i.e. generic business logic work) that reads from the associated database, (e.g. t11, t12, ... , t1N, ..., tM1, tM2, ..., tMN). The queries are written directly in SQL dialect, i.e. no &amp;quot;ORM&amp;quot; framework. We are mantaining both posgresql and mssql to date, duplicating the queries when needed. We plan to be non-agnognic and pick only one dialect, I think posgressql being free.&lt;/p&gt;\n\n&lt;p&gt;The logic open all the database connections at the start, then iterate over the tasks and exploits the open connections. If between tasks a timeout is reached, the connection is checked again and re-opened. Sometimes the connections are not closed properly and the connections are managed at low level directly with the available python drivers.&lt;/p&gt;\n\n&lt;p&gt;After some thinking, I came with the following steps for the re-design:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Order the (database, task) pairs in order to group by database and run the associated tasks in order, i.e. sequentially.&lt;/li&gt;\n&lt;li&gt;Open and closing the database connection inside the &amp;quot;group by for loop&amp;quot; so that the logic to manage the connection is somehow limited to the loop iteration, this should help the transition and re-design by having more control.&lt;/li&gt;\n&lt;li&gt;Switch from using the low-level driver to a production-ready library already optimized for the maganement of pools of connections in a threaded/async way. I was thinking about SQL Alchemy for this task.&lt;/li&gt;\n&lt;li&gt;Re-designing the writing queries to be indipedent of each other. To date, some queries need to know the ID generated by a previous query, so they are runned in a non-atomic way (i.e. with autocommit set to true). I would like to set autocommit to false and commit only at the end of each task so to avoid corruping the database in the case if the task is stopped while running (to date we do not have control of this and sometimes  we find corruped data). How can I solve this problem?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I would like to have your ideas on this refactoring process, if you need to ask me more questions or have more information, feel free to ask me: I wish to brainstorm here and collect some experience from senior data engineers as I am learning the role and I would like to re-design this in a robust way.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173nluq", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nluq/refactoring_database_connection_management_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nluq/refactoring_database_connection_management_with/", "subreddit_subscribers": 132883, "created_utc": 1696843819.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWanted to ask for some tips on how to manage Airflow connections in an IaC way for a personal project.\n\nI want to set up my infra using Terraform and I'm looking for some programmatic way to feed credentials into Airflow instance without using UI. I was considering extracting credentials from Terraform's output json files and using Connection class to feed them into Airflow.\n\nIs it a reasonable approach or maybe you can share some more secure/efficient methods? ", "author_fullname": "t2_legrskylq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage connections in Airflow in an IaC way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nlfn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696843778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Wanted to ask for some tips on how to manage Airflow connections in an IaC way for a personal project.&lt;/p&gt;\n\n&lt;p&gt;I want to set up my infra using Terraform and I&amp;#39;m looking for some programmatic way to feed credentials into Airflow instance without using UI. I was considering extracting credentials from Terraform&amp;#39;s output json files and using Connection class to feed them into Airflow.&lt;/p&gt;\n\n&lt;p&gt;Is it a reasonable approach or maybe you can share some more secure/efficient methods? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173nlfn", "is_robot_indexable": true, "report_reasons": null, "author": "tmtrvprmr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nlfn/how_do_you_manage_connections_in_airflow_in_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nlfn/how_do_you_manage_connections_in_airflow_in_an/", "subreddit_subscribers": 132883, "created_utc": 1696843778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear all. \n\n&amp;#x200B;\n\nHope you are OK.\n\n&amp;#x200B;\n\nI was told by my boss, that if I see a course I need, there is a budget for it. It has to be kind of short, and specific. What I am using at work is Redshift, DBT, Qlik, and QuickSight. I don\u00b4t have a clue abour kafka, lambda, or arquitecture. My background is a data science(analytics bootcamp, and I am a DE junior improving a lot and already understanding more and more our data and making visualizations transforming data via DBT.\n\n&amp;#x200B;\n\nAny good place to look for a paid course? Any specific course which you recommend?\n\n&amp;#x200B;\n\nMany thanks :)", "author_fullname": "t2_s92yqnfm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice course to improve in my job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173k36c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696829484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear all. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hope you are OK.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I was told by my boss, that if I see a course I need, there is a budget for it. It has to be kind of short, and specific. What I am using at work is Redshift, DBT, Qlik, and QuickSight. I don\u00b4t have a clue abour kafka, lambda, or arquitecture. My background is a data science(analytics bootcamp, and I am a DE junior improving a lot and already understanding more and more our data and making visualizations transforming data via DBT.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any good place to look for a paid course? Any specific course which you recommend?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Many thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "173k36c", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious_Savior", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173k36c/advice_course_to_improve_in_my_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173k36c/advice_course_to_improve_in_my_job/", "subreddit_subscribers": 132883, "created_utc": 1696829484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, my co-worker has written an article about alerting in our platform (GoodData). As you might suspect, the article promotes our product, but on the other hand, I think it can provide you with some new perspectives on how to deal with alerts in data/BI tools.\n\nPlease check it out and let us know if you find it valuable! We are eager to discuss it.  \n\n\n[https://medium.com/gooddata-developers/alerts-are-dead-long-live-data-driven-actions-486998b58dca](https://medium.com/gooddata-developers/alerts-are-dead-long-live-data-driven-actions-486998b58dca)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u270d\ufe0f Article: Alerts Are Dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_173nqqz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696844394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, my co-worker has written an article about alerting in our platform (GoodData). As you might suspect, the article promotes our product, but on the other hand, I think it can provide you with some new perspectives on how to deal with alerts in data/BI tools.&lt;/p&gt;\n\n&lt;p&gt;Please check it out and let us know if you find it valuable! We are eager to discuss it.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/gooddata-developers/alerts-are-dead-long-live-data-driven-actions-486998b58dca\"&gt;https://medium.com/gooddata-developers/alerts-are-dead-long-live-data-driven-actions-486998b58dca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?auto=webp&amp;s=9dad9baf3c1012766177702d394293ae8dd13622", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e053ab0c60a7e30ba581a27e9c42035aee5b4643", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a84b6f48123ec2298872e4a3d7f58ded64815fde", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef7aa2cbc6bf0216e1afd7d5ac616d208a2a9e56", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a45b9b0856995d9edeb136424aa130035d5436a6", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6aa73517955d527d03a97c03d342abaa1bcbde05", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/C_nlrFinIxZmtc33ifX-0JpeH0s3E2g5N0786MRP7IA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87ad79b7690b4e1626609421852ab6eb18d9d7be", "width": 1080, "height": 567}], "variants": {}, "id": "vsg3D5DApgZiwi1CyIlPbeQyyWP73HkihJneJ7hmGnI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "173nqqz", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/173nqqz/article_alerts_are_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/173nqqz/article_alerts_are_dead/", "subreddit_subscribers": 132883, "created_utc": 1696844394.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}