{"kind": "Listing", "data": {"after": "t3_16y59mv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -&gt; dbt-&gt;Redshift", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tool, you regret buying or deploying in the data infrastructure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xxu15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696256235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -&amp;gt; dbt-&amp;gt;Redshift&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xxu15", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xxu15/any_tool_you_regret_buying_or_deploying_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xxu15/any_tool_you_regret_buying_or_deploying_in_the/", "subreddit_subscribers": 131692, "created_utc": 1696256235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working at a company that is trying to make a push for I4.0. Whether or not you think thats a buzzword, there are tons of companies making the push towards smart manufacturing. My question, is it worth it?\n\nMaybe it is only because we are new to it and I'm not seeing the full benefits but cost of collection (sensors, PLCs, etc) is often expensive. If you want production data to be captured constantly (good counts, scrap counts, etc.) you need redundant networking/power.\n\nMoving data to the cloud is unbelievably expensive at this volume and SCADAs/Historians are equally expensive. The hardware requires highly skilled employees to set up and maintain the system.\n\nAll this for managerial metrics and the ability to root cause faster? Maybe if you're good enough get some predictive maintenance?\n\nTo the others in the industry, is this all worth it? Have you seen the value add at your own job?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To those that work in manufacturing, do you believe IoT (and heavy collection of manufacturing data) is a fad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xyt26", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696258506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working at a company that is trying to make a push for I4.0. Whether or not you think thats a buzzword, there are tons of companies making the push towards smart manufacturing. My question, is it worth it?&lt;/p&gt;\n\n&lt;p&gt;Maybe it is only because we are new to it and I&amp;#39;m not seeing the full benefits but cost of collection (sensors, PLCs, etc) is often expensive. If you want production data to be captured constantly (good counts, scrap counts, etc.) you need redundant networking/power.&lt;/p&gt;\n\n&lt;p&gt;Moving data to the cloud is unbelievably expensive at this volume and SCADAs/Historians are equally expensive. The hardware requires highly skilled employees to set up and maintain the system.&lt;/p&gt;\n\n&lt;p&gt;All this for managerial metrics and the ability to root cause faster? Maybe if you&amp;#39;re good enough get some predictive maintenance?&lt;/p&gt;\n\n&lt;p&gt;To the others in the industry, is this all worth it? Have you seen the value add at your own job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xyt26", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xyt26/to_those_that_work_in_manufacturing_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xyt26/to_those_that_work_in_manufacturing_do_you/", "subreddit_subscribers": 131692, "created_utc": 1696258506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thanks to the community for being so helpful and supportive!\n\nI am considering to start a data engineering consulting. My niche is e-commerce business.\n\nWhat to hear what the community thinks about this? \n\nIs this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? \n\nThanks again!", "author_fullname": "t2_hlf7js20w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xob54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696224918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the community for being so helpful and supportive!&lt;/p&gt;\n\n&lt;p&gt;I am considering to start a data engineering consulting. My niche is e-commerce business.&lt;/p&gt;\n\n&lt;p&gt;What to hear what the community thinks about this? &lt;/p&gt;\n\n&lt;p&gt;Is this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? &lt;/p&gt;\n\n&lt;p&gt;Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xob54", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dream-420", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xob54/data_engineering_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xob54/data_engineering_consulting/", "subreddit_subscribers": 131692, "created_utc": 1696224918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?\n\nI was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.", "author_fullname": "t2_66knxh65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you rather do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xfsg6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696201028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?&lt;/p&gt;\n\n&lt;p&gt;I was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xfsg6", "is_robot_indexable": true, "report_reasons": null, "author": "WarNeverChanges1997", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "subreddit_subscribers": 131692, "created_utc": 1696201028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am dealing with small to medium sized customers looking for a basic data warehouse solution for Power BI reports. They generally have several GB up to a maximum of a 100 GB of data to store, and several dozens of users.\n\nFor these small scale solutions, is it really better to use OLAP databases like Synapse, Snowflake, Redshift? It seems like a simple Azure database or PostgreSQL as a service is much cheaper and would offer sufficient performance? Am I missing something?\n\nI have been involved with Snowflake solutions twice before, but in both cases the client had a negative perception of the costs. Or could I configure Snowflake in such a way that it runs cheaply?", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I really need an OLAP database for a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzlm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696260420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am dealing with small to medium sized customers looking for a basic data warehouse solution for Power BI reports. They generally have several GB up to a maximum of a 100 GB of data to store, and several dozens of users.&lt;/p&gt;\n\n&lt;p&gt;For these small scale solutions, is it really better to use OLAP databases like Synapse, Snowflake, Redshift? It seems like a simple Azure database or PostgreSQL as a service is much cheaper and would offer sufficient performance? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;I have been involved with Snowflake solutions twice before, but in both cases the client had a negative perception of the costs. Or could I configure Snowflake in such a way that it runs cheaply?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xzlm4", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xzlm4/do_i_really_need_an_olap_database_for_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xzlm4/do_i_really_need_an_olap_database_for_a_data/", "subreddit_subscribers": 131692, "created_utc": 1696260420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my latest blog post, I've summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn't attend can benefit from the latest updates on data governance and cost management.  \n\n\nData Governance Highlights:  \n1) Data Quality Monitoring  \n2) Query Constraint Policies  \n3) Data Access Policies  \n4) Data Governance UI  \n5) Classification UI  \n6) Global PII Classification  \n\n\nCost Control Highlights:  \n1) Budget  \n2) Warehouse Utilization  \n3) SnowLens  \n\n\nBlog Link - [Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023](https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\n\nI want to thank the event organizers and the insightful speakers:  \n\n\n1) [Hemant Raorane](https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I), Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n2) [Sachin Gangwar](https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n3) [Wasim El-Omari](https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc), Principal Architect, Security, Field CTO Office, [Snowflake](https://www.linkedin.com/company/snowflake-computing/)  \n4) [Pawan Mall](https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n5) [Vikash K.](https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4), Senior Data Cloud Architect | GSI Partners, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n\n\nYour expertise will shape how we handle data in our projects.  \n\n\n[\\#SnowflakeSummit2023](https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataGovernance](https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#CostControl](https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#TechTrends](https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataSecurity](https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#Snowflake](https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#SnowflakeDevelopers](https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) ", "author_fullname": "t2_hhlcq19gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key takeaways from Snowflake Summit 2023 in Bangalore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z371joftmqrb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=632d25e3e54af280ce7616d3debab386c63f0568"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bfbd5f8788550e35d5658dcefb105c1c63d6b0d"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f77fa2d109d22fad0d51dbf0bcd975b9114bd1d5"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f028485b825f179d5aed019f12d7568ebb1adaf2"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b201bb9b074bf1f4348fa10aa0d65cd0ada5e4c4"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e153a28fa2528f892f5df29d19402d8c80efb2c"}], "s": {"y": 1200, "x": 1600, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90"}, "id": "z371joftmqrb1"}}, "name": "t3_16xprjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GTriMGw7dhnIVYinyVqUBxw7GM_RpAGKtQqnNof47ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696230074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my latest blog post, I&amp;#39;ve summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn&amp;#39;t attend can benefit from the latest updates on data governance and cost management.  &lt;/p&gt;\n\n&lt;p&gt;Data Governance Highlights:&lt;br/&gt;\n1) Data Quality Monitoring&lt;br/&gt;\n2) Query Constraint Policies&lt;br/&gt;\n3) Data Access Policies&lt;br/&gt;\n4) Data Governance UI&lt;br/&gt;\n5) Classification UI&lt;br/&gt;\n6) Global PII Classification  &lt;/p&gt;\n\n&lt;p&gt;Cost Control Highlights:&lt;br/&gt;\n1) Budget&lt;br/&gt;\n2) Warehouse Utilization&lt;br/&gt;\n3) SnowLens  &lt;/p&gt;\n\n&lt;p&gt;Blog Link - &lt;a href=\"https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005\"&gt;Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\"&gt;https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to thank the event organizers and the insightful speakers:  &lt;/p&gt;\n\n&lt;p&gt;1) &lt;a href=\"https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I\"&gt;Hemant Raorane&lt;/a&gt;, Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n2) &lt;a href=\"https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w\"&gt;Sachin Gangwar&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n3) &lt;a href=\"https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc\"&gt;Wasim El-Omari&lt;/a&gt;, Principal Architect, Security, Field CTO Office, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt;&lt;br/&gt;\n4) &lt;a href=\"https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0\"&gt;Pawan Mall&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n5) &lt;a href=\"https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4\"&gt;Vikash K.&lt;/a&gt;, Senior Data Cloud Architect | GSI Partners, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India  &lt;/p&gt;\n\n&lt;p&gt;Your expertise will shape how we handle data in our projects.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeSummit2023&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataGovernance&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#CostControl&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataManagement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#TechTrends&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataSecurity&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#Snowflake&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeDevelopers&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?auto=webp&amp;s=87203ce307f9b72fb094e85035ea3a41a5c8c384", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9637f60474d774efa820fda8f5974253eaefa50", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83868552d3590e90338962cb8d1672ccbaa73828", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4b715c2453ce59b28c1dcce84f0153ab57bd1f0", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9699bdf7ecc3f6608279ad7cb3f1411d89fc0424", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c95fed484686b328184f7151ca8e51384b94448", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20407829974bcc45a6f50e4809fca58564a96763", "width": 1080, "height": 565}], "variants": {}, "id": "4x9_UIUovtScdP6OlrszrrtgeXaSEDyQR_m-Sv_lbvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xprjr", "is_robot_indexable": true, "report_reasons": null, "author": "ijiv_s", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "subreddit_subscribers": 131692, "created_utc": 1696230074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just looking for a bit of feedback on this matter. \n\nI just started in a new company, and 2 weeks in I was assigned to a new project. The project consists of fetching data from a known CRM Rest API, which have a good amount of documentation. Based on a  list of abstracts, my task was to find a proper endpoint and develop the code to fetch everything needed and save raw files in S3, while validating against a schema. \n\nI was given 1 month to develop everything alone, except the Terraform script for the infrastructure. Basic documentation, CI CD, docker image generation and upload to registry, Python coding with logging and memory leak safeguards. \n\nI thought it was hard but achievable. But when the scope changed everything derailed. The change was with additional level of detail of some datasets. \n\nThe project is being delivered 2 weeks late, and I'm getting a negative feedback because of the delay. \n\nI feel that I worked a lot to get that done on time, and I can't see where I could have done better. Specially being so new to the company and getting used to the processes and people. \n\nIs 6 weeks too much time for a project this big? Is the company asking too much of me?", "author_fullname": "t2_hcybngxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rant about my new job. I need a bit of insight from other DEs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y2nwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696267625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just looking for a bit of feedback on this matter. &lt;/p&gt;\n\n&lt;p&gt;I just started in a new company, and 2 weeks in I was assigned to a new project. The project consists of fetching data from a known CRM Rest API, which have a good amount of documentation. Based on a  list of abstracts, my task was to find a proper endpoint and develop the code to fetch everything needed and save raw files in S3, while validating against a schema. &lt;/p&gt;\n\n&lt;p&gt;I was given 1 month to develop everything alone, except the Terraform script for the infrastructure. Basic documentation, CI CD, docker image generation and upload to registry, Python coding with logging and memory leak safeguards. &lt;/p&gt;\n\n&lt;p&gt;I thought it was hard but achievable. But when the scope changed everything derailed. The change was with additional level of detail of some datasets. &lt;/p&gt;\n\n&lt;p&gt;The project is being delivered 2 weeks late, and I&amp;#39;m getting a negative feedback because of the delay. &lt;/p&gt;\n\n&lt;p&gt;I feel that I worked a lot to get that done on time, and I can&amp;#39;t see where I could have done better. Specially being so new to the company and getting used to the processes and people. &lt;/p&gt;\n\n&lt;p&gt;Is 6 weeks too much time for a project this big? Is the company asking too much of me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16y2nwa", "is_robot_indexable": true, "report_reasons": null, "author": "Gh0sthy1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y2nwa/rant_about_my_new_job_i_need_a_bit_of_insight/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y2nwa/rant_about_my_new_job_i_need_a_bit_of_insight/", "subreddit_subscribers": 131692, "created_utc": 1696267625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting Started with Airflow for Beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16xw41s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Getting Started with Airflow for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xUKIL7zsjos/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16xw41s", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfZ7wJhL9utiPsC-TD-zD8lu-CTQx4ysaSk8Sv6M6fA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696251805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/xUKIL7zsjos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?auto=webp&amp;s=57532a9798dd0f63f82f98e6bdad2b089b1be608", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa7e8c5e146fd23eb95bfc63815b710a803be4e6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15475b8c73de173bfc254994ecfb17fcdb78c882", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40b1b167450b074097af8c9efc000061a07aed17", "width": 320, "height": 240}], "variants": {}, "id": "LmI8hdoYDnBSx0i5v80cfgwiuAE39fOk0kw_zg9f3lw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xw41s", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xw41s/getting_started_with_airflow_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/xUKIL7zsjos", "subreddit_subscribers": 131692, "created_utc": 1696251805.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Getting Started with Airflow for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xUKIL7zsjos/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer with a few years experience in SQL/NoSQL. I've designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.\n\nUnfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I'm trying to get my career back on track. I've been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I'm wondering if you suggest getting deep into that, or what else, and i'd appreciate links to courses/tuts.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_edkp97o8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting back into DB world, should I learn Vector DBs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe9jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer with a few years experience in SQL/NoSQL. I&amp;#39;ve designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I&amp;#39;m trying to get my career back on track. I&amp;#39;ve been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I&amp;#39;m wondering if you suggest getting deep into that, or what else, and i&amp;#39;d appreciate links to courses/tuts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe9jr", "is_robot_indexable": true, "report_reasons": null, "author": "no_deal_111", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "subreddit_subscribers": 131692, "created_utc": 1696197530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Goodbye Spark. Hello Polars + Delta Lake.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzcqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uoju8UTP0tZAIdqZc3iW3ef3oNtUOsvWrwPRLKKEcso.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696259823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/goodbye-spark-hello-polars-delta", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?auto=webp&amp;s=e585ee336c489e1e9d450ed5159b115f114faf2a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5c6d53daaae3bdf1918e25f3f0270fc6dc7f597", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa672a15ff78edbc9baca55c9808ad1f74530570", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=754d2c0745079122b89f58f096f81dd3529333b2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=233e82aa2321f7d05735593d9a37da1064b50bc0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=91e26d7ce3a240db9569e53c0674a9b3db878065", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69f818883fada4b287989c118a7cd66802bf6540", "width": 1080, "height": 540}], "variants": {}, "id": "PURvM--d-7MgOYB6-qnYgEtr-OWAeVREK0gXETFQvXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xzcqx", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xzcqx/goodbye_spark_hello_polars_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/goodbye-spark-hello-polars-delta", "subreddit_subscribers": 131692, "created_utc": 1696259823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\"Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?\"\n\nKindly advise video courses and or books \n\nDatacamp\nMike chambers \n\nIf someone has experience please share", "author_fullname": "t2_peoywkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe83b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Kindly advise video courses and or books &lt;/p&gt;\n\n&lt;p&gt;Datacamp\nMike chambers &lt;/p&gt;\n\n&lt;p&gt;If someone has experience please share&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe83b", "is_robot_indexable": true, "report_reasons": null, "author": "angadaws", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe83b/feature_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe83b/feature_engineering/", "subreddit_subscribers": 131692, "created_utc": 1696197435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello Everyone,\n\nI'm an Industrial Engineer that has been working on data for the last couple of years. 1 year as a Data Analyst and almost 2 years as an Analytics Specialist. I ended up on this area by learning on my own and getting some certifications (also there wasn't as much analysts as there are now).\n\nAnyways, as it happens to a lot, my position is kind of ambiguous and I personally would define it as an ETL and dashboard developer. I create initial queries using SQL and then transform the data using Alteryx and feed it to a tableau or power BI dashboard that I create.  \nA regular workflow could have 10 different EDW sources and a couple of excel files. The initial data is generally around 400M records and ends of being 100K - 4M record. So I have to prepare the data for myself to use later.\n\nI want to get deeper into the Data engineering part of it and eventually look for data engineer jobs, I started my research today to determine the next logical steps but depending on the source, the suggestions are very different. I would like to study a masters eventually but I can\u00b4t do it right now.\n\nSo, my question is: In your opinion and taking into account your personal experience and preferences and **without saying \"it depends\"**, given my described background. which tool/software/process do you think would be the most valuable to learn to be competitive?\n\nAlso, if you know of a good youtube video that explains what it actually is to be a data engineer, it would be appreciated, most of the content I saw seemed like it was made by a 20 yr old influencer trying to get likes.", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from Analytics Specialist to Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y5790", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696273467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an Industrial Engineer that has been working on data for the last couple of years. 1 year as a Data Analyst and almost 2 years as an Analytics Specialist. I ended up on this area by learning on my own and getting some certifications (also there wasn&amp;#39;t as much analysts as there are now).&lt;/p&gt;\n\n&lt;p&gt;Anyways, as it happens to a lot, my position is kind of ambiguous and I personally would define it as an ETL and dashboard developer. I create initial queries using SQL and then transform the data using Alteryx and feed it to a tableau or power BI dashboard that I create.&lt;br/&gt;\nA regular workflow could have 10 different EDW sources and a couple of excel files. The initial data is generally around 400M records and ends of being 100K - 4M record. So I have to prepare the data for myself to use later.&lt;/p&gt;\n\n&lt;p&gt;I want to get deeper into the Data engineering part of it and eventually look for data engineer jobs, I started my research today to determine the next logical steps but depending on the source, the suggestions are very different. I would like to study a masters eventually but I can\u00b4t do it right now.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: In your opinion and taking into account your personal experience and preferences and &lt;strong&gt;without saying &amp;quot;it depends&amp;quot;&lt;/strong&gt;, given my described background. which tool/software/process do you think would be the most valuable to learn to be competitive?&lt;/p&gt;\n\n&lt;p&gt;Also, if you know of a good youtube video that explains what it actually is to be a data engineer, it would be appreciated, most of the content I saw seemed like it was made by a 20 yr old influencer trying to get likes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16y5790", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y5790/transitioning_from_analytics_specialist_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y5790/transitioning_from_analytics_specialist_to_data/", "subreddit_subscribers": 131692, "created_utc": 1696273467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I am comfortable with setting up a pipeline, it runs more or less as I expect but I am not really confident when it comes to configure it, automate to the fullest, parametrize, orchestrate maybe, so all the MS docs and most of youtube/udemy tackles the topic vaguely, just letting you dip the toes and while it'll make you know the tool, it doesnt make you being independent. Now I landed on client site, created some pipelines and need to make them config driven, so there is no further development at UAT's, it should be prone to any changes, additions etc. Are there any prototypes/projects with descriptions, where those things have been discussed in depth, ETL configs etc? Or some blog posts? Thank you in advance", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get good at config driven pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xuvb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696248221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am comfortable with setting up a pipeline, it runs more or less as I expect but I am not really confident when it comes to configure it, automate to the fullest, parametrize, orchestrate maybe, so all the MS docs and most of youtube/udemy tackles the topic vaguely, just letting you dip the toes and while it&amp;#39;ll make you know the tool, it doesnt make you being independent. Now I landed on client site, created some pipelines and need to make them config driven, so there is no further development at UAT&amp;#39;s, it should be prone to any changes, additions etc. Are there any prototypes/projects with descriptions, where those things have been discussed in depth, ETL configs etc? Or some blog posts? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xuvb6", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xuvb6/how_to_get_good_at_config_driven_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xuvb6/how_to_get_good_at_config_driven_pipelines/", "subreddit_subscribers": 131692, "created_utc": 1696248221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI am running pyspark code on my local machine and trying to understand how to get rid of pyspark warning- 23/10/02 11:31:14 WARN TaskSetManager: Stage 36 contains a task of very large size (2886 KiB). The maximum recommended task size is 1000 KiB.\n\nI read multiple posts suggesting using repartition or parallelize functions. But when I try the repartition function the large size value does not change at all. And I haven't tried parallelize because I do not want the dataframe to convert into RDD.\n\nSo I did a comparing test: \n\nCASE I: Code:\n\n    scada = spark.read.csv('../processed/scada_data.csv') \n    scada.count() \n\nOutput: \n\n    337776 \n\nCASE II: Code:\n\n    test = pd.read_csv('../processed/scada_data.csv') \n    spark_scada = spark.createDataFrame(test) \n    spark_scada.count() \n\nOutput:\n\n    23/10/02 11:31:14 WARN TaskSetManager: Stage 36 contains a task of very large size (2886 KiB). The maximum recommended task size is 1000 KiB. \n    337776 \n\nInterestingly, I do not get the warning when directly reading the csv using pyspark. But if I read it using pandas and then convert it to a spark dataframe, I get the warning. So, just to understand better - If I was to read csv using pandas then convert to spark dataframe (I know I dont have to but just to understand) what can I do to get rid of the warning, like the same way there is no warning when reading directly through pyspark?", "author_fullname": "t2_hywcxnb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark question about warning - Stage contains a task of very large size.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y537v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696273221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running pyspark code on my local machine and trying to understand how to get rid of pyspark warning- 23/10/02 11:31:14 WARN TaskSetManager: Stage 36 contains a task of very large size (2886 KiB). The maximum recommended task size is 1000 KiB.&lt;/p&gt;\n\n&lt;p&gt;I read multiple posts suggesting using repartition or parallelize functions. But when I try the repartition function the large size value does not change at all. And I haven&amp;#39;t tried parallelize because I do not want the dataframe to convert into RDD.&lt;/p&gt;\n\n&lt;p&gt;So I did a comparing test: &lt;/p&gt;\n\n&lt;p&gt;CASE I: Code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;scada = spark.read.csv(&amp;#39;../processed/scada_data.csv&amp;#39;) \nscada.count() \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Output: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;337776 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;CASE II: Code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;test = pd.read_csv(&amp;#39;../processed/scada_data.csv&amp;#39;) \nspark_scada = spark.createDataFrame(test) \nspark_scada.count() \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;23/10/02 11:31:14 WARN TaskSetManager: Stage 36 contains a task of very large size (2886 KiB). The maximum recommended task size is 1000 KiB. \n337776 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Interestingly, I do not get the warning when directly reading the csv using pyspark. But if I read it using pandas and then convert it to a spark dataframe, I get the warning. So, just to understand better - If I was to read csv using pandas then convert to spark dataframe (I know I dont have to but just to understand) what can I do to get rid of the warning, like the same way there is no warning when reading directly through pyspark?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16y537v", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Role_8051", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y537v/pyspark_question_about_warning_stage_contains_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y537v/pyspark_question_about_warning_stage_contains_a/", "subreddit_subscribers": 131692, "created_utc": 1696273221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say we have a column called previous_order_id that gives the order_id of the previous customer for a given order. It is defined as (using snowflake):\n\n    lag(order_id, 1, 0) over (partition by customer_id order by created_at asc) as previous_email_id\n\nHow would we test the values are correct? \n\nI guess in general, if we generate a SQL based report, how do we know the report has generated the correct values? Aside from testing nulls, accepted values etc.", "author_fullname": "t2_ske3s9us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing complex logic of SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y0fcs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696262318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a column called previous_order_id that gives the order_id of the previous customer for a given order. It is defined as (using snowflake):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lag(order_id, 1, 0) over (partition by customer_id order by created_at asc) as previous_email_id\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How would we test the values are correct? &lt;/p&gt;\n\n&lt;p&gt;I guess in general, if we generate a SQL based report, how do we know the report has generated the correct values? Aside from testing nulls, accepted values etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16y0fcs", "is_robot_indexable": true, "report_reasons": null, "author": "iamanoob38", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y0fcs/testing_complex_logic_of_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y0fcs/testing_complex_logic_of_sql/", "subreddit_subscribers": 131692, "created_utc": 1696262318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I started a job as a Data Entry/ Integration clerk for a manufacturing company, I had an internship as Solution Architect, where I briefly touched upon data pipeline and did a Data analytics bootcamp, but don't have much experience in this regard.\nThe company I'm working is quite small and atm is trying to migrate from a bespoke, old software toward the Microsoft ecosystem. They are bringing in a business analyst and some other people and I have been given charge of the data mapping/ migration, CRM side of things (with support of the BA). I was wondering if any of you have any resource I should consult, and could make this easier.\nSo far I read up some introductiory content\nThank you", "author_fullname": "t2_8v5n5r3my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any resource useful for data mapping and data migration effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y5mcn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696274441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I started a job as a Data Entry/ Integration clerk for a manufacturing company, I had an internship as Solution Architect, where I briefly touched upon data pipeline and did a Data analytics bootcamp, but don&amp;#39;t have much experience in this regard.\nThe company I&amp;#39;m working is quite small and atm is trying to migrate from a bespoke, old software toward the Microsoft ecosystem. They are bringing in a business analyst and some other people and I have been given charge of the data mapping/ migration, CRM side of things (with support of the BA). I was wondering if any of you have any resource I should consult, and could make this easier.\nSo far I read up some introductiory content\nThank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16y5mcn", "is_robot_indexable": true, "report_reasons": null, "author": "Zabadabadoodles", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y5mcn/any_resource_useful_for_data_mapping_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y5mcn/any_resource_useful_for_data_mapping_and_data/", "subreddit_subscribers": 131692, "created_utc": 1696274441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Economics of a Data Mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16xylzj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/asYHsIY2kWUJFwtiE2r846XwB1kMey6DWhbL0qhH6RU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696258006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/the-economics-of-a-data-mesh?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?auto=webp&amp;s=7c34009cf87a9174cefcac1ea03e4e8915ebe8ec", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f32b9a9cd7b57620422d8eda54ea146ef1886234", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44dd3db79dcc5c718e4c70adedbd3fd157d6c9ed", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=977ed1c2fd28853d7d2cd2955ad926ae1c141923", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c04e0b8d6ca89d0628ba471d9e580492926f8957", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ddb0864bca29658a528df99596a5cf9a251a1efd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ff2c7a04498f0549c3bf53b29cc99e9078e8295", "width": 1080, "height": 540}], "variants": {}, "id": "NcNSsLXEhXs6fOgmhbwaZYHPgD_-CjFwtLmWBLgasOU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xylzj", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xylzj/the_economics_of_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/the-economics-of-a-data-mesh?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 131692, "created_utc": 1696258006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just trying to do some calculations on data pipelines via Synapse, mainly the costings for activity runs.\n\nForEach &amp; Switches (iterations &amp; conditionals), do they have to be included in the cost as a activity run cost? They don't fall under Azure Integration Runtime or Self Hosted so I assume not?\n\nThanks!", "author_fullname": "t2_2ybq8ird", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics - Data Pipeline - Costings on Activity Runs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xt4op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696242822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just trying to do some calculations on data pipelines via Synapse, mainly the costings for activity runs.&lt;/p&gt;\n\n&lt;p&gt;ForEach &amp;amp; Switches (iterations &amp;amp; conditionals), do they have to be included in the cost as a activity run cost? They don&amp;#39;t fall under Azure Integration Runtime or Self Hosted so I assume not?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xt4op", "is_robot_indexable": true, "report_reasons": null, "author": "downsy2019", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xt4op/azure_synapse_analytics_data_pipeline_costings_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xt4op/azure_synapse_analytics_data_pipeline_costings_on/", "subreddit_subscribers": 131692, "created_utc": 1696242822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.", "author_fullname": "t2_muzoa5tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Java still a requirement in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xqihp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696232898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xqihp", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate-Box-7", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "subreddit_subscribers": 131692, "created_utc": 1696232898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \nWe make use of the Debezium PostgreSQL connector but we don't appreciate that it cannot deliver DDL operations. (citation [https://debezium.io/documentation/reference/stable/connectors/postgresql.html](https://debezium.io/documentation/reference/stable/connectors/postgresql.html))\n\nOur expectation was that we could simply hook up Debezium and our PostgreSQL instance and get near-real-time \"mirroring\" to Kafka. We eventually need this mirroring to end up in Redshift, but that's out of scope of this question.\n\nQuestion: are there any alternatives to Debezium's PostgreSQL connector that do capture DDL operations? Some constraints:\n\n\\- cannot send data to a third party. Everything must take place on prem.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Debezium PostgreSQL connector?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xoddt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696225138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nWe make use of the Debezium PostgreSQL connector but we don&amp;#39;t appreciate that it cannot deliver DDL operations. (citation &lt;a href=\"https://debezium.io/documentation/reference/stable/connectors/postgresql.html\"&gt;https://debezium.io/documentation/reference/stable/connectors/postgresql.html&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Our expectation was that we could simply hook up Debezium and our PostgreSQL instance and get near-real-time &amp;quot;mirroring&amp;quot; to Kafka. We eventually need this mirroring to end up in Redshift, but that&amp;#39;s out of scope of this question.&lt;/p&gt;\n\n&lt;p&gt;Question: are there any alternatives to Debezium&amp;#39;s PostgreSQL connector that do capture DDL operations? Some constraints:&lt;/p&gt;\n\n&lt;p&gt;- cannot send data to a third party. Everything must take place on prem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xoddt", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xoddt/alternative_to_debezium_postgresql_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xoddt/alternative_to_debezium_postgresql_connector/", "subreddit_subscribers": 131692, "created_utc": 1696225138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an s3 bucket with about 5+ million JSON files loaded per day (across our different client folders).\n\n* clientA - 9/25 - 120k files\n* clientB - 9/25 - 1.2M files  (1 file per userid)\n\neach JSON is small &lt; 10kb each.\n\nI have to retrieve a couple key-value pairs from the json and export it back to a new s3 bucket (combined). so from 1.2M files, the output should maybe be &lt;50 files in parquet/csv format.\n\n* example: extract these fields from JSON\n   * \"activated\": true and \"userid\": 123\n* expected results: tabular parquet/csv file\n   * activated, userid\n\n|activated|userid|\n|:-|:-|\n|true|123|\n|false|456|\n\nthis only needs to run once per day (batch).\n\n**current environment:**\n\nHybrid - SQL Server on-prem and Airflow (on-prem) and Redshift, primarily. we actually need to load this to SQL Server but we can use Redshift as a  processing middle-man if we need to.\n\nwe have the option of using Glue/Spark.\n\n**rough plan**\n\n1. glue job using pyspark to read the files for the previous day\n2. parse the json to get what we need\n3. export it back out to another s3 bucket  in gzipped csv\n   1. each brand will get its own folder of exported files\n\nAny ideas on this approach?\n\n* is there a way to trigger the glue job for a specific day (if we wanted to rerun a day?)\n* it'd be nice if there's an error that it can alert but continue with the other clients (non-blocking)\n\nWe don't have an EMR cluster (don't really have a need). I realize we can use EMR Serverless as well. Perhaps we can use that with our on-prem Airflow instance instead\n\n**Another option -**\n\n1. use Redshift's COPY command and [JSONPaths filters](https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-copy-from-json.html) to load the s3 files to a staging table on Redshift\n2. Then UNLOAD the entire table (all the records) to csv so it can be imported into SQL Server\n   1. this has the benefit of being \"free\" since our cluster is always running.\n\n&amp;#x200B;\n\nedit: updated post with some more details.\n\n&amp;#x200B;", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about using Glue/Spark to process millions of JSON files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y7wpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696281893.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696279632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an s3 bucket with about 5+ million JSON files loaded per day (across our different client folders).&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;clientA - 9/25 - 120k files&lt;/li&gt;\n&lt;li&gt;clientB - 9/25 - 1.2M files  (1 file per userid)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;each JSON is small &amp;lt; 10kb each.&lt;/p&gt;\n\n&lt;p&gt;I have to retrieve a couple key-value pairs from the json and export it back to a new s3 bucket (combined). so from 1.2M files, the output should maybe be &amp;lt;50 files in parquet/csv format.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;example: extract these fields from JSON\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;activated&amp;quot;: true and &amp;quot;userid&amp;quot;: 123&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;expected results: tabular parquet/csv file\n\n&lt;ul&gt;\n&lt;li&gt;activated, userid&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;activated&lt;/th&gt;\n&lt;th align=\"left\"&gt;userid&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;true&lt;/td&gt;\n&lt;td align=\"left\"&gt;123&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;false&lt;/td&gt;\n&lt;td align=\"left\"&gt;456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;this only needs to run once per day (batch).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;current environment:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hybrid - SQL Server on-prem and Airflow (on-prem) and Redshift, primarily. we actually need to load this to SQL Server but we can use Redshift as a  processing middle-man if we need to.&lt;/p&gt;\n\n&lt;p&gt;we have the option of using Glue/Spark.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;rough plan&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;glue job using pyspark to read the files for the previous day&lt;/li&gt;\n&lt;li&gt;parse the json to get what we need&lt;/li&gt;\n&lt;li&gt;export it back out to another s3 bucket  in gzipped csv\n\n&lt;ol&gt;\n&lt;li&gt;each brand will get its own folder of exported files&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any ideas on this approach?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;is there a way to trigger the glue job for a specific day (if we wanted to rerun a day?)&lt;/li&gt;\n&lt;li&gt;it&amp;#39;d be nice if there&amp;#39;s an error that it can alert but continue with the other clients (non-blocking)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We don&amp;#39;t have an EMR cluster (don&amp;#39;t really have a need). I realize we can use EMR Serverless as well. Perhaps we can use that with our on-prem Airflow instance instead&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Another option -&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;use Redshift&amp;#39;s COPY command and &lt;a href=\"https://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-copy-from-json.html\"&gt;JSONPaths filters&lt;/a&gt; to load the s3 files to a staging table on Redshift&lt;/li&gt;\n&lt;li&gt;Then UNLOAD the entire table (all the records) to csv so it can be imported into SQL Server\n\n&lt;ol&gt;\n&lt;li&gt;this has the benefit of being &amp;quot;free&amp;quot; since our cluster is always running.&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit: updated post with some more details.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16y7wpn", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y7wpn/question_about_using_gluespark_to_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y7wpn/question_about_using_gluespark_to_process/", "subreddit_subscribers": 131692, "created_utc": 1696279632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I try to understand what are the benefits to convert DICOM images to PNG's.  \nContext:  \nI have DICOM images which I already extracted the useful meta-data I want to use.  \nThose images are for a task, classification-detection pipeline of some disease.\n\nSo  as I already asked, what are the benefits of converting those DICOM  files to PNG's rather then just using pydicom and the dicom pixel\\_array?\n\nReason I ask this is because I saw many top 5 users on kaggle do this when dealing with DICOM images.\n\nIf  I understand how networks actually works, they get as input an array of  pixels as floating point numbers no? So what's the differences between  DICOM pixel\\_array to PNG's pixel array and numpy array or tensor? both  are eventually will be fed to the network as a tensor of floating  numbers.\n\nIs the reason is because PNG's are usually faster to train?\n\nIs the reason is because PNG's have more libraries support for preprocessing / augmentation / etc. ?\n\nIs  the reason is because PNG's are the format many pre-trained models  expect to? (I write this knowing it's 99% not true, as mentioned the  tensor thing)\n\nThanks in Advance, and Please, forgive my English (I could use AI tools to fix it but I feel addicted already)", "author_fullname": "t2_5vngcus6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of converting DICOM images to PNG's", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y6r61", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696277029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I try to understand what are the benefits to convert DICOM images to PNG&amp;#39;s.&lt;br/&gt;\nContext:&lt;br/&gt;\nI have DICOM images which I already extracted the useful meta-data I want to use.&lt;br/&gt;\nThose images are for a task, classification-detection pipeline of some disease.&lt;/p&gt;\n\n&lt;p&gt;So  as I already asked, what are the benefits of converting those DICOM  files to PNG&amp;#39;s rather then just using pydicom and the dicom pixel_array?&lt;/p&gt;\n\n&lt;p&gt;Reason I ask this is because I saw many top 5 users on kaggle do this when dealing with DICOM images.&lt;/p&gt;\n\n&lt;p&gt;If  I understand how networks actually works, they get as input an array of  pixels as floating point numbers no? So what&amp;#39;s the differences between  DICOM pixel_array to PNG&amp;#39;s pixel array and numpy array or tensor? both  are eventually will be fed to the network as a tensor of floating  numbers.&lt;/p&gt;\n\n&lt;p&gt;Is the reason is because PNG&amp;#39;s are usually faster to train?&lt;/p&gt;\n\n&lt;p&gt;Is the reason is because PNG&amp;#39;s have more libraries support for preprocessing / augmentation / etc. ?&lt;/p&gt;\n\n&lt;p&gt;Is  the reason is because PNG&amp;#39;s are the format many pre-trained models  expect to? (I write this knowing it&amp;#39;s 99% not true, as mentioned the  tensor thing)&lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance, and Please, forgive my English (I could use AI tools to fix it but I feel addicted already)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16y6r61", "is_robot_indexable": true, "report_reasons": null, "author": "01jasper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y6r61/benefits_of_converting_dicom_images_to_pngs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y6r61/benefits_of_converting_dicom_images_to_pngs/", "subreddit_subscribers": 131692, "created_utc": 1696277029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started my career in 2017 working as Backend Developer, however, two years ago I decided to move to a Data Engineer role in my previous company. I requested the switch to the company because I wanted to try something new and I'm passionate about the data world and everything its involved. \n\nHowever, two years later I was layoff with some other team members. Now, we just two years of experience as Data Engineer it's getting difficult to find a new job remotely or even in my country (I'm from Latin America, I worked for US based companies though staff company). \n\nThese days I've been study and getting certified in some tools like Apache Kafka, dbt and Azure, maybe this would help me to find something. I got the Databricks Apache Spark recently, but I worked with Spark for two years. \n\nAnyway, I've been thinking to switch back to backend. \n\nWhat would you recommend me, what your thoughts? \n\nJust want to read some comments or maybe similar experiences. ", "author_fullname": "t2_7hlpzyga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't find a job (switched from Backend to Data)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y6c83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696276083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started my career in 2017 working as Backend Developer, however, two years ago I decided to move to a Data Engineer role in my previous company. I requested the switch to the company because I wanted to try something new and I&amp;#39;m passionate about the data world and everything its involved. &lt;/p&gt;\n\n&lt;p&gt;However, two years later I was layoff with some other team members. Now, we just two years of experience as Data Engineer it&amp;#39;s getting difficult to find a new job remotely or even in my country (I&amp;#39;m from Latin America, I worked for US based companies though staff company). &lt;/p&gt;\n\n&lt;p&gt;These days I&amp;#39;ve been study and getting certified in some tools like Apache Kafka, dbt and Azure, maybe this would help me to find something. I got the Databricks Apache Spark recently, but I worked with Spark for two years. &lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;ve been thinking to switch back to backend. &lt;/p&gt;\n\n&lt;p&gt;What would you recommend me, what your thoughts? &lt;/p&gt;\n\n&lt;p&gt;Just want to read some comments or maybe similar experiences. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16y6c83", "is_robot_indexable": true, "report_reasons": null, "author": "Still_Young8611", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y6c83/cant_find_a_job_switched_from_backend_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y6c83/cant_find_a_job_switched_from_backend_to_data/", "subreddit_subscribers": 131692, "created_utc": 1696276083.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nI was looking for some advice whether to pursue AWS data analytics cert. I have been in BI field for a while and i have already completed the GCP data engineer cert. At work i don't have enough opportunities to work on GCP . I have done some small pet projects on gcp but not consistently motivated to do these on my own. \n\nWhat is the best way to move ? I always appreciate your advise. \n\n&amp;#x200B;\n\nthanks", "author_fullname": "t2_a1u8biz4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data analytics cert - advise needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y63xd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696275552.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;I was looking for some advice whether to pursue AWS data analytics cert. I have been in BI field for a while and i have already completed the GCP data engineer cert. At work i don&amp;#39;t have enough opportunities to work on GCP . I have done some small pet projects on gcp but not consistently motivated to do these on my own. &lt;/p&gt;\n\n&lt;p&gt;What is the best way to move ? I always appreciate your advise. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16y63xd", "is_robot_indexable": true, "report_reasons": null, "author": "Iffy-diffy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y63xd/aws_data_analytics_cert_advise_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y63xd/aws_data_analytics_cert_advise_needed/", "subreddit_subscribers": 131692, "created_utc": 1696275552.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team really likes Sigma and would love to integrate it in our stack but their price is not public which makes me think they just say prices based on their perception of how much a company needs it. So my question is: is Sigma in your stack and how much do you pay for it?", "author_fullname": "t2_axgif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sigma pricing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y59mv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696273625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team really likes Sigma and would love to integrate it in our stack but their price is not public which makes me think they just say prices based on their perception of how much a company needs it. So my question is: is Sigma in your stack and how much do you pay for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16y59mv", "is_robot_indexable": true, "report_reasons": null, "author": "Batto1300", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y59mv/sigma_pricing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y59mv/sigma_pricing/", "subreddit_subscribers": 131692, "created_utc": 1696273625.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}