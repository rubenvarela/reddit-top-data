{"kind": "Listing", "data": {"after": "t3_16xr03q", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most are pretty aware of dbt, but technology moves fast. Has any serious competitor come up? Curious if anyone has tried sqlmesh - what's your first impression?\n\nContex:\nJust generally curious", "author_fullname": "t2_hj2y0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs sqlmesh vs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xbiar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696191264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most are pretty aware of dbt, but technology moves fast. Has any serious competitor come up? Curious if anyone has tried sqlmesh - what&amp;#39;s your first impression?&lt;/p&gt;\n\n&lt;p&gt;Contex:\nJust generally curious&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xbiar", "is_robot_indexable": true, "report_reasons": null, "author": "Cryptojacob", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xbiar/dbt_vs_sqlmesh_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xbiar/dbt_vs_sqlmesh_vs/", "subreddit_subscribers": 131665, "created_utc": 1696191264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -&gt; dbt-&gt;Redshift", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tool, you regret buying or deploying in the data infrastructure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xxu15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696256235.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -&amp;gt; dbt-&amp;gt;Redshift&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xxu15", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xxu15/any_tool_you_regret_buying_or_deploying_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xxu15/any_tool_you_regret_buying_or_deploying_in_the/", "subreddit_subscribers": 131665, "created_utc": 1696256235.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working at a company that is trying to make a push for I4.0. Whether or not you think thats a buzzword, there are tons of companies making the push towards smart manufacturing. My question, is it worth it?\n\nMaybe it is only because we are new to it and I'm not seeing the full benefits but cost of collection (sensors, PLCs, etc) is often expensive. If you want production data to be captured constantly (good counts, scrap counts, etc.) you need redundant networking/power.\n\nMoving data to the cloud is unbelievably expensive at this volume and SCADAs/Historians are equally expensive. The hardware requires highly skilled employees to set up and maintain the system.\n\nAll this for managerial metrics and the ability to root cause faster? Maybe if you're good enough get some predictive maintenance?\n\nTo the others in the industry, is this all worth it? Have you seen the value add at your own job?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To those that work in manufacturing, do you believe IoT (and heavy collection of manufacturing data) is a fad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xyt26", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696258506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working at a company that is trying to make a push for I4.0. Whether or not you think thats a buzzword, there are tons of companies making the push towards smart manufacturing. My question, is it worth it?&lt;/p&gt;\n\n&lt;p&gt;Maybe it is only because we are new to it and I&amp;#39;m not seeing the full benefits but cost of collection (sensors, PLCs, etc) is often expensive. If you want production data to be captured constantly (good counts, scrap counts, etc.) you need redundant networking/power.&lt;/p&gt;\n\n&lt;p&gt;Moving data to the cloud is unbelievably expensive at this volume and SCADAs/Historians are equally expensive. The hardware requires highly skilled employees to set up and maintain the system.&lt;/p&gt;\n\n&lt;p&gt;All this for managerial metrics and the ability to root cause faster? Maybe if you&amp;#39;re good enough get some predictive maintenance?&lt;/p&gt;\n\n&lt;p&gt;To the others in the industry, is this all worth it? Have you seen the value add at your own job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xyt26", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xyt26/to_those_that_work_in_manufacturing_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xyt26/to_those_that_work_in_manufacturing_do_you/", "subreddit_subscribers": 131665, "created_utc": 1696258506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?\n\nI was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.", "author_fullname": "t2_66knxh65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you rather do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xfsg6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696201028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?&lt;/p&gt;\n\n&lt;p&gt;I was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xfsg6", "is_robot_indexable": true, "report_reasons": null, "author": "WarNeverChanges1997", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "subreddit_subscribers": 131665, "created_utc": 1696201028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.\n\nAfter some time, I have come up with three solutions that I would like to share:\n\n* When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.\n* Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.\n* Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.\n\nBasically, I explained all three solutions in greater details in my newsletter [here](https://dbtips.substack.com/p/get-the-ultimate-developer-experience).\n\nDo you have any other tips of improving DX you can share?", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to improve your developer experience when working with dbt (and not only)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x9maj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696186924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.&lt;/p&gt;\n\n&lt;p&gt;After some time, I have come up with three solutions that I would like to share:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.&lt;/li&gt;\n&lt;li&gt;Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.&lt;/li&gt;\n&lt;li&gt;Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, I explained all three solutions in greater details in my newsletter &lt;a href=\"https://dbtips.substack.com/p/get-the-ultimate-developer-experience\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Do you have any other tips of improving DX you can share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?auto=webp&amp;s=d3bb661896828bac4429562f6ce7fff4ef505422", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5af75ad43a854078e3cea834f3ef698663e88cdc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37ae9fc5f3b20c68d9199ed55882ebca83ef2855", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5685fef764bd1fa720328ebd972cc947f012bd86", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c958b3293f06f32b4616718b83399f29154edee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89fca96637cd1490f814b4b891f7bdc567c62f1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f972567e2eba31f2c54f818bdce4b34235f400e", "width": 1080, "height": 540}], "variants": {}, "id": "ntcEH-D8yVlbOqdDYiN0TVlVBkXGXM6b3WJWBcIF5sw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16x9maj", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "subreddit_subscribers": 131665, "created_utc": 1696186924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my latest blog post, I've summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn't attend can benefit from the latest updates on data governance and cost management.  \n\n\nData Governance Highlights:  \n1) Data Quality Monitoring  \n2) Query Constraint Policies  \n3) Data Access Policies  \n4) Data Governance UI  \n5) Classification UI  \n6) Global PII Classification  \n\n\nCost Control Highlights:  \n1) Budget  \n2) Warehouse Utilization  \n3) SnowLens  \n\n\nBlog Link - [Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023](https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\n\nI want to thank the event organizers and the insightful speakers:  \n\n\n1) [Hemant Raorane](https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I), Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n2) [Sachin Gangwar](https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n3) [Wasim El-Omari](https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc), Principal Architect, Security, Field CTO Office, [Snowflake](https://www.linkedin.com/company/snowflake-computing/)  \n4) [Pawan Mall](https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n5) [Vikash K.](https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4), Senior Data Cloud Architect | GSI Partners, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n\n\nYour expertise will shape how we handle data in our projects.  \n\n\n[\\#SnowflakeSummit2023](https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataGovernance](https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#CostControl](https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#TechTrends](https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataSecurity](https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#Snowflake](https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#SnowflakeDevelopers](https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) ", "author_fullname": "t2_hhlcq19gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key takeaways from Snowflake Summit 2023 in Bangalore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z371joftmqrb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=632d25e3e54af280ce7616d3debab386c63f0568"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bfbd5f8788550e35d5658dcefb105c1c63d6b0d"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f77fa2d109d22fad0d51dbf0bcd975b9114bd1d5"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f028485b825f179d5aed019f12d7568ebb1adaf2"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b201bb9b074bf1f4348fa10aa0d65cd0ada5e4c4"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e153a28fa2528f892f5df29d19402d8c80efb2c"}], "s": {"y": 1200, "x": 1600, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90"}, "id": "z371joftmqrb1"}}, "name": "t3_16xprjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GTriMGw7dhnIVYinyVqUBxw7GM_RpAGKtQqnNof47ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696230074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my latest blog post, I&amp;#39;ve summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn&amp;#39;t attend can benefit from the latest updates on data governance and cost management.  &lt;/p&gt;\n\n&lt;p&gt;Data Governance Highlights:&lt;br/&gt;\n1) Data Quality Monitoring&lt;br/&gt;\n2) Query Constraint Policies&lt;br/&gt;\n3) Data Access Policies&lt;br/&gt;\n4) Data Governance UI&lt;br/&gt;\n5) Classification UI&lt;br/&gt;\n6) Global PII Classification  &lt;/p&gt;\n\n&lt;p&gt;Cost Control Highlights:&lt;br/&gt;\n1) Budget&lt;br/&gt;\n2) Warehouse Utilization&lt;br/&gt;\n3) SnowLens  &lt;/p&gt;\n\n&lt;p&gt;Blog Link - &lt;a href=\"https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005\"&gt;Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\"&gt;https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to thank the event organizers and the insightful speakers:  &lt;/p&gt;\n\n&lt;p&gt;1) &lt;a href=\"https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I\"&gt;Hemant Raorane&lt;/a&gt;, Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n2) &lt;a href=\"https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w\"&gt;Sachin Gangwar&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n3) &lt;a href=\"https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc\"&gt;Wasim El-Omari&lt;/a&gt;, Principal Architect, Security, Field CTO Office, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt;&lt;br/&gt;\n4) &lt;a href=\"https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0\"&gt;Pawan Mall&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n5) &lt;a href=\"https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4\"&gt;Vikash K.&lt;/a&gt;, Senior Data Cloud Architect | GSI Partners, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India  &lt;/p&gt;\n\n&lt;p&gt;Your expertise will shape how we handle data in our projects.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeSummit2023&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataGovernance&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#CostControl&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataManagement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#TechTrends&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataSecurity&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#Snowflake&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeDevelopers&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?auto=webp&amp;s=87203ce307f9b72fb094e85035ea3a41a5c8c384", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9637f60474d774efa820fda8f5974253eaefa50", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83868552d3590e90338962cb8d1672ccbaa73828", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4b715c2453ce59b28c1dcce84f0153ab57bd1f0", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9699bdf7ecc3f6608279ad7cb3f1411d89fc0424", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c95fed484686b328184f7151ca8e51384b94448", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20407829974bcc45a6f50e4809fca58564a96763", "width": 1080, "height": 565}], "variants": {}, "id": "4x9_UIUovtScdP6OlrszrrtgeXaSEDyQR_m-Sv_lbvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xprjr", "is_robot_indexable": true, "report_reasons": null, "author": "ijiv_s", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "subreddit_subscribers": 131665, "created_utc": 1696230074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting Started with Airflow for Beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16xw41s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Getting Started with Airflow for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xUKIL7zsjos/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16xw41s", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IfZ7wJhL9utiPsC-TD-zD8lu-CTQx4ysaSk8Sv6M6fA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696251805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/xUKIL7zsjos", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?auto=webp&amp;s=57532a9798dd0f63f82f98e6bdad2b089b1be608", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa7e8c5e146fd23eb95bfc63815b710a803be4e6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15475b8c73de173bfc254994ecfb17fcdb78c882", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T4YyMc6VC2UOui-2zwFVrE9aBp71kYlVV1JqoszmaYY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=40b1b167450b074097af8c9efc000061a07aed17", "width": 320, "height": 240}], "variants": {}, "id": "LmI8hdoYDnBSx0i5v80cfgwiuAE39fOk0kw_zg9f3lw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xw41s", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xw41s/getting_started_with_airflow_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/xUKIL7zsjos", "subreddit_subscribers": 131665, "created_utc": 1696251805.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Getting Started with Airflow for Beginners", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xUKIL7zsjos?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Getting Started with Airflow for Beginners\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xUKIL7zsjos/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thanks to the community for being so helpful and supportive!\n\nI am considering to start a data engineering consulting. My niche is e-commerce business.\n\nWhat to hear what the community thinks about this? \n\nIs this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? \n\nThanks again!", "author_fullname": "t2_hlf7js20w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xob54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696224918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the community for being so helpful and supportive!&lt;/p&gt;\n\n&lt;p&gt;I am considering to start a data engineering consulting. My niche is e-commerce business.&lt;/p&gt;\n\n&lt;p&gt;What to hear what the community thinks about this? &lt;/p&gt;\n\n&lt;p&gt;Is this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? &lt;/p&gt;\n\n&lt;p&gt;Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xob54", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dream-420", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xob54/data_engineering_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xob54/data_engineering_consulting/", "subreddit_subscribers": 131665, "created_utc": 1696224918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is a lot of overlap between data engineering and analytics for these jobs, so I'll post in both subs. It's my first job out of college, and down the line I'm really looking to be a full fledged tech professional, whatever that means. I'm looking at 2 vastly different offers. They are both at hospital systems, remote isn't an option for either. And I'm not declining either job just because its not remote, I have no leverage as an unemployed college grad desparate for a job. Pay and commute are similar.\n\nOffer 1: Mostly analytics, using SQL and drag-drop software for ETL processes. More emphasis is placed on understanding the stakeholder needs rather than coding. Data architecture is well established by third party vendor. Workload seems fairly low, easy-going office environment. No on call, typical 9-5, 4 days in office.\n\nOffer 2: Getting my hands dirty with Python, SQL, and Linux command line for ETL processes, existing data architecture needs lots of work. Job duties also include providing basic IT phone support for hospital employees. On call, unsure of frequency. 9-5, 5 days in office.\n\nOffer 1 is nice because of the work-life balance, easy and established reporting process is already in place, but it's no code. Offer 2 is enticing because of the amount of raw coding I'll be doing, but also having to answer phone calls from nurses and doctors to troubleshoot basic issues and being on call is a huge downside. Thoughts?", "author_fullname": "t2_5pddzf1hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First job out of college? What would you choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xitxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696209933.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696208771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a lot of overlap between data engineering and analytics for these jobs, so I&amp;#39;ll post in both subs. It&amp;#39;s my first job out of college, and down the line I&amp;#39;m really looking to be a full fledged tech professional, whatever that means. I&amp;#39;m looking at 2 vastly different offers. They are both at hospital systems, remote isn&amp;#39;t an option for either. And I&amp;#39;m not declining either job just because its not remote, I have no leverage as an unemployed college grad desparate for a job. Pay and commute are similar.&lt;/p&gt;\n\n&lt;p&gt;Offer 1: Mostly analytics, using SQL and drag-drop software for ETL processes. More emphasis is placed on understanding the stakeholder needs rather than coding. Data architecture is well established by third party vendor. Workload seems fairly low, easy-going office environment. No on call, typical 9-5, 4 days in office.&lt;/p&gt;\n\n&lt;p&gt;Offer 2: Getting my hands dirty with Python, SQL, and Linux command line for ETL processes, existing data architecture needs lots of work. Job duties also include providing basic IT phone support for hospital employees. On call, unsure of frequency. 9-5, 5 days in office.&lt;/p&gt;\n\n&lt;p&gt;Offer 1 is nice because of the work-life balance, easy and established reporting process is already in place, but it&amp;#39;s no code. Offer 2 is enticing because of the amount of raw coding I&amp;#39;ll be doing, but also having to answer phone calls from nurses and doctors to troubleshoot basic issues and being on call is a huge downside. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xitxb", "is_robot_indexable": true, "report_reasons": null, "author": "knewtonslol", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xitxb/first_job_out_of_college_what_would_you_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xitxb/first_job_out_of_college_what_would_you_choose/", "subreddit_subscribers": 131665, "created_utc": 1696208771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer with a few years experience in SQL/NoSQL. I've designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.\n\nUnfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I'm trying to get my career back on track. I've been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I'm wondering if you suggest getting deep into that, or what else, and i'd appreciate links to courses/tuts.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_edkp97o8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting back into DB world, should I learn Vector DBs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe9jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer with a few years experience in SQL/NoSQL. I&amp;#39;ve designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I&amp;#39;m trying to get my career back on track. I&amp;#39;ve been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I&amp;#39;m wondering if you suggest getting deep into that, or what else, and i&amp;#39;d appreciate links to courses/tuts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe9jr", "is_robot_indexable": true, "report_reasons": null, "author": "no_deal_111", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "subreddit_subscribers": 131665, "created_utc": 1696197530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\"Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?\"\n\nKindly advise video courses and or books \n\nDatacamp\nMike chambers \n\nIf someone has experience please share", "author_fullname": "t2_peoywkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe83b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Kindly advise video courses and or books &lt;/p&gt;\n\n&lt;p&gt;Datacamp\nMike chambers &lt;/p&gt;\n\n&lt;p&gt;If someone has experience please share&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe83b", "is_robot_indexable": true, "report_reasons": null, "author": "angadaws", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe83b/feature_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe83b/feature_engineering/", "subreddit_subscribers": 131665, "created_utc": 1696197435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am dealing with small to medium sized customers looking for a basic data warehouse solution for Power BI reports. They generally have several GB up to a maximum of a 100 GB of data to store, and several dozens of users.\n\nFor these small scale solutions, is it really better to use OLAP databases like Synapse, Snowflake, Redshift? It seems like a simple Azure database or PostgreSQL as a service is much cheaper and would offer sufficient performance? Am I missing something?\n\nI have been involved with Snowflake solutions twice before, but in both cases the client had a negative perception of the costs. Or could I configure Snowflake in such a way that it runs cheaply?", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I really need an OLAP database for a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzlm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696260420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am dealing with small to medium sized customers looking for a basic data warehouse solution for Power BI reports. They generally have several GB up to a maximum of a 100 GB of data to store, and several dozens of users.&lt;/p&gt;\n\n&lt;p&gt;For these small scale solutions, is it really better to use OLAP databases like Synapse, Snowflake, Redshift? It seems like a simple Azure database or PostgreSQL as a service is much cheaper and would offer sufficient performance? Am I missing something?&lt;/p&gt;\n\n&lt;p&gt;I have been involved with Snowflake solutions twice before, but in both cases the client had a negative perception of the costs. Or could I configure Snowflake in such a way that it runs cheaply?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xzlm4", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xzlm4/do_i_really_need_an_olap_database_for_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xzlm4/do_i_really_need_an_olap_database_for_a_data/", "subreddit_subscribers": 131665, "created_utc": 1696260420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hdte75ow1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Goodbye Spark. Hello Polars + Delta Lake.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzcqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uoju8UTP0tZAIdqZc3iW3ef3oNtUOsvWrwPRLKKEcso.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696259823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/goodbye-spark-hello-polars-delta", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?auto=webp&amp;s=e585ee336c489e1e9d450ed5159b115f114faf2a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5c6d53daaae3bdf1918e25f3f0270fc6dc7f597", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa672a15ff78edbc9baca55c9808ad1f74530570", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=754d2c0745079122b89f58f096f81dd3529333b2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=233e82aa2321f7d05735593d9a37da1064b50bc0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=91e26d7ce3a240db9569e53c0674a9b3db878065", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/8bn5fKYCKOZmmY4iCQy1Iva_CHlYcdIAXfm7TeL4H8Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69f818883fada4b287989c118a7cd66802bf6540", "width": 1080, "height": 540}], "variants": {}, "id": "PURvM--d-7MgOYB6-qnYgEtr-OWAeVREK0gXETFQvXQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xzcqx", "is_robot_indexable": true, "report_reasons": null, "author": "dataengineeringdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xzcqx/goodbye_spark_hello_polars_delta_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/goodbye-spark-hello-polars-delta", "subreddit_subscribers": 131665, "created_utc": 1696259823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I am comfortable with setting up a pipeline, it runs more or less as I expect but I am not really confident when it comes to configure it, automate to the fullest, parametrize, orchestrate maybe, so all the MS docs and most of youtube/udemy tackles the topic vaguely, just letting you dip the toes and while it'll make you know the tool, it doesnt make you being independent. Now I landed on client site, created some pipelines and need to make them config driven, so there is no further development at UAT's, it should be prone to any changes, additions etc. Are there any prototypes/projects with descriptions, where those things have been discussed in depth, ETL configs etc? Or some blog posts? Thank you in advance", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get good at config driven pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xuvb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696248221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am comfortable with setting up a pipeline, it runs more or less as I expect but I am not really confident when it comes to configure it, automate to the fullest, parametrize, orchestrate maybe, so all the MS docs and most of youtube/udemy tackles the topic vaguely, just letting you dip the toes and while it&amp;#39;ll make you know the tool, it doesnt make you being independent. Now I landed on client site, created some pipelines and need to make them config driven, so there is no further development at UAT&amp;#39;s, it should be prone to any changes, additions etc. Are there any prototypes/projects with descriptions, where those things have been discussed in depth, ETL configs etc? Or some blog posts? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xuvb6", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xuvb6/how_to_get_good_at_config_driven_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xuvb6/how_to_get_good_at_config_driven_pipelines/", "subreddit_subscribers": 131665, "created_utc": 1696248221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking for a database for all our laboratory data, our requirements are that the database must be capable of handling multiple data types (sensor data, images), easily accessible to everyone and most importantly has the functionality to link different databases. We would have a primary database that will have information about multiple processes and then each process has its own database. Whatever data is inserted in the primary database should get updated automatically on the respective process specific secondary database. Been trying to look for something that does these, any suggestions would be appreciated! Thank you.", "author_fullname": "t2_ndudpuk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source data management platform for academia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xcbec", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696193080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking for a database for all our laboratory data, our requirements are that the database must be capable of handling multiple data types (sensor data, images), easily accessible to everyone and most importantly has the functionality to link different databases. We would have a primary database that will have information about multiple processes and then each process has its own database. Whatever data is inserted in the primary database should get updated automatically on the respective process specific secondary database. Been trying to look for something that does these, any suggestions would be appreciated! Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xcbec", "is_robot_indexable": true, "report_reasons": null, "author": "chipsandcokerule", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xcbec/open_source_data_management_platform_for_academia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xcbec/open_source_data_management_platform_for_academia/", "subreddit_subscribers": 131665, "created_utc": 1696193080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.", "author_fullname": "t2_muzoa5tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Java still a requirement in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xqihp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696232898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xqihp", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate-Box-7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "subreddit_subscribers": 131665, "created_utc": 1696232898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nIm leading a team of a few part time contractors to build out my company's first Data Warehouse and we are trying to determine the best solution to orchestrate DBT transformations. We work within an AWS ecosystem and have strict rules about maintaining GDPR compliance, which requires that our data remain in Canada. This has proven to be a pretty limiting factor since all of the cloud orchestration tools either dont have Canada as a hosting region or they gets exponentially more expensive to host there (example with Astronomer. This is due to the fact that you are unable to deactivate your cluster, so it runs 24/7 at $2/hour. They claim they are releasing a feature next month to control that, but I cant depend on a sales reps word for this decision). \n\nOur data is small in size so we can use that to our advantage, since many of the options come out to be more affordable. The decision im working through is whether or not we should leverage MWAA (which would be about $4-5k/year with our current needs) or try another solution within the AWS ecosystem like step functions or fargate. Im finding this choice difficult as I do not have experience with these AWS products and I dont want to risk over engineering here.\n\nAny advice would be appreciated :)", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating DBT Jobs with Airflow/Step functions/Fargate/etc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xbu4w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696191987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;Im leading a team of a few part time contractors to build out my company&amp;#39;s first Data Warehouse and we are trying to determine the best solution to orchestrate DBT transformations. We work within an AWS ecosystem and have strict rules about maintaining GDPR compliance, which requires that our data remain in Canada. This has proven to be a pretty limiting factor since all of the cloud orchestration tools either dont have Canada as a hosting region or they gets exponentially more expensive to host there (example with Astronomer. This is due to the fact that you are unable to deactivate your cluster, so it runs 24/7 at $2/hour. They claim they are releasing a feature next month to control that, but I cant depend on a sales reps word for this decision). &lt;/p&gt;\n\n&lt;p&gt;Our data is small in size so we can use that to our advantage, since many of the options come out to be more affordable. The decision im working through is whether or not we should leverage MWAA (which would be about $4-5k/year with our current needs) or try another solution within the AWS ecosystem like step functions or fargate. Im finding this choice difficult as I do not have experience with these AWS products and I dont want to risk over engineering here.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xbu4w", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xbu4w/orchestrating_dbt_jobs_with_airflowstep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xbu4w/orchestrating_dbt_jobs_with_airflowstep/", "subreddit_subscribers": 131665, "created_utc": 1696191987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just looking for a bit of feedback on this matter. \n\nI just started in a new company, and 2 weeks in I was assigned to a new project. The project consists of fetching data from a known CRM Rest API, which have a good amount of documentation. Based on a  list of abstracts, my task was to find a proper endpoint and develop the code to fetch everything needed and save raw files in S3, while validating against a schema. \n\nI was given 1 month to develop everything alone, except the Terraform script for the infrastructure. Basic documentation, CI CD, docker image generation and upload to registry, Python coding with logging and memory leak safeguards. \n\nI thought it was hard but achievable. But when the scope changed everything derailed. The change was with additional level of detail of some datasets. \n\nThe project is being delivered 2 weeks late, and I'm getting a negative feedback because of the delay. \n\nI feel that I worked a lot to get that done on time, and I can't see where I could have done better. Specially being so new to the company and getting used to the processes and people. \n\nIs 6 weeks too much time for a project this big? Is the company asking too much of me?", "author_fullname": "t2_hcybngxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rant about my new job. I need a bit of insight from other DEs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y2nwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696267625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just looking for a bit of feedback on this matter. &lt;/p&gt;\n\n&lt;p&gt;I just started in a new company, and 2 weeks in I was assigned to a new project. The project consists of fetching data from a known CRM Rest API, which have a good amount of documentation. Based on a  list of abstracts, my task was to find a proper endpoint and develop the code to fetch everything needed and save raw files in S3, while validating against a schema. &lt;/p&gt;\n\n&lt;p&gt;I was given 1 month to develop everything alone, except the Terraform script for the infrastructure. Basic documentation, CI CD, docker image generation and upload to registry, Python coding with logging and memory leak safeguards. &lt;/p&gt;\n\n&lt;p&gt;I thought it was hard but achievable. But when the scope changed everything derailed. The change was with additional level of detail of some datasets. &lt;/p&gt;\n\n&lt;p&gt;The project is being delivered 2 weeks late, and I&amp;#39;m getting a negative feedback because of the delay. &lt;/p&gt;\n\n&lt;p&gt;I feel that I worked a lot to get that done on time, and I can&amp;#39;t see where I could have done better. Specially being so new to the company and getting used to the processes and people. &lt;/p&gt;\n\n&lt;p&gt;Is 6 weeks too much time for a project this big? Is the company asking too much of me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16y2nwa", "is_robot_indexable": true, "report_reasons": null, "author": "Gh0sthy1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y2nwa/rant_about_my_new_job_i_need_a_bit_of_insight/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y2nwa/rant_about_my_new_job_i_need_a_bit_of_insight/", "subreddit_subscribers": 131665, "created_utc": 1696267625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's say we have a column called previous_order_id that gives the order_id of the previous customer for a given order. It is defined as (using snowflake):\n\n    lag(order_id, 1, 0) over (partition by customer_id order by created_at asc) as previous_email_id\n\nHow would we test the values are correct? \n\nI guess in general, if we generate a SQL based report, how do we know the report has generated the correct values? Aside from testing nulls, accepted values etc.", "author_fullname": "t2_ske3s9us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Testing complex logic of SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y0fcs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696262318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say we have a column called previous_order_id that gives the order_id of the previous customer for a given order. It is defined as (using snowflake):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;lag(order_id, 1, 0) over (partition by customer_id order by created_at asc) as previous_email_id\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How would we test the values are correct? &lt;/p&gt;\n\n&lt;p&gt;I guess in general, if we generate a SQL based report, how do we know the report has generated the correct values? Aside from testing nulls, accepted values etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16y0fcs", "is_robot_indexable": true, "report_reasons": null, "author": "iamanoob38", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16y0fcs/testing_complex_logic_of_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16y0fcs/testing_complex_logic_of_sql/", "subreddit_subscribers": 131665, "created_utc": 1696262318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, currently we are using Fivetran to get our Hubspot data and put it into a Fivetran managed BigQuery dataset. However, I remember I've seen somewhere that Google Cloud has some tool that can do basically the same without too much development needed. I'm so confused with all these Google Cloud services names for data analytics and engineering.", "author_fullname": "t2_7chglbbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the Google Cloud tool alternative to Fivetran?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzqni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696260739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, currently we are using Fivetran to get our Hubspot data and put it into a Fivetran managed BigQuery dataset. However, I remember I&amp;#39;ve seen somewhere that Google Cloud has some tool that can do basically the same without too much development needed. I&amp;#39;m so confused with all these Google Cloud services names for data analytics and engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xzqni", "is_robot_indexable": true, "report_reasons": null, "author": "Computingss", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xzqni/what_is_the_google_cloud_tool_alternative_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xzqni/what_is_the_google_cloud_tool_alternative_to/", "subreddit_subscribers": 131665, "created_utc": 1696260739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Economics of a Data Mesh", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16xylzj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/asYHsIY2kWUJFwtiE2r846XwB1kMey6DWhbL0qhH6RU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696258006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/the-economics-of-a-data-mesh?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?auto=webp&amp;s=7c34009cf87a9174cefcac1ea03e4e8915ebe8ec", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f32b9a9cd7b57620422d8eda54ea146ef1886234", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44dd3db79dcc5c718e4c70adedbd3fd157d6c9ed", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=977ed1c2fd28853d7d2cd2955ad926ae1c141923", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c04e0b8d6ca89d0628ba471d9e580492926f8957", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ddb0864bca29658a528df99596a5cf9a251a1efd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7XfgEcuDydnUKT-IAgTAhJmRgPkLmqnlp4g6z2X5NO0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ff2c7a04498f0549c3bf53b29cc99e9078e8295", "width": 1080, "height": 540}], "variants": {}, "id": "NcNSsLXEhXs6fOgmhbwaZYHPgD_-CjFwtLmWBLgasOU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xylzj", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xylzj/the_economics_of_a_data_mesh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/the-economics-of-a-data-mesh?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 131665, "created_utc": 1696258006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there! I'm reading some documentation about Databricks and how to optimize the table sizes. I've found that the default (and sometimes recommended) value of individual files when using OPTIMIZE is 1 GB (and I understand the rationale for this file size) but then found out that there is another feature called AUTO OPTIMIZE that will set the file size at 128 MB in order to *accelerate write-intensive operations (as mentioned in* [here](https://docs.databricks.com/en/delta/tune-file-size.html#autotune-workload)). \n\nI'm a little bit confused about why having smaller files would be good for write operations (I'm guessing it may have something to do with parallelization, but not sure), and also confirm that (if I understood correctly) we want large files for improving reads on tables but small files for improving writes.\n\nCould anyone shed some light on this questions?\n\n ", "author_fullname": "t2_zn4k7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1 GB (OPTIMIZE) vs 128 MB (AUTO OPTIMIZE) file size in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xyl6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696257954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! I&amp;#39;m reading some documentation about Databricks and how to optimize the table sizes. I&amp;#39;ve found that the default (and sometimes recommended) value of individual files when using OPTIMIZE is 1 GB (and I understand the rationale for this file size) but then found out that there is another feature called AUTO OPTIMIZE that will set the file size at 128 MB in order to &lt;em&gt;accelerate write-intensive operations (as mentioned in&lt;/em&gt; &lt;a href=\"https://docs.databricks.com/en/delta/tune-file-size.html#autotune-workload\"&gt;here&lt;/a&gt;). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little bit confused about why having smaller files would be good for write operations (I&amp;#39;m guessing it may have something to do with parallelization, but not sure), and also confirm that (if I understood correctly) we want large files for improving reads on tables but small files for improving writes.&lt;/p&gt;\n\n&lt;p&gt;Could anyone shed some light on this questions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?auto=webp&amp;s=9dd59568b8579947f05ce66ee028655ef14e64d6", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=99613d282007d0bcc41947bc7f0846da94adca04", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400ef45c57444e53fb95c1358e9a0b6419c3112e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ed83d9a6c1afb35b8be4de3f85b722298d1c3d6", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=768e111879e31b88e5a61b81d8d367edaa5e5351", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2a359111feb6e4d3ffa529f6614614a63914c4e", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/weHg2vXuXJrvIzt7BevX9in3FR2J78iQjABoSHU6aQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6e5d40f18830851f93eb2158f465da573a5df80", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xyl6t", "is_robot_indexable": true, "report_reasons": null, "author": "DUM00", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xyl6t/1_gb_optimize_vs_128_mb_auto_optimize_file_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xyl6t/1_gb_optimize_vs_128_mb_auto_optimize_file_size/", "subreddit_subscribers": 131665, "created_utc": 1696257954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know for sql there is data lemur, leetcode, hacker rank but for big data , visualisation etc where to prepare that? Should I use chat gpt ? Is it a good idea", "author_fullname": "t2_8d8kaiqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some resources for interview preparation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xx761", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696254694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know for sql there is data lemur, leetcode, hacker rank but for big data , visualisation etc where to prepare that? Should I use chat gpt ? Is it a good idea&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16xx761", "is_robot_indexable": true, "report_reasons": null, "author": "lemmeguessindian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xx761/what_are_some_resources_for_interview_preparation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xx761/what_are_some_resources_for_interview_preparation/", "subreddit_subscribers": 131665, "created_utc": 1696254694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just trying to do some calculations on data pipelines via Synapse, mainly the costings for activity runs.\n\nForEach &amp; Switches (iterations &amp; conditionals), do they have to be included in the cost as a activity run cost? They don't fall under Azure Integration Runtime or Self Hosted so I assume not?\n\nThanks!", "author_fullname": "t2_2ybq8ird", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Synapse Analytics - Data Pipeline - Costings on Activity Runs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xt4op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696242822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just trying to do some calculations on data pipelines via Synapse, mainly the costings for activity runs.&lt;/p&gt;\n\n&lt;p&gt;ForEach &amp;amp; Switches (iterations &amp;amp; conditionals), do they have to be included in the cost as a activity run cost? They don&amp;#39;t fall under Azure Integration Runtime or Self Hosted so I assume not?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xt4op", "is_robot_indexable": true, "report_reasons": null, "author": "downsy2019", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xt4op/azure_synapse_analytics_data_pipeline_costings_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xt4op/azure_synapse_analytics_data_pipeline_costings_on/", "subreddit_subscribers": 131665, "created_utc": 1696242822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This script illustrates advanced features of Apache Iceberg tables, including branching, tagging, and time-travel. It starts with setting up a sample database and tables, followed by detailed examples of branching operations, querying branches/tags and writing to branches.\n\n\n``` sql jsx  title=\"Let's create a database and some sample tables for the queries\"\nCREATE DATABASE IF NOT EXISTS common_queries_demo_db;\n\n\nCREATE TABLE IF NOT EXISTS iceberg_demo_db.employees_mysql_external\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n    url \"jdbc:mysql://iomete-tutorial.cetmtjnompsh.eu-central-1.rds.amazonaws.com:3306/employees\",\n    dbtable \"employees.employees\",\n    driver 'com.mysql.cj.jdbc.Driver',\n    user 'tutorial_user',\n    password '9tVDVEKp'\n);\n\nCREATE TABLE iceberg_demo_db.employees\nAS\nSELECT *\nFROM iceberg_demo_db.employees_mysql_external;\n-- Schema: emp_no INT, birth_date DATE, first_name STRING, last_name STRING, gender STRING, hire_date DATE\n```\n\n## Branching DDL\n\n```sql jsx  title=\"CREATE audit branch at the latest snapshot with default retention\"\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit;\n```\n\n```sql jsx  title=\"CREATE audit branch at snapshot 1234 with default retention.\"\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit\n    AS OF VERSION 1234;\n```\n\n```sql jsx  title=\"CREATE audit branch at snapshot 1234, retain audit branch for 30 days\"\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit\n    AS OF VERSION 1234 RETAIN 30 DAYS;\n```\n\n```sql jsx  title=\"CREATE historical-tag at the latest snapshot with default retention\"\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag;\n```\n\n```sql jsx  title=\"CREATE historical-tag at snapshot 1234 with default retention\"\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag\n    AS OF VERSION 1234;\n```\n\n```sql jsx  title=\"CREATE historical_tag at snapshot 1234, retain historical_tag for 365 days\"\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag\n    AS OF VERSION 1234 RETAIN 365 DAYS;\n```\n\n```sql jsx  title=\"Replace branch audit's version to 1234 and retention to 60 days\"\nALTER TABLE iceberg_demo_db.employees REPLACE BRANCH audit\n    AS OF VERSION 1234 RETAIN 60 DAYS;\n```\n\n```sql jsx  title=\"Replace branch audit's version to the latest and retention to 60 days\"\nALTER TABLE iceberg_demo_db.employees REPLACE BRANCH audit\n    RETAIN 60 DAYS;\n```\n```sql jsx  title=\"Drop branch audit\"\nALTER TABLE iceberg_demo_db.employees DROP BRANCH audit;\n```\n\n```sql jsx  title=\"Drop tag historical_tag\"\nALTER TABLE iceberg_demo_db.employees DROP TAG historical_tag;\n```\n\n## Branching Queries\n\n```sql jsx  title=\"Prepare branch and tag for the queries\"\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit_branch;\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag;\n```\n\n```sql jsx  title=\"Query the snapshots, their timestamps, and their IDs\"\nselect * from iceberg_demo_db.employees.history;\n```\n\n```sql jsx  title=\"Query branches and tags\"\nSELECT * FROM iceberg_demo_db.employees.refs;\n```\n\n```sql jsx  title=\"Time travel to October 26, 1986 at 01:21:00\"\nSELECT * FROM iceberg_demo_db.employees TIMESTAMP AS OF '1986-10-26 01:21:00';\n```\n\n```sql jsx  title=Timestamps can also be supplied as a Unix timestamp, in seconds\"\nSELECT * FROM iceberg_demo_db.employees TIMESTAMP AS OF 499162860;\n```\n\n```sql jsx  title=\"Time travel to snapshot with id 10963874102873L\"\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF 10963874102873;\n```\n\n```sql jsx  title=\"Time travel to the head snapshot of audit_branch\"\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF 'audit_branch' LIMIT 100;\n```\n\n```sql jsx  title=\"You can use this syntax as well: &lt;db_name&gt;.&lt;table_name&gt;.branch_&lt;branch_name&gt;\"\nSELECT * FROM iceberg_demo_db.employees.branch_audit_branch LIMIT 100;\n```\n\n```sql jsx  title=\"Time travel to the snapshot referenced by the tag historical_tag\"\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF 'historical_tag' LIMIT 100;\n```\n\n```sql jsx  title=\"You can use this syntax as well: &lt;db_name&gt;.&lt;table_name&gt;.tag_&lt;tag_name&gt;\"\nSELECT * FROM iceberg_demo_db.employees.tag_historical_tag LIMIT 100;\n```\n\n\n## Writing to Branches\n\n```sql \nALTER TABLE iceberg_demo_db.employees CREATE BRANCH branch_audit;\n```\n```sql jsx  title=\"INSERT into {audit_branch}. The main branch stays unchanged\"\nINSERT INTO iceberg_demo_db.employees.branch_audit_branch (\n    emp_no,\n    first_name,\n    last_name,\n    gender,\n    birth_date,\n    hire_date\n)\nVALUES (\n    1,\n    'John-Branched',\n    'Doe',\n    'M',\n    CAST('2000-10-01' AS DATE),\n    CAST('2000-10-01' AS DATE)\n);\n```\n```sql jsx  title=\"Main branch doesn't return any row for emp_no=1\"\nSELECT * FROM iceberg_demo_db.employees WHERE emp_no=1;\n```\n```sql jsx  title=\"Branch branch_audit returns the inserted row for emp_no=1\"\nSELECT * FROM iceberg_demo_db.employees.branch_audit_branch WHERE emp_no=1;\n\n-- Branch write is supported for INSERT, UPDATE, DELETE, and MERGE INTO.\n\n-- MERGE INTO iceberg_demo_db.employees.branch_audit_branch t\n\n-- UPDATE iceberg_demo_db.employees.branch_audit_branch AS t1 ...\n\n-- DELETE FROM iceberg_demo_db.employees.branch_audit_branch WHERE emp_no = 2;\n\n```\n```sql jsx  title=\"Clean up\"\nDROP TABLE iceberg_demo_db.employees PURGE;\nDROP TABLE iceberg_demo_db.employees_mysql_external;\n\nDROP DATABASE iceberg_demo_db;\n```\n\nFor more details [Iceberg Features - Branching, Tagging and Time-Travel](https://iomete.com/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=post)", "author_fullname": "t2_8v89x0vgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg Advanced Features - Branching, Tagging and Time-Travel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xr03q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696234770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This script illustrates advanced features of Apache Iceberg tables, including branching, tagging, and time-travel. It starts with setting up a sample database and tables, followed by detailed examples of branching operations, querying branches/tags and writing to branches.&lt;/p&gt;\n\n&lt;p&gt;``` sql jsx  title=&amp;quot;Let&amp;#39;s create a database and some sample tables for the queries&amp;quot;\nCREATE DATABASE IF NOT EXISTS common_queries_demo_db;&lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE IF NOT EXISTS iceberg_demo_db.employees_mysql_external\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n    url &amp;quot;jdbc:mysql://iomete-tutorial.cetmtjnompsh.eu-central-1.rds.amazonaws.com:3306/employees&amp;quot;,\n    dbtable &amp;quot;employees.employees&amp;quot;,\n    driver &amp;#39;com.mysql.cj.jdbc.Driver&amp;#39;,\n    user &amp;#39;tutorial_user&amp;#39;,\n    password &amp;#39;9tVDVEKp&amp;#39;\n);&lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE iceberg_demo_db.employees\nAS\nSELECT *\nFROM iceberg_demo_db.employees_mysql_external;\n-- Schema: emp_no INT, birth_date DATE, first_name STRING, last_name STRING, gender STRING, hire_date DATE\n```&lt;/p&gt;\n\n&lt;h2&gt;Branching DDL&lt;/h2&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE audit branch at the latest snapshot with default retention&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE audit branch at snapshot 1234 with default retention.&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit\n    AS OF VERSION 1234;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE audit branch at snapshot 1234, retain audit branch for 30 days&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit\n    AS OF VERSION 1234 RETAIN 30 DAYS;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE historical-tag at the latest snapshot with default retention&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE historical-tag at snapshot 1234 with default retention&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag\n    AS OF VERSION 1234;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;CREATE historical_tag at snapshot 1234, retain historical_tag for 365 days&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag\n    AS OF VERSION 1234 RETAIN 365 DAYS;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Replace branch audit&amp;#39;s version to 1234 and retention to 60 days&amp;quot;\nALTER TABLE iceberg_demo_db.employees REPLACE BRANCH audit\n    AS OF VERSION 1234 RETAIN 60 DAYS;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Replace branch audit&amp;#39;s version to the latest and retention to 60 days&amp;quot;\nALTER TABLE iceberg_demo_db.employees REPLACE BRANCH audit\n    RETAIN 60 DAYS;\n&lt;/code&gt;\n&lt;code&gt;sql jsx  title=&amp;quot;Drop branch audit&amp;quot;\nALTER TABLE iceberg_demo_db.employees DROP BRANCH audit;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Drop tag historical_tag&amp;quot;\nALTER TABLE iceberg_demo_db.employees DROP TAG historical_tag;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;h2&gt;Branching Queries&lt;/h2&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Prepare branch and tag for the queries&amp;quot;\nALTER TABLE iceberg_demo_db.employees CREATE BRANCH audit_branch;\nALTER TABLE iceberg_demo_db.employees CREATE TAG historical_tag;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Query the snapshots, their timestamps, and their IDs&amp;quot;\nselect * from iceberg_demo_db.employees.history;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Query branches and tags&amp;quot;\nSELECT * FROM iceberg_demo_db.employees.refs;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Time travel to October 26, 1986 at 01:21:00&amp;quot;\nSELECT * FROM iceberg_demo_db.employees TIMESTAMP AS OF &amp;#39;1986-10-26 01:21:00&amp;#39;;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=Timestamps can also be supplied as a Unix timestamp, in seconds&amp;quot;\nSELECT * FROM iceberg_demo_db.employees TIMESTAMP AS OF 499162860;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Time travel to snapshot with id 10963874102873L&amp;quot;\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF 10963874102873;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Time travel to the head snapshot of audit_branch&amp;quot;\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF &amp;#39;audit_branch&amp;#39; LIMIT 100;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;You can use this syntax as well: &amp;lt;db_name&amp;gt;.&amp;lt;table_name&amp;gt;.branch_&amp;lt;branch_name&amp;gt;&amp;quot;\nSELECT * FROM iceberg_demo_db.employees.branch_audit_branch LIMIT 100;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;Time travel to the snapshot referenced by the tag historical_tag&amp;quot;\nSELECT * FROM iceberg_demo_db.employees VERSION AS OF &amp;#39;historical_tag&amp;#39; LIMIT 100;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sql jsx  title=&amp;quot;You can use this syntax as well: &amp;lt;db_name&amp;gt;.&amp;lt;table_name&amp;gt;.tag_&amp;lt;tag_name&amp;gt;&amp;quot;\nSELECT * FROM iceberg_demo_db.employees.tag_historical_tag LIMIT 100;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;h2&gt;Writing to Branches&lt;/h2&gt;\n\n&lt;p&gt;&lt;code&gt;sql \nALTER TABLE iceberg_demo_db.employees CREATE BRANCH branch_audit;\n&lt;/code&gt;\n&lt;code&gt;sql jsx  title=&amp;quot;INSERT into {audit_branch}. The main branch stays unchanged&amp;quot;\nINSERT INTO iceberg_demo_db.employees.branch_audit_branch (\n    emp_no,\n    first_name,\n    last_name,\n    gender,\n    birth_date,\n    hire_date\n)\nVALUES (\n    1,\n    &amp;#39;John-Branched&amp;#39;,\n    &amp;#39;Doe&amp;#39;,\n    &amp;#39;M&amp;#39;,\n    CAST(&amp;#39;2000-10-01&amp;#39; AS DATE),\n    CAST(&amp;#39;2000-10-01&amp;#39; AS DATE)\n);\n&lt;/code&gt;\n&lt;code&gt;sql jsx  title=&amp;quot;Main branch doesn&amp;#39;t return any row for emp_no=1&amp;quot;\nSELECT * FROM iceberg_demo_db.employees WHERE emp_no=1;\n&lt;/code&gt;\n```sql jsx  title=&amp;quot;Branch branch_audit returns the inserted row for emp_no=1&amp;quot;\nSELECT * FROM iceberg_demo_db.employees.branch_audit_branch WHERE emp_no=1;&lt;/p&gt;\n\n&lt;p&gt;-- Branch write is supported for INSERT, UPDATE, DELETE, and MERGE INTO.&lt;/p&gt;\n\n&lt;p&gt;-- MERGE INTO iceberg_demo_db.employees.branch_audit_branch t&lt;/p&gt;\n\n&lt;p&gt;-- UPDATE iceberg_demo_db.employees.branch_audit_branch AS t1 ...&lt;/p&gt;\n\n&lt;p&gt;-- DELETE FROM iceberg_demo_db.employees.branch_audit_branch WHERE emp_no = 2;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n&lt;/code&gt;sql jsx  title=&amp;quot;Clean up&amp;quot;\nDROP TABLE iceberg_demo_db.employees PURGE;\nDROP TABLE iceberg_demo_db.employees_mysql_external;&lt;/p&gt;\n\n&lt;p&gt;DROP DATABASE iceberg_demo_db;\n```&lt;/p&gt;\n\n&lt;p&gt;For more details &lt;a href=\"https://iomete.com/?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=post\"&gt;Iceberg Features - Branching, Tagging and Time-Travel&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?auto=webp&amp;s=af5e777f23593e5bb38dc3811422059eeefa4741", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff3e3c0b4da57c60a643839add92dd13b796357", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37dcf99cf18d66b6cd7eef712ad50c387eff6d35", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2f9a8eba52fb8084cb591ea0baef498b8ebea07", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb82fa9bb9cadd1fe1d84d16fa4430a4ab22667f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=411d99d558ad4df3a293dd01c5c7316f314fa5c8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/gT2MfCXk6SgYZgEb7pS10x7y0UsRlSe9P8l41qneiRo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4335ab833720fb241a4717cae0993d3e324f686", "width": 1080, "height": 567}], "variants": {}, "id": "rex-r0064JV_QCfALD_KXuSs33wCWVXAKbGFRkN-Rzc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xr03q", "is_robot_indexable": true, "report_reasons": null, "author": "dataguy777", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xr03q/iceberg_advanced_features_branching_tagging_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xr03q/iceberg_advanced_features_branching_tagging_and/", "subreddit_subscribers": 131665, "created_utc": 1696234770.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}