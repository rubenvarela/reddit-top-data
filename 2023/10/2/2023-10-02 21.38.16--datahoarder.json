{"kind": "Listing", "data": {"after": "t3_16xpiq8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I decided to run my own SSD data retention experiment. It's quite limited and a bit anecdotal, but still it's a data point.\n\nI used four cheap Leven 128GB SATA 2.5\" TLC SSD's.\n\nTwo of those SSD's I torture tested by writing over 280TB each with random data. This was painful because these SSD's had no DRAM cache and averaged about 60 MB/sec, and I did give a few minutes of breather time between each SSD fill and file delete, so took over 3 months to complete. I just used an old laptop I had and put it to work using a PowerShell script.\n\nAverage temperature of SSD's during write was about 63C.\n\nThe other two SSD's I kept fresh, unused, to be used as control SSD's.\n\nAfter the tortured SSD's were complete, I then wrote the same data to each SSD, WORN and FRESH ones, took an MD5 hash of the data and verified all the data matches on each SSD.\n\nI then tucked those SSD's in an anti-static bag in my home office averaging probably 24-25C year round. This was September 2, 2022.\n\nThe plan is to read one each of a torture SSD and a fresh SSD after 1 year, then the other set after 2 years. Then let those both sit for another 2 years and then read that data.\n\nHere's the schedule:\n\n    SSD 1 -  WORN:  1 YEAR 2023SEP\n                    3 YEAR 2025SEP\n    \n    SSD 3 - FRESH:  1 YEAR 2023SEP\n                    3 YEAR 2025SEP\n    \n    SSD 2 -  WORN:  2 YEAR 2024SEP\n                    4 YEAR 2026SEP\n    \n    SSD 4 - FRESH:  2 YEAR 2024SEP\n                    4 YEAR 2026SEP\n\nToday, October 1, 2023 (about a month later than originally anticipated) I read the first set of disks and verified hashes, and **EVERYTHING VALIDATED FINE!**\n\nSo I guess in a year we'll see the 2 year results.\n\nHere's images at Imgur if you're interested: https://imgur.com/a/x06TpxR", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1 Year Update - SSD / NAND Data Retention Unpowered", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xk59l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 253, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 253, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696212256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I decided to run my own SSD data retention experiment. It&amp;#39;s quite limited and a bit anecdotal, but still it&amp;#39;s a data point.&lt;/p&gt;\n\n&lt;p&gt;I used four cheap Leven 128GB SATA 2.5&amp;quot; TLC SSD&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Two of those SSD&amp;#39;s I torture tested by writing over 280TB each with random data. This was painful because these SSD&amp;#39;s had no DRAM cache and averaged about 60 MB/sec, and I did give a few minutes of breather time between each SSD fill and file delete, so took over 3 months to complete. I just used an old laptop I had and put it to work using a PowerShell script.&lt;/p&gt;\n\n&lt;p&gt;Average temperature of SSD&amp;#39;s during write was about 63C.&lt;/p&gt;\n\n&lt;p&gt;The other two SSD&amp;#39;s I kept fresh, unused, to be used as control SSD&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;After the tortured SSD&amp;#39;s were complete, I then wrote the same data to each SSD, WORN and FRESH ones, took an MD5 hash of the data and verified all the data matches on each SSD.&lt;/p&gt;\n\n&lt;p&gt;I then tucked those SSD&amp;#39;s in an anti-static bag in my home office averaging probably 24-25C year round. This was September 2, 2022.&lt;/p&gt;\n\n&lt;p&gt;The plan is to read one each of a torture SSD and a fresh SSD after 1 year, then the other set after 2 years. Then let those both sit for another 2 years and then read that data.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the schedule:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SSD 1 -  WORN:  1 YEAR 2023SEP\n                3 YEAR 2025SEP\n\nSSD 3 - FRESH:  1 YEAR 2023SEP\n                3 YEAR 2025SEP\n\nSSD 2 -  WORN:  2 YEAR 2024SEP\n                4 YEAR 2026SEP\n\nSSD 4 - FRESH:  2 YEAR 2024SEP\n                4 YEAR 2026SEP\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Today, October 1, 2023 (about a month later than originally anticipated) I read the first set of disks and verified hashes, and &lt;strong&gt;EVERYTHING VALIDATED FINE!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I guess in a year we&amp;#39;ll see the 2 year results.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s images at Imgur if you&amp;#39;re interested: &lt;a href=\"https://imgur.com/a/x06TpxR\"&gt;https://imgur.com/a/x06TpxR&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?auto=webp&amp;s=c99044e4bfc328959fdb620caaad32e4e1035661", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3a8e16a654df6757e597375612255e78c630393", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ecfc7fc920cf82e467bd8d5a048be17f1347d3e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=12a72a856bed6b09c6398c9237ce5561f37ce1a8", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb9d2cb16561996eda0a3048341fb5f2054f2ced", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db62cbf33a40ab8d281b3ae92c98e0750764e7ae", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/lbXRg3jvqc7QIZKG-w7PyUoh5qOYNv6Nk1aFKeStGGQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c15e1373a9bff4f120033961edf968a57b739941", "width": 1080, "height": 810}], "variants": {}, "id": "i0RaEeSS6L0sRX0Uv8wrzM6pd4-GsibtpYQ4GtimNXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "16xk59l", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16xk59l/1_year_update_ssd_nand_data_retention_unpowered/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xk59l/1_year_update_ssd_nand_data_retention_unpowered/", "subreddit_subscribers": 704438, "created_utc": 1696212256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is the downside of buying SMR (except the low speeds)?\n\nDo SMR drives more prone to bit-rot because of the way they hold data?", "author_fullname": "t2_i5ldbq630", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the downside of buying SMR (except the low speeds)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xf4mg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696199475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the downside of buying SMR (except the low speeds)?&lt;/p&gt;\n\n&lt;p&gt;Do SMR drives more prone to bit-rot because of the way they hold data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xf4mg", "is_robot_indexable": true, "report_reasons": null, "author": "Material-XS", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16xf4mg/what_is_the_downside_of_buying_smr_except_the_low/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xf4mg/what_is_the_downside_of_buying_smr_except_the_low/", "subreddit_subscribers": 704438, "created_utc": 1696199475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So... in 2019 or something like so I decided to watch 1 movie for every year since 1895 till $current\\_year but sad true is most fun for me is looking for movies, downloading them (so far I have only one screen recording case) and adding into Kodi. \n\n&amp;#x200B;\n\nAny tips how keep it low as downloading and catologing movies took quite a time", "author_fullname": "t2_11am0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Movie downloading addiction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xzy69", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696261217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So... in 2019 or something like so I decided to watch 1 movie for every year since 1895 till $current_year but sad true is most fun for me is looking for movies, downloading them (so far I have only one screen recording case) and adding into Kodi. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any tips how keep it low as downloading and catologing movies took quite a time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xzy69", "is_robot_indexable": true, "report_reasons": null, "author": "wytrzeszcz", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xzy69/movie_downloading_addiction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xzy69/movie_downloading_addiction/", "subreddit_subscribers": 704438, "created_utc": 1696261217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Every few years, Pioneer releases a new generation of **internal Blu-ray drives**, (usually) consisting of a high-end drive, a middle-range model and a low-end drive.\n\nThen, they release **external versions** of the middle and low-end models, but never an external version of the high-end drive.\n\nIt's really weird!\n\nBecause computer cases with 5.25\" bays are being phased out, I'm forced to look at external drives, but I'm not satisfied with purchasing a lower-quality model when a better one exists.\n\nHope someone has some info on this. Thanks!", "author_fullname": "t2_arkymz1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Pioneer never incorporates their high-end internal drives into external enclosures?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xgig0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696202711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every few years, Pioneer releases a new generation of &lt;strong&gt;internal Blu-ray drives&lt;/strong&gt;, (usually) consisting of a high-end drive, a middle-range model and a low-end drive.&lt;/p&gt;\n\n&lt;p&gt;Then, they release &lt;strong&gt;external versions&lt;/strong&gt; of the middle and low-end models, but never an external version of the high-end drive.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s really weird!&lt;/p&gt;\n\n&lt;p&gt;Because computer cases with 5.25&amp;quot; bays are being phased out, I&amp;#39;m forced to look at external drives, but I&amp;#39;m not satisfied with purchasing a lower-quality model when a better one exists.&lt;/p&gt;\n\n&lt;p&gt;Hope someone has some info on this. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xgig0", "is_robot_indexable": true, "report_reasons": null, "author": "alexpinkish", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xgig0/why_pioneer_never_incorporates_their_highend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xgig0/why_pioneer_never_incorporates_their_highend/", "subreddit_subscribers": 704438, "created_utc": 1696202711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf\n\n(EDIT: Published January 2023 so fairly recent)\n\nIt addresses SSD endurance (long term unpowered storage) and generally how SSD's operate.\n\nBut regarding HDD's, particularly interesting is addressing the Hard Drive Workload Rating and why it's the same regardless of capacity (cherry picked specific entries below):\n\n&gt; Why Doesn\u2019t Workload Increase with Capacity?\n&gt; \n&gt; There is also no ability to wear-level the amount of data that is transferred through any specific head. While populating a greater number of heads and platters in a higher-capacity drive will mean that on average each head will transfer less data, the higher number of components provides more potential points of failure. \n&gt; \n&gt; The failure of a single head is typically deemed a failure condition for the entire drive. For example, if a large drive containing 20 heads experiences a failure of only 1 head the drive must be taken out of service and replaced.\n&gt; \n&gt; Across a spectrum where a drive has 6 heads and 3 platters up through a drive which has 20 heads and 10 platters, the failure rates for a given host workload are close to identical.\n&gt; \n&gt; In a single-drive scenario, an HDD that is operated beyond its workload rating does not mean that there is any specific concern about its ability to perform its duty. A drive that is beyond its workload rating that is still operating properly\u2014and for which SMART or SAS log data do not show any red flags\u2014is not expected to individually fail simply by being beyond the rating. HDD workload ratings are best understood as a population-level prediction of failure rates.", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interesting White Paper about SSD Endurance and HDD Workload Ratings.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y013y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "White Paper", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1696261669.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696261395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf\"&gt;https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/white-paper/white-paper-ssd-endurance-and-hdd-workloads.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(EDIT: Published January 2023 so fairly recent)&lt;/p&gt;\n\n&lt;p&gt;It addresses SSD endurance (long term unpowered storage) and generally how SSD&amp;#39;s operate.&lt;/p&gt;\n\n&lt;p&gt;But regarding HDD&amp;#39;s, particularly interesting is addressing the Hard Drive Workload Rating and why it&amp;#39;s the same regardless of capacity (cherry picked specific entries below):&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Why Doesn\u2019t Workload Increase with Capacity?&lt;/p&gt;\n\n&lt;p&gt;There is also no ability to wear-level the amount of data that is transferred through any specific head. While populating a greater number of heads and platters in a higher-capacity drive will mean that on average each head will transfer less data, the higher number of components provides more potential points of failure. &lt;/p&gt;\n\n&lt;p&gt;The failure of a single head is typically deemed a failure condition for the entire drive. For example, if a large drive containing 20 heads experiences a failure of only 1 head the drive must be taken out of service and replaced.&lt;/p&gt;\n\n&lt;p&gt;Across a spectrum where a drive has 6 heads and 3 platters up through a drive which has 20 heads and 10 platters, the failure rates for a given host workload are close to identical.&lt;/p&gt;\n\n&lt;p&gt;In a single-drive scenario, an HDD that is operated beyond its workload rating does not mean that there is any specific concern about its ability to perform its duty. A drive that is beyond its workload rating that is still operating properly\u2014and for which SMART or SAS log data do not show any red flags\u2014is not expected to individually fail simply by being beyond the rating. HDD workload ratings are best understood as a population-level prediction of failure rates.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "16y013y", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16y013y/interesting_white_paper_about_ssd_endurance_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y013y/interesting_white_paper_about_ssd_endurance_and/", "subreddit_subscribers": 704438, "created_utc": 1696261395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nLong time reading over the posts here, looking for some advice, all the help is welcome. Sorry for the long post.\n\nI have a home setup of currently 3 RAID 1 arrays being one of two 2TB drives and two arrays of 4TB drives each. The oldest drive has 7.2 years of powered on hours, the youngest has about 3 years of powered on hours. All drives are SATA and I run mdadm on Ubuntu Server 20.04, the filesystem is ext4 on all drives. The machine is a desktop Intel i7 8700 with 32GB RAM non-ECC.\n\nAs the storage units are in a low free space condition and few file modifications, I started to backup less frequently and have some old backups lying around, but as I am starting to migrate everything, I bought a Seagate One Touch 20TB USB drive (STLC20000400) and I have backed up almost everything. I intend to buy another backup drive in order to have at least two backups as I know RAID is not backup (I just don't know which secondary backup should I use, I think I must not buy another drive exactly equal as the one I already have).\n\nI am planning to replace the six drives with a single RAID 1 setup made of two Seagate Exos 20TB, but I would like to have a more robust setup in terms of protection against data corruption, bit rot and such things. I recently have been more aware of that and I am afraid of these possibilities (I thought I was already more or less protected).\n\nReading about lots of filesystems besides ext4, I get ZFS(XFS?) and BTRFS. I see people complaining about BTRFS saying that it is not mature enough or has some bugs and others about ZFS saying that you must have ECC RAM in order to be safe, because some people have already lost data because of bad integrity checks.\n\nBut one thing I didn't hear much about is about different block sizes that SAS drives offer. Exos 20TB has SATA and SAS options and those SAS options support 512, 520 and 528 and also 4096, 4160 and 4224 which is unusual for me as I have only contact with consumer grade equipment. I read that those block sizes are for some kind of hardware level checksum or data parity check. How does that work? Does it work with a simple setup like Ubuntu Server and a consumer grade machine without ECC RAM and running software RAID 1 mdadm?\n\nIf yes, is it a better solution than adopting BTRFS and ZFS(XFS?)? I find a lot of articles about ZFS but not XFS, only the ones saying that XFS is the open source implementation of ZFS, but nothing much more about it. Which SAS card do you recommend to connect these drives? I intend to keep using Ubuntu as I use the machine for other purposes like running web applications and other tasks along with data storage.\n\nHere is the Seagate Exos datasheet: https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-channel-DS2080-2111GB-en\\_EM.pdf\n\nSorry for the long post, first time posting here \ud83d\ude05\n\nThanks for the help in advance.\n\nEdit: grammar correction", "author_fullname": "t2_rd9c6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bit rot prevention and overall integrity... which filesystem or kind of drive to choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xkbx4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696251456.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696212760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Long time reading over the posts here, looking for some advice, all the help is welcome. Sorry for the long post.&lt;/p&gt;\n\n&lt;p&gt;I have a home setup of currently 3 RAID 1 arrays being one of two 2TB drives and two arrays of 4TB drives each. The oldest drive has 7.2 years of powered on hours, the youngest has about 3 years of powered on hours. All drives are SATA and I run mdadm on Ubuntu Server 20.04, the filesystem is ext4 on all drives. The machine is a desktop Intel i7 8700 with 32GB RAM non-ECC.&lt;/p&gt;\n\n&lt;p&gt;As the storage units are in a low free space condition and few file modifications, I started to backup less frequently and have some old backups lying around, but as I am starting to migrate everything, I bought a Seagate One Touch 20TB USB drive (STLC20000400) and I have backed up almost everything. I intend to buy another backup drive in order to have at least two backups as I know RAID is not backup (I just don&amp;#39;t know which secondary backup should I use, I think I must not buy another drive exactly equal as the one I already have).&lt;/p&gt;\n\n&lt;p&gt;I am planning to replace the six drives with a single RAID 1 setup made of two Seagate Exos 20TB, but I would like to have a more robust setup in terms of protection against data corruption, bit rot and such things. I recently have been more aware of that and I am afraid of these possibilities (I thought I was already more or less protected).&lt;/p&gt;\n\n&lt;p&gt;Reading about lots of filesystems besides ext4, I get ZFS(XFS?) and BTRFS. I see people complaining about BTRFS saying that it is not mature enough or has some bugs and others about ZFS saying that you must have ECC RAM in order to be safe, because some people have already lost data because of bad integrity checks.&lt;/p&gt;\n\n&lt;p&gt;But one thing I didn&amp;#39;t hear much about is about different block sizes that SAS drives offer. Exos 20TB has SATA and SAS options and those SAS options support 512, 520 and 528 and also 4096, 4160 and 4224 which is unusual for me as I have only contact with consumer grade equipment. I read that those block sizes are for some kind of hardware level checksum or data parity check. How does that work? Does it work with a simple setup like Ubuntu Server and a consumer grade machine without ECC RAM and running software RAID 1 mdadm?&lt;/p&gt;\n\n&lt;p&gt;If yes, is it a better solution than adopting BTRFS and ZFS(XFS?)? I find a lot of articles about ZFS but not XFS, only the ones saying that XFS is the open source implementation of ZFS, but nothing much more about it. Which SAS card do you recommend to connect these drives? I intend to keep using Ubuntu as I use the machine for other purposes like running web applications and other tasks along with data storage.&lt;/p&gt;\n\n&lt;p&gt;Here is the Seagate Exos datasheet: &lt;a href=\"https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-channel-DS2080-2111GB-en%5C_EM.pdf\"&gt;https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-channel-DS2080-2111GB-en\\_EM.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long post, first time posting here \ud83d\ude05&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help in advance.&lt;/p&gt;\n\n&lt;p&gt;Edit: grammar correction&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "About 15TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xkbx4", "is_robot_indexable": true, "report_reasons": null, "author": "Shypers", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16xkbx4/bit_rot_prevention_and_overall_integrity_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xkbx4/bit_rot_prevention_and_overall_integrity_which/", "subreddit_subscribers": 704438, "created_utc": 1696212760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am currently trying to think of a long-term storage solution that a \"fairly\" non-technical person could retrieve photos and video from 30 to 40 years from now. \n\nI am attempting to build a generational digital archiving solution that would allow my currently young children (and potential grandchildren) to access the photos and videos that we take of us now.\n\nCurrently, we make hard copies of all of our photos and store them in fire-proof folders. However, nothing is currently done with video.\n\nI would be very interested to know the solutions others have come up with, best practices, and how others have solved this issue.", "author_fullname": "t2_3xwzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best long-term (30-year) storage solution for photos and video with the (most likely) easiest retrieval?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xmoz7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696219599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently trying to think of a long-term storage solution that a &amp;quot;fairly&amp;quot; non-technical person could retrieve photos and video from 30 to 40 years from now. &lt;/p&gt;\n\n&lt;p&gt;I am attempting to build a generational digital archiving solution that would allow my currently young children (and potential grandchildren) to access the photos and videos that we take of us now.&lt;/p&gt;\n\n&lt;p&gt;Currently, we make hard copies of all of our photos and store them in fire-proof folders. However, nothing is currently done with video.&lt;/p&gt;\n\n&lt;p&gt;I would be very interested to know the solutions others have come up with, best practices, and how others have solved this issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xmoz7", "is_robot_indexable": true, "report_reasons": null, "author": "Leg0z", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xmoz7/best_longterm_30year_storage_solution_for_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xmoz7/best_longterm_30year_storage_solution_for_photos/", "subreddit_subscribers": 704438, "created_utc": 1696219599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "EDIT: Turns out timestamps got corrected after copy/verify process was done in Teracopy, with except of root folder (but it's just 1 folder, so that's fine)\n\nRight now I'm in halfway of copying data from hdd to ssd using Teracopy and realized that folders in ssd have refreshed modified date stamps (files are fine), despite Teracopy having enabled timestamps in folder/filer transfer settings. I'd rather not redo copying file, 1TB+ of small files from hdd takes lot of time :( Is it something that Teracopy will correct after copy proccess is done, or is it some bug? If not, how can I fix it quickly with some window tool app?\n\nhttps://preview.redd.it/bcl9nebtoorb1.png?width=181&amp;format=png&amp;auto=webp&amp;s=fded662878a4fea8954a43e44a1aeb64c9602d94", "author_fullname": "t2_af77w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Correcting date stamps of folders after copying from hdd to ssd with Teracopy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 137, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bcl9nebtoorb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/bcl9nebtoorb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed0a0719dc3cc7955e0ea523b2eec494fe020b8a"}], "s": {"y": 178, "x": 181, "u": "https://preview.redd.it/bcl9nebtoorb1.png?width=181&amp;format=png&amp;auto=webp&amp;s=fded662878a4fea8954a43e44a1aeb64c9602d94"}, "id": "bcl9nebtoorb1"}}, "name": "t3_16xi2sg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8zrnkkjlqRZVUNKozriiQt00pIOE5txnkhOxRv2N31U.jpg", "edited": 1696240933.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696206717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: Turns out timestamps got corrected after copy/verify process was done in Teracopy, with except of root folder (but it&amp;#39;s just 1 folder, so that&amp;#39;s fine)&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m in halfway of copying data from hdd to ssd using Teracopy and realized that folders in ssd have refreshed modified date stamps (files are fine), despite Teracopy having enabled timestamps in folder/filer transfer settings. I&amp;#39;d rather not redo copying file, 1TB+ of small files from hdd takes lot of time :( Is it something that Teracopy will correct after copy proccess is done, or is it some bug? If not, how can I fix it quickly with some window tool app?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bcl9nebtoorb1.png?width=181&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fded662878a4fea8954a43e44a1aeb64c9602d94\"&gt;https://preview.redd.it/bcl9nebtoorb1.png?width=181&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fded662878a4fea8954a43e44a1aeb64c9602d94&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xi2sg", "is_robot_indexable": true, "report_reasons": null, "author": "magictrashbox", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xi2sg/correcting_date_stamps_of_folders_after_copying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xi2sg/correcting_date_stamps_of_folders_after_copying/", "subreddit_subscribers": 704438, "created_utc": 1696206717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all! I'm trying to build my first NAS, and wanted to make sure I wasn't missing anything before pulling the trigger!\n\n&amp;#x200B;\n\n[https://ca.pcpartpicker.com/list/WWvqmD](https://ca.pcpartpicker.com/list/WWvqmD)\n\n* i3-10100 (for Plex QuickSync, although I was hoping for something cheaper)\n* 16Gb DDR4-3200 ram. It's RGB, so you know it's fast /s (had it lying around)\n* Asus prime H570M-Plus\n* 2.5/10 Gb/s ethernet adapter (Later buy, had trouble finding a cheaper mobo with 2.5)\n* Cooler master N400\n* eVGA 500w 80+ Gold\n\n&amp;#x200B;\n\nI want to throw unraid on it, and use it to host some docker services, plex, and a perforce server. Is there anything on here I should worry will come back to bite me, or anything that seems over/under spec?\n\nAlso for storage, is there any way for me to buy a single 12TB drive, and upgrade to RAID5 in the future with 2 additional drives?  From what I understand, you can't go from RAID0/RAID1 to RAID5 without just transferring data over to a new set of drives.", "author_fullname": "t2_ndu0t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First-time NAS builder - Will this suffice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y5a5i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696273656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all! I&amp;#39;m trying to build my first NAS, and wanted to make sure I wasn&amp;#39;t missing anything before pulling the trigger!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ca.pcpartpicker.com/list/WWvqmD\"&gt;https://ca.pcpartpicker.com/list/WWvqmD&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;i3-10100 (for Plex QuickSync, although I was hoping for something cheaper)&lt;/li&gt;\n&lt;li&gt;16Gb DDR4-3200 ram. It&amp;#39;s RGB, so you know it&amp;#39;s fast /s (had it lying around)&lt;/li&gt;\n&lt;li&gt;Asus prime H570M-Plus&lt;/li&gt;\n&lt;li&gt;2.5/10 Gb/s ethernet adapter (Later buy, had trouble finding a cheaper mobo with 2.5)&lt;/li&gt;\n&lt;li&gt;Cooler master N400&lt;/li&gt;\n&lt;li&gt;eVGA 500w 80+ Gold&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to throw unraid on it, and use it to host some docker services, plex, and a perforce server. Is there anything on here I should worry will come back to bite me, or anything that seems over/under spec?&lt;/p&gt;\n\n&lt;p&gt;Also for storage, is there any way for me to buy a single 12TB drive, and upgrade to RAID5 in the future with 2 additional drives?  From what I understand, you can&amp;#39;t go from RAID0/RAID1 to RAID5 without just transferring data over to a new set of drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y5a5i", "is_robot_indexable": true, "report_reasons": null, "author": "GriMw0lf69", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y5a5i/firsttime_nas_builder_will_this_suffice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y5a5i/firsttime_nas_builder_will_this_suffice/", "subreddit_subscribers": 704438, "created_utc": 1696273656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm currently using some of the circuit boards from shucking WD externals and I'm trying to reduce some of my cable mess.\n\n[splitter 1](https://www.amazon.ca/Splitter-Female-Adppter-Security-Cameras/dp/B07VPVC1H1/ref=sr_1_12?keywords=dc%2Bpower%2Bsplitter&amp;qid=1696204055&amp;s=electronics&amp;sr=1-12&amp;th=1)\n\n[splitter 2](https://www.amazon.ca/ANLINK-Female-Splitter-Adapter-Cameras/dp/B09CV9HF4H/ref=sr_1_8?keywords=dc%2Bpower%2Bsplitter&amp;qid=1696204055&amp;s=electronics&amp;sr=1-8&amp;th=1)\n\n[dc power adapter](https://i.imgur.com/KEVYuXu.jpg)\n\nwould either of the dc power splitters work for my external hard drives?", "author_fullname": "t2_99qdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do dc power splitters work for external hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xhcld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696204832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using some of the circuit boards from shucking WD externals and I&amp;#39;m trying to reduce some of my cable mess.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.ca/Splitter-Female-Adppter-Security-Cameras/dp/B07VPVC1H1/ref=sr_1_12?keywords=dc%2Bpower%2Bsplitter&amp;amp;qid=1696204055&amp;amp;s=electronics&amp;amp;sr=1-12&amp;amp;th=1\"&gt;splitter 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.ca/ANLINK-Female-Splitter-Adapter-Cameras/dp/B09CV9HF4H/ref=sr_1_8?keywords=dc%2Bpower%2Bsplitter&amp;amp;qid=1696204055&amp;amp;s=electronics&amp;amp;sr=1-8&amp;amp;th=1\"&gt;splitter 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/KEVYuXu.jpg\"&gt;dc power adapter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;would either of the dc power splitters work for my external hard drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?auto=webp&amp;s=c2895ba38d376406e0d0f41a35f1982b9bfc4c37", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f932826bcd6306491fd532610077b77e6937199a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=51680f1fa5527ed7f8f3fead7d367fa0bab172f2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fa0f4b7c91a8322205247a5319ccdbf23086f9c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36dde8a87404aad8397f77218309564dee5db042", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b9991f3809c195e4145e01b9d61d111b37550c1", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/06BlTdIAUY7paZFl-dOA2jxpY5GMSUZ9ALj9cGvldIk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9383f441069f604ee6f16565b8dd8056159948b", "width": 1080, "height": 810}], "variants": {}, "id": "EgtYngi09-rHpoM_89JG4QsX5I_7CPV0ysiyrj8Rj3I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "64TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xhcld", "is_robot_indexable": true, "report_reasons": null, "author": "MrPsych", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/16xhcld/do_dc_power_splitters_work_for_external_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xhcld/do_dc_power_splitters_work_for_external_hard/", "subreddit_subscribers": 704438, "created_utc": 1696204832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://serverpartdeals.com/collections/manufacturer-recertified-drives/products/western-digital-ultrastar-dc-hc530-wuh721414aln6l4-0f31278-14tb-7-2k-rpm-sata-6gb-s-4kn-3-5-recertified-hard-drive", "author_fullname": "t2_2oq4ldty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is thuds recertified drive good and worth buying WD ultra star", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y80xb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696279901.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://serverpartdeals.com/collections/manufacturer-recertified-drives/products/western-digital-ultrastar-dc-hc530-wuh721414aln6l4-0f31278-14tb-7-2k-rpm-sata-6gb-s-4kn-3-5-recertified-hard-drive\"&gt;https://serverpartdeals.com/collections/manufacturer-recertified-drives/products/western-digital-ultrastar-dc-hc530-wuh721414aln6l4-0f31278-14tb-7-2k-rpm-sata-6gb-s-4kn-3-5-recertified-hard-drive&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?auto=webp&amp;s=bef394b7ca298f17a9073e324ff90d7bf4585db9", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3812e62d0e4310f92b0be8b4740ff8d06db61b95", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e2046089b3278a19626493460c1feb009bee5c8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=704d5e935c6454567d01db7d7ede39d754c869f4", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b58b89993ff1e99925ba300f5776049227babe7c", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ca9c4e56ccd6af73afb71c82d6e27dd88eb2cb2", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/TkyP760JhMl_BfXj6JTEvhRPNZYm3ShFR99adLJ00pE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de4c30037a0a6759fa4f133bceefb6bd1b9b0e6d", "width": 1080, "height": 1080}], "variants": {}, "id": "QQdeUyNg_ZOcYoIdgxZ9d5zzvrSlp0F5d13aCQ__si8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y80xb", "is_robot_indexable": true, "report_reasons": null, "author": "thephillman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y80xb/is_thuds_recertified_drive_good_and_worth_buying/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y80xb/is_thuds_recertified_drive_good_and_worth_buying/", "subreddit_subscribers": 704438, "created_utc": 1696279901.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI've recently been going through my movie collection and reorganizing it to be easier to use.\nOne of the ways I was trying to make it easier was putting movie series together like all the Die Hard movies in one folder or the Jurassic Park movies together.\n\nThe problem is, I want the movies to remain in order which would be easiest to do by putting a number in front of the filename for the order they should be in, but I don't like this method.\n\nI was wondering if any of you guys have a better method for keeping movies in order in the one folder? I've been looking around for a better way but all I've found are methods for movies in their own folders as storage for Plex and similar.\n\nThanks ahead of time for any help :)", "author_fullname": "t2_2k7tgdh2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organising Movie Series in Folders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xsjfq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696240672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently been going through my movie collection and reorganizing it to be easier to use.\nOne of the ways I was trying to make it easier was putting movie series together like all the Die Hard movies in one folder or the Jurassic Park movies together.&lt;/p&gt;\n\n&lt;p&gt;The problem is, I want the movies to remain in order which would be easiest to do by putting a number in front of the filename for the order they should be in, but I don&amp;#39;t like this method.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if any of you guys have a better method for keeping movies in order in the one folder? I&amp;#39;ve been looking around for a better way but all I&amp;#39;ve found are methods for movies in their own folders as storage for Plex and similar.&lt;/p&gt;\n\n&lt;p&gt;Thanks ahead of time for any help :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xsjfq", "is_robot_indexable": true, "report_reasons": null, "author": "AmIHumanMaybe", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xsjfq/organising_movie_series_in_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xsjfq/organising_movie_series_in_folders/", "subreddit_subscribers": 704438, "created_utc": 1696240672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nSo I bought an old server with 32gb ddr3, xeon 1230v3\nI have an raidcontroller with 4gbddr4, capable for ssd cache. and backup battery which is doing nothing at the moment.\n\nApplication I want to use it mainly for synchronized writes.\n\nQuestion: Can I combine the raid controller so data is safer/lower latencey etc...\n\n\n- possible to use as hba controller with raid cache. Pool sync turn off?\n- also hdd in raid 5 give single disk volume to zfs? Problems with zfs if raid 5 gets expanded?\n\n- Just give up leave default settings because simpler is better?. \n\nThx\n\nSorry if this question is answered befor..\n\n\n-", "author_fullname": "t2_nfjbku4y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So, Another zfs, Zil question...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xr1hg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696234918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;So I bought an old server with 32gb ddr3, xeon 1230v3\nI have an raidcontroller with 4gbddr4, capable for ssd cache. and backup battery which is doing nothing at the moment.&lt;/p&gt;\n\n&lt;p&gt;Application I want to use it mainly for synchronized writes.&lt;/p&gt;\n\n&lt;p&gt;Question: Can I combine the raid controller so data is safer/lower latencey etc...&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;possible to use as hba controller with raid cache. Pool sync turn off?&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;also hdd in raid 5 give single disk volume to zfs? Problems with zfs if raid 5 gets expanded?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Just give up leave default settings because simpler is better?. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thx&lt;/p&gt;\n\n&lt;p&gt;Sorry if this question is answered befor..&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xr1hg", "is_robot_indexable": true, "report_reasons": null, "author": "Sad_Faithlessness873", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xr1hg/so_another_zfs_zil_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xr1hg/so_another_zfs_zil_question/", "subreddit_subscribers": 704438, "created_utc": 1696234918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Years ago it was primarily just HDDs, but getting burned on data loss a few times I decided optical discs were the way to go. From the CD-R to DVD, and later on Dual layer BDRs... I'm now again on the lookout for a stable storage solution that won't kill my wallet in the long-term. \n\nI popped in a few BD-DL and BDR backups, and my drives couldn't even begin to read the first byte. This had me in a panic - these were discs stored in a relatively stable environment, and not used frequently, simply in a binder sleeve, and maybe pulled once or twice a year. Well, seems to be the last time for this batch. \n\nThey were burned &lt; 2-3yrs ago, it never occurred to me they would just fail to read in that length of time. My intention was to eventually start buying batches of HDDs and migrate them to this as a permanent solution, something hopefully more shelf stable - maybe doubles or triples of each unit especially with HDD storage getting cheaper and cheaper. \n\nAm I setting myself up for failure yet again? I've only one other BD drive to consider testing with, but it's nowhere near me at this moment. There are other discs that appear to read just fine, which is great and not so great at the same time, as it just further confirms my suspicion that these burned discs have somehow managed to become faulty within a few years span...\n\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nEDIT 1: So I'm considering this as a backup solution for the moment:\nThree HDD arrays, 2 as master copies, 1 as an actively used array for a media library. \nThen, investing into M-Disc media as well, easily the biggest cost in this endeavor. \n\nHDD arrays will be referred to as the following: \n\n* HDD 1   &lt;---master copy 1 - shelved\n\n* HDD 2   &lt;---master copy 2 - shelved\n\n* HDD 3   &lt;---Daily used copy (media library, photos, etc)\n\n\nThe idea would be to periodically, every 6-9 months or year, rewrite the data between HDD 1, 2, &amp; 3. Just taking a free weekend to rewrite these drives with the same data to mitigate any problems regarding magnetic degradation. \n\n\n* Verify data integrity across the three drives and confirm that they continue to match. \n\n* Overwrite 3, with 1. Verify across all three again they continue to match. \n\n* Overwrite 1, with 2. Verify again. \n\n* Overwrite 2, with 1. Verify again, and each write preceded with a low format. \n\n\nMeanwhile, the library is backed up onto M-discs if proven to be a viable long-term solution. \nWish tape was an affordable option... \n\nMoney being the biggest factor. I've long wanted to build my collection and just have a running NAS to whip up a show on the fly. Currently I just pull what I want from the disc, until again today, I ran into my copies from the past 2-3 years not being wholly readable and ended up not even being able to pull a single file from the disc. It certainly sees the original folder structure, and files within, but gets stuck at 0%, windows perpetually calculating how long it's going to take to begin its copy process. Meanwhile I pull a different disc of the same make, and it starts right away with no issue. The previous disc, its entire batch were all found to exihibit the same behavior in the meantime... signaling to me it's that batch, from the same spindle of BD-DL discs that altogether failed...", "author_fullname": "t2_hnjew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to find yet another backup solution...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xjv25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696217267.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696211510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Years ago it was primarily just HDDs, but getting burned on data loss a few times I decided optical discs were the way to go. From the CD-R to DVD, and later on Dual layer BDRs... I&amp;#39;m now again on the lookout for a stable storage solution that won&amp;#39;t kill my wallet in the long-term. &lt;/p&gt;\n\n&lt;p&gt;I popped in a few BD-DL and BDR backups, and my drives couldn&amp;#39;t even begin to read the first byte. This had me in a panic - these were discs stored in a relatively stable environment, and not used frequently, simply in a binder sleeve, and maybe pulled once or twice a year. Well, seems to be the last time for this batch. &lt;/p&gt;\n\n&lt;p&gt;They were burned &amp;lt; 2-3yrs ago, it never occurred to me they would just fail to read in that length of time. My intention was to eventually start buying batches of HDDs and migrate them to this as a permanent solution, something hopefully more shelf stable - maybe doubles or triples of each unit especially with HDD storage getting cheaper and cheaper. &lt;/p&gt;\n\n&lt;p&gt;Am I setting myself up for failure yet again? I&amp;#39;ve only one other BD drive to consider testing with, but it&amp;#39;s nowhere near me at this moment. There are other discs that appear to read just fine, which is great and not so great at the same time, as it just further confirms my suspicion that these burned discs have somehow managed to become faulty within a few years span...&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;EDIT 1: So I&amp;#39;m considering this as a backup solution for the moment:\nThree HDD arrays, 2 as master copies, 1 as an actively used array for a media library. \nThen, investing into M-Disc media as well, easily the biggest cost in this endeavor. &lt;/p&gt;\n\n&lt;p&gt;HDD arrays will be referred to as the following: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;HDD 1   &amp;lt;---master copy 1 - shelved&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;HDD 2   &amp;lt;---master copy 2 - shelved&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;HDD 3   &amp;lt;---Daily used copy (media library, photos, etc)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The idea would be to periodically, every 6-9 months or year, rewrite the data between HDD 1, 2, &amp;amp; 3. Just taking a free weekend to rewrite these drives with the same data to mitigate any problems regarding magnetic degradation. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Verify data integrity across the three drives and confirm that they continue to match. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Overwrite 3, with 1. Verify across all three again they continue to match. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Overwrite 1, with 2. Verify again. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Overwrite 2, with 1. Verify again, and each write preceded with a low format. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Meanwhile, the library is backed up onto M-discs if proven to be a viable long-term solution. \nWish tape was an affordable option... &lt;/p&gt;\n\n&lt;p&gt;Money being the biggest factor. I&amp;#39;ve long wanted to build my collection and just have a running NAS to whip up a show on the fly. Currently I just pull what I want from the disc, until again today, I ran into my copies from the past 2-3 years not being wholly readable and ended up not even being able to pull a single file from the disc. It certainly sees the original folder structure, and files within, but gets stuck at 0%, windows perpetually calculating how long it&amp;#39;s going to take to begin its copy process. Meanwhile I pull a different disc of the same make, and it starts right away with no issue. The previous disc, its entire batch were all found to exihibit the same behavior in the meantime... signaling to me it&amp;#39;s that batch, from the same spindle of BD-DL discs that altogether failed...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xjv25", "is_robot_indexable": true, "report_reasons": null, "author": "Redoubt9000", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xjv25/looking_to_find_yet_another_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xjv25/looking_to_find_yet_another_backup_solution/", "subreddit_subscribers": 704438, "created_utc": 1696211510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a substantial collection of physical DVDs, which I have converted to 1:1 ISOs. Now I am setting up JellyFin as a home media system. It will not play ISOs , but will play MKVs. \n\nI am using Dumbofab DVD ripper to convert the ISOs to MKV's that are lossless, but their docs are outdated in so much as what output formats for MKV to choose. \n\nConsidering that I am converting DVDs (  480i or 480p ) to MKV, is there any advantage of using the higher output configurations? Should I use the preset MKV, MKV HD or MKV 4k? \n\nI am not interested in any compression. I want my 1:1 ISOs to be losslessly converted to MKV, however my lizard brain tells me that you can't make 4K out of 480p and it would just be a waste of file space ( I hoard! ) with no improvement in quality. I have been known to be wrong. Any suggestions? I have burned up Google but had no direct answer. ", "author_fullname": "t2_yppex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ISO conversion question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ximfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696208200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a substantial collection of physical DVDs, which I have converted to 1:1 ISOs. Now I am setting up JellyFin as a home media system. It will not play ISOs , but will play MKVs. &lt;/p&gt;\n\n&lt;p&gt;I am using Dumbofab DVD ripper to convert the ISOs to MKV&amp;#39;s that are lossless, but their docs are outdated in so much as what output formats for MKV to choose. &lt;/p&gt;\n\n&lt;p&gt;Considering that I am converting DVDs (  480i or 480p ) to MKV, is there any advantage of using the higher output configurations? Should I use the preset MKV, MKV HD or MKV 4k? &lt;/p&gt;\n\n&lt;p&gt;I am not interested in any compression. I want my 1:1 ISOs to be losslessly converted to MKV, however my lizard brain tells me that you can&amp;#39;t make 4K out of 480p and it would just be a waste of file space ( I hoard! ) with no improvement in quality. I have been known to be wrong. Any suggestions? I have burned up Google but had no direct answer. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16ximfx", "is_robot_indexable": true, "report_reasons": null, "author": "Delchi", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16ximfx/iso_conversion_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16ximfx/iso_conversion_question/", "subreddit_subscribers": 704438, "created_utc": 1696208200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI need advice on a major project. I have to scan 200 articles (approximately 10 pages each) so around 2000 pages. I then need to convert the scanned articles into a word document or some other digital text format. The reason for this is that I want to upload the text from scanned articles on a website. So three steps:\n\n1) Scan a hard copy of an article\n2) Convert the scan to a digital text file of some sort\n3) Upload the digital text to a website\n\nA few questions:\n\nA) What is the most efficient way to accomplish this? \nB) Is there a program that will do both steps and 1 and 2 simultaneously?\nC) Is there a way to do all of this on my phone with an app?\n\nThanks you!!", "author_fullname": "t2_b1on3h09", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed for archive - scanning articles for website p", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y87v0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696280348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I need advice on a major project. I have to scan 200 articles (approximately 10 pages each) so around 2000 pages. I then need to convert the scanned articles into a word document or some other digital text format. The reason for this is that I want to upload the text from scanned articles on a website. So three steps:&lt;/p&gt;\n\n&lt;p&gt;1) Scan a hard copy of an article\n2) Convert the scan to a digital text file of some sort\n3) Upload the digital text to a website&lt;/p&gt;\n\n&lt;p&gt;A few questions:&lt;/p&gt;\n\n&lt;p&gt;A) What is the most efficient way to accomplish this? \nB) Is there a program that will do both steps and 1 and 2 simultaneously?\nC) Is there a way to do all of this on my phone with an app?&lt;/p&gt;\n\n&lt;p&gt;Thanks you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y87v0", "is_robot_indexable": true, "report_reasons": null, "author": "Fragrant-Cap-4462", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y87v0/advice_needed_for_archive_scanning_articles_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y87v0/advice_needed_for_archive_scanning_articles_for/", "subreddit_subscribers": 704438, "created_utc": 1696280348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been running openmediaserver for many years. I have a 4 drive configuration using snapraid (3 data drives + one parity drive) with mergerfs.  I use the NAS for storing photos, music and a few movies/videos/torrents.  I sometimes use bitorrent on the NAS and I have a plex server on the nas. Updating OMS and Plex is a pain though. It always has some complications. I'm at the point that I need to do a major update and will likely just start from scratch and do a fresh install.  I currently store about 1TB of data on the NAS and I have a separate 3TB drive for backups.\n\nI need to downsize to a smaller place and I can go three ways. 1) Update and keep my OMS NAS (it's in a typical desktop case), 2) get something like a synology NAS, or 3) just get a 3TB drive and put it in my desktop.\n\nI'm really leaning towards option 3, I'm testing running plex media server on my desktop and it works well. Updates can be done very easily.  Option 2 is somewhat appealing because it's a bit more plug n play than my OMS and it is smaller in size.  OMS is becoming less appealing because of the technical side of running it and the space it takes up. As well, file transfers are pretty slow on it.\n\nDowntime isn't important so I don't think I really need redundancy from raid, but maybe I misunderstand what a mirrored drive might give me. Like if I don't have a mirrored drive and my drive slowly starts crapping out, maybe I won't notice it and I'll even start making backups with missing data.\n\nGiven my main use (storage and serving media files via plex or UPNP), option 3 seems good. Is there any other thoughts that maybe I'm missing? \n\n&amp;#x200B;", "author_fullname": "t2_haxjv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Move from NAS to a single drive in my Desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y7jmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696278789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running openmediaserver for many years. I have a 4 drive configuration using snapraid (3 data drives + one parity drive) with mergerfs.  I use the NAS for storing photos, music and a few movies/videos/torrents.  I sometimes use bitorrent on the NAS and I have a plex server on the nas. Updating OMS and Plex is a pain though. It always has some complications. I&amp;#39;m at the point that I need to do a major update and will likely just start from scratch and do a fresh install.  I currently store about 1TB of data on the NAS and I have a separate 3TB drive for backups.&lt;/p&gt;\n\n&lt;p&gt;I need to downsize to a smaller place and I can go three ways. 1) Update and keep my OMS NAS (it&amp;#39;s in a typical desktop case), 2) get something like a synology NAS, or 3) just get a 3TB drive and put it in my desktop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really leaning towards option 3, I&amp;#39;m testing running plex media server on my desktop and it works well. Updates can be done very easily.  Option 2 is somewhat appealing because it&amp;#39;s a bit more plug n play than my OMS and it is smaller in size.  OMS is becoming less appealing because of the technical side of running it and the space it takes up. As well, file transfers are pretty slow on it.&lt;/p&gt;\n\n&lt;p&gt;Downtime isn&amp;#39;t important so I don&amp;#39;t think I really need redundancy from raid, but maybe I misunderstand what a mirrored drive might give me. Like if I don&amp;#39;t have a mirrored drive and my drive slowly starts crapping out, maybe I won&amp;#39;t notice it and I&amp;#39;ll even start making backups with missing data.&lt;/p&gt;\n\n&lt;p&gt;Given my main use (storage and serving media files via plex or UPNP), option 3 seems good. Is there any other thoughts that maybe I&amp;#39;m missing? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y7jmu", "is_robot_indexable": true, "report_reasons": null, "author": "dougshmish", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y7jmu/move_from_nas_to_a_single_drive_in_my_desktop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y7jmu/move_from_nas_to_a_single_drive_in_my_desktop/", "subreddit_subscribers": 704438, "created_utc": 1696278789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have some .mkv videos and want to reduce the space they occupy, so i want to convert it to MP4, but don\u00b4t want to lose any audio quality. Is there a way to do it?", "author_fullname": "t2_1xrwfw4m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can I convert MKV to MP4 without losing audio quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16y7czw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696278385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some .mkv videos and want to reduce the space they occupy, so i want to convert it to MP4, but don\u00b4t want to lose any audio quality. Is there a way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y7czw", "is_robot_indexable": true, "report_reasons": null, "author": "SushiVoador", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y7czw/can_i_convert_mkv_to_mp4_without_losing_audio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y7czw/can_i_convert_mkv_to_mp4_without_losing_audio/", "subreddit_subscribers": 704438, "created_utc": 1696278385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "About 6 months ago, I purchased an Orico 3559RU3 [Orico 80TB Enterprise Raid Enclosure](https://www.newegg.com/orico-3559rus3-bk/p/0VN-0003-00023?Item=9SIA1DS0CD0629), and filled it with 3 [Seagate 16TB HDD Enterprise Edition](https://www.amazon.com/gp/product/B07SPFPKF4/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1), and set it up as a RAID 5 with Orico HW RAID Manager.\n\nI pretty much just used it to dump everything from multiple externals, so it's all in once place for storage.  As my computer is running off a 2 TB SSD and watching things off that was easier.  But I'm now going back and trying to organize things, and watch some movies from it, and I can't.  The movies and TV shows buffer every minute or so.  Also, as I'm trying to move files from my C: to the RAID, it's incredibly slow, often getting locked up on a file for minutes at a time (sometimes it's a small file, sometimes it's a big file).  \n\nThis data solution is causing more trouble than benefit, since if I want to watch a movie from the drive, I need to copy it to my C drive, which takes several minutes to watch it.  \n\nMy two questions are:\n\n1) If there maybe something I did wrong that's causing this to run so slowly? I know other people run Plex servers off their RAIDs, and that was the plan, but I definitely can't.\n\n2) If this solution is not going to be able to work for my needs (being able to watch files off of it, whether locally or via Plex), is there a safe and easy way to dismantle 21TB of data on a RAID without having to buy new drives?\n\nThank you in advance.  I used to know a lot about computers, but I've forgotten so much and so much has changed since my last build that I don't have any idea what I'm doing anymore. ", "author_fullname": "t2_7pokn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow read and write speeds on my RAID 5. Can't watch videos off of it without lagging", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y4qyy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696272404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About 6 months ago, I purchased an Orico 3559RU3 &lt;a href=\"https://www.newegg.com/orico-3559rus3-bk/p/0VN-0003-00023?Item=9SIA1DS0CD0629\"&gt;Orico 80TB Enterprise Raid Enclosure&lt;/a&gt;, and filled it with 3 &lt;a href=\"https://www.amazon.com/gp/product/B07SPFPKF4/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1\"&gt;Seagate 16TB HDD Enterprise Edition&lt;/a&gt;, and set it up as a RAID 5 with Orico HW RAID Manager.&lt;/p&gt;\n\n&lt;p&gt;I pretty much just used it to dump everything from multiple externals, so it&amp;#39;s all in once place for storage.  As my computer is running off a 2 TB SSD and watching things off that was easier.  But I&amp;#39;m now going back and trying to organize things, and watch some movies from it, and I can&amp;#39;t.  The movies and TV shows buffer every minute or so.  Also, as I&amp;#39;m trying to move files from my C: to the RAID, it&amp;#39;s incredibly slow, often getting locked up on a file for minutes at a time (sometimes it&amp;#39;s a small file, sometimes it&amp;#39;s a big file).  &lt;/p&gt;\n\n&lt;p&gt;This data solution is causing more trouble than benefit, since if I want to watch a movie from the drive, I need to copy it to my C drive, which takes several minutes to watch it.  &lt;/p&gt;\n\n&lt;p&gt;My two questions are:&lt;/p&gt;\n\n&lt;p&gt;1) If there maybe something I did wrong that&amp;#39;s causing this to run so slowly? I know other people run Plex servers off their RAIDs, and that was the plan, but I definitely can&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;2) If this solution is not going to be able to work for my needs (being able to watch files off of it, whether locally or via Plex), is there a safe and easy way to dismantle 21TB of data on a RAID without having to buy new drives?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.  I used to know a lot about computers, but I&amp;#39;ve forgotten so much and so much has changed since my last build that I don&amp;#39;t have any idea what I&amp;#39;m doing anymore. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y4qyy", "is_robot_indexable": true, "report_reasons": null, "author": "ojuditho", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y4qyy/slow_read_and_write_speeds_on_my_raid_5_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y4qyy/slow_read_and_write_speeds_on_my_raid_5_cant/", "subreddit_subscribers": 704438, "created_utc": 1696272404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Question  \nthis mega link contains many folders which are crucial as oxygen to me, im afraid one day the link gets corrupted or so, so i want to make a back up of it, but as far as i know that mega has a 5gb download limit per day, so whats your say?", "author_fullname": "t2_5l3isyb1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can i make a back up of a 1TB mega link to an external hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16y3bo4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696269156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Question&lt;br/&gt;\nthis mega link contains many folders which are crucial as oxygen to me, im afraid one day the link gets corrupted or so, so i want to make a back up of it, but as far as i know that mega has a 5gb download limit per day, so whats your say?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16y3bo4", "is_robot_indexable": true, "report_reasons": null, "author": "doepual", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16y3bo4/how_can_i_make_a_back_up_of_a_1tb_mega_link_to_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16y3bo4/how_can_i_make_a_back_up_of_a_1tb_mega_link_to_an/", "subreddit_subscribers": 704438, "created_utc": 1696269156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As the title states, I need some ideas for storing around 50TB of video streams which grow by 8TB per month. \n\nI can\u2019t upload the streams to a public site. Although there may be some interest in them, I would violate copyright. So this is not an option. \n\nOVH\u2018s cold archive is the cheapest option I have found so far. The only drawback is, I can\u2019t use the material whenever I want to. \n\nSo now I\u2019m wondering: How to store and keep them accessible at the lowest cost? \n\nI\u2019m mostly using my Jellyfin server to access the files. They are all MP4. \n\nAny cloud storage would be capable too. \n\nAppreciate any idea!", "author_fullname": "t2_b7a8w160", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to store video streams?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xz61o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696259373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states, I need some ideas for storing around 50TB of video streams which grow by 8TB per month. &lt;/p&gt;\n\n&lt;p&gt;I can\u2019t upload the streams to a public site. Although there may be some interest in them, I would violate copyright. So this is not an option. &lt;/p&gt;\n\n&lt;p&gt;OVH\u2018s cold archive is the cheapest option I have found so far. The only drawback is, I can\u2019t use the material whenever I want to. &lt;/p&gt;\n\n&lt;p&gt;So now I\u2019m wondering: How to store and keep them accessible at the lowest cost? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m mostly using my Jellyfin server to access the files. They are all MP4. &lt;/p&gt;\n\n&lt;p&gt;Any cloud storage would be capable too. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any idea!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xz61o", "is_robot_indexable": true, "report_reasons": null, "author": "_c0der", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xz61o/where_to_store_video_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xz61o/where_to_store_video_streams/", "subreddit_subscribers": 704438, "created_utc": 1696259373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use Corsair's RM750x power supply on my server.  As the storage needs increase, I was looking for an extra SATA cable for the power supply.  In our country, these original accessories are either unavailable or cost a lot (shipping + import duty).  But luckily, I found the type 3 cable at a local store for a great price.  Both cost 7 USD.  So without a second thought, I bought it.  Because according to these websites type 3 and type 4 SATA cables are the same.\n\n [PSU CABLE COMPATIBILITY (corsair.com)](https://www.corsair.com/us/en/s/psu-cable-compatibility) \n\n [Corsair\u00ae PSU Type 3 Cables Pinout \u2013 PC Mods (pc-mods.com)](https://pc-mods.com/blogs/psu-pinout-repository/corsair-psu-type-3-cables-pinout) \n\n [Corsair\u00ae PSU Type 4 Cables Pinout \u2013 PC Mods (pc-mods.com)](https://pc-mods.com/blogs/psu-pinout-repository/corsair-psu-type-4-cables-pinout) \n\nBut when I came home and compared the two, I saw that they were all the same except for one pin in the middle.  \n\nhttps://preview.redd.it/3q3td68tzprb1.jpg?width=2340&amp;format=pjpg&amp;auto=webp&amp;s=c90e6239c3f55e82f9d583f889863f022e3a654c\n\n&amp;#x200B;\n\nI tested it on an old hard drive.  There was no issue.  My question is, my server is running 24/7, can I\u00a0use\u00a0this\u00a0cable?", "author_fullname": "t2_acowsey3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Corsair type 3 SATA cable on RM750x, Sanity check.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3q3td68tzprb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=32b6d35381a8eacf959749df44730cdbaf576360"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=807f45c6bfb1295f7ab3c4ddfd69a35f8411a30d"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=89f71a5d092c3da3969ac908eb96c51e3d626beb"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d880cd81a734083dc07a69e616239eb4d9bb09f6"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=81309375c69b926e51abe760ebd364bb34ba9da7"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a77bb0bb8f60f53fe2a687fc570f62b43d600272"}], "s": {"y": 4160, "x": 2340, "u": "https://preview.redd.it/3q3td68tzprb1.jpg?width=2340&amp;format=pjpg&amp;auto=webp&amp;s=c90e6239c3f55e82f9d583f889863f022e3a654c"}, "id": "3q3td68tzprb1"}}, "name": "t3_16xnj8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/P6YzNWlA0xBPOILYKDmE3dPycmNGA6QRwSuLo6b7fmw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696222366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Corsair&amp;#39;s RM750x power supply on my server.  As the storage needs increase, I was looking for an extra SATA cable for the power supply.  In our country, these original accessories are either unavailable or cost a lot (shipping + import duty).  But luckily, I found the type 3 cable at a local store for a great price.  Both cost 7 USD.  So without a second thought, I bought it.  Because according to these websites type 3 and type 4 SATA cables are the same.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.corsair.com/us/en/s/psu-cable-compatibility\"&gt;PSU CABLE COMPATIBILITY (corsair.com)&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pc-mods.com/blogs/psu-pinout-repository/corsair-psu-type-3-cables-pinout\"&gt;Corsair\u00ae PSU Type 3 Cables Pinout \u2013 PC Mods (pc-mods.com)&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pc-mods.com/blogs/psu-pinout-repository/corsair-psu-type-4-cables-pinout\"&gt;Corsair\u00ae PSU Type 4 Cables Pinout \u2013 PC Mods (pc-mods.com)&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;But when I came home and compared the two, I saw that they were all the same except for one pin in the middle.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3q3td68tzprb1.jpg?width=2340&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c90e6239c3f55e82f9d583f889863f022e3a654c\"&gt;https://preview.redd.it/3q3td68tzprb1.jpg?width=2340&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c90e6239c3f55e82f9d583f889863f022e3a654c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I tested it on an old hard drive.  There was no issue.  My question is, my server is running 24/7, can I\u00a0use\u00a0this\u00a0cable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xnj8p", "is_robot_indexable": true, "report_reasons": null, "author": "ZealousidealCup4095", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xnj8p/corsair_type_3_sata_cable_on_rm750x_sanity_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xnj8p/corsair_type_3_sata_cable_on_rm750x_sanity_check/", "subreddit_subscribers": 704438, "created_utc": 1696222366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there software that will keep records of which files got moved where, so that if I later look at a file in a destination directory I can look up where that file was copied/moved from? GUI preferred but CLI ok if that's all that's available. Linux or Windows.\n\nI want to consolidate backups from multiple computers onto one backup server computer(which will then be backed up with a cloud service). I want to be able to say which was the source of each file after I sort things by topic/type on the backup computer. I plan to have \"source\" directories like From_Laptop, From_Desktop1 From_Desktop2, From_PortableHDD, where I dump everything from those devices into, then I will move them into topic-separated folders like Backup_Job, Backup_Game_WoW, Backup_Game_CS, Backup_Family etc.\n\nThe software only needs to work on one computer, it doesn't need to be able to work across multiple devices since I'll be moving things to a local directory first.\n\nAnything like that exist?", "author_fullname": "t2_ui7qs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software for tracking where files were copied/moved?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xlnyl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696216475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there software that will keep records of which files got moved where, so that if I later look at a file in a destination directory I can look up where that file was copied/moved from? GUI preferred but CLI ok if that&amp;#39;s all that&amp;#39;s available. Linux or Windows.&lt;/p&gt;\n\n&lt;p&gt;I want to consolidate backups from multiple computers onto one backup server computer(which will then be backed up with a cloud service). I want to be able to say which was the source of each file after I sort things by topic/type on the backup computer. I plan to have &amp;quot;source&amp;quot; directories like From_Laptop, From_Desktop1 From_Desktop2, From_PortableHDD, where I dump everything from those devices into, then I will move them into topic-separated folders like Backup_Job, Backup_Game_WoW, Backup_Game_CS, Backup_Family etc.&lt;/p&gt;\n\n&lt;p&gt;The software only needs to work on one computer, it doesn&amp;#39;t need to be able to work across multiple devices since I&amp;#39;ll be moving things to a local directory first.&lt;/p&gt;\n\n&lt;p&gt;Anything like that exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xlnyl", "is_robot_indexable": true, "report_reasons": null, "author": "ggggthrowawaygggg", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xlnyl/software_for_tracking_where_files_were_copiedmoved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xlnyl/software_for_tracking_where_files_were_copiedmoved/", "subreddit_subscribers": 704438, "created_utc": 1696216475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm running a Debian based NAS with 2 4TB drives, which are currently not holding any files and are unformatted.\n\nThe purpose of this system:\n\n1. Seed files 24/7\n2. Stream content from this device to other local devices\n\nIdeal requirements:\n\n* Remove one of the drives and plug it into another system without ruining my file system or being unusable on another computer.\n* Ability to point to one place for the files, i.e. /NAS/Media/ containing all the movies which happen to be stored on two different drives.\n* Behaving as \"one large drive\" is not necessary, but I would strongly prefer not to have to setup all of my other devices to point to two different drives on the same system for the same type of content.\n\nI was going to setup an LVM for this, but was warned that LVM is neither flexible nor safe, with catastrophic problems if a drive fails/is removed.\n\nIs this a realistic setup? If not, how else would you go about managing this two-drive system?", "author_fullname": "t2_cc458uu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it realistic to setup a NAS with these qualities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xlekk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696215725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a Debian based NAS with 2 4TB drives, which are currently not holding any files and are unformatted.&lt;/p&gt;\n\n&lt;p&gt;The purpose of this system:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Seed files 24/7&lt;/li&gt;\n&lt;li&gt;Stream content from this device to other local devices&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ideal requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Remove one of the drives and plug it into another system without ruining my file system or being unusable on another computer.&lt;/li&gt;\n&lt;li&gt;Ability to point to one place for the files, i.e. /NAS/Media/ containing all the movies which happen to be stored on two different drives.&lt;/li&gt;\n&lt;li&gt;Behaving as &amp;quot;one large drive&amp;quot; is not necessary, but I would strongly prefer not to have to setup all of my other devices to point to two different drives on the same system for the same type of content.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I was going to setup an LVM for this, but was warned that LVM is neither flexible nor safe, with catastrophic problems if a drive fails/is removed.&lt;/p&gt;\n\n&lt;p&gt;Is this a realistic setup? If not, how else would you go about managing this two-drive system?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xlekk", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingAndy", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xlekk/is_it_realistic_to_setup_a_nas_with_these/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xlekk/is_it_realistic_to_setup_a_nas_with_these/", "subreddit_subscribers": 704438, "created_utc": 1696215725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When I bought my Toshiba 1TB External HDD, it wouldn\u2019t let me drag and drop into it. I could only backup with time machine into it and even at that it was slightly glitchy. I decided to buy a SanDisk SSD instead and transfer my HDD files into it so I can return the HDD eventually.\n\nHowever, after I erased my APFS Toshiba drive with disk utility so I could return it the next day, it just kinda fixed itself. It\u2019s now letting me drag and drop into it. It was super annoying before this and even with using terminal commands to fix permissions and contradicting permissions in get info vs terminal, it still wasn\u2019t working. But now it just magically does.\n\nMy worry is, what if the permission issues in the beginning were early sign of a faulty drive and this fix is temporary? Is that possible?\n\nAlso, my SanDisk SSD has survived the washer and is new, but Im also worried about potential water damage there even though it has worked fine 2 days after that (except for one time it unmounted itself).\n\nI\u2019m not sure what to do or what to choose between Toshiba HDD/SanDisk SSD or if I should buy a new external drive altogether to be completely safe. Any advice is appreciated!!\n\nEdit: if relevant at all, the toshiba drive was like $67 with tax, while the sandisk is $100 without tax", "author_fullname": "t2_56p7hm9o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keep HDD or Keep SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xpiq8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696231508.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696229190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I bought my Toshiba 1TB External HDD, it wouldn\u2019t let me drag and drop into it. I could only backup with time machine into it and even at that it was slightly glitchy. I decided to buy a SanDisk SSD instead and transfer my HDD files into it so I can return the HDD eventually.&lt;/p&gt;\n\n&lt;p&gt;However, after I erased my APFS Toshiba drive with disk utility so I could return it the next day, it just kinda fixed itself. It\u2019s now letting me drag and drop into it. It was super annoying before this and even with using terminal commands to fix permissions and contradicting permissions in get info vs terminal, it still wasn\u2019t working. But now it just magically does.&lt;/p&gt;\n\n&lt;p&gt;My worry is, what if the permission issues in the beginning were early sign of a faulty drive and this fix is temporary? Is that possible?&lt;/p&gt;\n\n&lt;p&gt;Also, my SanDisk SSD has survived the washer and is new, but Im also worried about potential water damage there even though it has worked fine 2 days after that (except for one time it unmounted itself).&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not sure what to do or what to choose between Toshiba HDD/SanDisk SSD or if I should buy a new external drive altogether to be completely safe. Any advice is appreciated!!&lt;/p&gt;\n\n&lt;p&gt;Edit: if relevant at all, the toshiba drive was like $67 with tax, while the sandisk is $100 without tax&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16xpiq8", "is_robot_indexable": true, "report_reasons": null, "author": "hploxx", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16xpiq8/keep_hdd_or_keep_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16xpiq8/keep_hdd_or_keep_ssd/", "subreddit_subscribers": 704438, "created_utc": 1696229190.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}