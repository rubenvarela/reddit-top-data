{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most are pretty aware of dbt, but technology moves fast. Has any serious competitor come up? Curious if anyone has tried sqlmesh - what's your first impression?\n\nContex:\nJust generally curious", "author_fullname": "t2_hj2y0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt vs sqlmesh vs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xbiar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696191264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most are pretty aware of dbt, but technology moves fast. Has any serious competitor come up? Curious if anyone has tried sqlmesh - what&amp;#39;s your first impression?&lt;/p&gt;\n\n&lt;p&gt;Contex:\nJust generally curious&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xbiar", "is_robot_indexable": true, "report_reasons": null, "author": "Cryptojacob", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xbiar/dbt_vs_sqlmesh_vs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xbiar/dbt_vs_sqlmesh_vs/", "subreddit_subscribers": 131587, "created_utc": 1696191264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don't want a complex solution which involves many components and there's no need to scale computing beyond a single node.\n\nit seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.\n\nour preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn't require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.\n\nanyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?", "author_fullname": "t2_mojp1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to query JSON and parquet files on S3 for data verification and pipeline building?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wysjd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696159977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a bunch of files on S3 (minio) primarly in JSON and parquet format. We are searching for a simple solution to do some ad hoc sql queries against those files to verify data and see the structure. mainly for the engineers to build the data pipelines in dbt/dagster. we don&amp;#39;t want a complex solution which involves many components and there&amp;#39;s no need to scale computing beyond a single node.&lt;/p&gt;\n\n&lt;p&gt;it seems there are many solutions with a hive metastore like trino/presto. but this requires multiple tools and manual registration of the single files.&lt;/p&gt;\n\n&lt;p&gt;our preferred way would be something which we can point to a s3 bucket and it shows all the files as e.g. views in the database. it shouldn&amp;#39;t require to manually specify the schema/structure of the files. just lookup the structure on read. the files are usually only around 1-20MB. so really small scale.&lt;/p&gt;\n\n&lt;p&gt;anyone have an idea which tool could help us here? And what are you using to engineer your pipeline queries from unstructured data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wysjd", "is_robot_indexable": true, "report_reasons": null, "author": "OneCyrus", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wysjd/what_is_the_best_way_to_query_json_and_parquet/", "subreddit_subscribers": 131587, "created_utc": 1696159977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Planning to buy a Nuc 11 Enthusiast instead of a Steam Deck to upskill myself as well as not screwing my Gaming PC . Are there any ideas where i can start this journey . My aim to setup homelab is :  \n\n1. Networking side with OpenWrt , pfSense \n2. Containers/Virtualization  with Dockers and Vmware \n3. Server Side with Proxmox\n\nShare if its sufficient to run multiple VMs and Containers with this thing as well as share how do you use homelab for upskilling your DE skills .", "author_fullname": "t2_cl5vl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any DEs here have homelabs to upskill , make a dedicated dev env, and do other server side stuff ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x6mf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696180108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Planning to buy a Nuc 11 Enthusiast instead of a Steam Deck to upskill myself as well as not screwing my Gaming PC . Are there any ideas where i can start this journey . My aim to setup homelab is :  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Networking side with OpenWrt , pfSense &lt;/li&gt;\n&lt;li&gt;Containers/Virtualization  with Dockers and Vmware &lt;/li&gt;\n&lt;li&gt;Server Side with Proxmox&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Share if its sufficient to run multiple VMs and Containers with this thing as well as share how do you use homelab for upskilling your DE skills .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x6mf6", "is_robot_indexable": true, "report_reasons": null, "author": "Redxer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x6mf6/any_des_here_have_homelabs_to_upskill_make_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x6mf6/any_des_here_have_homelabs_to_upskill_make_a/", "subreddit_subscribers": 131587, "created_utc": 1696180108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Context**\n\nWe have data pipelines that are a mixture of SQL / python as they are a combo of:\n\n \\- more 'data engineering-y' transformations at the start (SQL)\n\n \\- more 'data science-y' operations further down (python)\n\nWe run on GCP so given we aren't at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.\n\nOrganising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.\n\nHowever, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I'd like to bring it in to the pipelines.\n\n**Question**\n\nHow do you introduce dbt to this infrastructure?\n\nBy having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?\n\nOr by starting with dbt's recommended monorepo, but then meaning a pipeline's logic is now split between:\n\n \\- pipeline repo\n\n \\- that pipeline's section in the dbt mono-repo\n\nI haven't been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?", "author_fullname": "t2_576k5sn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt mono-repo vs individual pipeline repos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16wy2vv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696157594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We have data pipelines that are a mixture of SQL / python as they are a combo of:&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data engineering-y&amp;#39; transformations at the start (SQL)&lt;/p&gt;\n\n&lt;p&gt;- more &amp;#39;data science-y&amp;#39; operations further down (python)&lt;/p&gt;\n\n&lt;p&gt;We run on GCP so given we aren&amp;#39;t at serious scale yet this manifests itself as cloudbuild to deploy our infra which consists of GCP workflows orchestrating http python cloud functions/run on top of Big Query.&lt;/p&gt;\n\n&lt;p&gt;Organising all of that logic into 1 repo per pipeline appears to make sense as we combine all business logic together (in python) along with its orchestration, triggers and deployment code - this allows us to release/version a pipeline repo and deploy that unit of logic all at once.&lt;/p&gt;\n\n&lt;p&gt;However, our sql script management is currently just a bunch of parameterised sql queries inside a file in each repo (and sometimes manual view creation in the GCP console) that lacks the nice features of data testing, lineage, versioning etc that dbt involves so I&amp;#39;d like to bring it in to the pipelines.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you introduce dbt to this infrastructure?&lt;/p&gt;\n\n&lt;p&gt;By having a dbt project within each pipeline repo (and thus forgoing cross-pipeline dependencies in the lineage)?&lt;/p&gt;\n\n&lt;p&gt;Or by starting with dbt&amp;#39;s recommended monorepo, but then meaning a pipeline&amp;#39;s logic is now split between:&lt;/p&gt;\n\n&lt;p&gt;- pipeline repo&lt;/p&gt;\n\n&lt;p&gt;- that pipeline&amp;#39;s section in the dbt mono-repo&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to find much on this on line and was wondering if anyone else has encountered this and how they tackled it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16wy2vv", "is_robot_indexable": true, "report_reasons": null, "author": "mjam03", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16wy2vv/dbt_monorepo_vs_individual_pipeline_repos/", "subreddit_subscribers": 131587, "created_utc": 1696157594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was looking through old posts on this subreddit about system design and came across a comment a couple years ago that discussed a useful scaling exercise to practice for DE interviews: creating a pipeline that ingests 1MB at first, then 1GB, then 10GB, 100GB, 1TB, etc. and then talking about challenges along the way.\n\nI was wondering if this community had some ideas about things to consider as you get further and further up the throughput ladder. Here's a few I've compiled (I assumed the volume at an hourly rate):\n\n&amp;#x200B;\n\n* **@ 1MB / hour**\n   * ingestion: either batch or streaming is possible depending on the nature of the data and our business requirements. Orchestration and processing can live on same machine comfortably.\n   * Throughput is relatively small and should not require distributed processing. Libraries like pandas or numpy would be sufficient for most operations\n   * loading into a relational store or data warehouse should be trivial, though we still need to adopt best practices for designing our schema, managing indexes, etc.\n* **@ 1 GB / hour**\n   * Batch and streaming are both possible, but examine the data to find the most efficient approach. If the data is a single 1GB-sized file arriving hourly, it could be processed in batch, but it wouldn't be ideal to read the whole thing into memory on a lone machine. If the data is from an external source, we also have to pay attention to network I/O. Better to partition the data and have multiple machines read it in parallel. If instead the data is comprised of several small log files or messages in the KB-level, try consuming from an event broker.\n   * Processing data with Pandas on a single machine is possible if scaling vertically, but not ideal. Should switch to a small Spark cluster, or something like Dask. Again, depends on the transformations.\n   * Tools for logging, monitoring pipeline health, and analyzing resource utilization are recommended. (Should be recommended at all levels, but becomes more and more necessary as data scales)\n   * Using an optimized storage format is recommended for large data files (e.g. parquet, avro)\n   * If writing to a relational db, need to be mindful of our transactions/sec and not create strain on the server. (use load balancer and connection pooling)\n* **@ 10 GB / hour**\n   * Horizontal scaling preferred over vertical scaling. Should use a distributed cluster regardless of batch or streaming requirements.\n   * During processing, make sure our joins/transformations aren't creating uneven shards and resulting in bottlenecks on our nodes.\n   * Have strong data governance policies in place for data quality checks, data observability, data lineage, etc.\n   * Continuous monitoring of resource and CPU utilization of the cluster, notifications when thresholds are breached (again, useful at all levels). Also create pipelines for centralized log analysis (with ElasticSearch perhaps?)\n   * Properly partition data in data lake or relational store, with strategies for rolling off data as costs build up.\n   * Optimize compression and indexing wherever possible.\n* **@ 100 GB / hour**\n   * Proper configuration, load balancing, and partitioning of the event broker is essential\n   * Critical to have a properly tuned cluster that can auto-scale to accommodate job size as costs increase.\n   * Watch for bottlenecks in processing, OutOfMemory exceptions are likely if improper join strategies are used.\n   * Clean data, especially data deduplication, is critical for reducing redundant processing.\n   * Writing to traditional relational dbs may struggle to keep up with volume of writes. Distributed databases may be preferred (e.g. Cassandra).\n   * Employ caching liberally, both in serving queries and in processing data\n   * Optimizing queries is crucial, as poorly written SQL can result in long execution and resource contention.\n* **@ 1 TB / hour**\n   * Efficiency in configuring compute and storage is a must. Improperly tuned cloud services can be hugely expensive.\n   * Distributed databases/DWH typically required.\n   * Use an appropriate partitioning strategy in data lake\n   * Avoid processing data that is not necessary for the business, and move data that isn't used to cheaper, long-term storage.\n   * Optimize data model and indexing strategy for efficient queries.\n   * Good data retention policies prevent expensive, unmanageable database growth.\n   * Monitoring and alerting systems should be sophisticated and battle-tested to track overall resource utilization.\n\n# Above all, know how the business plans to use the data, as that will have the biggest influence on design!\n\n**Considerations at all levels:**\n\n* caching\n* security and privacy\n* metadata management\n* CI/CD, testing\n* redundancy and fault-tolerance\n* labor and maintenance overhead\n* cost-complexity ratio\n\n&amp;#x200B;\n\nAnyone have anything else to add? In an interview, I would obviously flesh out a lot of these bullet points.", "author_fullname": "t2_5zyhb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scaling exercise for DE interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x7m4d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696182404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking through old posts on this subreddit about system design and came across a comment a couple years ago that discussed a useful scaling exercise to practice for DE interviews: creating a pipeline that ingests 1MB at first, then 1GB, then 10GB, 100GB, 1TB, etc. and then talking about challenges along the way.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if this community had some ideas about things to consider as you get further and further up the throughput ladder. Here&amp;#39;s a few I&amp;#39;ve compiled (I assumed the volume at an hourly rate):&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;@ 1MB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ingestion: either batch or streaming is possible depending on the nature of the data and our business requirements. Orchestration and processing can live on same machine comfortably.&lt;/li&gt;\n&lt;li&gt;Throughput is relatively small and should not require distributed processing. Libraries like pandas or numpy would be sufficient for most operations&lt;/li&gt;\n&lt;li&gt;loading into a relational store or data warehouse should be trivial, though we still need to adopt best practices for designing our schema, managing indexes, etc.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 1 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Batch and streaming are both possible, but examine the data to find the most efficient approach. If the data is a single 1GB-sized file arriving hourly, it could be processed in batch, but it wouldn&amp;#39;t be ideal to read the whole thing into memory on a lone machine. If the data is from an external source, we also have to pay attention to network I/O. Better to partition the data and have multiple machines read it in parallel. If instead the data is comprised of several small log files or messages in the KB-level, try consuming from an event broker.&lt;/li&gt;\n&lt;li&gt;Processing data with Pandas on a single machine is possible if scaling vertically, but not ideal. Should switch to a small Spark cluster, or something like Dask. Again, depends on the transformations.&lt;/li&gt;\n&lt;li&gt;Tools for logging, monitoring pipeline health, and analyzing resource utilization are recommended. (Should be recommended at all levels, but becomes more and more necessary as data scales)&lt;/li&gt;\n&lt;li&gt;Using an optimized storage format is recommended for large data files (e.g. parquet, avro)&lt;/li&gt;\n&lt;li&gt;If writing to a relational db, need to be mindful of our transactions/sec and not create strain on the server. (use load balancer and connection pooling)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 10 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Horizontal scaling preferred over vertical scaling. Should use a distributed cluster regardless of batch or streaming requirements.&lt;/li&gt;\n&lt;li&gt;During processing, make sure our joins/transformations aren&amp;#39;t creating uneven shards and resulting in bottlenecks on our nodes.&lt;/li&gt;\n&lt;li&gt;Have strong data governance policies in place for data quality checks, data observability, data lineage, etc.&lt;/li&gt;\n&lt;li&gt;Continuous monitoring of resource and CPU utilization of the cluster, notifications when thresholds are breached (again, useful at all levels). Also create pipelines for centralized log analysis (with ElasticSearch perhaps?)&lt;/li&gt;\n&lt;li&gt;Properly partition data in data lake or relational store, with strategies for rolling off data as costs build up.&lt;/li&gt;\n&lt;li&gt;Optimize compression and indexing wherever possible.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 100 GB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Proper configuration, load balancing, and partitioning of the event broker is essential&lt;/li&gt;\n&lt;li&gt;Critical to have a properly tuned cluster that can auto-scale to accommodate job size as costs increase.&lt;/li&gt;\n&lt;li&gt;Watch for bottlenecks in processing, OutOfMemory exceptions are likely if improper join strategies are used.&lt;/li&gt;\n&lt;li&gt;Clean data, especially data deduplication, is critical for reducing redundant processing.&lt;/li&gt;\n&lt;li&gt;Writing to traditional relational dbs may struggle to keep up with volume of writes. Distributed databases may be preferred (e.g. Cassandra).&lt;/li&gt;\n&lt;li&gt;Employ caching liberally, both in serving queries and in processing data&lt;/li&gt;\n&lt;li&gt;Optimizing queries is crucial, as poorly written SQL can result in long execution and resource contention.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;@ 1 TB / hour&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Efficiency in configuring compute and storage is a must. Improperly tuned cloud services can be hugely expensive.&lt;/li&gt;\n&lt;li&gt;Distributed databases/DWH typically required.&lt;/li&gt;\n&lt;li&gt;Use an appropriate partitioning strategy in data lake&lt;/li&gt;\n&lt;li&gt;Avoid processing data that is not necessary for the business, and move data that isn&amp;#39;t used to cheaper, long-term storage.&lt;/li&gt;\n&lt;li&gt;Optimize data model and indexing strategy for efficient queries.&lt;/li&gt;\n&lt;li&gt;Good data retention policies prevent expensive, unmanageable database growth.&lt;/li&gt;\n&lt;li&gt;Monitoring and alerting systems should be sophisticated and battle-tested to track overall resource utilization.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Above all, know how the business plans to use the data, as that will have the biggest influence on design!&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Considerations at all levels:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;caching&lt;/li&gt;\n&lt;li&gt;security and privacy&lt;/li&gt;\n&lt;li&gt;metadata management&lt;/li&gt;\n&lt;li&gt;CI/CD, testing&lt;/li&gt;\n&lt;li&gt;redundancy and fault-tolerance&lt;/li&gt;\n&lt;li&gt;labor and maintenance overhead&lt;/li&gt;\n&lt;li&gt;cost-complexity ratio&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone have anything else to add? In an interview, I would obviously flesh out a lot of these bullet points.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16x7m4d", "is_robot_indexable": true, "report_reasons": null, "author": "LurkLurkington", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x7m4d/scaling_exercise_for_de_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x7m4d/scaling_exercise_for_de_interviews/", "subreddit_subscribers": 131587, "created_utc": 1696182404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.\n\nAfter some time, I have come up with three solutions that I would like to share:\n\n* When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.\n* Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.\n* Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.\n\nBasically, I explained all three solutions in greater details in my newsletter [here](https://dbtips.substack.com/p/get-the-ultimate-developer-experience).\n\nDo you have any other tips of improving DX you can share?", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to improve your developer experience when working with dbt (and not only)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x9maj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1696186924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;About a year ago, I began to realize how suboptimal my experience was while working with dbt and other data engineering projects in my company. As a result, I started exploring ways to improve this situation.&lt;/p&gt;\n\n&lt;p&gt;After some time, I have come up with three solutions that I would like to share:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When working with dbt specifically, use \u201cdbt Power User\u201d plugin for VSCode to get contextual information, suggestions, data lineage for dbt models, etc. It makes the workflow really enjoyable.&lt;/li&gt;\n&lt;li&gt;Incorporate SQLFluff for linting and formatting SQL code. It should bring a consistent code to your project and you finally stop arguing about leading vs trailing commas.&lt;/li&gt;\n&lt;li&gt;Also implement pre-commit hooks, small scripts that run before committing code to Github. These script usually perform various checks on the project, such as YAML validity, documentation and test availability, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, I explained all three solutions in greater details in my newsletter &lt;a href=\"https://dbtips.substack.com/p/get-the-ultimate-developer-experience\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Do you have any other tips of improving DX you can share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?auto=webp&amp;s=d3bb661896828bac4429562f6ce7fff4ef505422", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5af75ad43a854078e3cea834f3ef698663e88cdc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=37ae9fc5f3b20c68d9199ed55882ebca83ef2855", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5685fef764bd1fa720328ebd972cc947f012bd86", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c958b3293f06f32b4616718b83399f29154edee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89fca96637cd1490f814b4b891f7bdc567c62f1f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s0TIqqlC6I0ChDA1sGkNSIbFqZTLX1cxIH_yrPkaYj0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f972567e2eba31f2c54f818bdce4b34235f400e", "width": 1080, "height": 540}], "variants": {}, "id": "ntcEH-D8yVlbOqdDYiN0TVlVBkXGXM6b3WJWBcIF5sw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16x9maj", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x9maj/how_to_improve_your_developer_experience_when/", "subreddit_subscribers": 131587, "created_utc": 1696186924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer with a few years experience in SQL/NoSQL. I've designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.\n\nUnfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I'm trying to get my career back on track. I've been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I'm wondering if you suggest getting deep into that, or what else, and i'd appreciate links to courses/tuts.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_edkp97o8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting back into DB world, should I learn Vector DBs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe9jr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer with a few years experience in SQL/NoSQL. I&amp;#39;ve designed/diagnosed high throughput TPS systems, warehousing clusters, HA, DR, DBA, code reviews.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately due to health reasons I took a few years off with just occasionally solving a brain teaser on LeetCode. I&amp;#39;m trying to get my career back on track. I&amp;#39;ve been reading a lot about AI and frankly i think its the future of search. This quickly drew me to Vector DBs and I&amp;#39;m wondering if you suggest getting deep into that, or what else, and i&amp;#39;d appreciate links to courses/tuts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe9jr", "is_robot_indexable": true, "report_reasons": null, "author": "no_deal_111", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe9jr/getting_back_into_db_world_should_i_learn_vector/", "subreddit_subscribers": 131587, "created_utc": 1696197530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?\n\nI was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.", "author_fullname": "t2_66knxh65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you rather do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xfsg6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696201028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would you rather work on different projects where you implement something and then go to the next project (consulting) or work in a team with a single project developing a single data ecosystem?&lt;/p&gt;\n\n&lt;p&gt;I was wondering what DEs in this sub rather do. I\u2019ve  doing the first one (consulting) for a while, but I was curious about how is the day in the life outside of consulting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xfsg6", "is_robot_indexable": true, "report_reasons": null, "author": "WarNeverChanges1997", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xfsg6/what_would_you_rather_do/", "subreddit_subscribers": 131587, "created_utc": 1696201028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't see any pricing details for glue workflows but I would assume there's an extra cost since it's providing additional functionality. I only see pricing based on DPUs for Glue jobs. Does this mean that if I use Glue Workflows to orchestrate glue jobs, I will only pay for the cost of the glue jobs and the orchestration will just be free? If that's the case, is there any benefit to using Airflow for orchestration of glue jobs if glue workflows can be used?", "author_fullname": "t2_jx5l6r0ay", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any added cost to using Glue workflows with Glue jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x6tkl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696180572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t see any pricing details for glue workflows but I would assume there&amp;#39;s an extra cost since it&amp;#39;s providing additional functionality. I only see pricing based on DPUs for Glue jobs. Does this mean that if I use Glue Workflows to orchestrate glue jobs, I will only pay for the cost of the glue jobs and the orchestration will just be free? If that&amp;#39;s the case, is there any benefit to using Airflow for orchestration of glue jobs if glue workflows can be used?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x6tkl", "is_robot_indexable": true, "report_reasons": null, "author": "IllRepresentative858", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x6tkl/is_there_any_added_cost_to_using_glue_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x6tkl/is_there_any_added_cost_to_using_glue_workflows/", "subreddit_subscribers": 131587, "created_utc": 1696180572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is a lot of overlap between data engineering and analytics for these jobs, so I'll post in both subs. It's my first job out of college, and down the line I'm really looking to be a full fledged tech professional, whatever that means. I'm looking at 2 vastly different offers. They are both at hospital systems, remote isn't an option for either. And I'm not declining either job just because its not remote, I have no leverage as an unemployed college grad desparate for a job. Pay and commute are similar.\n\nOffer 1: Mostly analytics, using SQL and drag-drop software for ETL processes. More emphasis is placed on understanding the stakeholder needs rather than coding. Data architecture is well established by third party vendor. Workload seems fairly low, easy-going office environment. No on call, typical 9-5, 4 days in office.\n\nOffer 2: Getting my hands dirty with Python, SQL, and Linux command line for ETL processes, existing data architecture needs lots of work. Job duties also include providing basic IT phone support for hospital employees. On call, unsure of frequency. 9-5, 5 days in office.\n\nOffer 1 is nice because of the work-life balance, easy and established reporting process is already in place, but it's no code. Offer 2 is enticing because of the amount of raw coding I'll be doing, but also having to answer phone calls from nurses and doctors to troubleshoot basic issues and being on call is a huge downside. Thoughts?", "author_fullname": "t2_5pddzf1hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First job out of college? What would you choose?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xitxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1696209933.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696208771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is a lot of overlap between data engineering and analytics for these jobs, so I&amp;#39;ll post in both subs. It&amp;#39;s my first job out of college, and down the line I&amp;#39;m really looking to be a full fledged tech professional, whatever that means. I&amp;#39;m looking at 2 vastly different offers. They are both at hospital systems, remote isn&amp;#39;t an option for either. And I&amp;#39;m not declining either job just because its not remote, I have no leverage as an unemployed college grad desparate for a job. Pay and commute are similar.&lt;/p&gt;\n\n&lt;p&gt;Offer 1: Mostly analytics, using SQL and drag-drop software for ETL processes. More emphasis is placed on understanding the stakeholder needs rather than coding. Data architecture is well established by third party vendor. Workload seems fairly low, easy-going office environment. No on call, typical 9-5, 4 days in office.&lt;/p&gt;\n\n&lt;p&gt;Offer 2: Getting my hands dirty with Python, SQL, and Linux command line for ETL processes, existing data architecture needs lots of work. Job duties also include providing basic IT phone support for hospital employees. On call, unsure of frequency. 9-5, 5 days in office.&lt;/p&gt;\n\n&lt;p&gt;Offer 1 is nice because of the work-life balance, easy and established reporting process is already in place, but it&amp;#39;s no code. Offer 2 is enticing because of the amount of raw coding I&amp;#39;ll be doing, but also having to answer phone calls from nurses and doctors to troubleshoot basic issues and being on call is a huge downside. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xitxb", "is_robot_indexable": true, "report_reasons": null, "author": "knewtonslol", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xitxb/first_job_out_of_college_what_would_you_choose/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xitxb/first_job_out_of_college_what_would_you_choose/", "subreddit_subscribers": 131587, "created_utc": 1696208771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\"Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?\"\n\nKindly advise video courses and or books \n\nDatacamp\nMike chambers \n\nIf someone has experience please share", "author_fullname": "t2_peoywkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xe83b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696197435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Seeking guidance on learning feature engineering. Should I prioritize Python libraries for feature engineering, like Scikit-learn and MLlib, while maintaining a basic understanding of machine learning, given my background in executing PySpark projects using Databricks? Any recommendations on efficient ways to dive into feature engineering?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Kindly advise video courses and or books &lt;/p&gt;\n\n&lt;p&gt;Datacamp\nMike chambers &lt;/p&gt;\n\n&lt;p&gt;If someone has experience please share&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xe83b", "is_robot_indexable": true, "report_reasons": null, "author": "angadaws", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xe83b/feature_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xe83b/feature_engineering/", "subreddit_subscribers": 131587, "created_utc": 1696197435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my latest blog post, I've summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn't attend can benefit from the latest updates on data governance and cost management.  \n\n\nData Governance Highlights:  \n1) Data Quality Monitoring  \n2) Query Constraint Policies  \n3) Data Access Policies  \n4) Data Governance UI  \n5) Classification UI  \n6) Global PII Classification  \n\n\nCost Control Highlights:  \n1) Budget  \n2) Warehouse Utilization  \n3) SnowLens  \n\n\nBlog Link - [Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023](https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\n\nI want to thank the event organizers and the insightful speakers:  \n\n\n1) [Hemant Raorane](https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I), Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n2) [Sachin Gangwar](https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n3) [Wasim El-Omari](https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc), Principal Architect, Security, Field CTO Office, [Snowflake](https://www.linkedin.com/company/snowflake-computing/)  \n4) [Pawan Mall](https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0), Senior Sales Engineer, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n5) [Vikash K.](https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4), Senior Data Cloud Architect | GSI Partners, [Snowflake](https://www.linkedin.com/company/snowflake-computing/) India  \n\n\nYour expertise will shape how we handle data in our projects.  \n\n\n[\\#SnowflakeSummit2023](https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataGovernance](https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#CostControl](https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataManagement](https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#TechTrends](https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataAnalytics](https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#DataSecurity](https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#Snowflake](https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) [\\#SnowflakeDevelopers](https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944) ", "author_fullname": "t2_hhlcq19gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key takeaways from Snowflake Summit 2023 in Bangalore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "media_metadata": {"z371joftmqrb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=632d25e3e54af280ce7616d3debab386c63f0568"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9bfbd5f8788550e35d5658dcefb105c1c63d6b0d"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f77fa2d109d22fad0d51dbf0bcd975b9114bd1d5"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f028485b825f179d5aed019f12d7568ebb1adaf2"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b201bb9b074bf1f4348fa10aa0d65cd0ada5e4c4"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e153a28fa2528f892f5df29d19402d8c80efb2c"}], "s": {"y": 1200, "x": 1600, "u": "https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90"}, "id": "z371joftmqrb1"}}, "name": "t3_16xprjr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GTriMGw7dhnIVYinyVqUBxw7GM_RpAGKtQqnNof47ck.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696230074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my latest blog post, I&amp;#39;ve summarized the key takeaways from Snowflake Summit 2023 in Bangalore, so that those who couldn&amp;#39;t attend can benefit from the latest updates on data governance and cost management.  &lt;/p&gt;\n\n&lt;p&gt;Data Governance Highlights:&lt;br/&gt;\n1) Data Quality Monitoring&lt;br/&gt;\n2) Query Constraint Policies&lt;br/&gt;\n3) Data Access Policies&lt;br/&gt;\n4) Data Governance UI&lt;br/&gt;\n5) Classification UI&lt;br/&gt;\n6) Global PII Classification  &lt;/p&gt;\n\n&lt;p&gt;Cost Control Highlights:&lt;br/&gt;\n1) Budget&lt;br/&gt;\n2) Warehouse Utilization&lt;br/&gt;\n3) SnowLens  &lt;/p&gt;\n\n&lt;p&gt;Blog Link - &lt;a href=\"https://medium.com/bi3-technologies/unlocking-the-power-of-data-governance-and-privacy-insights-from-snowflake-summit-2023-7ffcb697e005\"&gt;Unlocking the Power of Data Governance and Cost Control: Insights from Snowflake Summit 2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90\"&gt;https://preview.redd.it/z371joftmqrb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7aa178f639e8751472f63f1d63fcc22805cd9d90&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to thank the event organizers and the insightful speakers:  &lt;/p&gt;\n\n&lt;p&gt;1) &lt;a href=\"https://www.linkedin.com/in/ACoAABMXMIIB6tUTKUjbYz1CqBz2jS3NCnSpM7I\"&gt;Hemant Raorane&lt;/a&gt;, Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n2) &lt;a href=\"https://www.linkedin.com/in/ACoAAAZGouMBkudPlBY9PD76sXVTAdnwWQNQm9w\"&gt;Sachin Gangwar&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n3) &lt;a href=\"https://www.linkedin.com/in/ACoAAC-O1KYBhH78QJa92-7ggpODUPFVS3jvgAc\"&gt;Wasim El-Omari&lt;/a&gt;, Principal Architect, Security, Field CTO Office, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt;&lt;br/&gt;\n4) &lt;a href=\"https://www.linkedin.com/in/ACoAAAEq2hkBAI8EwgK_ROKoaULMSB-72GQrnS0\"&gt;Pawan Mall&lt;/a&gt;, Senior Sales Engineer, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India&lt;br/&gt;\n5) &lt;a href=\"https://www.linkedin.com/in/ACoAACea-JUBwLCCA2Iqcbrpm_F6rHUrEKHpHA4\"&gt;Vikash K.&lt;/a&gt;, Senior Data Cloud Architect | GSI Partners, &lt;a href=\"https://www.linkedin.com/company/snowflake-computing/\"&gt;Snowflake&lt;/a&gt; India  &lt;/p&gt;\n\n&lt;p&gt;Your expertise will shape how we handle data in our projects.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakesummit2023&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeSummit2023&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datagovernance&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataGovernance&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=costcontrol&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#CostControl&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datamanagement&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataManagement&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=techtrends&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#TechTrends&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataanalytics&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataAnalytics&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datasecurity&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#DataSecurity&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflake&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#Snowflake&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=snowflakedevelopers&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7114183004519890944\"&gt;#SnowflakeDevelopers&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?auto=webp&amp;s=87203ce307f9b72fb094e85035ea3a41a5c8c384", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9637f60474d774efa820fda8f5974253eaefa50", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83868552d3590e90338962cb8d1672ccbaa73828", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4b715c2453ce59b28c1dcce84f0153ab57bd1f0", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9699bdf7ecc3f6608279ad7cb3f1411d89fc0424", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c95fed484686b328184f7151ca8e51384b94448", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/mOnZdotm_swfXKikgDWKR7RoivQD5XTwI3LWgQELgoQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20407829974bcc45a6f50e4809fca58564a96763", "width": 1080, "height": 565}], "variants": {}, "id": "4x9_UIUovtScdP6OlrszrrtgeXaSEDyQR_m-Sv_lbvk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xprjr", "is_robot_indexable": true, "report_reasons": null, "author": "ijiv_s", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xprjr/key_takeaways_from_snowflake_summit_2023_in/", "subreddit_subscribers": 131587, "created_utc": 1696230074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thanks to the community for being so helpful and supportive!\n\nI am considering to start a data engineering consulting. My niche is e-commerce business.\n\nWhat to hear what the community thinks about this? \n\nIs this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? \n\nThanks again!", "author_fullname": "t2_hlf7js20w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering consulting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xob54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696224918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the community for being so helpful and supportive!&lt;/p&gt;\n\n&lt;p&gt;I am considering to start a data engineering consulting. My niche is e-commerce business.&lt;/p&gt;\n\n&lt;p&gt;What to hear what the community thinks about this? &lt;/p&gt;\n\n&lt;p&gt;Is this a bad decision to focus on building a company around DE consulting? \nWhat are the key challenges I need to prepare for?\nWhat industries to target?\nHow to grow my business? &lt;/p&gt;\n\n&lt;p&gt;Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16xob54", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dream-420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xob54/data_engineering_consulting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xob54/data_engineering_consulting/", "subreddit_subscribers": 131587, "created_utc": 1696224918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are looking for a database for all our laboratory data, our requirements are that the database must be capable of handling multiple data types (sensor data, images), easily accessible to everyone and most importantly has the functionality to link different databases. We would have a primary database that will have information about multiple processes and then each process has its own database. Whatever data is inserted in the primary database should get updated automatically on the respective process specific secondary database. Been trying to look for something that does these, any suggestions would be appreciated! Thank you.", "author_fullname": "t2_ndudpuk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open source data management platform for academia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xcbec", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696193080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are looking for a database for all our laboratory data, our requirements are that the database must be capable of handling multiple data types (sensor data, images), easily accessible to everyone and most importantly has the functionality to link different databases. We would have a primary database that will have information about multiple processes and then each process has its own database. Whatever data is inserted in the primary database should get updated automatically on the respective process specific secondary database. Been trying to look for something that does these, any suggestions would be appreciated! Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xcbec", "is_robot_indexable": true, "report_reasons": null, "author": "chipsandcokerule", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xcbec/open_source_data_management_platform_for_academia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xcbec/open_source_data_management_platform_for_academia/", "subreddit_subscribers": 131587, "created_utc": 1696193080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, ive been a data analyst in a FMCG corpo in a smaller BI team, working with microstrategy dwh, azure snowflake, databricks, and SAp, with strong python and sql(downstream analytical) skills. Ive worked in pretty mature data environment, where i had all the raw and unaggregated data \"served\" and could choose using DWH program for ad hoc small extractions, creating dashboards in it aswell, or doing longer/repeating processes with python/sql and extraction to excel.\nI mostly had self made python  libraries and programs which automated SAP and azure connections/sql script running and all the data transformation, excel storing and mail sending. Before i came in the office, processes were very slow and manually made. This is my background in short.\n\nOn new job ill be working in production and sales company, as DE, on AWS, and first time working on  \"other side\" of sql. Ill be working mostly on extraction of data from excel from various departments, some scraping, some data from softwares/web site. I believe its not as data mature company as one where i was before. As i understood, python is 90% of work. \n\nIll also have a smaller part of the job using tableu and creating dashboards. \n\nCurrently reading fundamentals of data engineering. Will focus on sources, how data is used and pipelines when i start working. Ill try to learn AWS and more about cloud. \n\nWhat good info, sources, suggestions and tips do you have for me? Thank you in advance", "author_fullname": "t2_nj1ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Landed new job, from DA to DE would like to hear your advice for first job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x84y6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696183611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, ive been a data analyst in a FMCG corpo in a smaller BI team, working with microstrategy dwh, azure snowflake, databricks, and SAp, with strong python and sql(downstream analytical) skills. Ive worked in pretty mature data environment, where i had all the raw and unaggregated data &amp;quot;served&amp;quot; and could choose using DWH program for ad hoc small extractions, creating dashboards in it aswell, or doing longer/repeating processes with python/sql and extraction to excel.\nI mostly had self made python  libraries and programs which automated SAP and azure connections/sql script running and all the data transformation, excel storing and mail sending. Before i came in the office, processes were very slow and manually made. This is my background in short.&lt;/p&gt;\n\n&lt;p&gt;On new job ill be working in production and sales company, as DE, on AWS, and first time working on  &amp;quot;other side&amp;quot; of sql. Ill be working mostly on extraction of data from excel from various departments, some scraping, some data from softwares/web site. I believe its not as data mature company as one where i was before. As i understood, python is 90% of work. &lt;/p&gt;\n\n&lt;p&gt;Ill also have a smaller part of the job using tableu and creating dashboards. &lt;/p&gt;\n\n&lt;p&gt;Currently reading fundamentals of data engineering. Will focus on sources, how data is used and pipelines when i start working. Ill try to learn AWS and more about cloud. &lt;/p&gt;\n\n&lt;p&gt;What good info, sources, suggestions and tips do you have for me? Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16x84y6", "is_robot_indexable": true, "report_reasons": null, "author": "Kichmad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x84y6/landed_new_job_from_da_to_de_would_like_to_hear/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16x84y6/landed_new_job_from_da_to_de_would_like_to_hear/", "subreddit_subscribers": 131587, "created_utc": 1696183611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Tips. Optimizing JDBC data source reads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16x7kis", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/p1GjGScxcKmQ-FPaqLmcZc3MHdyx7rPDYi802AF_oDE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1696182306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "luminousmen.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://luminousmen.com/post/spark-tips-optimizing-jdbc-data-source-reads", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?auto=webp&amp;s=6feb316599e137f192368d3c5f8a09812a2fb9b3", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb47d235eb134e4426e30eb7cc852b9c1767bd28", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=042a2aae04d7b9c5e900bd441411b71f0d3fe891", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6a7cc51735acd26f5ae97f3b0fbcac4f6ca11e4", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QEMI5q4IwnMz4l6gRxc8YFGNhwM5v8MuF8yNJm3e1T4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b490b48832c9be9449eb2a5452e626717f703562", "width": 640, "height": 480}], "variants": {}, "id": "FAw-sDyD8fCBR3nlCjPbH2rgCvhf8lX7QDTmW7bd0yU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16x7kis", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x7kis/spark_tips_optimizing_jdbc_data_source_reads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://luminousmen.com/post/spark-tips-optimizing-jdbc-data-source-reads", "subreddit_subscribers": 131587, "created_utc": 1696182306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Oct 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e", "t3_16x4y7c"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1696176058.352, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16x4y7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1696176058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Meetups\"&gt;Data Engineering Meetups&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?auto=webp&amp;s=02e6018b7f945f491d0b3a2effc39732c734f1e1", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dccc2af8931f0a3ac9ada660b34e9cba537b2fd1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a24a390861290f321df46393893e52524fe7623f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53c673cdf215f673c496a58a993c4eac464fc2bb", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/c1tDGdXyXGdsLQ6pwjWc8mXKh5-H7Rix5dx2etHXDU4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db34a5d7735459993d15b53c8a1b7aff4d3b2ec4", "width": 640, "height": 333}], "variants": {}, "id": "FmCaWz0_zzuMIYjO5Y9qwrC5XQ9HEDt0Z1CdPOgLQk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16x4y7c", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/16x4y7c/monthly_general_discussion_oct_2023/", "subreddit_subscribers": 131587, "created_utc": 1696176058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,  \nWe make use of the Debezium PostgreSQL connector but we don't appreciate that it cannot deliver DDL operations. (citation [https://debezium.io/documentation/reference/stable/connectors/postgresql.html](https://debezium.io/documentation/reference/stable/connectors/postgresql.html))\n\nOur expectation was that we could simply hook up Debezium and our PostgreSQL instance and get near-real-time \"mirroring\" to Kafka. We eventually need this mirroring to end up in Redshift, but that's out of scope of this question.\n\nQuestion: are there any alternatives to Debezium's PostgreSQL connector that do capture DDL operations? Some constraints:\n\n\\- cannot send data to a third party. Everything must take place on prem.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to Debezium PostgreSQL connector?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xoddt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696225138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nWe make use of the Debezium PostgreSQL connector but we don&amp;#39;t appreciate that it cannot deliver DDL operations. (citation &lt;a href=\"https://debezium.io/documentation/reference/stable/connectors/postgresql.html\"&gt;https://debezium.io/documentation/reference/stable/connectors/postgresql.html&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Our expectation was that we could simply hook up Debezium and our PostgreSQL instance and get near-real-time &amp;quot;mirroring&amp;quot; to Kafka. We eventually need this mirroring to end up in Redshift, but that&amp;#39;s out of scope of this question.&lt;/p&gt;\n\n&lt;p&gt;Question: are there any alternatives to Debezium&amp;#39;s PostgreSQL connector that do capture DDL operations? Some constraints:&lt;/p&gt;\n\n&lt;p&gt;- cannot send data to a third party. Everything must take place on prem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16xoddt", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xoddt/alternative_to_debezium_postgresql_connector/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xoddt/alternative_to_debezium_postgresql_connector/", "subreddit_subscribers": 131587, "created_utc": 1696225138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.", "author_fullname": "t2_muzoa5tk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Java still a requirement in DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16xqihp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696232898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an expert in python, SQL and Bash. Do data engineers still used Java? Since MapReduce is kinda phased out by Spark/PySpark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16xqihp", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate-Box-7", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xqihp/java_still_a_requirement_in_de/", "subreddit_subscribers": 131587, "created_utc": 1696232898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nIm leading a team of a few part time contractors to build out my company's first Data Warehouse and we are trying to determine the best solution to orchestrate DBT transformations. We work within an AWS ecosystem and have strict rules about maintaining GDPR compliance, which requires that our data remain in Canada. This has proven to be a pretty limiting factor since all of the cloud orchestration tools either dont have Canada as a hosting region or they gets exponentially more expensive to host there (example with Astronomer. This is due to the fact that you are unable to deactivate your cluster, so it runs 24/7 at $2/hour. They claim they are releasing a feature next month to control that, but I cant depend on a sales reps word for this decision). \n\nOur data is small in size so we can use that to our advantage, since many of the options come out to be more affordable. The decision im working through is whether or not we should leverage MWAA (which would be about $4-5k/year with our current needs) or try another solution within the AWS ecosystem like step functions or fargate. Im finding this choice difficult as I do not have experience with these AWS products and I dont want to risk over engineering here.\n\nAny advice would be appreciated :)", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrating DBT Jobs with Airflow/Step functions/Fargate/etc", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xbu4w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696191987.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;Im leading a team of a few part time contractors to build out my company&amp;#39;s first Data Warehouse and we are trying to determine the best solution to orchestrate DBT transformations. We work within an AWS ecosystem and have strict rules about maintaining GDPR compliance, which requires that our data remain in Canada. This has proven to be a pretty limiting factor since all of the cloud orchestration tools either dont have Canada as a hosting region or they gets exponentially more expensive to host there (example with Astronomer. This is due to the fact that you are unable to deactivate your cluster, so it runs 24/7 at $2/hour. They claim they are releasing a feature next month to control that, but I cant depend on a sales reps word for this decision). &lt;/p&gt;\n\n&lt;p&gt;Our data is small in size so we can use that to our advantage, since many of the options come out to be more affordable. The decision im working through is whether or not we should leverage MWAA (which would be about $4-5k/year with our current needs) or try another solution within the AWS ecosystem like step functions or fargate. Im finding this choice difficult as I do not have experience with these AWS products and I dont want to risk over engineering here.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xbu4w", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xbu4w/orchestrating_dbt_jobs_with_airflowstep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xbu4w/orchestrating_dbt_jobs_with_airflowstep/", "subreddit_subscribers": 131587, "created_utc": 1696191987.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! Has anyone recently attempted the Qlik replicate cert? If yes - can you please share any resources you used to prepare? Struggling to find practice tests online / any course material other than Qlik\u2019s continuous classroom offering. \n\nThanks!", "author_fullname": "t2_fwuj5vp8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Qlik Replicate Certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xjavi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696210041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Has anyone recently attempted the Qlik replicate cert? If yes - can you please share any resources you used to prepare? Struggling to find practice tests online / any course material other than Qlik\u2019s continuous classroom offering. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xjavi", "is_robot_indexable": true, "report_reasons": null, "author": "SuccotashPopular9660", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xjavi/qlik_replicate_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xjavi/qlik_replicate_certification/", "subreddit_subscribers": 131587, "created_utc": 1696210041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The question is pretty simple and straightforward but I don\u2019t understand how people teach themselves anything. I guess I am just confused how to study/learn something on your own without any true guidance? I am trying to learn sql and eventually want to move onto learning tableau/power bi and python by myself. I don\u2019t understand how to start and what to do. Can someone help here? In general, I guess how do people teach themselves and become proficient in anything?", "author_fullname": "t2_11bng6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you teach yourself sql and python? I am confused.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16xhh96", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.31, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1696205143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The question is pretty simple and straightforward but I don\u2019t understand how people teach themselves anything. I guess I am just confused how to study/learn something on your own without any true guidance? I am trying to learn sql and eventually want to move onto learning tableau/power bi and python by myself. I don\u2019t understand how to start and what to do. Can someone help here? In general, I guess how do people teach themselves and become proficient in anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16xhh96", "is_robot_indexable": true, "report_reasons": null, "author": "imperba", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16xhh96/how_do_you_teach_yourself_sql_and_python_i_am/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16xhh96/how_do_you_teach_yourself_sql_and_python_i_am/", "subreddit_subscribers": 131587, "created_utc": 1696205143.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}