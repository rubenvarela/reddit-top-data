{"kind": "Listing", "data": {"after": "t3_14qnmcl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4aynv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We Found &amp; Saved 10 Years of Lost Video Game History (Noclip)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_14ptfic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 398, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7KKCWGN2fBs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"We Found &amp;amp; Saved 10 YEARS of Lost Video Game History\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "We Found &amp; Saved 10 YEARS of Lost Video Game History", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7KKCWGN2fBs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"We Found &amp;amp; Saved 10 YEARS of Lost Video Game History\"&gt;&lt;/iframe&gt;", "author_name": "Noclip - Video Game Documentaries", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7KKCWGN2fBs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@NoclipDocs"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7KKCWGN2fBs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"We Found &amp;amp; Saved 10 YEARS of Lost Video Game History\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/14ptfic", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 398, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Kau1FiObgR52XG6i9SrqNAlAA5KYu4av8EPVdU4zHYc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688415140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/7KKCWGN2fBs", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gqQ1qRJcox5fUVxon-SykmOG4pMQmIl_j7HXQeDIa94.jpg?auto=webp&amp;v=enabled&amp;s=00d7a4d5ca0be327d45d18447a2e32012db74d74", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/gqQ1qRJcox5fUVxon-SykmOG4pMQmIl_j7HXQeDIa94.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddcb6dfa93bf29a823b0eaba453d040f91cb410d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/gqQ1qRJcox5fUVxon-SykmOG4pMQmIl_j7HXQeDIa94.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a46d1ee1b433b77418f93bf270f0e6622c5bed78", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/gqQ1qRJcox5fUVxon-SykmOG4pMQmIl_j7HXQeDIa94.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cedf2dc17ddfa365fd5a1efb491b7f6b138f40c", "width": 320, "height": 240}], "variants": {}, "id": "wTvWCcHI3cmprPQ-uFKGZ-4GLsR8NXz8BYOS3GO2yk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ptfic", "is_robot_indexable": true, "report_reasons": null, "author": "retrac1324", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ptfic/we_found_saved_10_years_of_lost_video_game/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://youtu.be/7KKCWGN2fBs", "subreddit_subscribers": 691056, "created_utc": 1688415140.0, "num_crossposts": 2, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "We Found &amp; Saved 10 YEARS of Lost Video Game History", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7KKCWGN2fBs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"We Found &amp;amp; Saved 10 YEARS of Lost Video Game History\"&gt;&lt;/iframe&gt;", "author_name": "Noclip - Video Game Documentaries", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7KKCWGN2fBs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@NoclipDocs"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A lot of my data is transcoded for streamers(plex). I'm getting ready to build a new server and trying to decide if I should use RAMDisk to save my NVME drive or give me any performance. All the information I can find is from years ago.\n\nTo give you an idea we sometimes have up to 30 streams going at a time. Nothing 4k.", "author_fullname": "t2_speo3xmt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are RAMdisks worth it at all now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qfd94", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688478374.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688477638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of my data is transcoded for streamers(plex). I&amp;#39;m getting ready to build a new server and trying to decide if I should use RAMDisk to save my NVME drive or give me any performance. All the information I can find is from years ago.&lt;/p&gt;\n\n&lt;p&gt;To give you an idea we sometimes have up to 30 streams going at a time. Nothing 4k.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14qfd94", "is_robot_indexable": true, "report_reasons": null, "author": "Vile-X", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qfd94/are_ramdisks_worth_it_at_all_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qfd94/are_ramdisks_worth_it_at_all_now/", "subreddit_subscribers": 691056, "created_utc": 1688477638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.synology.com/en-global/products/status/eoa-software-packages-and-utilities](https://www.synology.com/en-global/products/status/eoa-software-packages-and-utilities)\n\nI'm fairly sure that the existing 2 archives include all the affected packages and utilities. ", "author_fullname": "t2_33srm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology deleting more old packages and utilities on Aug 31", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q25j9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688437238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.synology.com/en-global/products/status/eoa-software-packages-and-utilities\"&gt;https://www.synology.com/en-global/products/status/eoa-software-packages-and-utilities&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly sure that the existing 2 archives include all the affected packages and utilities. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?auto=webp&amp;v=enabled&amp;s=7cf285b613153d35a4ea7785d0c36e4642167bb7", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e67ce412859b6d81936e77a948dc1c7fedc5f3c5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=434d2fd7c71bf8bdbb42eb5b2bece1bee3eb9805", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4be00e6f1bce287fcd69c69b7338b275e7c8fd32", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abe38ecb2ae546225c1cfc51c975d8ee7464bda8", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8827807950f667fe52a0c7c2951360ab84cf5de3", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/wegydO5WX1iklEez2qxzrsSrzeLO37WSWl9TDE1utMI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85bff2163eb0ffea1add5e9594bdfbc859fd06a0", "width": 1080, "height": 565}], "variants": {}, "id": "mqRAmjgCM1Tj12KBJxqH4zVVbxJTCi9kJINbuyh9dSQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "186TB local", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14q25j9", "is_robot_indexable": true, "report_reasons": null, "author": "DaveR007", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14q25j9/synology_deleting_more_old_packages_and_utilities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14q25j9/synology_deleting_more_old_packages_and_utilities/", "subreddit_subscribers": 691056, "created_utc": 1688437238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know there was a big CMR vs SMR kerfuffle a few years ago, but I think this may be a new finding? I recently purchased two WD Red Plus 10TB NAS drives to add to my storage array. Per usual, I booted up each drive in HDAT2 to check smart attributes to ensure drive integrity post delivery. When doing so, I noticed something interesting I\u2019ve never seen in HDAT before. The drive type was labeled \u201cSMR-DM.\u201d I also ran HDAT on an old 1TB WD Caviar Black I have and it shows as SATA, not SMR-DM.\n\nI looked at the western digital listing again, and it explicitly says the drive time is CMR. I searched some of the \u201cCMR vs. SMR\u201d sites and confirmed this should be CMR. Am I possibly using/reading HDAT wrong, or is WD trying to pull a fast one here? Is there any other testing I can do to further differentiate?\n\nLink to images: https://imgur.com/a/rcc2Ll7\n\nEDIT:\nI added the disks to the spool and started a 5TB write to the pool ( 3 x two disk mirrors). Write speeds seem to average 90 MB/s 750GBs into the write. Added a screenshot to the images. WD says \u201cup to 215 MB/s, but at least it\u2019s not doing 40MB/s? Still seems unclear to me.", "author_fullname": "t2_2xtxup6c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD SMR Masquerading as CMR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pzwz5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688442859.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688430799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there was a big CMR vs SMR kerfuffle a few years ago, but I think this may be a new finding? I recently purchased two WD Red Plus 10TB NAS drives to add to my storage array. Per usual, I booted up each drive in HDAT2 to check smart attributes to ensure drive integrity post delivery. When doing so, I noticed something interesting I\u2019ve never seen in HDAT before. The drive type was labeled \u201cSMR-DM.\u201d I also ran HDAT on an old 1TB WD Caviar Black I have and it shows as SATA, not SMR-DM.&lt;/p&gt;\n\n&lt;p&gt;I looked at the western digital listing again, and it explicitly says the drive time is CMR. I searched some of the \u201cCMR vs. SMR\u201d sites and confirmed this should be CMR. Am I possibly using/reading HDAT wrong, or is WD trying to pull a fast one here? Is there any other testing I can do to further differentiate?&lt;/p&gt;\n\n&lt;p&gt;Link to images: &lt;a href=\"https://imgur.com/a/rcc2Ll7\"&gt;https://imgur.com/a/rcc2Ll7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EDIT:\nI added the disks to the spool and started a 5TB write to the pool ( 3 x two disk mirrors). Write speeds seem to average 90 MB/s 750GBs into the write. Added a screenshot to the images. WD says \u201cup to 215 MB/s, but at least it\u2019s not doing 40MB/s? Still seems unclear to me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?auto=webp&amp;v=enabled&amp;s=99edb654cea534d67d076d47b6899a3f7e481e9a", "width": 1092, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a8bbd2f6178cfb4506c18e338fde99937e48035", "width": 108, "height": 202}, {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bdc99e62e36d6bf048152ff83b11d1ceb8eca05", "width": 216, "height": 405}, {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3796043d1afa612dcb7f215c4631693dd237cd0", "width": 320, "height": 600}, {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a6b532ab7afa63c3d4c80b3d47b02d4c987a9b1", "width": 640, "height": 1200}, {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5e41783fc2e2312edaadf8718a9c06bf21e05dd", "width": 960, "height": 1800}, {"url": "https://external-preview.redd.it/do7GbpYhfaOIB6Qp6QPHrndlEIUJbCfBK01fUlHIdG0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb64f68d201d7a5da1b0f5f929a0d8dd8a020da0", "width": 1080, "height": 2025}], "variants": {}, "id": "YdFEOSh6ISMI32cYO00AKssp3dLNU3lMogdK_wDx8as"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14pzwz5", "is_robot_indexable": true, "report_reasons": null, "author": "ipad_pilot", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14pzwz5/wd_smr_masquerading_as_cmr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14pzwz5/wd_smr_masquerading_as_cmr/", "subreddit_subscribers": 691056, "created_utc": 1688430799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3wef6thkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 980 PRO SSD 2TB for $99.99 - 52% Off XPG 2TB GAMMIX S70 Blade SSD, for $99.99 - Early Prime Day Deals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qgda0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1688480050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/HelpMeFindDeals/status/1676232084451627008", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14qgda0", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Command15", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qgda0/samsung_980_pro_ssd_2tb_for_9999_52_off_xpg_2tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/HelpMeFindDeals/status/1676232084451627008", "subreddit_subscribers": 691056, "created_utc": 1688480050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I love collecting mature videos and love the idea of collecting a copy of an entire series.  With a long weekend I decided to focus on 1 series.  There is a studio that has multiple series so I decided to focus on the \"MyHotGirl\" series (actual name is different of course).\n\nHere is the workflow I came up with.\n\nCATALOG\n\nThe first problem with a 'collection' is you need to know all the possible titles.\n\nI went to the official studio website and discovered they have separate web pages for each series as a catalog/index to the videos. Great. I wanted to create a local copy of these pages so I could scan for titles, dates, actors, etc. at my leisure.\n\nFor series that occupy 2-5 web pages I could simply go to the page, right-clicked and did \"Save As..\" and saved each page as a single file with .html extension.  The \"MyHotGirl\" series has been around for years and had 45 web pages.  So I wrote a simple Python script that calculated the Page 1,2,3 URL, did a Request and saved each to a local Catalog file.\n\nThen I wrote another Python script to use BeautifulSoup. It opens each Catalog file and searches for redirects or \"a href\" .  There are tons but eventually I discover the pattern that the pages use for a video and made my python script only print these.\n\nBUT - I don't want to link/play the video on the studio site.  I want the Release date, title AROUND each title.  I am trying to build a Catalog.\n\nKnowing one of the URL's I went back to the Catalog site and right-clicked on the page and did \"View Source\".  Then I searched for one of the URL's.\n\nNow - I look above and below the url in the source of the web page.  I am NOT a web page designer but I could see rows with the Title of the video and the Release Date for the video.\n\nMy goal was to derive the likely file name for each title in this format:\n\nMyHotGirl.YY.MM.DD.Title Title Title Actor Actor Actor\n\nI fought with BeautifulSoup to get it to extract all 3 parts of the information but failed.  I could get the URL's of the videos. I could get all Date strings or I could get all text that included video titles - but a lot of other crap as well.\n\nUSING CHATGPT\n\nEach video was encased in SPAN, CLASS, DIV and other mark up. I copied and pasted all the enclosing HTML into a text editor.\n\nI then changed the website name to something other than the Porn site and also cleaned up the Title strings so it was not adult.\n\nThen I fired up ChatGPT and asked it this question:\n\n&gt;I need to use Python3 and BeautifulSoup to extract the Date, Title and Series name from this HTML:  (Paste my cleaned up snippet here)\n\nTo my shock and delight - It spit out some Python code that could actually work.\n\nThen I added:\n\n&gt;I need the code above to handle multiple items on the web page and extract all the dates titles and series.\n\nAgain - it spit out code like this that now included a loop:\n\n    soup = BeautifulSoup(url, 'html.parser')\n    redirects = []\n    video_items = soup.select('.thumb.item')\n    \n    for item in video_items:\n        a_tag = item.select_one('a.thumb__top')\n        video_url = a_tag['href']\n        img_tag = item.select_one('.thumb__img img')\n        alt_text = img_tag['alt']\n        date_text = item.select_one(.thumb__date')\n        #print(\"Video URL:\", video_url)\n        #print(\"Alt Text:\", alt_text)\n        #print(\"Date Text:\", date_text)\n    \n\nSuddenly I could now get the Series, Date and Title as a set for each video on each catalog page.\n\nI ran the code in a loop to print out a line of text for each that looked like this to what I call a \"Catalog File\"\n\n    MyHotGirl.15.03.11.Title Title Title|N      &lt;-- 'N' means Not in my collection\n    MyHotGirl.15.04.03.Title Title Title|N\n    ...\n\nNow I had a text file that should match the file names I already had.  I found a list of most of my videos and searched for \"MyHotGirl\". For each row I had - I found that row in my Catalog file and changed \"|N\" to \"|Y\" to indicate I had the video in my collection.\n\n&amp;#x200B;\n\nQUESTION\n\nIs this post interesting to the group? It is very nerdy and technical.  But I thought the workflow or order of operations might be of interest.  The same workflow could be used for many other studios and series.\n\nI will continue if there is interest.\n\n&amp;#x200B;", "author_fullname": "t2_1jno0tn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My long weekend data hoard project - workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qli4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688491905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I love collecting mature videos and love the idea of collecting a copy of an entire series.  With a long weekend I decided to focus on 1 series.  There is a studio that has multiple series so I decided to focus on the &amp;quot;MyHotGirl&amp;quot; series (actual name is different of course).&lt;/p&gt;\n\n&lt;p&gt;Here is the workflow I came up with.&lt;/p&gt;\n\n&lt;p&gt;CATALOG&lt;/p&gt;\n\n&lt;p&gt;The first problem with a &amp;#39;collection&amp;#39; is you need to know all the possible titles.&lt;/p&gt;\n\n&lt;p&gt;I went to the official studio website and discovered they have separate web pages for each series as a catalog/index to the videos. Great. I wanted to create a local copy of these pages so I could scan for titles, dates, actors, etc. at my leisure.&lt;/p&gt;\n\n&lt;p&gt;For series that occupy 2-5 web pages I could simply go to the page, right-clicked and did &amp;quot;Save As..&amp;quot; and saved each page as a single file with .html extension.  The &amp;quot;MyHotGirl&amp;quot; series has been around for years and had 45 web pages.  So I wrote a simple Python script that calculated the Page 1,2,3 URL, did a Request and saved each to a local Catalog file.&lt;/p&gt;\n\n&lt;p&gt;Then I wrote another Python script to use BeautifulSoup. It opens each Catalog file and searches for redirects or &amp;quot;a href&amp;quot; .  There are tons but eventually I discover the pattern that the pages use for a video and made my python script only print these.&lt;/p&gt;\n\n&lt;p&gt;BUT - I don&amp;#39;t want to link/play the video on the studio site.  I want the Release date, title AROUND each title.  I am trying to build a Catalog.&lt;/p&gt;\n\n&lt;p&gt;Knowing one of the URL&amp;#39;s I went back to the Catalog site and right-clicked on the page and did &amp;quot;View Source&amp;quot;.  Then I searched for one of the URL&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Now - I look above and below the url in the source of the web page.  I am NOT a web page designer but I could see rows with the Title of the video and the Release Date for the video.&lt;/p&gt;\n\n&lt;p&gt;My goal was to derive the likely file name for each title in this format:&lt;/p&gt;\n\n&lt;p&gt;MyHotGirl.YY.MM.DD.Title Title Title Actor Actor Actor&lt;/p&gt;\n\n&lt;p&gt;I fought with BeautifulSoup to get it to extract all 3 parts of the information but failed.  I could get the URL&amp;#39;s of the videos. I could get all Date strings or I could get all text that included video titles - but a lot of other crap as well.&lt;/p&gt;\n\n&lt;p&gt;USING CHATGPT&lt;/p&gt;\n\n&lt;p&gt;Each video was encased in SPAN, CLASS, DIV and other mark up. I copied and pasted all the enclosing HTML into a text editor.&lt;/p&gt;\n\n&lt;p&gt;I then changed the website name to something other than the Porn site and also cleaned up the Title strings so it was not adult.&lt;/p&gt;\n\n&lt;p&gt;Then I fired up ChatGPT and asked it this question:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I need to use Python3 and BeautifulSoup to extract the Date, Title and Series name from this HTML:  (Paste my cleaned up snippet here)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;To my shock and delight - It spit out some Python code that could actually work.&lt;/p&gt;\n\n&lt;p&gt;Then I added:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I need the code above to handle multiple items on the web page and extract all the dates titles and series.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Again - it spit out code like this that now included a loop:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;soup = BeautifulSoup(url, &amp;#39;html.parser&amp;#39;)\nredirects = []\nvideo_items = soup.select(&amp;#39;.thumb.item&amp;#39;)\n\nfor item in video_items:\n    a_tag = item.select_one(&amp;#39;a.thumb__top&amp;#39;)\n    video_url = a_tag[&amp;#39;href&amp;#39;]\n    img_tag = item.select_one(&amp;#39;.thumb__img img&amp;#39;)\n    alt_text = img_tag[&amp;#39;alt&amp;#39;]\n    date_text = item.select_one(.thumb__date&amp;#39;)\n    #print(&amp;quot;Video URL:&amp;quot;, video_url)\n    #print(&amp;quot;Alt Text:&amp;quot;, alt_text)\n    #print(&amp;quot;Date Text:&amp;quot;, date_text)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Suddenly I could now get the Series, Date and Title as a set for each video on each catalog page.&lt;/p&gt;\n\n&lt;p&gt;I ran the code in a loop to print out a line of text for each that looked like this to what I call a &amp;quot;Catalog File&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;MyHotGirl.15.03.11.Title Title Title|N      &amp;lt;-- &amp;#39;N&amp;#39; means Not in my collection\nMyHotGirl.15.04.03.Title Title Title|N\n...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Now I had a text file that should match the file names I already had.  I found a list of most of my videos and searched for &amp;quot;MyHotGirl&amp;quot;. For each row I had - I found that row in my Catalog file and changed &amp;quot;|N&amp;quot; to &amp;quot;|Y&amp;quot; to indicate I had the video in my collection.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;QUESTION&lt;/p&gt;\n\n&lt;p&gt;Is this post interesting to the group? It is very nerdy and technical.  But I thought the workflow or order of operations might be of interest.  The same workflow could be used for many other studios and series.&lt;/p&gt;\n\n&lt;p&gt;I will continue if there is interest.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qli4g", "is_robot_indexable": true, "report_reasons": null, "author": "ShamBawk33", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qli4g/my_long_weekend_data_hoard_project_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qli4g/my_long_weekend_data_hoard_project_workflow/", "subreddit_subscribers": 691056, "created_utc": 1688491905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been exploring different methods for remotely accessing my PC from my iPad while outside of my home network, and I'd love to gather insights from fellow tech enthusiasts. Initially, I looked into options like TeamViewer, VNC, and SSH, but I found that TeamViewer's performance fell short of expectations.\n\nI've come across some concerns regarding the security of VNC without a VPN when accessing a PC outside of the home network. So, I'm curious to know if there's a preferred method for remote access that ensures both convenience and security.", "author_fullname": "t2_psrwiqn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best way to remotely access a pc outside without a VPN?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qko51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688490013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been exploring different methods for remotely accessing my PC from my iPad while outside of my home network, and I&amp;#39;d love to gather insights from fellow tech enthusiasts. Initially, I looked into options like TeamViewer, VNC, and SSH, but I found that TeamViewer&amp;#39;s performance fell short of expectations.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come across some concerns regarding the security of VNC without a VPN when accessing a PC outside of the home network. So, I&amp;#39;m curious to know if there&amp;#39;s a preferred method for remote access that ensures both convenience and security.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qko51", "is_robot_indexable": true, "report_reasons": null, "author": "g3ger1ub", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qko51/the_best_way_to_remotely_access_a_pc_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qko51/the_best_way_to_remotely_access_a_pc_outside/", "subreddit_subscribers": 691056, "created_utc": 1688490013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have about 300 gigs of Steam game demos I want to put on Archive.org. Are there any files that could contain personal details linking back to my account within the files?\n\nI'd like to stress these are literally **just** the demos. These aren't the full games.", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anything I should remove from Steam demos before uploading to archive.org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qmghg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 300 gigs of Steam game demos I want to put on Archive.org. Are there any files that could contain personal details linking back to my account within the files?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to stress these are literally &lt;strong&gt;just&lt;/strong&gt; the demos. These aren&amp;#39;t the full games.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmghg", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmghg/is_there_anything_i_should_remove_from_steam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmghg/is_there_anything_i_should_remove_from_steam/", "subreddit_subscribers": 691056, "created_utc": 1688494123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've normally have done dual HD in RAID 1 in my computers in the past. I'm looking at possible doing a 5 bay set up but was wondering if I could just get 3 more of the drives I currently have and put them in and set the box up so I could move the data over and then wipe the two drives in my computer and add them to the box and have a 5 drive at the end. I know Drobo could do something like that, but wasn't sure if other companies could.", "author_fullname": "t2_1ftqbdxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First External Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qn7s1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688495900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve normally have done dual HD in RAID 1 in my computers in the past. I&amp;#39;m looking at possible doing a 5 bay set up but was wondering if I could just get 3 more of the drives I currently have and put them in and set the box up so I could move the data over and then wipe the two drives in my computer and add them to the box and have a 5 drive at the end. I know Drobo could do something like that, but wasn&amp;#39;t sure if other companies could.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qn7s1", "is_robot_indexable": true, "report_reasons": null, "author": "shadow1013", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qn7s1/first_external_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qn7s1/first_external_backup/", "subreddit_subscribers": 691056, "created_utc": 1688495900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I remember reading about the internet archive making a copy of all of their data in 2017, and storing the storage units somewhere... but I can't find a single article on this now. Can someone help me find this?", "author_fullname": "t2_4i9ed589", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a physical copy of the entire internet archive somewhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qlzj9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688493057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember reading about the internet archive making a copy of all of their data in 2017, and storing the storage units somewhere... but I can&amp;#39;t find a single article on this now. Can someone help me find this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "what is a hard drive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qlzj9", "is_robot_indexable": true, "report_reasons": null, "author": "Anoyint", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qlzj9/is_there_a_physical_copy_of_the_entire_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qlzj9/is_there_a_physical_copy_of_the_entire_internet/", "subreddit_subscribers": 691056, "created_utc": 1688493057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I finally got my [model-less IBM drive array](https://www.ebay.com/itm/394654841834) working with my old finicky HP Gen7 servers, so that's 12x 3.5\" bays I finally can use in my lab. I just the other day pulled the trigger on a Chenbro 4U chassis from a guy on r/homelabsales. And that IBM drive array has dropped in price to only $109.99, so I might pick another one up in the next couple of months. That gives me 48x 3.5\" bays, for a total of 960TB raw storage capacity if I went with all 20TB drives (which I eventually will). Sure, it's gonna take me 3-4 years to acquire that many 20TB drives, but....to have almost enough bays already to support that amount of storage? I'm hyped.\n\nI'm thinking RAID 6 or something similar, I only realistically need 2-4 drive parity since most of it is uh....Linux ISOs and whatnot...for the end setup. So I figure if I get 2 more of those IBM drive arrays, I'd be at 60 bays. That's 1200TB of raw storage capacity. If I did a ZFS pool with triple parity, I'd have a little over 1PB of storage. Any thoughts or input on this insane setup? It's a long way from being done and I'm more than willing to tweak things based on community advice.\n\n&amp;#x200B;\n\nEDIT: After hours of Googling I finally found the actual genuine manual for this unit from IBM. Turns out it's only SAS1 :( so definitely not gonna get more of them.", "author_fullname": "t2_mr4t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I might be closer to my 1PB raw storage capacity goal than I thought.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qfemb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688487560.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688477732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got my &lt;a href=\"https://www.ebay.com/itm/394654841834\"&gt;model-less IBM drive array&lt;/a&gt; working with my old finicky HP Gen7 servers, so that&amp;#39;s 12x 3.5&amp;quot; bays I finally can use in my lab. I just the other day pulled the trigger on a Chenbro 4U chassis from a guy on &lt;a href=\"/r/homelabsales\"&gt;r/homelabsales&lt;/a&gt;. And that IBM drive array has dropped in price to only $109.99, so I might pick another one up in the next couple of months. That gives me 48x 3.5&amp;quot; bays, for a total of 960TB raw storage capacity if I went with all 20TB drives (which I eventually will). Sure, it&amp;#39;s gonna take me 3-4 years to acquire that many 20TB drives, but....to have almost enough bays already to support that amount of storage? I&amp;#39;m hyped.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking RAID 6 or something similar, I only realistically need 2-4 drive parity since most of it is uh....Linux ISOs and whatnot...for the end setup. So I figure if I get 2 more of those IBM drive arrays, I&amp;#39;d be at 60 bays. That&amp;#39;s 1200TB of raw storage capacity. If I did a ZFS pool with triple parity, I&amp;#39;d have a little over 1PB of storage. Any thoughts or input on this insane setup? It&amp;#39;s a long way from being done and I&amp;#39;m more than willing to tweak things based on community advice.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EDIT: After hours of Googling I finally found the actual genuine manual for this unit from IBM. Turns out it&amp;#39;s only SAS1 :( so definitely not gonna get more of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1d_974DQtKT373NMYXynWEd-cF7UlUaDMw6EWr0jG_o.jpg?auto=webp&amp;v=enabled&amp;s=80d5c77194b924814330abf5ec8ac0d2c5ef38e7", "width": 373, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/1d_974DQtKT373NMYXynWEd-cF7UlUaDMw6EWr0jG_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9883d7d69b1d5c58a9e912ea5ef838f85500d54a", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/1d_974DQtKT373NMYXynWEd-cF7UlUaDMw6EWr0jG_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe10cc7a1070a8126b8d3ff0a48abef124df4040", "width": 216, "height": 289}, {"url": "https://external-preview.redd.it/1d_974DQtKT373NMYXynWEd-cF7UlUaDMw6EWr0jG_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=856398e88d5f7c3590782bc0c890e2067e186c89", "width": 320, "height": 428}], "variants": {}, "id": "NGKrtYORI9NDgJcsA-BHQIW1Jr9Rrb5T6I6SluYQPvw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1PB goal", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14qfemb", "is_robot_indexable": true, "report_reasons": null, "author": "grabmyrooster", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qfemb/i_might_be_closer_to_my_1pb_raw_storage_capacity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qfemb/i_might_be_closer_to_my_1pb_raw_storage_capacity/", "subreddit_subscribers": 691056, "created_utc": 1688477732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Guys I have around 55TB data of data in google drive and want to move it to Dropbox. My internet is not much fast so looking for a VPS that can help me move the data as soon as possible. Looking at the posts no bandwidth is unlimited Can anyone recommend me something if anyone have experienced something like that? What should i do about it? ", "author_fullname": "t2_4g1pv66o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q7sam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688454484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys I have around 55TB data of data in google drive and want to move it to Dropbox. My internet is not much fast so looking for a VPS that can help me move the data as soon as possible. Looking at the posts no bandwidth is unlimited Can anyone recommend me something if anyone have experienced something like that? What should i do about it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14q7sam", "is_robot_indexable": true, "report_reasons": null, "author": "MSZ10", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14q7sam/data_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14q7sam/data_migration/", "subreddit_subscribers": 691056, "created_utc": 1688454484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Recently I have started to collect some Xbox, Xbox 360 and Xbox one game discs. Since they are all used, some of them may have some minor scratches on them.\n\nSo I need to check some of them to see if they are 100% readable.\n\nThe best way to do this is to calculate the MD5Sum of the discs and compare it against other peoples' MD5Sums at redump. \n\nOne way of calculating the MD5Sum of the disc is to make an .iso image file of the disc and then calculate the MD5Sum of the .iso file. There are plenty of programs that can do this.\n\nMy question is, Can I skip making an .iso image file and directly calculate the MD5Sum of the discs ? is there a utility for that on Windows ?\n\n&amp;#x200B;\n\nAny other way to make sure the discs are 100% readable ? ", "author_fullname": "t2_cb7fdihh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make sure an Xbox disc is 100% readable ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qnmfl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688496794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I have started to collect some Xbox, Xbox 360 and Xbox one game discs. Since they are all used, some of them may have some minor scratches on them.&lt;/p&gt;\n\n&lt;p&gt;So I need to check some of them to see if they are 100% readable.&lt;/p&gt;\n\n&lt;p&gt;The best way to do this is to calculate the MD5Sum of the discs and compare it against other peoples&amp;#39; MD5Sums at redump. &lt;/p&gt;\n\n&lt;p&gt;One way of calculating the MD5Sum of the disc is to make an .iso image file of the disc and then calculate the MD5Sum of the .iso file. There are plenty of programs that can do this.&lt;/p&gt;\n\n&lt;p&gt;My question is, Can I skip making an .iso image file and directly calculate the MD5Sum of the discs ? is there a utility for that on Windows ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other way to make sure the discs are 100% readable ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qnmfl", "is_robot_indexable": true, "report_reasons": null, "author": "l3gi0n0fH3ll", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qnmfl/how_to_make_sure_an_xbox_disc_is_100_readable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qnmfl/how_to_make_sure_an_xbox_disc_is_100_readable/", "subreddit_subscribers": 691056, "created_utc": 1688496794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Saw this offer on Amazon. Anyone has experience with this drive?", "author_fullname": "t2_2res6mzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a good drive for the price?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_14qngms", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mgs0JN5BSPv7eJ6YzxuwPng0zbfN9xl6t8QqHpg1riQ.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688496431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saw this offer on Amazon. Anyone has experience with this drive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ri0spihsuz9b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?auto=webp&amp;v=enabled&amp;s=a24f83f6d5f7d588bbb24c50e8a6531eafdc3d2f", "width": 1064, "height": 3031}, "resolutions": [{"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=891cb7e819e75bea255dbfb0f2f897cd7a1ee4e5", "width": 108, "height": 216}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4592f4441f965ea784d55344a0e9441b3918dd6c", "width": 216, "height": 432}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4ec8ae106784ff80b6e05f600e9e443e2e11e9a", "width": 320, "height": 640}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe9778a013d8ab5d2ac2909ef33a8c04b6de4cd6", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/ri0spihsuz9b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9b72ae33cc4895359315442757e22a29fa26752", "width": 960, "height": 1920}], "variants": {}, "id": "YNpqV0JG5PnX2k7w2CcVqECtDx4cbT_ah2MQUTyvfFI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "150 TB in Google Drive ", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qngms", "is_robot_indexable": true, "report_reasons": null, "author": "tempoguyx", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14qngms/is_it_a_good_drive_for_the_price/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ri0spihsuz9b1.jpg", "subreddit_subscribers": 691056, "created_utc": 1688496431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 3 of these drives and having to reach around all the time is a pain in the butt to unplug when I am done using them\n\nI dont want to use a hub since transferring from drive to drive on a hub is going to be really slow. So I am wanting to run them one by one into my motherboard since I have enough usb ports avail.\n\nDo they make single switches for each drive?\n\nAny other ideas on how to do this?", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Usb Seagate Expansion Drive help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qnazx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688496097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 of these drives and having to reach around all the time is a pain in the butt to unplug when I am done using them&lt;/p&gt;\n\n&lt;p&gt;I dont want to use a hub since transferring from drive to drive on a hub is going to be really slow. So I am wanting to run them one by one into my motherboard since I have enough usb ports avail.&lt;/p&gt;\n\n&lt;p&gt;Do they make single switches for each drive?&lt;/p&gt;\n\n&lt;p&gt;Any other ideas on how to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qnazx", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qnazx/usb_seagate_expansion_drive_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qnazx/usb_seagate_expansion_drive_help/", "subreddit_subscribers": 691056, "created_utc": 1688496097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought 3x WD 16gb ultrastar (renewed) drives from amazon $169 ea..  seems like a pretty solid deal, they came nicely packed and have reconditioned printed on side of the drive..  the smart table info is zero'd out on them, i figure its maybe like used drives from elsewhere.   needless to say i am doing the full burn-in/testing recommended by truenas before i put anything on them...\n\nI am definitely skeptical about the origin of these..  Anyone else using these and can share their experience? Anything I should watch out for?  Thanks.", "author_fullname": "t2_1wjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions on renewed HDDs from Amazon?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qmzcb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688495336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought 3x WD 16gb ultrastar (renewed) drives from amazon $169 ea..  seems like a pretty solid deal, they came nicely packed and have reconditioned printed on side of the drive..  the smart table info is zero&amp;#39;d out on them, i figure its maybe like used drives from elsewhere.   needless to say i am doing the full burn-in/testing recommended by truenas before i put anything on them...&lt;/p&gt;\n\n&lt;p&gt;I am definitely skeptical about the origin of these..  Anyone else using these and can share their experience? Anything I should watch out for?  Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmzcb", "is_robot_indexable": true, "report_reasons": null, "author": "dss", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmzcb/opinions_on_renewed_hdds_from_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmzcb/opinions_on_renewed_hdds_from_amazon/", "subreddit_subscribers": 691056, "created_utc": 1688495336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I know there are a lot of posts about keeping an archive of personal emails. I've read through many of them and think I have found a solution that will work for me. However, I'm just knowledgeable enough to figure out a solution, but not knowledgeable enough to have good context on the practicality of the solution or if I've missed something that will cause me problems down the road. I'd be super appreciative if anyone can offer any insight into if the goals/ solutions shown below make sense. Thanks in advance!  \n\n\nGoals: \n\nI want to archive my personal emails from the last 15 years or so across a gmail account and a few personal accounts I have through a personal website. The goal is to keep these old emails incase I want to reference them and maybe eventually organize them further, off of my computer/ servers to save disk space/ server space, and easily accessible/ searchable on an ssd drive. Additionally, I want to have the emails that are currently not archived to be accessible on my normal devices.   \n\n\nMy Solution/ process: \n\nI downloaded Thunderbird and saved it to a fairly decent sized SSD drive.   \nI connected my email account to Thunderbird with IMAP.  \nIn Thunderbird, I'll create local subfolders for each Email account I want to archive.  \nWhen I wan't to archive a bunch of emails, I'll drag/ drop them from the inbox to the local folder. \n\nI haven't dug into it, but maybe I can set up an archiving rule in Thunderbird to transfer emails older than a certain time period to the local folder. I'm assuming this would automatically happen when I open Thunderbird after being disconnected for some time.  \n\n\nThis seems fairly straight forward, but would love to know if I'm missing anything important.   \n\n\nThanks again!", "author_fullname": "t2_7xu217zo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portable Thunderbird Email Archive - Any Red Flags?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qmkb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I know there are a lot of posts about keeping an archive of personal emails. I&amp;#39;ve read through many of them and think I have found a solution that will work for me. However, I&amp;#39;m just knowledgeable enough to figure out a solution, but not knowledgeable enough to have good context on the practicality of the solution or if I&amp;#39;ve missed something that will cause me problems down the road. I&amp;#39;d be super appreciative if anyone can offer any insight into if the goals/ solutions shown below make sense. Thanks in advance!  &lt;/p&gt;\n\n&lt;p&gt;Goals: &lt;/p&gt;\n\n&lt;p&gt;I want to archive my personal emails from the last 15 years or so across a gmail account and a few personal accounts I have through a personal website. The goal is to keep these old emails incase I want to reference them and maybe eventually organize them further, off of my computer/ servers to save disk space/ server space, and easily accessible/ searchable on an ssd drive. Additionally, I want to have the emails that are currently not archived to be accessible on my normal devices.   &lt;/p&gt;\n\n&lt;p&gt;My Solution/ process: &lt;/p&gt;\n\n&lt;p&gt;I downloaded Thunderbird and saved it to a fairly decent sized SSD drive.&lt;br/&gt;\nI connected my email account to Thunderbird with IMAP.&lt;br/&gt;\nIn Thunderbird, I&amp;#39;ll create local subfolders for each Email account I want to archive.&lt;br/&gt;\nWhen I wan&amp;#39;t to archive a bunch of emails, I&amp;#39;ll drag/ drop them from the inbox to the local folder. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t dug into it, but maybe I can set up an archiving rule in Thunderbird to transfer emails older than a certain time period to the local folder. I&amp;#39;m assuming this would automatically happen when I open Thunderbird after being disconnected for some time.  &lt;/p&gt;\n\n&lt;p&gt;This seems fairly straight forward, but would love to know if I&amp;#39;m missing anything important.   &lt;/p&gt;\n\n&lt;p&gt;Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmkb6", "is_robot_indexable": true, "report_reasons": null, "author": "JacksonIsBillCarson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmkb6/portable_thunderbird_email_archive_any_red_flags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmkb6/portable_thunderbird_email_archive_any_red_flags/", "subreddit_subscribers": 691056, "created_utc": 1688494368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Every day I worry bout my old hard drive failing and losing the files to this project I've been working on for so so long. If you guys have any recommendations for an external drive that will backup the entirety of my old hard-drive, and preserve this project without being too hard on my old hard-drive during the transfer, it'd be appreciated. \n\nMy old hard-drive is only 1 TB btw (fine with buying a 4TB external tho for later backups) ", "author_fullname": "t2_3scsgz3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best external drive for PC backup (old harddrive contains 8-year digital media project)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qmhok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688494206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every day I worry bout my old hard drive failing and losing the files to this project I&amp;#39;ve been working on for so so long. If you guys have any recommendations for an external drive that will backup the entirety of my old hard-drive, and preserve this project without being too hard on my old hard-drive during the transfer, it&amp;#39;d be appreciated. &lt;/p&gt;\n\n&lt;p&gt;My old hard-drive is only 1 TB btw (fine with buying a 4TB external tho for later backups) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qmhok", "is_robot_indexable": true, "report_reasons": null, "author": "Spaloonbabagoon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qmhok/best_external_drive_for_pc_backup_old_harddrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qmhok/best_external_drive_for_pc_backup_old_harddrive/", "subreddit_subscribers": 691056, "created_utc": 1688494206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am looking for a high endurance ssd that performs well. When comparing a Samsung 870 EVO 2TB to a Samsung PM893 1TB (roughly the same price) I noticed that the EVO is a 3-bit MLC vs the data center oriented PM893 is a TLC drive. I was under the impression that MLC is superior to TLC in terms of endurance so now I am confused even more on which to go for? Can anyone shed some light on what I am missing?\n\n[https://i.imgur.com/LVrYZsR.png](https://i.imgur.com/LVrYZsR.png)\n\nscreenshot of the side by side comparison.", "author_fullname": "t2_bguqd9mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung mlc vs tlc endurance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qiqxo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688485612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am looking for a high endurance ssd that performs well. When comparing a Samsung 870 EVO 2TB to a Samsung PM893 1TB (roughly the same price) I noticed that the EVO is a 3-bit MLC vs the data center oriented PM893 is a TLC drive. I was under the impression that MLC is superior to TLC in terms of endurance so now I am confused even more on which to go for? Can anyone shed some light on what I am missing?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.imgur.com/LVrYZsR.png\"&gt;https://i.imgur.com/LVrYZsR.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;screenshot of the side by side comparison.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?auto=webp&amp;v=enabled&amp;s=046a5c68a1f16470e50806b5a1717fe921bf70e0", "width": 1793, "height": 831}, "resolutions": [{"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9f056401726a3f040163ee68b91ade4c6239999", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e1ac6bd93f601adc19e2d54808d2f2791e06e9b", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=053cf9c643253c65b682d035fa5c5472986fb42f", "width": 320, "height": 148}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c20f62007aaa7126b2306056e9fb5be5bd33080a", "width": 640, "height": 296}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=759ceaf00b147db4a0e932718980d1347f297fc9", "width": 960, "height": 444}, {"url": "https://external-preview.redd.it/VAYeVxRRU65J-RWplwo5niHWaDydnQPldk6SKp8DSHQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b65a3a8c91b3a98a3f3d2c82c217e20e1ac3f86", "width": 1080, "height": 500}], "variants": {}, "id": "DoBVfJeD_491-95tHhIp2KtEPAk_VBKtIhriezCU3YQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qiqxo", "is_robot_indexable": true, "report_reasons": null, "author": "itidi0t", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qiqxo/samsung_mlc_vs_tlc_endurance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qiqxo/samsung_mlc_vs_tlc_endurance/", "subreddit_subscribers": 691056, "created_utc": 1688485612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i have 30k images, mostly art, downloaded from all over the web. it's a pain in the ass adding metadata after i've downloaded them- i add the author of the art and then use the tags as well. is there a tool that lets me streamline the process? add the tags as im downloading the images? up to this point ive just 'right clicked, save image as'.", "author_fullname": "t2_xcwt2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tool to edit metadata on images as you download them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qf8e1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688477292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have 30k images, mostly art, downloaded from all over the web. it&amp;#39;s a pain in the ass adding metadata after i&amp;#39;ve downloaded them- i add the author of the art and then use the tags as well. is there a tool that lets me streamline the process? add the tags as im downloading the images? up to this point ive just &amp;#39;right clicked, save image as&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14qf8e1", "is_robot_indexable": true, "report_reasons": null, "author": "iakr", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qf8e1/tool_to_edit_metadata_on_images_as_you_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qf8e1/tool_to_edit_metadata_on_images_as_you_download/", "subreddit_subscribers": 691056, "created_utc": 1688477292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I find more often than not that i'm left without answers because every other subreddit is protesting and locking access.\n\nSince i have quiete a large server for movies and stuff i thougt i might be able to backup reddit.\n\nBut how would i find a the entire Reddit from weeks ago?", "author_fullname": "t2_7zn1nmj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Possible to download reddit from before protest-blackout?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qcw3y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688470694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find more often than not that i&amp;#39;m left without answers because every other subreddit is protesting and locking access.&lt;/p&gt;\n\n&lt;p&gt;Since i have quiete a large server for movies and stuff i thougt i might be able to backup reddit.&lt;/p&gt;\n\n&lt;p&gt;But how would i find a the entire Reddit from weeks ago?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qcw3y", "is_robot_indexable": true, "report_reasons": null, "author": "plantinspace", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qcw3y/possible_to_download_reddit_from_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qcw3y/possible_to_download_reddit_from_before/", "subreddit_subscribers": 691056, "created_utc": 1688470694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently purchased a 6TB HC550 hard disk drive (HDD) on eBay for 80\u20ac, which was advertised as \"new\". Upon receiving the item, I ran a SMART analysis and discovered that the HDD has 251 power-on hours and 378 start-stop counts already registered.\n\nI reached out to the seller to discuss this matter. According to the seller, the HDD is indeed new and the recorded hours are likely manufacturer testing hours. They offered a 10% refund, which would be 8\u20ac, or a full refund if I return the HDD.\n\nFrom my understanding, it seems unusual for a new HDD to have these SMART statistics. So, I thought I'd ask for your opinions and experiences.\n\nIn your view, is it possible for a new HDD to have these statistics due to manufacturer testing? Or do these statistics indicate previous usage?\n\nI'm attaching the SMART statistics taken from my Synology DS220 for reference at the end of this post.\n\nI appreciate any input or advice you can offer!\n\nThank you so much.\n\nhttps://imgur.com/a/mll0bB2", "author_fullname": "t2_pvtn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bought a \"New\" HDD on eBay with 251 Power-On Hours and 378 Start-Stop Counts - Need Your Opinions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qckoy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688469753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently purchased a 6TB HC550 hard disk drive (HDD) on eBay for 80\u20ac, which was advertised as &amp;quot;new&amp;quot;. Upon receiving the item, I ran a SMART analysis and discovered that the HDD has 251 power-on hours and 378 start-stop counts already registered.&lt;/p&gt;\n\n&lt;p&gt;I reached out to the seller to discuss this matter. According to the seller, the HDD is indeed new and the recorded hours are likely manufacturer testing hours. They offered a 10% refund, which would be 8\u20ac, or a full refund if I return the HDD.&lt;/p&gt;\n\n&lt;p&gt;From my understanding, it seems unusual for a new HDD to have these SMART statistics. So, I thought I&amp;#39;d ask for your opinions and experiences.&lt;/p&gt;\n\n&lt;p&gt;In your view, is it possible for a new HDD to have these statistics due to manufacturer testing? Or do these statistics indicate previous usage?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m attaching the SMART statistics taken from my Synology DS220 for reference at the end of this post.&lt;/p&gt;\n\n&lt;p&gt;I appreciate any input or advice you can offer!&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/mll0bB2\"&gt;https://imgur.com/a/mll0bB2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ne30huEi9GaingZcchxdSmECEuIogq8vJRr9Cfc5IoY.jpg?auto=webp&amp;v=enabled&amp;s=79eec4136b70521e3bf6ac8de6fc34985c92cedb", "width": 956, "height": 781}, "resolutions": [{"url": "https://external-preview.redd.it/Ne30huEi9GaingZcchxdSmECEuIogq8vJRr9Cfc5IoY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb0681c4e2103aeee154d3da67421ffc052db3fd", "width": 108, "height": 88}, {"url": "https://external-preview.redd.it/Ne30huEi9GaingZcchxdSmECEuIogq8vJRr9Cfc5IoY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3f8011a3c2c4ae5d24dbea94fbdd60fde9b8c12", "width": 216, "height": 176}, {"url": "https://external-preview.redd.it/Ne30huEi9GaingZcchxdSmECEuIogq8vJRr9Cfc5IoY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=262ce6d3c1a641ea1098431a919a6233243d7c69", "width": 320, "height": 261}, {"url": "https://external-preview.redd.it/Ne30huEi9GaingZcchxdSmECEuIogq8vJRr9Cfc5IoY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b82fc3617e99212acd59ad35f080c44415c9b4ec", "width": 640, "height": 522}], "variants": {}, "id": "yszEFq2iGXyhx4BHcQ2t14LCzihHAxrjRXKHGUiEHrU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qckoy", "is_robot_indexable": true, "report_reasons": null, "author": "Ayonx", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qckoy/bought_a_new_hdd_on_ebay_with_251_poweron_hours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qckoy/bought_a_new_hdd_on_ebay_with_251_poweron_hours/", "subreddit_subscribers": 691056, "created_utc": 1688469753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The only place I found is the National Park [website](https://www.nps.gov/civilwar/search-soldiers.htm) but scraping this website will take forever (endless scrolling needed).  I was wondering if there is a copy of the databse somewhere.\nThanks!", "author_fullname": "t2_nakkq8ax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a copy of Civil War Soldiers database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q71uo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688452113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The only place I found is the National Park &lt;a href=\"https://www.nps.gov/civilwar/search-soldiers.htm\"&gt;website&lt;/a&gt; but scraping this website will take forever (endless scrolling needed).  I was wondering if there is a copy of the databse somewhere.\nThanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14q71uo", "is_robot_indexable": true, "report_reasons": null, "author": "secaps123", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14q71uo/is_there_a_copy_of_civil_war_soldiers_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14q71uo/is_there_a_copy_of_civil_war_soldiers_database/", "subreddit_subscribers": 691056, "created_utc": 1688452113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to save them using the Video DownloadHelper Firefox extension but that no longer appears to work.", "author_fullname": "t2_131dxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know of a way to rip vids from Justforfans?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q0p55", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688433037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to save them using the Video DownloadHelper Firefox extension but that no longer appears to work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14q0p55", "is_robot_indexable": true, "report_reasons": null, "author": "magnumXLs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14q0p55/anyone_know_of_a_way_to_rip_vids_from_justforfans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14q0p55/anyone_know_of_a_way_to_rip_vids_from_justforfans/", "subreddit_subscribers": 691056, "created_utc": 1688433037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "New to Jellyfin, never used the server before. Downloaded and installed on Windows 10 64 bit, but won't open when I try to open. Jellyfin task is visible in TM but nothing is opening. Couldn't find a solution on the web, any help is appreciated! ", "author_fullname": "t2_9aj1cs1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jellyfin not opening", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qnmcl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688496789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to Jellyfin, never used the server before. Downloaded and installed on Windows 10 64 bit, but won&amp;#39;t open when I try to open. Jellyfin task is visible in TM but nothing is opening. Couldn&amp;#39;t find a solution on the web, any help is appreciated! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14qnmcl", "is_robot_indexable": true, "report_reasons": null, "author": "SamSepi0l599", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14qnmcl/jellyfin_not_opening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14qnmcl/jellyfin_not_opening/", "subreddit_subscribers": 691056, "created_utc": 1688496789.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}