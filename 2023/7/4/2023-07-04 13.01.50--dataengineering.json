{"kind": "Listing", "data": {"after": "t3_14pkxnz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am preparing for an entry level data engineering position. I have one year working experience as a data analyst. \n\nPlease provide your feedbacks on the roadmap.", "author_fullname": "t2_csjsu6koh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering roadmap", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14pll13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "ups": 197, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 197, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jL8UItWJxagnjkNloXcDZX6dcCzSpHoqjeBH_ql7piM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688396724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am preparing for an entry level data engineering position. I have one year working experience as a data analyst. &lt;/p&gt;\n\n&lt;p&gt;Please provide your feedbacks on the roadmap.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/7kclbs2bmr9b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?auto=webp&amp;v=enabled&amp;s=46ef94456a1c8727406813b965c9061f97352c70", "width": 3472, "height": 4640}, "resolutions": [{"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16113d7ce764dd63512fd0bb07988dbf31f197e6", "width": 108, "height": 144}, {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5af46c867ba7b334a528b147f711697cc211b078", "width": 216, "height": 288}, {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02d760182f2150b4198d29cfb0873a05cadbc403", "width": 320, "height": 427}, {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cbe4815529eb8708f795c5d0cb99229b0a79036", "width": 640, "height": 855}, {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e54146509ba2d3472f56a7a82fee4c189924f06", "width": 960, "height": 1282}, {"url": "https://preview.redd.it/7kclbs2bmr9b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c2997e24f765b264e844f1866a37d2bd5fcc101", "width": 1080, "height": 1443}], "variants": {}, "id": "ikvY8yyleeYjXyErPZeGcKKi9PSoW3NRR3Uy4yslXLg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pll13", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Article6420", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14pll13/data_engineering_roadmap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/7kclbs2bmr9b1.jpg", "subreddit_subscribers": 113970, "created_utc": 1688396724.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically the title, I wanted to know if smart ETL\u2019ing, or great cloud skills makes us into the top 1% in our field. What in your opinion is the skills required to be in the top 1% of data engineering field?", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What skills is top 1% in data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pzuls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688430620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title, I wanted to know if smart ETL\u2019ing, or great cloud skills makes us into the top 1% in our field. What in your opinion is the skills required to be in the top 1% of data engineering field?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pzuls", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pzuls/what_skills_is_top_1_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pzuls/what_skills_is_top_1_in_data_engineering/", "subreddit_subscribers": 113970, "created_utc": 1688430620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reducing Data Platform Cost by $2M", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_14pk4p6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JXT4AQ4CpOoTqPuXzm5WLmoZjFrHy_3WTEPDupL7diA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688393434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "engineering.razorpay.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://engineering.razorpay.com/reducing-data-platform-cost-by-2m-d8f82285c4ae", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?auto=webp&amp;v=enabled&amp;s=8c742f369597d3d947855897009efd7825c79a1d", "width": 1200, "height": 629}, "resolutions": [{"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ace638e75bf26863e67c1cc5685e9362441bad5b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=125a40902b2236dfc963860df4a9f539f3512d31", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a00d25272676560bca66a957fe99803fffbc3ab", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ec688bce1de544b46d4ed955d96724160b2cb7d", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ba7ea8942a35faa775cdb37cd9f70ff54f2b4e3", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/UQv39PHs0KRzJq4oePPhN6VxEobV7d1cxJGCV0mwmzc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a192023fc0e37e8a08b5c556351df6bc9ee0c489", "width": 1080, "height": 566}], "variants": {}, "id": "Bmc-lCRdmqgT8FMUGr2lVxOtONwc7bFZMlWtQPRYcIg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14pk4p6", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pk4p6/reducing_data_platform_cost_by_2m/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://engineering.razorpay.com/reducing-data-platform-cost-by-2m-d8f82285c4ae", "subreddit_subscribers": 113970, "created_utc": 1688393434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know basic python. I can connect to different databases fetch data, transform it and load to other database. \nI want to become good at python. So, my question would be if DSA is even required for becoming a data engineer and if yes then how much?", "author_fullname": "t2_csjsu6koh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much DSA is required for a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ph895", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688386130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know basic python. I can connect to different databases fetch data, transform it and load to other database. \nI want to become good at python. So, my question would be if DSA is even required for becoming a data engineer and if yes then how much?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ph895", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Article6420", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/14ph895/how_much_dsa_is_required_for_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ph895/how_much_dsa_is_required_for_a_data_engineer/", "subreddit_subscribers": 113970, "created_utc": 1688386130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone interviewed DE candidates and \u2014 in response to them answering a SQL interview question with a window function \u2014 asked them how to solve it without the window function? If so, why? To me, that doesn\u2019t seem like a value added constraint to add to the interview.", "author_fullname": "t2_bqrxlxtj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not using window functions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pmlt0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688399089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone interviewed DE candidates and \u2014 in response to them answering a SQL interview question with a window function \u2014 asked them how to solve it without the window function? If so, why? To me, that doesn\u2019t seem like a value added constraint to add to the interview.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14pmlt0", "is_robot_indexable": true, "report_reasons": null, "author": "data_questions", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pmlt0/not_using_window_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pmlt0/not_using_window_functions/", "subreddit_subscribers": 113970, "created_utc": 1688399089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there! I've done some data analysis based on Glassdoor job postings for the search term \"data engineer\" I looked at around 3,000 job entries from Europe, USA + Canada, South-East Asia + Oceania.\n\nHere's the analysis I conducted:\nYou can find it here: *[+100 insights - Data Engineer \ud83e\udded\ud83d\uddfa\ufe0f](https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#%F0%9F%8C%9F-Introduction)*\n\nI based my research on various sources like blogs, courses, and vlogs to figure out what companies mean when they use the title \"data engineer\". The data science roles can be vague, and responsibilities tend to get mixed up. So, I'm here to gather valuable feedback.\n\n___\n\nOne area where I have the most doubts is regarding the selected technologies. I'm not sure if I've categorized them correctly or if I've chosen the right ones. You can check the tech here: *[Tech Knowledge Required](https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#14.-Tech-Knowledge-Required)*\n\nHere are the specific questions I have according to tech:\n\n1. Is Databricks more of a \"cloud platform\" or a \"data integration &amp; processing platform\"?\n2. Are the *\"Tech Knowledge Required\"* categories well-defined?\n3. Are there any redundant categories?\n4. Are there any missing or unnecessary tech categories?\n5. Do you think there's anything else I should add?\n\t\nPlease point out any misspelled words or confusing phrases in my analysis. Feel free to provide any additional comments apart from the technology aspect.\n\n___\n\nLastly, I have a premium question for you. I've heard that in most companies, a \"Data Scientist\" is essentially just a \"Data Engineer\" with added responsibilities like creating regressions and data classifications. I'm curious about your insights or personal experience with this. Or does it depend on the company?\n\nThanks a lot for taking the time to read through this!\n\n___\n\nI'm a fresh Redditor, so I apologize if I violated any rule here ;)", "author_fullname": "t2_yf843", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey! I've Analyzed Thousands of Data Engineer Job Postings - Feedback Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q92b9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688460799.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688458714.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! I&amp;#39;ve done some data analysis based on Glassdoor job postings for the search term &amp;quot;data engineer&amp;quot; I looked at around 3,000 job entries from Europe, USA + Canada, South-East Asia + Oceania.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the analysis I conducted:\nYou can find it here: &lt;em&gt;&lt;a href=\"https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#%F0%9F%8C%9F-Introduction\"&gt;+100 insights - Data Engineer \ud83e\udded\ud83d\uddfa\ufe0f&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I based my research on various sources like blogs, courses, and vlogs to figure out what companies mean when they use the title &amp;quot;data engineer&amp;quot;. The data science roles can be vague, and responsibilities tend to get mixed up. So, I&amp;#39;m here to gather valuable feedback.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;One area where I have the most doubts is regarding the selected technologies. I&amp;#39;m not sure if I&amp;#39;ve categorized them correctly or if I&amp;#39;ve chosen the right ones. You can check the tech here: &lt;em&gt;&lt;a href=\"https://www.kaggle.com/code/lukkardata/100-insights-data-engineer#14.-Tech-Knowledge-Required\"&gt;Tech Knowledge Required&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are the specific questions I have according to tech:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is Databricks more of a &amp;quot;cloud platform&amp;quot; or a &amp;quot;data integration &amp;amp; processing platform&amp;quot;?&lt;/li&gt;\n&lt;li&gt;Are the &lt;em&gt;&amp;quot;Tech Knowledge Required&amp;quot;&lt;/em&gt; categories well-defined?&lt;/li&gt;\n&lt;li&gt;Are there any redundant categories?&lt;/li&gt;\n&lt;li&gt;Are there any missing or unnecessary tech categories?&lt;/li&gt;\n&lt;li&gt;Do you think there&amp;#39;s anything else I should add?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please point out any misspelled words or confusing phrases in my analysis. Feel free to provide any additional comments apart from the technology aspect.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Lastly, I have a premium question for you. I&amp;#39;ve heard that in most companies, a &amp;quot;Data Scientist&amp;quot; is essentially just a &amp;quot;Data Engineer&amp;quot; with added responsibilities like creating regressions and data classifications. I&amp;#39;m curious about your insights or personal experience with this. Or does it depend on the company?&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for taking the time to read through this!&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I&amp;#39;m a fresh Redditor, so I apologize if I violated any rule here ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?auto=webp&amp;v=enabled&amp;s=06fa2c14b6ccb76528c85e988a38335ee8d4e9a8", "width": 160, "height": 160}, "resolutions": [{"url": "https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55a56d0e849f51b511a0d930de53bbed87e211f7", "width": 108, "height": 108}], "variants": {}, "id": "p15coSqe7L8wApjnVlwASEYE50BcnmvRuPbSVpGUPaM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14q92b9", "is_robot_indexable": true, "report_reasons": null, "author": "Lukkar", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q92b9/hey_ive_analyzed_thousands_of_data_engineer_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q92b9/hey_ive_analyzed_thousands_of_data_engineer_job/", "subreddit_subscribers": 113970, "created_utc": 1688458714.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, everyone! A few months ago I started to learn both Python (Pandas, PySpark) and SQL from a company that was hiring people that go to that 2 months \"bootcamp\".  \n  \nThere, I learned the basics of SQL and Python, and we did an end to end on Azure with Databricks and a Kaggle dataset in the form of an SQL database for the end of the course.  \n  \nI started working supposedly as a data engineer for that same company, since I did a great job and only a few were qualified for the job: me being one of them (I really put a lot of effort and endless hours for that purpose).That being said, I'm galaxies far from being a proficient DE.  \n  \nSo, I was assigned to an awful project for a company that works on prem, with awful xlsx files generated by an outdated software, and I'm in charge of giving support of Power BI. A few weeks ago I knew close to nothing about Power BI, dax and all the stuff involved, so that's really unfair.  \n  \nNo one is judging my progress, I'm doing well so far, but that's not what I signed for: power Bi dashboard support and documentation basically. Apart from being awful it is not giving me any helpful experience as a DE.  \n  \nSo, for people with experience, is this playing by the rules? Is this normal? What would you do?", "author_fullname": "t2_j7j4f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hired as DE but working as Analyst. WWYD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pkrzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688394906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone! A few months ago I started to learn both Python (Pandas, PySpark) and SQL from a company that was hiring people that go to that 2 months &amp;quot;bootcamp&amp;quot;.  &lt;/p&gt;\n\n&lt;p&gt;There, I learned the basics of SQL and Python, and we did an end to end on Azure with Databricks and a Kaggle dataset in the form of an SQL database for the end of the course.  &lt;/p&gt;\n\n&lt;p&gt;I started working supposedly as a data engineer for that same company, since I did a great job and only a few were qualified for the job: me being one of them (I really put a lot of effort and endless hours for that purpose).That being said, I&amp;#39;m galaxies far from being a proficient DE.  &lt;/p&gt;\n\n&lt;p&gt;So, I was assigned to an awful project for a company that works on prem, with awful xlsx files generated by an outdated software, and I&amp;#39;m in charge of giving support of Power BI. A few weeks ago I knew close to nothing about Power BI, dax and all the stuff involved, so that&amp;#39;s really unfair.  &lt;/p&gt;\n\n&lt;p&gt;No one is judging my progress, I&amp;#39;m doing well so far, but that&amp;#39;s not what I signed for: power Bi dashboard support and documentation basically. Apart from being awful it is not giving me any helpful experience as a DE.  &lt;/p&gt;\n\n&lt;p&gt;So, for people with experience, is this playing by the rules? Is this normal? What would you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14pkrzu", "is_robot_indexable": true, "report_reasons": null, "author": "Marawishka", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pkrzu/hired_as_de_but_working_as_analyst_wwyd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pkrzu/hired_as_de_but_working_as_analyst_wwyd/", "subreddit_subscribers": 113970, "created_utc": 1688394906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently working as DE in a small team with the only technical person. We have bunch of GUI based ETL jobs and python pandas files( eat up lot of memory) that are triggered using cron on an EC2 server. I have been tasked with revamping the workflow orchestration. Two reasons for doing this: alerting users in case of job failures and better/efficient workflow management. I looked at few solutions and found two possible solutions: Airflow or AWS lambda. Which one among these would be the best (or any other)\nP.S: trying to move from DE to SDE/SWE, so want to implement a solution that can help me highlight on my resume for SDE/SWE positions", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best workflow solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pxq7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688425041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working as DE in a small team with the only technical person. We have bunch of GUI based ETL jobs and python pandas files( eat up lot of memory) that are triggered using cron on an EC2 server. I have been tasked with revamping the workflow orchestration. Two reasons for doing this: alerting users in case of job failures and better/efficient workflow management. I looked at few solutions and found two possible solutions: Airflow or AWS lambda. Which one among these would be the best (or any other)\nP.S: trying to move from DE to SDE/SWE, so want to implement a solution that can help me highlight on my resume for SDE/SWE positions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pxq7v", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pxq7v/best_workflow_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pxq7v/best_workflow_solution/", "subreddit_subscribers": 113970, "created_utc": 1688425041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nLooking to get more knowledge on data warehousing and I've seen the Kimball books mentioned a couple of times and was just wondering if anyone could point me in the direction of the best one to get for a beginner please", "author_fullname": "t2_44gx5087", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kimball books", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q9f0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688459926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Looking to get more knowledge on data warehousing and I&amp;#39;ve seen the Kimball books mentioned a couple of times and was just wondering if anyone could point me in the direction of the best one to get for a beginner please&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14q9f0u", "is_robot_indexable": true, "report_reasons": null, "author": "rogerbarario", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q9f0u/kimball_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q9f0u/kimball_books/", "subreddit_subscribers": 113970, "created_utc": 1688459926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Complex calculation to be done in tableau and minimalistic aggregation to be done in backend is my understanding. I am still speculative what shouldn't be done in the visualization layer and what should be done in visualization layer", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformation in Tableau vs Backend [ETL]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q3yo0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688442530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Complex calculation to be done in tableau and minimalistic aggregation to be done in backend is my understanding. I am still speculative what shouldn&amp;#39;t be done in the visualization layer and what should be done in visualization layer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14q3yo0", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q3yo0/transformation_in_tableau_vs_backend_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q3yo0/transformation_in_tableau_vs_backend_etl/", "subreddit_subscribers": 113970, "created_utc": 1688442530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have set up a pipeline which uses SCD Type 1 and overwrites whenever there is a change to Table A. Table A contains a mapping which is used to calculate an important business metric. When new mappings are added/removed (which happens every few months or so) the new mapping is used historically and makes the historical records of this metric wrong in Table B.\n\nI can see that the solution to this issue is to use SCD Type 2 (recording the start and end dates and apply the correct mapping for any given date) but can't figure out how to do this practically. I could work out how to apply SCD Type 2 as there's some info online about doing so but haven't been able to find much on actually utilising this feature.\n\nThe calculation for the metric is already somewhat complex due to some nuances that needed to be included and I can't imagine handling applying the mapping to specific dates on top - especially given that handling of nuances involve messing around with the mappings to generate extra data points that are to be used in the business metric calculation.\n\nI appreciate that this description is somewhat abstract but I was wondering if there are any good resources on actually applying SCD Type 2 into data transformations or is it just a matter of playing around with the code until I can solve all the issues and get the metric to calculate properly?", "author_fullname": "t2_n937n0g6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing and using SCD Type 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pnufd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688401917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have set up a pipeline which uses SCD Type 1 and overwrites whenever there is a change to Table A. Table A contains a mapping which is used to calculate an important business metric. When new mappings are added/removed (which happens every few months or so) the new mapping is used historically and makes the historical records of this metric wrong in Table B.&lt;/p&gt;\n\n&lt;p&gt;I can see that the solution to this issue is to use SCD Type 2 (recording the start and end dates and apply the correct mapping for any given date) but can&amp;#39;t figure out how to do this practically. I could work out how to apply SCD Type 2 as there&amp;#39;s some info online about doing so but haven&amp;#39;t been able to find much on actually utilising this feature.&lt;/p&gt;\n\n&lt;p&gt;The calculation for the metric is already somewhat complex due to some nuances that needed to be included and I can&amp;#39;t imagine handling applying the mapping to specific dates on top - especially given that handling of nuances involve messing around with the mappings to generate extra data points that are to be used in the business metric calculation.&lt;/p&gt;\n\n&lt;p&gt;I appreciate that this description is somewhat abstract but I was wondering if there are any good resources on actually applying SCD Type 2 into data transformations or is it just a matter of playing around with the code until I can solve all the issues and get the metric to calculate properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14pnufd", "is_robot_indexable": true, "report_reasons": null, "author": "piri9825", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pnufd/implementing_and_using_scd_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pnufd/implementing_and_using_scd_type_2/", "subreddit_subscribers": 113970, "created_utc": 1688401917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\\*Hi All!\\*\n\n  \n\n\nLately, I've been immersing myself in Snowflake tool and exploring its capabilities. Along the way, I've compiled a collection of queries that I've executed and organized them on GitHub. It's a resource for anyone looking to refresh their understanding of the basic concepts.\n\n  \n\n\nHere's the link to the GitHub repository: \\[Snowflake Queries Repository\\](https://github.com/dattapadal/Snowflake_tutorial.git)\n\n  \n\n\nThanks", "author_fullname": "t2_138wofcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake basic concepts with queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q8rts", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688457760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*Hi All!*&lt;/p&gt;\n\n&lt;p&gt;Lately, I&amp;#39;ve been immersing myself in Snowflake tool and exploring its capabilities. Along the way, I&amp;#39;ve compiled a collection of queries that I&amp;#39;ve executed and organized them on GitHub. It&amp;#39;s a resource for anyone looking to refresh their understanding of the basic concepts.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to the GitHub repository: [Snowflake Queries Repository](&lt;a href=\"https://github.com/dattapadal/Snowflake_tutorial.git\"&gt;https://github.com/dattapadal/Snowflake_tutorial.git&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?auto=webp&amp;v=enabled&amp;s=ee0b5e8c05946a835350ed8272a10fd6e84bd6fd", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58c2a3e02e2d0305738b37f02e38628e7100030d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae6104a3cf487c5b8dd2911570335ede54101b3a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17fa74f417a0afec7c0fa2ee474689cb60c7fcf4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1137f0707b0a04891814efbe90fbb47374648c9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=147aaed4cca00816d4f8926d0f6c189b49ca15e2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/W6xO7n7RE-FsiAyRm0_56YyETqfsvYRbpt_Y6okUWfI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f6c08d3a0da7a93c1da9b486ef7cab338ef6e3b", "width": 1080, "height": 540}], "variants": {}, "id": "VNtyUw_auHa3mR_aUhtUyTw1gYG26LlQwNZoU08Ww2Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "14q8rts", "is_robot_indexable": true, "report_reasons": null, "author": "akashTheTraveller", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q8rts/snowflake_basic_concepts_with_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q8rts/snowflake_basic_concepts_with_queries/", "subreddit_subscribers": 113970, "created_utc": 1688457760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi DE community, I have a question that's bothering me for a while. While I am a Lead Analyst, but I mostly do DE and Analytics Engg work...tech stack being Snowflake, dbt, Airflow.... Will my designation affect my future job search as a Data Engineer.", "author_fullname": "t2_2q90ftwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Doing DE but designation is Lead Analyst. Will that affect next job search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pol99", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688403650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi DE community, I have a question that&amp;#39;s bothering me for a while. While I am a Lead Analyst, but I mostly do DE and Analytics Engg work...tech stack being Snowflake, dbt, Airflow.... Will my designation affect my future job search as a Data Engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14pol99", "is_robot_indexable": true, "report_reasons": null, "author": "ppdas", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pol99/doing_de_but_designation_is_lead_analyst_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pol99/doing_de_but_designation_is_lead_analyst_will/", "subreddit_subscribers": 113970, "created_utc": 1688403650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Confused between Azure , AWS and GCP and I'm getting mixed Opinions. Any guidance would be appreciated.", "author_fullname": "t2_3hhrw7ib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Cloud certificate for foundations will be the best in terms of growth and applicability for data engineering and Machine learning engineering roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pnzv7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688402276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Confused between Azure , AWS and GCP and I&amp;#39;m getting mixed Opinions. Any guidance would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pnzv7", "is_robot_indexable": true, "report_reasons": null, "author": "AnishNehete", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pnzv7/which_cloud_certificate_for_foundations_will_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pnzv7/which_cloud_certificate_for_foundations_will_be/", "subreddit_subscribers": 113970, "created_utc": 1688402276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Reddit,\n\nI am building a DWH for a client that has most of its data stored in Azure Cosmos containers. Each items, but not all are stored with GUIDs as IDs.\n\n&amp;#x200B;\n\nI am tasked with making a data warehouse to store historical changes, but also to enable reporting in Power BI. I have developed a theoretical pipeline inspired by Databricks Medallion Structure that loads the data into seperate SQL tables using ADF for orchestration:\n\n\\-  staging SQL tables with ID's, data in json structure and a timestamp\n\n\\- A bronze layer, which checks for changes in the data between the staging tables and present table. It will skip unchanges records, and create a new row with updated set time\\_from and time\\_to columns in order to form SCD2s.\n\n\\- A silver layer, where most of the data will be cleaned &amp; transformed, a tabular schema will be established using  statitically defined SQL queries with data types.  \n\n\\- Gold layer, where I was considering making a star schema inspired by Kimballs 3NF. This would involve making fact tables and add surrogate keys either in this layer or the prior, maybe using a mapping table, as I could imagine that the data from the source could change relatively often.\n\n&amp;#x200B;\n\nSo my issue is, how valid is this approach or a thing of the past? I come from a prior Power BI consultancy where we strictly used Kimballs architecture, but I was never taught how to establish the Fact tables. Additionally, I am unsure how to establish a fail-proof method of adding surrogate keys and handle changing dimensions in this regards. \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8112m7hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Surrogate keys and building fact tables in datawarehouses a thing of the past?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14qczkc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688470969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Reddit,&lt;/p&gt;\n\n&lt;p&gt;I am building a DWH for a client that has most of its data stored in Azure Cosmos containers. Each items, but not all are stored with GUIDs as IDs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am tasked with making a data warehouse to store historical changes, but also to enable reporting in Power BI. I have developed a theoretical pipeline inspired by Databricks Medallion Structure that loads the data into seperate SQL tables using ADF for orchestration:&lt;/p&gt;\n\n&lt;p&gt;-  staging SQL tables with ID&amp;#39;s, data in json structure and a timestamp&lt;/p&gt;\n\n&lt;p&gt;- A bronze layer, which checks for changes in the data between the staging tables and present table. It will skip unchanges records, and create a new row with updated set time_from and time_to columns in order to form SCD2s.&lt;/p&gt;\n\n&lt;p&gt;- A silver layer, where most of the data will be cleaned &amp;amp; transformed, a tabular schema will be established using  statitically defined SQL queries with data types.  &lt;/p&gt;\n\n&lt;p&gt;- Gold layer, where I was considering making a star schema inspired by Kimballs 3NF. This would involve making fact tables and add surrogate keys either in this layer or the prior, maybe using a mapping table, as I could imagine that the data from the source could change relatively often.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So my issue is, how valid is this approach or a thing of the past? I come from a prior Power BI consultancy where we strictly used Kimballs architecture, but I was never taught how to establish the Fact tables. Additionally, I am unsure how to establish a fail-proof method of adding surrogate keys and handle changing dimensions in this regards. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14qczkc", "is_robot_indexable": true, "report_reasons": null, "author": "Olafcitoo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14qczkc/are_surrogate_keys_and_building_fact_tables_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14qczkc/are_surrogate_keys_and_building_fact_tables_in/", "subreddit_subscribers": 113970, "created_utc": 1688470969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, in my work there is data we get / extract data from Postgresql, MySQL, etc with estimated schedule every 15 minutes with data we get incremental 1 day. The data that we get it's more than million. why 15 minutes, it's because we need to serve data fastly but not realtime to other transformation process. I want it stay incremental 1 day by incremental column.\n\nWe extract the data by using query in source db. Before i getting into airflow, we used pentaho / kettle for this ingestion / extraction. But, the problem when we extract data on airflow using pandas + psycopg to send it into aws S3 is getting slow and consume so much resource in cpu than im using pentaho before that more consume of memory and fast for getting data. I guess this is cause of Java Connector in pentaho that make it run faster (?) i dont know.\n\nThis is for old pipeline:\nPostgresql (Source) -&gt; Pentaho Extraction (JDBC ?) -&gt; Local System (CSV) -&gt; AWS S3 -&gt; Redshift\n\nNew Pipeline : \nPostgresql (Source) -&gt; Airflow Extraction (Pandas + psycopg) -&gt; Local System (CSV) -&gt; AWS S3 -&gt; Redshift\n\nOld Pipeline is faster, and resources consume less. When i tried full refresh on pentaho transformation it's never getting error about high memory, but when i tried airflow with pandas and psycopg it's get error high memory (memory leak).\n\nSo, i'm still want to use Airflow for this data extraction and will remove pentaho. But, i don't know what the best practice if we get data that having schedule will update every 15 minutes. Is there any Open Source tools or libraries that help this problem ? \n\nI Have tried but it's still not solving the problem:\n- Asynchronously get data using AIOPG / aiomysql but there is not really much improvement or nothing is improved by speed.\n- Apache Spark, first when i call SparkSession.builder.getOrCreate() every run schedule it's take time consuming about 30 - 60 seconds, secondly sometimes spark is fast but sometimes it's slow.\n- Airbyte, first it's good using JDBC and good when tried for full refresh. But, airbyte is not based query, because i dont want to get all the column table, and sometimes there is more than 2 incremental column so i preffered query based instead. The dbt transform in airbyte make it confuss me because is it like we get all the data with all the column first then we transform it ? so it's kinda ineffective or maybe time consuming\n\nI'm newbie in data engineer. That's why i want to know what maybe best practice for extraction data that schedule every 15 minutes with big data ? I still want to get data with incremental 1 day, fastly, less consume resource or getting no error when trying with full refresh data. (Full refresh data is not every 15 minutes but it trigger manually) \n\nWhat about your company data extraction method ? is anything wrong with my trial in async, airbyte, or spark that maybe im skipped ?\nThanks A lot", "author_fullname": "t2_8ejjs2r3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What best practices get / extract data that frequently update with a little big data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14qbe2n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688466192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, in my work there is data we get / extract data from Postgresql, MySQL, etc with estimated schedule every 15 minutes with data we get incremental 1 day. The data that we get it&amp;#39;s more than million. why 15 minutes, it&amp;#39;s because we need to serve data fastly but not realtime to other transformation process. I want it stay incremental 1 day by incremental column.&lt;/p&gt;\n\n&lt;p&gt;We extract the data by using query in source db. Before i getting into airflow, we used pentaho / kettle for this ingestion / extraction. But, the problem when we extract data on airflow using pandas + psycopg to send it into aws S3 is getting slow and consume so much resource in cpu than im using pentaho before that more consume of memory and fast for getting data. I guess this is cause of Java Connector in pentaho that make it run faster (?) i dont know.&lt;/p&gt;\n\n&lt;p&gt;This is for old pipeline:\nPostgresql (Source) -&amp;gt; Pentaho Extraction (JDBC ?) -&amp;gt; Local System (CSV) -&amp;gt; AWS S3 -&amp;gt; Redshift&lt;/p&gt;\n\n&lt;p&gt;New Pipeline : \nPostgresql (Source) -&amp;gt; Airflow Extraction (Pandas + psycopg) -&amp;gt; Local System (CSV) -&amp;gt; AWS S3 -&amp;gt; Redshift&lt;/p&gt;\n\n&lt;p&gt;Old Pipeline is faster, and resources consume less. When i tried full refresh on pentaho transformation it&amp;#39;s never getting error about high memory, but when i tried airflow with pandas and psycopg it&amp;#39;s get error high memory (memory leak).&lt;/p&gt;\n\n&lt;p&gt;So, i&amp;#39;m still want to use Airflow for this data extraction and will remove pentaho. But, i don&amp;#39;t know what the best practice if we get data that having schedule will update every 15 minutes. Is there any Open Source tools or libraries that help this problem ? &lt;/p&gt;\n\n&lt;p&gt;I Have tried but it&amp;#39;s still not solving the problem:\n- Asynchronously get data using AIOPG / aiomysql but there is not really much improvement or nothing is improved by speed.\n- Apache Spark, first when i call SparkSession.builder.getOrCreate() every run schedule it&amp;#39;s take time consuming about 30 - 60 seconds, secondly sometimes spark is fast but sometimes it&amp;#39;s slow.\n- Airbyte, first it&amp;#39;s good using JDBC and good when tried for full refresh. But, airbyte is not based query, because i dont want to get all the column table, and sometimes there is more than 2 incremental column so i preffered query based instead. The dbt transform in airbyte make it confuss me because is it like we get all the data with all the column first then we transform it ? so it&amp;#39;s kinda ineffective or maybe time consuming&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m newbie in data engineer. That&amp;#39;s why i want to know what maybe best practice for extraction data that schedule every 15 minutes with big data ? I still want to get data with incremental 1 day, fastly, less consume resource or getting no error when trying with full refresh data. (Full refresh data is not every 15 minutes but it trigger manually) &lt;/p&gt;\n\n&lt;p&gt;What about your company data extraction method ? is anything wrong with my trial in async, airbyte, or spark that maybe im skipped ?\nThanks A lot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14qbe2n", "is_robot_indexable": true, "report_reasons": null, "author": "azharizz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14qbe2n/what_best_practices_get_extract_data_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14qbe2n/what_best_practices_get_extract_data_that/", "subreddit_subscribers": 113970, "created_utc": 1688466192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have many streams and main business objects can be found in different flavours in different datastores (data mesh kinda architecture). The problem is ensuring that data quality stays good along the way as we can have troubles finding the root cause of bad / missing data. (not published event or bad consumed event, lag?)\n\nWhat would be a good way to monitor this, I'm feeling tracing, logs &amp; supervision are very tech solutions that are great for globally ensuring that everything is working, not so great for focused data supervision (we do not have very exploitable logs so fetching other app logs and yours through 1 splunk query is not possible), they can't really help when you particularly want to check quickly the sanity around 1 business object (it requires devops, dev access, takes time). \n\nI'm thinking that building a business object timeseries (dynamodb or cassandra for instance) for storing latest footprints pushed at end of processes of said business objects (id 1, gone stream A ok at timestamp t1...), so I could easily fetch by API what happened lately on these elements accross multiple apps &amp; datastores, would it be a good way or I am actually reinventing the wheel somewhere? The idea of having such timelines would also allow real time data health checks (for some random picked ones for instance) which I believe would be very valuable. Did anybody build something similar or could cover this data quality check?  \n", "author_fullname": "t2_fcsuc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to monitor business objects through EDA?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q82kg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688455444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have many streams and main business objects can be found in different flavours in different datastores (data mesh kinda architecture). The problem is ensuring that data quality stays good along the way as we can have troubles finding the root cause of bad / missing data. (not published event or bad consumed event, lag?)&lt;/p&gt;\n\n&lt;p&gt;What would be a good way to monitor this, I&amp;#39;m feeling tracing, logs &amp;amp; supervision are very tech solutions that are great for globally ensuring that everything is working, not so great for focused data supervision (we do not have very exploitable logs so fetching other app logs and yours through 1 splunk query is not possible), they can&amp;#39;t really help when you particularly want to check quickly the sanity around 1 business object (it requires devops, dev access, takes time). &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that building a business object timeseries (dynamodb or cassandra for instance) for storing latest footprints pushed at end of processes of said business objects (id 1, gone stream A ok at timestamp t1...), so I could easily fetch by API what happened lately on these elements accross multiple apps &amp;amp; datastores, would it be a good way or I am actually reinventing the wheel somewhere? The idea of having such timelines would also allow real time data health checks (for some random picked ones for instance) which I believe would be very valuable. Did anybody build something similar or could cover this data quality check?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14q82kg", "is_robot_indexable": true, "report_reasons": null, "author": "zenbeni", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q82kg/how_to_monitor_business_objects_through_eda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q82kg/how_to_monitor_business_objects_through_eda/", "subreddit_subscribers": 113970, "created_utc": 1688455444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are multiple data sources, from API to live stream data (e.g. mqtt).. I have to take this data, transform it and send it to another server microservice. \n\nI'm looking for a tool which is easy to use (also people without CS background are in my team), open source and easy to deploy in AWS (but I guess every tool is.. idk).. \n\nI would like to push for Kafka since it's smth I would learn to use. It doesn't seem really easy to deploy though, so I can imagine they could stop me on that. \n\nMy questions:\n\n- is kafka suited for such a use case? Or maybe overkill? \n\n- what other tools could I use? What about Airflow?", "author_fullname": "t2_aelhnnee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which ETL tool for this simple use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q75x4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688452484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are multiple data sources, from API to live stream data (e.g. mqtt).. I have to take this data, transform it and send it to another server microservice. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a tool which is easy to use (also people without CS background are in my team), open source and easy to deploy in AWS (but I guess every tool is.. idk).. &lt;/p&gt;\n\n&lt;p&gt;I would like to push for Kafka since it&amp;#39;s smth I would learn to use. It doesn&amp;#39;t seem really easy to deploy though, so I can imagine they could stop me on that. &lt;/p&gt;\n\n&lt;p&gt;My questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;is kafka suited for such a use case? Or maybe overkill? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;what other tools could I use? What about Airflow?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14q75x4", "is_robot_indexable": true, "report_reasons": null, "author": "Rogitus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q75x4/which_etl_tool_for_this_simple_use_case/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q75x4/which_etl_tool_for_this_simple_use_case/", "subreddit_subscribers": 113970, "created_utc": 1688452484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given a data set A for training an AI model, we would like to have a function/method that subsamples A and yields a subset B in a random manner, which will be used for mini-batch selection. Please write a function in pseudo-code which receives a set A and outputs a set B.\u201d  \nFor the completion of the task, please consider the following:  \n\n\n1. edge cases,\n2. how the function can be unit tested,\n3. responsibility of the function,\n\n&amp;#8203;\n\n    Ans:\n## I thought of one approach:\n    random_A = random.shuffle(A)\n    B = random_A[0:len(B)]    \n\nNeed your help to get an idea to different approach, unit testing this.", "author_fullname": "t2_3hgltcf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hello DE people Need your help to solve a simple problem as part of the assessment.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14q59ct", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688446459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given a data set A for training an AI model, we would like to have a function/method that subsamples A and yields a subset B in a random manner, which will be used for mini-batch selection. Please write a function in pseudo-code which receives a set A and outputs a set B.\u201d&lt;br/&gt;\nFor the completion of the task, please consider the following:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;edge cases,&lt;/li&gt;\n&lt;li&gt;how the function can be unit tested,&lt;/li&gt;\n&lt;li&gt;responsibility of the function,&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Ans:\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;I thought of one approach:&lt;/h2&gt;\n\n&lt;pre&gt;&lt;code&gt;random_A = random.shuffle(A)\nB = random_A[0:len(B)]    \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Need your help to get an idea to different approach, unit testing this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14q59ct", "is_robot_indexable": true, "report_reasons": null, "author": "Loser_Lanister", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14q59ct/hello_de_people_need_your_help_to_solve_a_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14q59ct/hello_de_people_need_your_help_to_solve_a_simple/", "subreddit_subscribers": 113970, "created_utc": 1688446459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I want to   \nautomate the refresh of data pulling from Azure AD  and build the pipeline that would visualize the analysis in Power bi   \n\n\nshould I use Synapse or is their a better solution?  \n ", "author_fullname": "t2_v2ah4w4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a Data Pipeline to Extract Data from Azure AD and Visualize it in Power BI.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pvhqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688419801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to&lt;br/&gt;\nautomate the refresh of data pulling from Azure AD  and build the pipeline that would visualize the analysis in Power bi   &lt;/p&gt;\n\n&lt;p&gt;should I use Synapse or is their a better solution?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pvhqy", "is_robot_indexable": true, "report_reasons": null, "author": "Expert__Bat", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pvhqy/creating_a_data_pipeline_to_extract_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pvhqy/creating_a_data_pipeline_to_extract_data_from/", "subreddit_subscribers": 113970, "created_utc": 1688419801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,  \n\n\nI have an interview with Specsavers for a Junior Data Engineer role, I'd like to know if anyone else here has gone through their process and can give some insight into what I should prep for.", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone interviewed at Specsavers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ptj64", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688415376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,  &lt;/p&gt;\n\n&lt;p&gt;I have an interview with Specsavers for a Junior Data Engineer role, I&amp;#39;d like to know if anyone else here has gone through their process and can give some insight into what I should prep for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14ptj64", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ptj64/has_anyone_interviewed_at_specsavers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ptj64/has_anyone_interviewed_at_specsavers/", "subreddit_subscribers": 113970, "created_utc": 1688415376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys I\u2019ve seen my graduates major in something and work in something else might even be the different industry , so if one majored in Data engineering, what can he transition in? Like can he later on be a cloud engineer? Or a blockchain engineer? Give me your thought and advices as I might major in DE , and thanks", "author_fullname": "t2_66m9v5u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering transitions.,", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pt0un", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688414189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys I\u2019ve seen my graduates major in something and work in something else might even be the different industry , so if one majored in Data engineering, what can he transition in? Like can he later on be a cloud engineer? Or a blockchain engineer? Give me your thought and advices as I might major in DE , and thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pt0un", "is_robot_indexable": true, "report_reasons": null, "author": "Carefull_eater", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pt0un/data_engineering_transitions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pt0un/data_engineering_transitions/", "subreddit_subscribers": 113970, "created_utc": 1688414189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Besides the obvious \"Data Engineers are Software Engineers\" and \"My Website uses a Database\".", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any real intersect between Data Engineering and Web Dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14po7bc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688402757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Besides the obvious &amp;quot;Data Engineers are Software Engineers&amp;quot; and &amp;quot;My Website uses a Database&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14po7bc", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14po7bc/any_real_intersect_between_data_engineering_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14po7bc/any_real_intersect_between_data_engineering_and/", "subreddit_subscribers": 113970, "created_utc": 1688402757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all!  \n\n\nSo, there's a requirement that requires me to group and filter data from multiple files stored in separate ADLS storage accounts.  \n\n\nThe way I have thought of it is to basically add the datasets in ADF studio and then create a data flow with filter/group by activity, and then add that data flow to a pipeline.\n\n  \nI was wondering if there was a better way of doing it?", "author_fullname": "t2_9d4i4lxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a better way for this pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pl3tw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688395654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!  &lt;/p&gt;\n\n&lt;p&gt;So, there&amp;#39;s a requirement that requires me to group and filter data from multiple files stored in separate ADLS storage accounts.  &lt;/p&gt;\n\n&lt;p&gt;The way I have thought of it is to basically add the datasets in ADF studio and then create a data flow with filter/group by activity, and then add that data flow to a pipeline.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there was a better way of doing it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14pl3tw", "is_robot_indexable": true, "report_reasons": null, "author": "New_Introduction_154", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pl3tw/is_there_a_better_way_for_this_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pl3tw/is_there_a_better_way_for_this_pipeline/", "subreddit_subscribers": 113970, "created_utc": 1688395654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are integrating data from multiple customers' databases into our DW, the data is staged in Snowflake. The source tables structure is same of all customers but coming from their own databases.\n\nNow how do you design dbt model for data vault load with 100s of source tables? Defining source of each table would be a very painful task just for 1 table, and we have to deal with 100s of tables.\n\nAny design tricks I can use to simplify dbt ?\n\n&amp;#x200B;", "author_fullname": "t2_9iyum30h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you design dbt for DV with 100s of sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14pkxnz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688395263.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are integrating data from multiple customers&amp;#39; databases into our DW, the data is staged in Snowflake. The source tables structure is same of all customers but coming from their own databases.&lt;/p&gt;\n\n&lt;p&gt;Now how do you design dbt model for data vault load with 100s of source tables? Defining source of each table would be a very painful task just for 1 table, and we have to deal with 100s of tables.&lt;/p&gt;\n\n&lt;p&gt;Any design tricks I can use to simplify dbt ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14pkxnz", "is_robot_indexable": true, "report_reasons": null, "author": "PrtScr1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14pkxnz/how_do_you_design_dbt_for_dv_with_100s_of_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14pkxnz/how_do_you_design_dbt_for_dv_with_100s_of_sources/", "subreddit_subscribers": 113970, "created_utc": 1688395263.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}