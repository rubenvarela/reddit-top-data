{"kind": "Listing", "data": {"after": "t3_150zkpk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Something like this could probably shift the paradigm of how we store files and create directories. Would you spend your free time organizing your files if you know that a simple search could get you exactly to what you need in a few seconds?\n\nIt's very similar to Spotify search, where you can type in part of the lyrics of a song and it will find you the song. Or in MacOS, you can search by text inside photos and it will grab all the images that contain that text (no more spending my weekends on meticulously tagging and organizing my screenshots folder).\n\nSo is there anything like that already out there that would achieve this especially on Windows?\n\n*Edit*: a local search engine ideally powered by LLMs which would search your entire local hard drive and find matches. LLM-powered means instead of searching using a very specific string of words, you'd be able to \"describe\" what you're looking for in your hard drive without mentioning the file name, tags, Metadata. \n\n```-- \"Find me that video where I was walking next to a river with ice cream in my hand\"```\n\nor \n\n```-- Can you look into my Obsidian notes and summarize all my notes that relate to concept XYZ. Also see if you can find any screenshots that I might have saved that might relate to this topic or check my offline YouTube video archive to see if you can find any snippets or talks that relate to this \"```", "author_fullname": "t2_3bnahjfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With all the AI hype and GPT stuff, are there already any tools/software that make searching and sorting files smarter and faster? Such as the MacOS search inside images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1511tpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": "", "subreddit_type": "public", "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689509621.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689499743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something like this could probably shift the paradigm of how we store files and create directories. Would you spend your free time organizing your files if you know that a simple search could get you exactly to what you need in a few seconds?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very similar to Spotify search, where you can type in part of the lyrics of a song and it will find you the song. Or in MacOS, you can search by text inside photos and it will grab all the images that contain that text (no more spending my weekends on meticulously tagging and organizing my screenshots folder).&lt;/p&gt;\n\n&lt;p&gt;So is there anything like that already out there that would achieve this especially on Windows?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Edit&lt;/em&gt;: a local search engine ideally powered by LLMs which would search your entire local hard drive and find matches. LLM-powered means instead of searching using a very specific string of words, you&amp;#39;d be able to &amp;quot;describe&amp;quot; what you&amp;#39;re looking for in your hard drive without mentioning the file name, tags, Metadata. &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-- &amp;quot;Find me that video where I was walking next to a river with ice cream in my hand&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;or &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-- Can you look into my Obsidian notes and summarize all my notes that relate to concept XYZ. Also see if you can find any screenshots that I might have saved that might relate to this topic or check my offline YouTube video archive to see if you can find any snippets or talks that relate to this &amp;quot;&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1511tpx", "is_robot_indexable": true, "report_reasons": null, "author": "egobamyasi", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1511tpx/with_all_the_ai_hype_and_gpt_stuff_are_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1511tpx/with_all_the_ai_hype_and_gpt_stuff_are_there/", "subreddit_subscribers": 692897, "created_utc": 1689499743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was reading an interesting study about 1.4 MILLION enterprise SSDs, the data was collected via their enterprise monitoring software for 2.5 years (via weekly telemetry data), and was published in 2020.\n\n* Study: [https://www.usenix.org/system/files/fast20-maneas.pdf](https://www.usenix.org/system/files/fast20-maneas.pdf)\n* Official Article (with video): [https://www.usenix.org/conference/fast20/presentation/maneas](https://www.usenix.org/conference/fast20/presentation/maneas)\n* Another Article: [https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/](https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/)\n\nThe study found interesting things, such as: Always update your firmware, because you will massively reduce the amount of data loss over time if you have the latest firmware.\n\n&amp;#x200B;\n\n**But the most interesting column is on page 8 of the PDF, middle graph \"(b) 3D-TLC drives\".**\n\nGroup \"A\" of that graph refers to all kinds of \"total failures\" of the drive (explained on page 5). There are two types of \"total failures\": Either that the drive reported actual on-board DRAM errors (ECC errors in the RAM itself) meaning the hardware was broken and unable to read data properly anymore, OR that the drive became completely non-responsive and vanished from the system. The most serious failures.\n\nSo, on page 8's graphs, they show that 800GB-3800GB 3D-TLC SSDs had a very low \"total drive failure\" rate. But as soon as you got to 8000GB and 15000GB, the drives had a MASSIVE increase in risk that the entire drive has hardware errors and dies, becomes non-responsive, etc.\n\n&amp;#x200B;\n\nHere are some relevant quotes about how larger capacities have lower reliability:\n\n&amp;#x200B;\n\n* \"We also looked for differences in the reasons for replacement between smaller and larger capacity drives and made an interesting observation: for the largest capacity drives, the rate of predictable failures (S.M.A.R.T. data) is lower than for smaller capacity drives. In contrast, the most severe failure reason, i.e., an unresponsive drive, occurs at a much higher rate for the larger capacity drives than for the smaller capacity drives.\"\n* \"Among the eMLC drives, the 3800GB and 3840GB capacities, and among the 3D-TLC drives, the 8TB and 15TB capacities, have very high rates of replacement due to an unresponsive drive, compared to smaller capacities. They also have a lower rate of replacements due to predictive failures. This means that the replacement rate associated with high capacity drives is not only bigger, but also has potentially more severe consequences.\"\n* \"It may be possible that the severe failures and unpredictability of such failures is an artifact of the larger DRAM footprint associated with large flash capacity, rather than the flash capacity itself. Potential for such impact could be mitigated by upcoming architectures such as Zoned Storage (ZNS) \\[4, 30\\] that obviate the need for large Flash Translation Layer (FTL) tables in DRAM and consequently reducing the DRAM footprint.\"\n* Summary: \"Drives with very large capacities not only see a higher replacement rate overall, but also see more severe failures and fewer of the (more benign) predictive failures.\"\n\n&amp;#x200B;\n\nI have transcribed the MEDIAN values of the 3D-TLC \"total drive failure\" rates (as a percentage of all drives in operation) as closely as I could from the graphs:\n\n&amp;#x200B;\n\n* Annual failure rates at category \"A\" (highest severity), as a percentage of all SSDs of that particular size and NAND type (3D-TLC):\n* 800 GB: 0.01875% (0.1875 drives per 1000 die every year)\n* 960 GB: 0.025% (0.25 drives per 1000 die every year)\n* 1600 GB: 0.0055% (0.055 drives per 1000 die every year)\n* 3800 GB: 0.02625% (0.2625 drives per 1000 die every year)\n* 8000 GB: 0.2275% (2.275 drives per 1000 die every year)\n* 15000 GB: 0.1625% (1.625 drives per 1000 die every year)\n* In summary: 8 TB+ SSDs have roughly 10x more \"total drive death\" events than smaller SSDs. And they have less S.M.A.R.T. warning signals before death, meaning the deaths are less predictable.\n\n&amp;#x200B;\n\nAs a bonus fact, they also studied the failure rate related to age:\n\n* \"We observe an unexpectedly long period of infant mortality with a shape that differs from the common \u201cbathtub\u201d model often used in reliability theory. The bathtub model assumes a short initial period of high failure rates, which then quickly drops.\"\n* \"Instead, we observe for both 3D-TLC and eMLC drives, a long period (12\u201315 months) of increasing failure rates, followed by a lengthy period (another 6\u201312 months) of slowly decreasing failure rates, before rates finally stabilize. That means that, given typical drive lifetimes of 5 years, drives spend 20-40% of their life in infant mortality.\"\n* In short, the \"infant mortality\" rate of SSDs is roughly 27 months (slightly over 2 years), before you can say \"this drive is safe, it won't suddenly die due to hardware failure\".\n* After those 27 months, they say that almost all failures are due to predictable errors that show up in S.M.A.R.T. data, such as NAND degradation/read errors.\n* Although keep in mind that the study didn't look at super long term data (5+ years) since this is enterprise, where drives are always replaced within a few years of deployment. So it is likely that a bunch of \"old age\" deaths would creep up at consumer usage where SSDs are kept for 5+ years.\n\n&amp;#x200B;\n\nSo... I am already torn between buying a 4 TB vs 8 TB Samsung PM9A3 enterprise drive (U.2 connector 2.5\"), and this study make me even more confused...\n\n[https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/](https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/)\n\n&amp;#x200B;\n\nI basically have fear of missing out. The 4 TB drive \"might not be big enough and then I have to buy yet another drive\", and the 8 TB drive is only 65% more expensive for double the capacity. It's a really good deal right now.\n\nOn the other hand, 8 TB is a ton of data to lose if the entire drive just completely dies (I only have redundant backups for the most important data), and if 8 TB+ drives still have vastly more catastrophic hardware failures, well, then that's bad...\n\nThoughts?", "author_fullname": "t2_dmkidisb", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "Enterprise SSD reliability: Does size still matter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150orlb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1689515674.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689459432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was reading an interesting study about 1.4 MILLION enterprise SSDs, the data was collected via their enterprise monitoring software for 2.5 years (via weekly telemetry data), and was published in 2020.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Study: &lt;a href=\"https://www.usenix.org/system/files/fast20-maneas.pdf\"&gt;https://www.usenix.org/system/files/fast20-maneas.pdf&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Official Article (with video): &lt;a href=\"https://www.usenix.org/conference/fast20/presentation/maneas\"&gt;https://www.usenix.org/conference/fast20/presentation/maneas&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Another Article: &lt;a href=\"https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/\"&gt;https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The study found interesting things, such as: Always update your firmware, because you will massively reduce the amount of data loss over time if you have the latest firmware.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;But the most interesting column is on page 8 of the PDF, middle graph &amp;quot;(b) 3D-TLC drives&amp;quot;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Group &amp;quot;A&amp;quot; of that graph refers to all kinds of &amp;quot;total failures&amp;quot; of the drive (explained on page 5). There are two types of &amp;quot;total failures&amp;quot;: Either that the drive reported actual on-board DRAM errors (ECC errors in the RAM itself) meaning the hardware was broken and unable to read data properly anymore, OR that the drive became completely non-responsive and vanished from the system. The most serious failures.&lt;/p&gt;\n\n&lt;p&gt;So, on page 8&amp;#39;s graphs, they show that 800GB-3800GB 3D-TLC SSDs had a very low &amp;quot;total drive failure&amp;quot; rate. But as soon as you got to 8000GB and 15000GB, the drives had a MASSIVE increase in risk that the entire drive has hardware errors and dies, becomes non-responsive, etc.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here are some relevant quotes about how larger capacities have lower reliability:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;We also looked for differences in the reasons for replacement between smaller and larger capacity drives and made an interesting observation: for the largest capacity drives, the rate of predictable failures (S.M.A.R.T. data) is lower than for smaller capacity drives. In contrast, the most severe failure reason, i.e., an unresponsive drive, occurs at a much higher rate for the larger capacity drives than for the smaller capacity drives.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Among the eMLC drives, the 3800GB and 3840GB capacities, and among the 3D-TLC drives, the 8TB and 15TB capacities, have very high rates of replacement due to an unresponsive drive, compared to smaller capacities. They also have a lower rate of replacements due to predictive failures. This means that the replacement rate associated with high capacity drives is not only bigger, but also has potentially more severe consequences.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;It may be possible that the severe failures and unpredictability of such failures is an artifact of the larger DRAM footprint associated with large flash capacity, rather than the flash capacity itself. Potential for such impact could be mitigated by upcoming architectures such as Zoned Storage (ZNS) [4, 30] that obviate the need for large Flash Translation Layer (FTL) tables in DRAM and consequently reducing the DRAM footprint.&amp;quot;&lt;/li&gt;\n&lt;li&gt;Summary: &amp;quot;Drives with very large capacities not only see a higher replacement rate overall, but also see more severe failures and fewer of the (more benign) predictive failures.&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have transcribed the MEDIAN values of the 3D-TLC &amp;quot;total drive failure&amp;quot; rates (as a percentage of all drives in operation) as closely as I could from the graphs:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Annual failure rates at category &amp;quot;A&amp;quot; (highest severity), as a percentage of all SSDs of that particular size and NAND type (3D-TLC):&lt;/li&gt;\n&lt;li&gt;800 GB: 0.01875% (0.1875 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;960 GB: 0.025% (0.25 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;1600 GB: 0.0055% (0.055 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;3800 GB: 0.02625% (0.2625 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;8000 GB: 0.2275% (2.275 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;15000 GB: 0.1625% (1.625 drives per 1000 die every year)&lt;/li&gt;\n&lt;li&gt;In summary: 8 TB+ SSDs have roughly 10x more &amp;quot;total drive death&amp;quot; events than smaller SSDs. And they have less S.M.A.R.T. warning signals before death, meaning the deaths are less predictable.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As a bonus fact, they also studied the failure rate related to age:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;We observe an unexpectedly long period of infant mortality with a shape that differs from the common \u201cbathtub\u201d model often used in reliability theory. The bathtub model assumes a short initial period of high failure rates, which then quickly drops.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Instead, we observe for both 3D-TLC and eMLC drives, a long period (12\u201315 months) of increasing failure rates, followed by a lengthy period (another 6\u201312 months) of slowly decreasing failure rates, before rates finally stabilize. That means that, given typical drive lifetimes of 5 years, drives spend 20-40% of their life in infant mortality.&amp;quot;&lt;/li&gt;\n&lt;li&gt;In short, the &amp;quot;infant mortality&amp;quot; rate of SSDs is roughly 27 months (slightly over 2 years), before you can say &amp;quot;this drive is safe, it won&amp;#39;t suddenly die due to hardware failure&amp;quot;.&lt;/li&gt;\n&lt;li&gt;After those 27 months, they say that almost all failures are due to predictable errors that show up in S.M.A.R.T. data, such as NAND degradation/read errors.&lt;/li&gt;\n&lt;li&gt;Although keep in mind that the study didn&amp;#39;t look at super long term data (5+ years) since this is enterprise, where drives are always replaced within a few years of deployment. So it is likely that a bunch of &amp;quot;old age&amp;quot; deaths would creep up at consumer usage where SSDs are kept for 5+ years.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So... I am already torn between buying a 4 TB vs 8 TB Samsung PM9A3 enterprise drive (U.2 connector 2.5&amp;quot;), and this study make me even more confused...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/\"&gt;https://www.zdnet.com/article/ssd-reliability-in-the-enterprise/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I basically have fear of missing out. The 4 TB drive &amp;quot;might not be big enough and then I have to buy yet another drive&amp;quot;, and the 8 TB drive is only 65% more expensive for double the capacity. It&amp;#39;s a really good deal right now.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, 8 TB is a ton of data to lose if the entire drive just completely dies (I only have redundant backups for the most important data), and if 8 TB+ drives still have vastly more catastrophic hardware failures, well, then that&amp;#39;s bad...&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "150orlb", "is_robot_indexable": true, "report_reasons": null, "author": "GoastRiter", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150orlb/enterprise_ssd_reliability_does_size_still_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150orlb/enterprise_ssd_reliability_does_size_still_matter/", "subreddit_subscribers": 692897, "created_utc": 1689459432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seems it would be wise to hoard programs/files that will be useful in offline scenarios. Something that converts imperial to metric for example, or common tables and charts for things like electrical and plumbing come to mind. Wikipedia mirror makes good sense. I've grabbed mauals for every tool and appliance I own. \n\nHayes repair manuals for cars take some digging but are available, the question is whether to grab only the cars you have, or all that are available? I've only bothered with certain brands but I can see an argument for downloading everything. \n\nPersonally I found some of the compilations like \"the ark\" to be full of junk. Open to suggestions for anything I mentioned above and more", "author_fullname": "t2_ay7tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Programs to hoard for offline computers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151btrm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689527960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems it would be wise to hoard programs/files that will be useful in offline scenarios. Something that converts imperial to metric for example, or common tables and charts for things like electrical and plumbing come to mind. Wikipedia mirror makes good sense. I&amp;#39;ve grabbed mauals for every tool and appliance I own. &lt;/p&gt;\n\n&lt;p&gt;Hayes repair manuals for cars take some digging but are available, the question is whether to grab only the cars you have, or all that are available? I&amp;#39;ve only bothered with certain brands but I can see an argument for downloading everything. &lt;/p&gt;\n\n&lt;p&gt;Personally I found some of the compilations like &amp;quot;the ark&amp;quot; to be full of junk. Open to suggestions for anything I mentioned above and more&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "124TB ZFS and Synology + Cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "151btrm", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/151btrm/programs_to_hoard_for_offline_computers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151btrm/programs_to_hoard_for_offline_computers/", "subreddit_subscribers": 692897, "created_utc": 1689527960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for some advice on how to revamp my backups. I've already went down several extremely weird paths but I think it's time to give up and get some help.\n\nMy machines look like this:\n1. TrueNAS (Source) : ~48TB\n2. Windows (\"GUI\" + Local Backup) : 3TB\n3. Orange Pi 5+ (Offsite Backup) : 1TB\n4. Raspberry Pi 4B (Offsite Backup) : 1TB\n5. Mango Pi Pro (Usage TBD) : ???\n6. Storj (Cloud Backup) : 150GB x2\n7. Oracle Cloud (Cloud Backup) : 150GB x2\n\nI'm looking to do partial backups since I can live without most of the data, but I really want to keep about 1TB of it safe from multiple copies going down simultaneously. I had originally planned on doing a PULL configuration from each target machine with read only access, so if TrueNAS gets compromised it doesn't matter because it barely knows the target machines exist. If one of the target machines gets compromised, it only has read access so it doesn't matter much either. But then maintenance and reporting gets a bit squirrely so that's not great either.... But I think a PUSH configuration on TrueNAS with a locked down user should achieve mostly the same effect. \n\nThe target machines (2?, 3, 4, 7) can run ZFS so in the event TrueNAS becomes compromised it can overwrite the backup folder, but that'll just fill up the drive while the snapshots hosted on the target machines are still intact. If the target machines become compromised then they don't have access to TrueNAS at all. I can't really do snapshots on Storj, but they're freebie accounts so I wasn't going to rely on them anyway.\n\nThis seems like a decent compromise between security and ease of use. I can monitor the status of my backups from a central location (TrueNAS's UI) without SSH'ing into half a dozen machines or figuring out grafana / prometheus. Also if the backup parameters change (what goes where, ignore files X, Y, Z, etc) then I can do that from one location as well. Also I can save TrueNAS's single configuration easily without worrying about which individual machines have which version of a backup script.\n\nI'm trying to avoid dedicated backup programs because I don't trust them or myself to set them up. I've been using Kopia which is easily accessible and intuitive, but I've also had it silently lose a chunk randomly corrupting the repository with literally no way of knowing from the GUI. There are a bunch of other programs out there like Dupliacy, Duplicati, restic, bacula but they all have their downsides. Some of them don't have a GUI for windows or are WAY too complicated which is a nogo for an idiot user like me. Or they just don't have some fairly basic features like backing up from a network location (Veaam windows...) But you know what hasn't failed me? ZFS. But since I don't want to replicate entire datasets everywhere and not every location is capable of running ZFS, that rules out ZFS send / recv. The rsync task probably works but I know for sure cloud sync works, so that is probably what I'll go with.\n\nSo to recap;\n1. TrueNAS -&gt; Cloud Sync Task -&gt; Bunch of storage -&gt; ZFS snapshot\n2. The accounts TrueNAS has access to will have no privleges outside of the backup folder\n\nDoes this make sense? Am I missing something? Thanks in advance!", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to redo some backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1515ivg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689511908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for some advice on how to revamp my backups. I&amp;#39;ve already went down several extremely weird paths but I think it&amp;#39;s time to give up and get some help.&lt;/p&gt;\n\n&lt;p&gt;My machines look like this:\n1. TrueNAS (Source) : ~48TB\n2. Windows (&amp;quot;GUI&amp;quot; + Local Backup) : 3TB\n3. Orange Pi 5+ (Offsite Backup) : 1TB\n4. Raspberry Pi 4B (Offsite Backup) : 1TB\n5. Mango Pi Pro (Usage TBD) : ???\n6. Storj (Cloud Backup) : 150GB x2\n7. Oracle Cloud (Cloud Backup) : 150GB x2&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to do partial backups since I can live without most of the data, but I really want to keep about 1TB of it safe from multiple copies going down simultaneously. I had originally planned on doing a PULL configuration from each target machine with read only access, so if TrueNAS gets compromised it doesn&amp;#39;t matter because it barely knows the target machines exist. If one of the target machines gets compromised, it only has read access so it doesn&amp;#39;t matter much either. But then maintenance and reporting gets a bit squirrely so that&amp;#39;s not great either.... But I think a PUSH configuration on TrueNAS with a locked down user should achieve mostly the same effect. &lt;/p&gt;\n\n&lt;p&gt;The target machines (2?, 3, 4, 7) can run ZFS so in the event TrueNAS becomes compromised it can overwrite the backup folder, but that&amp;#39;ll just fill up the drive while the snapshots hosted on the target machines are still intact. If the target machines become compromised then they don&amp;#39;t have access to TrueNAS at all. I can&amp;#39;t really do snapshots on Storj, but they&amp;#39;re freebie accounts so I wasn&amp;#39;t going to rely on them anyway.&lt;/p&gt;\n\n&lt;p&gt;This seems like a decent compromise between security and ease of use. I can monitor the status of my backups from a central location (TrueNAS&amp;#39;s UI) without SSH&amp;#39;ing into half a dozen machines or figuring out grafana / prometheus. Also if the backup parameters change (what goes where, ignore files X, Y, Z, etc) then I can do that from one location as well. Also I can save TrueNAS&amp;#39;s single configuration easily without worrying about which individual machines have which version of a backup script.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to avoid dedicated backup programs because I don&amp;#39;t trust them or myself to set them up. I&amp;#39;ve been using Kopia which is easily accessible and intuitive, but I&amp;#39;ve also had it silently lose a chunk randomly corrupting the repository with literally no way of knowing from the GUI. There are a bunch of other programs out there like Dupliacy, Duplicati, restic, bacula but they all have their downsides. Some of them don&amp;#39;t have a GUI for windows or are WAY too complicated which is a nogo for an idiot user like me. Or they just don&amp;#39;t have some fairly basic features like backing up from a network location (Veaam windows...) But you know what hasn&amp;#39;t failed me? ZFS. But since I don&amp;#39;t want to replicate entire datasets everywhere and not every location is capable of running ZFS, that rules out ZFS send / recv. The rsync task probably works but I know for sure cloud sync works, so that is probably what I&amp;#39;ll go with.&lt;/p&gt;\n\n&lt;p&gt;So to recap;\n1. TrueNAS -&amp;gt; Cloud Sync Task -&amp;gt; Bunch of storage -&amp;gt; ZFS snapshot\n2. The accounts TrueNAS has access to will have no privleges outside of the backup folder&lt;/p&gt;\n\n&lt;p&gt;Does this make sense? Am I missing something? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1515ivg", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1515ivg/looking_to_redo_some_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1515ivg/looking_to_redo_some_backups/", "subreddit_subscribers": 692897, "created_utc": 1689511908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 12TB (ST12000NM001G) and 2TB (ST2000LX001) HDD in my PC. The 12TB I bought new last October, while the 2TB I bought new around 4 years ago. The 2TB drive was first used for around 3 years in my PS4 for gaming.\r\n\r\nThis 1st July, CrystalDiskInfo is reporting Caution for the 12TB drive. This is because of 100 reallocated sectors. When I searched online about this, I found that this is something that happens, and it is okay to keep using the drive unless the number keeps going up. However, some expressed that anything other than 0 is a risk and the drive must be replaced.\r\n\r\nThe drive is fairly new and is not used constantly. When investigating further I found the following:\r\n\r\nThe 12TB drive has 100 current, 100 worst, 10 threshold, and 000000000038 raw value in CrystalDiskInfo for Reallocated Sectores Count. It is marked as yellow. However, the 2TB drive has 100 current, 100 worst, 36 threshold, and 000000000000 raw value in CrystalDiskInfo for Reallocated Sectores Count and marked as blue. How is the current and worst value the same with a different and all-zero raw value? How is it also not yellow if the other drive has the same current value and yellow?\r\n\r\nThings get even weirder when I check the same values with Samsung Magician. The ECC Error Rate is critical for the 12TB drive in Samsung magician but good in CrystalDiskInfo. The values are 12/2/0 for Current/Worst/Threshold and EE034D raw.\r\n\r\nCan someone help me understand why the CrystalDiskInfo is reporting Good and Caution for the same value and why Samsung Magician has a different opinion to the values?\r\n\r\nThanks", "author_fullname": "t2_2dhcdz13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interpreting SMART Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15163g5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689513493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 12TB (ST12000NM001G) and 2TB (ST2000LX001) HDD in my PC. The 12TB I bought new last October, while the 2TB I bought new around 4 years ago. The 2TB drive was first used for around 3 years in my PS4 for gaming.&lt;/p&gt;\n\n&lt;p&gt;This 1st July, CrystalDiskInfo is reporting Caution for the 12TB drive. This is because of 100 reallocated sectors. When I searched online about this, I found that this is something that happens, and it is okay to keep using the drive unless the number keeps going up. However, some expressed that anything other than 0 is a risk and the drive must be replaced.&lt;/p&gt;\n\n&lt;p&gt;The drive is fairly new and is not used constantly. When investigating further I found the following:&lt;/p&gt;\n\n&lt;p&gt;The 12TB drive has 100 current, 100 worst, 10 threshold, and 000000000038 raw value in CrystalDiskInfo for Reallocated Sectores Count. It is marked as yellow. However, the 2TB drive has 100 current, 100 worst, 36 threshold, and 000000000000 raw value in CrystalDiskInfo for Reallocated Sectores Count and marked as blue. How is the current and worst value the same with a different and all-zero raw value? How is it also not yellow if the other drive has the same current value and yellow?&lt;/p&gt;\n\n&lt;p&gt;Things get even weirder when I check the same values with Samsung Magician. The ECC Error Rate is critical for the 12TB drive in Samsung magician but good in CrystalDiskInfo. The values are 12/2/0 for Current/Worst/Threshold and EE034D raw.&lt;/p&gt;\n\n&lt;p&gt;Can someone help me understand why the CrystalDiskInfo is reporting Good and Caution for the same value and why Samsung Magician has a different opinion to the values?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15163g5", "is_robot_indexable": true, "report_reasons": null, "author": "ISuckAtNames387", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15163g5/interpreting_smart_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15163g5/interpreting_smart_data/", "subreddit_subscribers": 692897, "created_utc": 1689513493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have too much digital stuff. I am deleting everything I don\u2019t need. I\u2019ve about 10 years worth of stuff to go through, and it will take a long time. My fear of deleting anything first happened when my first computer got a virus and I lost everything + losing stuff due to files becoming corrupt. Even though my \u2018hoarding\u2019 is pretty mild, I don\u2019t like having to keep stuff \u2018just in case.\u2019 Especially if I genuinely don\u2019t need it anymore. \n\nMy digital clutter clutters my mind. If I see something and either don\u2019t need it or don\u2019t see myself reopening the file, I am getting rid of it. It makes me feel uncomfortable just deleting shit, but I have to delete and forget about it. I signed up to lots of accounts and emails as well, which are all in the process of getting deleted.\n\nYou guys might laugh at this but I have only around 2-5 TB worth of stuff, which might seem like nothing around here, but I find it overwhelming and extremely burdensome. Respect to all of you who are able to manage all that, jesus.\n\nEdit: Thanks guys, I didn\u2019t even know my post was approved because it got rejected at first due to my account being new. I really appreciate your comments and advice. It\u2019s great to relate to you all.", "author_fullname": "t2_d0lx00uz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am tired", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150x71e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689521066.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689483892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have too much digital stuff. I am deleting everything I don\u2019t need. I\u2019ve about 10 years worth of stuff to go through, and it will take a long time. My fear of deleting anything first happened when my first computer got a virus and I lost everything + losing stuff due to files becoming corrupt. Even though my \u2018hoarding\u2019 is pretty mild, I don\u2019t like having to keep stuff \u2018just in case.\u2019 Especially if I genuinely don\u2019t need it anymore. &lt;/p&gt;\n\n&lt;p&gt;My digital clutter clutters my mind. If I see something and either don\u2019t need it or don\u2019t see myself reopening the file, I am getting rid of it. It makes me feel uncomfortable just deleting shit, but I have to delete and forget about it. I signed up to lots of accounts and emails as well, which are all in the process of getting deleted.&lt;/p&gt;\n\n&lt;p&gt;You guys might laugh at this but I have only around 2-5 TB worth of stuff, which might seem like nothing around here, but I find it overwhelming and extremely burdensome. Respect to all of you who are able to manage all that, jesus.&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks guys, I didn\u2019t even know my post was approved because it got rejected at first due to my account being new. I really appreciate your comments and advice. It\u2019s great to relate to you all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "150x71e", "is_robot_indexable": true, "report_reasons": null, "author": "OkPoet6127", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150x71e/i_am_tired/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150x71e/i_am_tired/", "subreddit_subscribers": 692897, "created_utc": 1689483892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone! I've recently been looking all over the internet for an xml dump of fandom (formerly known as wikia) wikis from the years 2016 to 2018. I've seen dumps from 2015 and 2020, but the 2018 dump doesn't seem to have the specific wiki I'm looking for. Where would I look for more information?\n\n&amp;#x200B;", "author_fullname": "t2_21qnqkln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Find Website Dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150qz0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689465126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I&amp;#39;ve recently been looking all over the internet for an xml dump of fandom (formerly known as wikia) wikis from the years 2016 to 2018. I&amp;#39;ve seen dumps from 2015 and 2020, but the 2018 dump doesn&amp;#39;t seem to have the specific wiki I&amp;#39;m looking for. Where would I look for more information?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150qz0n", "is_robot_indexable": true, "report_reasons": null, "author": "TemporarilyResolute", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150qz0n/trying_to_find_website_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150qz0n/trying_to_find_website_dump/", "subreddit_subscribers": 692897, "created_utc": 1689465126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently got a Terramaster D4-300 4 bay DAS and some WD red 16TB drives for it. The noise from the HDDs (read/write noises and what I assume is preventive wear leveling based on some google searches) is very loud. I have also had a WD elements 8TB external drive for a few years and I keep it in the same location. You can hear that drive if you get close to it but the new 16 TB drives are substantially louder.\n\nSo I\u2019m wondering, is it normal that they would be so much louder than the 8TB? Or would the issue be more with the DAS? The external drive has plastic housing while this DAS is metal. I\u2019m also wondering if it\u2019s possible I didn\u2019t install them well, or should use screws instead of the plastic piece it comes with to snap them in. \n\nIf there isn\u2019t a good way to reduce the noise I\u2019m considering putting the DAS elsewhere and getting a long USB-C cable. But they don\u2019t seem to make data transfer cables more than a few feet long, so I\u2019m not sure if this is a great idea. \n\nAny advice on this would be appreciated.", "author_fullname": "t2_8fbeb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excessive noise with new DAS setup.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151d29d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689530881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got a Terramaster D4-300 4 bay DAS and some WD red 16TB drives for it. The noise from the HDDs (read/write noises and what I assume is preventive wear leveling based on some google searches) is very loud. I have also had a WD elements 8TB external drive for a few years and I keep it in the same location. You can hear that drive if you get close to it but the new 16 TB drives are substantially louder.&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m wondering, is it normal that they would be so much louder than the 8TB? Or would the issue be more with the DAS? The external drive has plastic housing while this DAS is metal. I\u2019m also wondering if it\u2019s possible I didn\u2019t install them well, or should use screws instead of the plastic piece it comes with to snap them in. &lt;/p&gt;\n\n&lt;p&gt;If there isn\u2019t a good way to reduce the noise I\u2019m considering putting the DAS elsewhere and getting a long USB-C cable. But they don\u2019t seem to make data transfer cables more than a few feet long, so I\u2019m not sure if this is a great idea. &lt;/p&gt;\n\n&lt;p&gt;Any advice on this would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151d29d", "is_robot_indexable": true, "report_reasons": null, "author": "Zander327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151d29d/excessive_noise_with_new_das_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151d29d/excessive_noise_with_new_das_setup/", "subreddit_subscribers": 692897, "created_utc": 1689530881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a number of laptop backups, very old (oldest ~8-9 yrs back) laptop backups, which were taken at random dates and a lot of the backup just repeats itself (~195 GB total). Basically a copy paste of most of the folders from the laptop to a harddrive at different points of time.\n\nI was going through the backups, and found a lot of photos, which I have never seen. I would love to find all such \"lost\" photos in these backups. And a way to de-dup the duplicate ones (there will be duplicates)\n\nThe other documents in these backups are pretty much useless for me, apart from the photos. Is there a quick way to extract photos from these huge backups?\n\nI tried searching for a software which could do this, but was unable to find one.\n\nI have a feeling that this might be a common use-case and that there must be a solution outside, just that I don't have the proper key-word to search with.\n\nAny help is appreciated!\n\n\n\nPS:\n- I also tried writing a small script which could extract photos (recursively search all files, check if image, move) but that gives me very very huge list of images (I think they are internal laptop/windows images, and are of no use to me). \n- Is there a good way of doing this? I could also filter on the image size, but these backups also contain a lot of small images aswell - whatsapp images, screenshots, etc. which are small in size too (so this only won't work).\n- One more disadvantage is - the script is very slow (or atleast I think that there must be better optimized way of doing this).\n- I would still not know if what I am writing will cover all the photos in the disk. Basically will have to spend a lot of time testing, running, etc.", "author_fullname": "t2_blvc1ovuy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting all photos from a laptop backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15108wx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689496045.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689494176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a number of laptop backups, very old (oldest ~8-9 yrs back) laptop backups, which were taken at random dates and a lot of the backup just repeats itself (~195 GB total). Basically a copy paste of most of the folders from the laptop to a harddrive at different points of time.&lt;/p&gt;\n\n&lt;p&gt;I was going through the backups, and found a lot of photos, which I have never seen. I would love to find all such &amp;quot;lost&amp;quot; photos in these backups. And a way to de-dup the duplicate ones (there will be duplicates)&lt;/p&gt;\n\n&lt;p&gt;The other documents in these backups are pretty much useless for me, apart from the photos. Is there a quick way to extract photos from these huge backups?&lt;/p&gt;\n\n&lt;p&gt;I tried searching for a software which could do this, but was unable to find one.&lt;/p&gt;\n\n&lt;p&gt;I have a feeling that this might be a common use-case and that there must be a solution outside, just that I don&amp;#39;t have the proper key-word to search with.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated!&lt;/p&gt;\n\n&lt;p&gt;PS:\n- I also tried writing a small script which could extract photos (recursively search all files, check if image, move) but that gives me very very huge list of images (I think they are internal laptop/windows images, and are of no use to me). \n- Is there a good way of doing this? I could also filter on the image size, but these backups also contain a lot of small images aswell - whatsapp images, screenshots, etc. which are small in size too (so this only won&amp;#39;t work).\n- One more disadvantage is - the script is very slow (or atleast I think that there must be better optimized way of doing this).\n- I would still not know if what I am writing will cover all the photos in the disk. Basically will have to spend a lot of time testing, running, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15108wx", "is_robot_indexable": true, "report_reasons": null, "author": "MeasurementSad6631", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15108wx/extracting_all_photos_from_a_laptop_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15108wx/extracting_all_photos_from_a_laptop_backup/", "subreddit_subscribers": 692897, "created_utc": 1689494176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently acquired a couple of 12-bay synology NASes loaded with drives, after a family friend passed away. \n\nUnsure if the contents are worth saving, but it appears the first enclosure I started messing with, a DS2415, is suffering from the \u201catom power problem\u201d, with blinking power and alert lights upon trying to startup.  I\u2019ve removed and numbered the drives for now. \n\nIs there any other way for me to view these drives, outside of getting them into a new 12-bay enclosure? Will that even work? I can\u2019t move them into the other enclosure I got from them because it is an expansion unit.", "author_fullname": "t2_ablys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reconstructing/salvaging a 12-drive RAID without the 12-bay enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150x4zh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689483717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently acquired a couple of 12-bay synology NASes loaded with drives, after a family friend passed away. &lt;/p&gt;\n\n&lt;p&gt;Unsure if the contents are worth saving, but it appears the first enclosure I started messing with, a DS2415, is suffering from the \u201catom power problem\u201d, with blinking power and alert lights upon trying to startup.  I\u2019ve removed and numbered the drives for now. &lt;/p&gt;\n\n&lt;p&gt;Is there any other way for me to view these drives, outside of getting them into a new 12-bay enclosure? Will that even work? I can\u2019t move them into the other enclosure I got from them because it is an expansion unit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150x4zh", "is_robot_indexable": true, "report_reasons": null, "author": "JayVeeBee", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150x4zh/reconstructingsalvaging_a_12drive_raid_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150x4zh/reconstructingsalvaging_a_12drive_raid_without/", "subreddit_subscribers": 692897, "created_utc": 1689483717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pictures and videos of pets of mine who have since passed away and I really don't want the timestamp on each file to be reset when copying the files over; this is especially a problem when transferring from my phone. Is there any way to make the necessary copies while keeping the original time and date intact?", "author_fullname": "t2_t42gx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to keep picture/video EXIF data intact when following the 3-2-1 rule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151gch3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689538600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pictures and videos of pets of mine who have since passed away and I really don&amp;#39;t want the timestamp on each file to be reset when copying the files over; this is especially a problem when transferring from my phone. Is there any way to make the necessary copies while keeping the original time and date intact?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151gch3", "is_robot_indexable": true, "report_reasons": null, "author": "sunbr0_7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151gch3/how_to_keep_picturevideo_exif_data_intact_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151gch3/how_to_keep_picturevideo_exif_data_intact_when/", "subreddit_subscribers": 692897, "created_utc": 1689538600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone ever tried looking at Snopes via the WayBackMachine?  Interestingly enough, theres no archives before 2021. I find that  strange as even niche websites usually do. Did they used to go by a  different domain affix than dot com?\n\nI  found the site when I was young around 2009 and wanted to look at the  old layout and collection of articles again. I know I can just look at  the archive, but frankly Im after the old aesthetic. Does anybody have  any ideas how to access the classic Snopes?", "author_fullname": "t2_4r1t92im", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice for archiving old Snopes website data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151dqu9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689532508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever tried looking at Snopes via the WayBackMachine?  Interestingly enough, theres no archives before 2021. I find that  strange as even niche websites usually do. Did they used to go by a  different domain affix than dot com?&lt;/p&gt;\n\n&lt;p&gt;I  found the site when I was young around 2009 and wanted to look at the  old layout and collection of articles again. I know I can just look at  the archive, but frankly Im after the old aesthetic. Does anybody have  any ideas how to access the classic Snopes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151dqu9", "is_robot_indexable": true, "report_reasons": null, "author": "EI_CEO_CFT", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151dqu9/seeking_advice_for_archiving_old_snopes_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151dqu9/seeking_advice_for_archiving_old_snopes_website/", "subreddit_subscribers": 692897, "created_utc": 1689532508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long ago PC only needed one power connector to the motherboard. Then as CPU started getting more powerful, came the need for extra 12v. Some of the higher end motherboard uses 2 of the 8 pin connectors.\n\nLong ago video cards could use power from the slot. Some of the powerful AGP needed extra power and had a 4 pin connector on it. Today's high end video card uses 2 8-pin PCIe power or even 12 pin connector.\n\nSome USB cards also needed extra power for driving external USB 3.x devices at over 2A per port.\n\nI was just reading into the recently released PCIe 5.0 NVME drive, it uses a hefty 12w of power. It got me thinking, could one day we find them with an extra connector such as the old floppy disk drive connector or something to get more power?  I think the max from m.2 port is 25w?", "author_fullname": "t2_y5py1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could NVME drives one day need external power connection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151a17m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689523596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long ago PC only needed one power connector to the motherboard. Then as CPU started getting more powerful, came the need for extra 12v. Some of the higher end motherboard uses 2 of the 8 pin connectors.&lt;/p&gt;\n\n&lt;p&gt;Long ago video cards could use power from the slot. Some of the powerful AGP needed extra power and had a 4 pin connector on it. Today&amp;#39;s high end video card uses 2 8-pin PCIe power or even 12 pin connector.&lt;/p&gt;\n\n&lt;p&gt;Some USB cards also needed extra power for driving external USB 3.x devices at over 2A per port.&lt;/p&gt;\n\n&lt;p&gt;I was just reading into the recently released PCIe 5.0 NVME drive, it uses a hefty 12w of power. It got me thinking, could one day we find them with an extra connector such as the old floppy disk drive connector or something to get more power?  I think the max from m.2 port is 25w?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "151a17m", "is_robot_indexable": true, "report_reasons": null, "author": "tomytronics", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/151a17m/could_nvme_drives_one_day_need_external_power/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151a17m/could_nvme_drives_one_day_need_external_power/", "subreddit_subscribers": 692897, "created_utc": 1689523596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i currently have 4 external hard drives all backing up various data, all with mismatched backup and stuff (I'll fix it later, this was completely unrelated to the post)\n\nare there any external hard drive bays made to store hard drives to keep them safe from physical damage? i just want a cheap solution to keeping them safe, not a costly one that connects to the internet and provides other knick knacks\n\nI'm searching for a temporary solution to keep my external HDDs safe from physical damage until i start earning enough to create better back ups\n\nfor more details:\n1x2 TB Seagate HDD (&lt;1 year old)\n2x1 TB Seagate HDD (between 1-4 years old)\n1x1.5 TB Seagate HDD (&gt;4 years old)", "author_fullname": "t2_85ypvsyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "safe storage solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1519l3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689522500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i currently have 4 external hard drives all backing up various data, all with mismatched backup and stuff (I&amp;#39;ll fix it later, this was completely unrelated to the post)&lt;/p&gt;\n\n&lt;p&gt;are there any external hard drive bays made to store hard drives to keep them safe from physical damage? i just want a cheap solution to keeping them safe, not a costly one that connects to the internet and provides other knick knacks&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m searching for a temporary solution to keep my external HDDs safe from physical damage until i start earning enough to create better back ups&lt;/p&gt;\n\n&lt;p&gt;for more details:\n1x2 TB Seagate HDD (&amp;lt;1 year old)\n2x1 TB Seagate HDD (between 1-4 years old)\n1x1.5 TB Seagate HDD (&amp;gt;4 years old)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1519l3p", "is_robot_indexable": true, "report_reasons": null, "author": "TriggeredTrigz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1519l3p/safe_storage_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1519l3p/safe_storage_solution/", "subreddit_subscribers": 692897, "created_utc": 1689522500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, Sorry for a newbie question. I did use the search, I found it hard to get past the recent Google restrictions posts, or maybe I really am the first to ask this, but I couldn't find an answer either way. I'm hoping you data storage exists can help me. \n\nI only keep about 1.5-2TB data on the paid for Google workspace thing. I have 2 accounts, one for myself and one for my wife. It's got some work stuff on but it's mostly personal crap and photos etc. I don't want to lose any of it really.\n\nIs there a way of syncing my Google data to say Wasabi or AWS or some other service without changing the way I use it now so that I have a second backup should Google lose it/ban me or whatever bad scenario might happen?\n\nDo I even need to worry about it?\n\nAny thoughts appreciated. I'm not against getting a local NAS or something if that's the best route, I'm also happy to pay another cloud provider. I care more about convenience than cost.", "author_fullname": "t2_4y08k82j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dual cloud backup, gcloud + ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1519ios", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689522332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, Sorry for a newbie question. I did use the search, I found it hard to get past the recent Google restrictions posts, or maybe I really am the first to ask this, but I couldn&amp;#39;t find an answer either way. I&amp;#39;m hoping you data storage exists can help me. &lt;/p&gt;\n\n&lt;p&gt;I only keep about 1.5-2TB data on the paid for Google workspace thing. I have 2 accounts, one for myself and one for my wife. It&amp;#39;s got some work stuff on but it&amp;#39;s mostly personal crap and photos etc. I don&amp;#39;t want to lose any of it really.&lt;/p&gt;\n\n&lt;p&gt;Is there a way of syncing my Google data to say Wasabi or AWS or some other service without changing the way I use it now so that I have a second backup should Google lose it/ban me or whatever bad scenario might happen?&lt;/p&gt;\n\n&lt;p&gt;Do I even need to worry about it?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts appreciated. I&amp;#39;m not against getting a local NAS or something if that&amp;#39;s the best route, I&amp;#39;m also happy to pay another cloud provider. I care more about convenience than cost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1519ios", "is_robot_indexable": true, "report_reasons": null, "author": "jeffreyshran", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1519ios/dual_cloud_backup_gcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1519ios/dual_cloud_backup_gcloud/", "subreddit_subscribers": 692897, "created_utc": 1689522332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Current main NAS setup:\n- 2 x 3TB mirror (WS reds, new drives) for important stuff\n- 2 x 1TB stripe (8y old drives) for plex media/torrents - \n\nBackup NAS setup:\n- 1 x 4TB (WD red, new drive) Backup for the important stuff (the media drives don't have a backup)\n\nOther unused, old drives I have:\n - 3 x 1TB WD blues\n\nNow, as I don't live in the US and drive prices are all over the place I can't just purchase 3 x 16TB drives an put all the stuff on them.\n\nThe main 3TB mirror setup is half full and won't need much expanding for the years to come. \nThe media drives are what concerns me, the stripe pool (2TB in total) is almost full and expanding every month. The safety of this pool is in the negative so to say, so I need to change something.\n\nWhat to do if you want to reuse as many drives as possible?\n- Purchase new 2 x 3TB drives (mirror) just for the media stuff?\n- Purchase two very large drives (mirror), put everything on them and use the 2 x 3TB ones in the backup machine to backup everything, not just the important stuff?\n- Use a different configuration altogether (non mirror)?", "author_fullname": "t2_1hsjxygy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on old/new drives combination for max value and safety", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1514uwo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689509909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current main NAS setup:\n- 2 x 3TB mirror (WS reds, new drives) for important stuff\n- 2 x 1TB stripe (8y old drives) for plex media/torrents - &lt;/p&gt;\n\n&lt;p&gt;Backup NAS setup:\n- 1 x 4TB (WD red, new drive) Backup for the important stuff (the media drives don&amp;#39;t have a backup)&lt;/p&gt;\n\n&lt;p&gt;Other unused, old drives I have:\n - 3 x 1TB WD blues&lt;/p&gt;\n\n&lt;p&gt;Now, as I don&amp;#39;t live in the US and drive prices are all over the place I can&amp;#39;t just purchase 3 x 16TB drives an put all the stuff on them.&lt;/p&gt;\n\n&lt;p&gt;The main 3TB mirror setup is half full and won&amp;#39;t need much expanding for the years to come. \nThe media drives are what concerns me, the stripe pool (2TB in total) is almost full and expanding every month. The safety of this pool is in the negative so to say, so I need to change something.&lt;/p&gt;\n\n&lt;p&gt;What to do if you want to reuse as many drives as possible?\n- Purchase new 2 x 3TB drives (mirror) just for the media stuff?\n- Purchase two very large drives (mirror), put everything on them and use the 2 x 3TB ones in the backup machine to backup everything, not just the important stuff?\n- Use a different configuration altogether (non mirror)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1514uwo", "is_robot_indexable": true, "report_reasons": null, "author": "DAndreyD", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1514uwo/advice_on_oldnew_drives_combination_for_max_value/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1514uwo/advice_on_oldnew_drives_combination_for_max_value/", "subreddit_subscribers": 692897, "created_utc": 1689509909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have been getting error messages since Friday evening when trying to use my iDrive e2 bucket in Frankfurt?\n\nIs the full region down or just my server?\n\nI have also noticed they have reduced their yearly price to $20 for 1TB (was $40 before)", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iDrive e2 Frankfurt down since Friday evening?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15136lt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689504451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have been getting error messages since Friday evening when trying to use my iDrive e2 bucket in Frankfurt?&lt;/p&gt;\n\n&lt;p&gt;Is the full region down or just my server?&lt;/p&gt;\n\n&lt;p&gt;I have also noticed they have reduced their yearly price to $20 for 1TB (was $40 before)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15136lt", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15136lt/idrive_e2_frankfurt_down_since_friday_evening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15136lt/idrive_e2_frankfurt_down_since_friday_evening/", "subreddit_subscribers": 692897, "created_utc": 1689504451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to download all photos from users albums in original HQ format. I understand there was a project but it was discontinued.\n\nAn example would be: [https://weibo.com/u/5732554220?tabtype=album](https://weibo.com/u/5732554220?tabtype=album) with an almost infinite scroll for several years of professional content.\n\nHopefully with a GUI as I'm big enough to admit I'm not savvy enough to use Gibhub projects.", "author_fullname": "t2_a7s673lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a current Weibo album downloader?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15124wf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689500833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to download all photos from users albums in original HQ format. I understand there was a project but it was discontinued.&lt;/p&gt;\n\n&lt;p&gt;An example would be: &lt;a href=\"https://weibo.com/u/5732554220?tabtype=album\"&gt;https://weibo.com/u/5732554220?tabtype=album&lt;/a&gt; with an almost infinite scroll for several years of professional content.&lt;/p&gt;\n\n&lt;p&gt;Hopefully with a GUI as I&amp;#39;m big enough to admit I&amp;#39;m not savvy enough to use Gibhub projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15124wf", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Distance740", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15124wf/is_there_a_current_weibo_album_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15124wf/is_there_a_current_weibo_album_downloader/", "subreddit_subscribers": 692897, "created_utc": 1689500833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My pool is a mix of duplicated and unduplicated files across 7 drives (52.8 TB total). My StableBit Scanner says I have 512 byte bad sector in one of the drives. DrivePool immediately copies unduplicated files on the drive with the bad sector across other drives. After it finishes, it did a remeasure and pool organization is green with \"file distribution not normal\". For some reason, the bad drive shows I have around 400GB unduplicated data, which I really want to backup just in case. My pool statistics shows:\n\n* 21.9 TB unduplicated\n* 24.6 TB duplicated\n* 6.05 TB unusable for duplication\n\nIs 512 byte bad sector something to worry about? Should I keep my drive or replace it? I'm not sure what to do next from here. Thank you in advance.\n\n&amp;#x200B;", "author_fullname": "t2_u50yq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "StableBit DrivePool/Scanner says I have 512 byte bad sector...what to do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15120jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689500416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My pool is a mix of duplicated and unduplicated files across 7 drives (52.8 TB total). My StableBit Scanner says I have 512 byte bad sector in one of the drives. DrivePool immediately copies unduplicated files on the drive with the bad sector across other drives. After it finishes, it did a remeasure and pool organization is green with &amp;quot;file distribution not normal&amp;quot;. For some reason, the bad drive shows I have around 400GB unduplicated data, which I really want to backup just in case. My pool statistics shows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;21.9 TB unduplicated&lt;/li&gt;\n&lt;li&gt;24.6 TB duplicated&lt;/li&gt;\n&lt;li&gt;6.05 TB unusable for duplication&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is 512 byte bad sector something to worry about? Should I keep my drive or replace it? I&amp;#39;m not sure what to do next from here. Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15120jl", "is_robot_indexable": true, "report_reasons": null, "author": "neohanime", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15120jl/stablebit_drivepoolscanner_says_i_have_512_byte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15120jl/stablebit_drivepoolscanner_says_i_have_512_byte/", "subreddit_subscribers": 692897, "created_utc": 1689500416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm having trouble **only** wgetting this URL : [https://forum.spacehey.com/topic?id=3959](https://forum.spacehey.com/topic?id=3959) and all the links that branch out of it, only 1 level away from the link.\n\n    When I run wget -e robots=off --recursive -np -k --html-extension \"https://forum.spacehey.com/topic?id=3959\"\n\nit starts downloading all of the forum pages and all the user profiles, even those that did not participate in that specific thread. I just want \\`topic?id=3959  \n to be downloaded, plus the user profiles or links that might branch out from the link, but no deeper than 1 level. Not sure if I'm explaining myself correctly.", "author_fullname": "t2_2rewrxo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150q5z7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689462973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having trouble &lt;strong&gt;only&lt;/strong&gt; wgetting this URL : &lt;a href=\"https://forum.spacehey.com/topic?id=3959\"&gt;https://forum.spacehey.com/topic?id=3959&lt;/a&gt; and all the links that branch out of it, only 1 level away from the link.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;When I run wget -e robots=off --recursive -np -k --html-extension &amp;quot;https://forum.spacehey.com/topic?id=3959&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;it starts downloading all of the forum pages and all the user profiles, even those that did not participate in that specific thread. I just want `topic?id=3959&lt;br/&gt;\n to be downloaded, plus the user profiles or links that might branch out from the link, but no deeper than 1 level. Not sure if I&amp;#39;m explaining myself correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?auto=webp&amp;s=3857ed03a465a31599b65c323030194450e8bc09", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=722e416e8ee490729a17976e9f4bbe7c3e078871", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f35418a5521952a5db5b0452b454e3a32b03a46", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d16b9e2c1149bb5979da229871a45ec1d23d7dfc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=536746646c1dded870ef8ae904c9414d295e6926", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=acee12e384beec774f373142492cf49d80ee2300", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8453b2a3662b55ed0db129bc72c0d660403065ee", "width": 1080, "height": 540}], "variants": {}, "id": "ztoUwoXJpe58a61I-q3Dg8eJzueXuf-NZZ1WQ20Kpgo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150q5z7", "is_robot_indexable": true, "report_reasons": null, "author": "iPodClassic7", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150q5z7/need_help_with_wget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150q5z7/need_help_with_wget/", "subreddit_subscribers": 692897, "created_utc": 1689462973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Why does the TerraMaster D4-300 consume more power than that of a low-powered NAS solution?\n\nFor example:\n\nThe TerraMaster D4-300 is a simple DAS (no CPU -- it's just a simple HDD docking station) has a power draw of around 45.6W. Whereas, a low-powered ARM-based NAS, like the Synology DS463 power draw ranges from  4.97W to 32.41W. Heck, even the DS463+ (which features an x86-based CPU) seems to use less power than the DAS with a range of 8W to 28W.\n\n&gt; **NOTE:** All of these devices support up to four (4) 3.5\" HDDs within their enclusures.", "author_fullname": "t2_429x3s08", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple DAS consumes more power than a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151cyko", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689530643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why does the TerraMaster D4-300 consume more power than that of a low-powered NAS solution?&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;p&gt;The TerraMaster D4-300 is a simple DAS (no CPU -- it&amp;#39;s just a simple HDD docking station) has a power draw of around 45.6W. Whereas, a low-powered ARM-based NAS, like the Synology DS463 power draw ranges from  4.97W to 32.41W. Heck, even the DS463+ (which features an x86-based CPU) seems to use less power than the DAS with a range of 8W to 28W.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; All of these devices support up to four (4) 3.5&amp;quot; HDDs within their enclusures.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151cyko", "is_robot_indexable": true, "report_reasons": null, "author": "etho201", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151cyko/simple_das_consumes_more_power_than_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151cyko/simple_das_consumes_more_power_than_a_nas/", "subreddit_subscribers": 692897, "created_utc": 1689530643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_eqim9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A small tool I wrote to rename files and keep them under a specified byte length", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_151c8dq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/u876Jb36vi1KZ77Etm_0g8WqUGWAf_dUHLOhVfZ_NMw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689528934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/mashariqk/rnx", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?auto=webp&amp;s=434fd8e1ace017eb9306385ad3211773f41971eb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f44ec9a94b7307cabbcd70ac9967bb1d3da0ae82", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=096b0a36ce29820cc1811886e81d20b98c17c2f5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b85613a33e7e4de4b0a83a93e589ef6804586225", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05064e18ac978f43eb11966a41f57d5e948e21e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0dd4870ddc4b84008b4d21143fbb6827f4f63d5b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=572f6fed2dcd592fb257cd587cf16e0ec5ae970c", "width": 1080, "height": 540}], "variants": {}, "id": "N5KjDDYIOFMLeNApx-JOg7PfTvgd3LNwev2zj9E40_Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151c8dq", "is_robot_indexable": true, "report_reasons": null, "author": "mashariq", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151c8dq/a_small_tool_i_wrote_to_rename_files_and_keep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/mashariqk/rnx", "subreddit_subscribers": 692897, "created_utc": 1689528934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did a quick search of /r/datahoarder and couldn\u2019t find any mention of this horrible restriction. \n\nReddit app used to save the full resolution image. Now, clicking \u201cdownload\u201d in the mobile app: \n\n* saves a JPEG to the camera roll with a maximum of 1164 px on the long edge\n* adds a huge title margin, ~100 px: \u201cFrom X community on Reddit\u201d\n* adds large ~50 px side and bottom margin\n* adds a large, orange Reddit logo (size is at least ~100 px, and transparency at least 50%)\n\nIs this why they\u2019re killing the third party apps? To foist this nonsense on us? \n\n\n*EDIT: wasn\u2019t sure entirely where to post this, but I figured it\u2019s undoubtedly an important topic to my fellow hoarders.*", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving images from the Reddit mobile app: Reddit is downscaling images to 1164 px on the long edge, AND adding destructive watermarking. WTAF!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151bv0h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689528339.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689528041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a quick search of &lt;a href=\"/r/datahoarder\"&gt;/r/datahoarder&lt;/a&gt; and couldn\u2019t find any mention of this horrible restriction. &lt;/p&gt;\n\n&lt;p&gt;Reddit app used to save the full resolution image. Now, clicking \u201cdownload\u201d in the mobile app: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;saves a JPEG to the camera roll with a maximum of 1164 px on the long edge&lt;/li&gt;\n&lt;li&gt;adds a huge title margin, ~100 px: \u201cFrom X community on Reddit\u201d&lt;/li&gt;\n&lt;li&gt;adds large ~50 px side and bottom margin&lt;/li&gt;\n&lt;li&gt;adds a large, orange Reddit logo (size is at least ~100 px, and transparency at least 50%)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is this why they\u2019re killing the third party apps? To foist this nonsense on us? &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EDIT: wasn\u2019t sure entirely where to post this, but I figured it\u2019s undoubtedly an important topic to my fellow hoarders.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151bv0h", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151bv0h/saving_images_from_the_reddit_mobile_app_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151bv0h/saving_images_from_the_reddit_mobile_app_reddit/", "subreddit_subscribers": 692897, "created_utc": 1689528041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an 6 yrs old WD 2TB HDD, dropped it by accident , still spinning no signs of head crash or any physical damage but does not show up . I was looking if it could be fixed and how much would it take . The data is really important but I cant really cannot spend too much for data extraction ", "author_fullname": "t2_brtfbspo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a crashed HDD, No head crash or physical damage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151avoh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689525683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an 6 yrs old WD 2TB HDD, dropped it by accident , still spinning no signs of head crash or any physical damage but does not show up . I was looking if it could be fixed and how much would it take . The data is really important but I cant really cannot spend too much for data extraction &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151avoh", "is_robot_indexable": true, "report_reasons": null, "author": "Top_Top_315", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151avoh/need_help_with_a_crashed_hdd_no_head_crash_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151avoh/need_help_with_a_crashed_hdd_no_head_crash_or/", "subreddit_subscribers": 692897, "created_utc": 1689525683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using Storage Spaces for years to store relatively unimportant data and it's worked fine with two 18TB internal drives and two 8TB external drives. I was finally running out of room so I decided to add two new 18TB drives attached via a USB Docking Station. Got the new drives/docking station, started to optimize, and immediately started getting errors that one of the old drives had failed. \n\nFast-forward a bit and I have to stop optimization, and after stopping it and reconnecting all drives, all the drives  read as \"OK.\" So I start trying to optimize again. This time things go even worse. One of the old drives read as \"failed\" and, for some reason, Storage Spaces automatically prepares it to remove even though I didn't tell it to, and I get a \"warning\" on one of the new drives. So optimization completes, and I try unplugging/replugging everything again, but this time it doesn't work. The old drive will only read \"ready to remove\" and new drives now both have Warning marks. So I remove the old drive and eventually reconnnect it using a different USB port. Now getting the message \"Error Unrecognized Configuration Reset Drive\" which, according to a Google search, will wipe the drive. \n\nMy Storage Spaces volume is still accessible but a lot of data is missing/inaccessible. Most of it I could get back if I wanted to put in the time/effort, but I'd be willing to pay some if I could recover it without having to manually find/download everything I'd saved, but I don't know what my options are. Any advice/recommendations? I know of, eg, ReClaime, but it's like $300 and I have no idea how successful it would be. I've got to think the data is recoverable given that none of the drives have actually failed and the data is still there. ", "author_fullname": "t2_b04kmo9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces Fails to Optimize: Data Lost? Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150zkpk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689491866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Storage Spaces for years to store relatively unimportant data and it&amp;#39;s worked fine with two 18TB internal drives and two 8TB external drives. I was finally running out of room so I decided to add two new 18TB drives attached via a USB Docking Station. Got the new drives/docking station, started to optimize, and immediately started getting errors that one of the old drives had failed. &lt;/p&gt;\n\n&lt;p&gt;Fast-forward a bit and I have to stop optimization, and after stopping it and reconnecting all drives, all the drives  read as &amp;quot;OK.&amp;quot; So I start trying to optimize again. This time things go even worse. One of the old drives read as &amp;quot;failed&amp;quot; and, for some reason, Storage Spaces automatically prepares it to remove even though I didn&amp;#39;t tell it to, and I get a &amp;quot;warning&amp;quot; on one of the new drives. So optimization completes, and I try unplugging/replugging everything again, but this time it doesn&amp;#39;t work. The old drive will only read &amp;quot;ready to remove&amp;quot; and new drives now both have Warning marks. So I remove the old drive and eventually reconnnect it using a different USB port. Now getting the message &amp;quot;Error Unrecognized Configuration Reset Drive&amp;quot; which, according to a Google search, will wipe the drive. &lt;/p&gt;\n\n&lt;p&gt;My Storage Spaces volume is still accessible but a lot of data is missing/inaccessible. Most of it I could get back if I wanted to put in the time/effort, but I&amp;#39;d be willing to pay some if I could recover it without having to manually find/download everything I&amp;#39;d saved, but I don&amp;#39;t know what my options are. Any advice/recommendations? I know of, eg, ReClaime, but it&amp;#39;s like $300 and I have no idea how successful it would be. I&amp;#39;ve got to think the data is recoverable given that none of the drives have actually failed and the data is still there. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150zkpk", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_War5435", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150zkpk/storage_spaces_fails_to_optimize_data_lost_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150zkpk/storage_spaces_fails_to_optimize_data_lost_help/", "subreddit_subscribers": 692897, "created_utc": 1689491866.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}