{"kind": "Listing", "data": {"after": "t3_150zkpk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Something like this could probably shift the paradigm of how we store files and create directories. Would you spend your free time organizing your files if you know that a simple search could get you exactly to what you need in a few seconds?\n\nIt's very similar to Spotify search, where you can type in part of the lyrics of a song and it will find you the song. Or in MacOS, you can search by text inside photos and it will grab all the images that contain that text (no more spending my weekends on meticulously tagging and organizing my screenshots folder).\n\nSo is there anything like that already out there that would achieve this especially on Windows?\n\n*Edit*: a local search engine ideally powered by LLMs which would search your entire local hard drive and find matches. LLM-powered means instead of searching using a very specific string of words, you'd be able to \"describe\" what you're looking for in your hard drive without mentioning the file name, tags, Metadata. \n\n```-- \"Find me that video where I was walking next to a river with ice cream in my hand\"```\n\nor \n\n```-- Can you look into my Obsidian notes and summarize all my notes that relate to concept XYZ. Also see if you can find any screenshots that I might have saved that might relate to this topic or check my offline YouTube video archive to see if you can find any snippets or talks that relate to this \"```", "author_fullname": "t2_3bnahjfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With all the AI hype and GPT stuff, are there already any tools/software that make searching and sorting files smarter and faster? Such as the MacOS search inside images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1511tpx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": "", "subreddit_type": "public", "ups": 117, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 117, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689509621.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689499743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Something like this could probably shift the paradigm of how we store files and create directories. Would you spend your free time organizing your files if you know that a simple search could get you exactly to what you need in a few seconds?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very similar to Spotify search, where you can type in part of the lyrics of a song and it will find you the song. Or in MacOS, you can search by text inside photos and it will grab all the images that contain that text (no more spending my weekends on meticulously tagging and organizing my screenshots folder).&lt;/p&gt;\n\n&lt;p&gt;So is there anything like that already out there that would achieve this especially on Windows?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Edit&lt;/em&gt;: a local search engine ideally powered by LLMs which would search your entire local hard drive and find matches. LLM-powered means instead of searching using a very specific string of words, you&amp;#39;d be able to &amp;quot;describe&amp;quot; what you&amp;#39;re looking for in your hard drive without mentioning the file name, tags, Metadata. &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-- &amp;quot;Find me that video where I was walking next to a river with ice cream in my hand&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;or &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-- Can you look into my Obsidian notes and summarize all my notes that relate to concept XYZ. Also see if you can find any screenshots that I might have saved that might relate to this topic or check my offline YouTube video archive to see if you can find any snippets or talks that relate to this &amp;quot;&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1511tpx", "is_robot_indexable": true, "report_reasons": null, "author": "egobamyasi", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1511tpx/with_all_the_ai_hype_and_gpt_stuff_are_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1511tpx/with_all_the_ai_hype_and_gpt_stuff_are_there/", "subreddit_subscribers": 692909, "created_utc": 1689499743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seems it would be wise to hoard programs/files that will be useful in offline scenarios. Something that converts imperial to metric for example, or common tables and charts for things like electrical and plumbing come to mind. Wikipedia mirror makes good sense. I've grabbed mauals for every tool and appliance I own. \n\nHayes repair manuals for cars take some digging but are available, the question is whether to grab only the cars you have, or all that are available? I've only bothered with certain brands but I can see an argument for downloading everything. \n\nPersonally I found some of the compilations like \"the ark\" to be full of junk. Open to suggestions for anything I mentioned above and more", "author_fullname": "t2_ay7tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Programs to hoard for offline computers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151btrm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689527960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems it would be wise to hoard programs/files that will be useful in offline scenarios. Something that converts imperial to metric for example, or common tables and charts for things like electrical and plumbing come to mind. Wikipedia mirror makes good sense. I&amp;#39;ve grabbed mauals for every tool and appliance I own. &lt;/p&gt;\n\n&lt;p&gt;Hayes repair manuals for cars take some digging but are available, the question is whether to grab only the cars you have, or all that are available? I&amp;#39;ve only bothered with certain brands but I can see an argument for downloading everything. &lt;/p&gt;\n\n&lt;p&gt;Personally I found some of the compilations like &amp;quot;the ark&amp;quot; to be full of junk. Open to suggestions for anything I mentioned above and more&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "124TB ZFS and Synology + Cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "151btrm", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/151btrm/programs_to_hoard_for_offline_computers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151btrm/programs_to_hoard_for_offline_computers/", "subreddit_subscribers": 692909, "created_utc": 1689527960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for some advice on how to revamp my backups. I've already went down several extremely weird paths but I think it's time to give up and get some help.\n\nMy machines look like this:\n1. TrueNAS (Source) : ~48TB\n2. Windows (\"GUI\" + Local Backup) : 3TB\n3. Orange Pi 5+ (Offsite Backup) : 1TB\n4. Raspberry Pi 4B (Offsite Backup) : 1TB\n5. Mango Pi Pro (Usage TBD) : ???\n6. Storj (Cloud Backup) : 150GB x2\n7. Oracle Cloud (Cloud Backup) : 150GB x2\n\nI'm looking to do partial backups since I can live without most of the data, but I really want to keep about 1TB of it safe from multiple copies going down simultaneously. I had originally planned on doing a PULL configuration from each target machine with read only access, so if TrueNAS gets compromised it doesn't matter because it barely knows the target machines exist. If one of the target machines gets compromised, it only has read access so it doesn't matter much either. But then maintenance and reporting gets a bit squirrely so that's not great either.... But I think a PUSH configuration on TrueNAS with a locked down user should achieve mostly the same effect. \n\nThe target machines (2?, 3, 4, 7) can run ZFS so in the event TrueNAS becomes compromised it can overwrite the backup folder, but that'll just fill up the drive while the snapshots hosted on the target machines are still intact. If the target machines become compromised then they don't have access to TrueNAS at all. I can't really do snapshots on Storj, but they're freebie accounts so I wasn't going to rely on them anyway.\n\nThis seems like a decent compromise between security and ease of use. I can monitor the status of my backups from a central location (TrueNAS's UI) without SSH'ing into half a dozen machines or figuring out grafana / prometheus. Also if the backup parameters change (what goes where, ignore files X, Y, Z, etc) then I can do that from one location as well. Also I can save TrueNAS's single configuration easily without worrying about which individual machines have which version of a backup script.\n\nI'm trying to avoid dedicated backup programs because I don't trust them or myself to set them up. I've been using Kopia which is easily accessible and intuitive, but I've also had it silently lose a chunk randomly corrupting the repository with literally no way of knowing from the GUI. There are a bunch of other programs out there like Dupliacy, Duplicati, restic, bacula but they all have their downsides. Some of them don't have a GUI for windows or are WAY too complicated which is a nogo for an idiot user like me. Or they just don't have some fairly basic features like backing up from a network location (Veaam windows...) But you know what hasn't failed me? ZFS. But since I don't want to replicate entire datasets everywhere and not every location is capable of running ZFS, that rules out ZFS send / recv. The rsync task probably works but I know for sure cloud sync works, so that is probably what I'll go with.\n\nSo to recap;\n1. TrueNAS -&gt; Cloud Sync Task -&gt; Bunch of storage -&gt; ZFS snapshot\n2. The accounts TrueNAS has access to will have no privleges outside of the backup folder\n\nDoes this make sense? Am I missing something? Thanks in advance!", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to redo some backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1515ivg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689511908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for some advice on how to revamp my backups. I&amp;#39;ve already went down several extremely weird paths but I think it&amp;#39;s time to give up and get some help.&lt;/p&gt;\n\n&lt;p&gt;My machines look like this:\n1. TrueNAS (Source) : ~48TB\n2. Windows (&amp;quot;GUI&amp;quot; + Local Backup) : 3TB\n3. Orange Pi 5+ (Offsite Backup) : 1TB\n4. Raspberry Pi 4B (Offsite Backup) : 1TB\n5. Mango Pi Pro (Usage TBD) : ???\n6. Storj (Cloud Backup) : 150GB x2\n7. Oracle Cloud (Cloud Backup) : 150GB x2&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to do partial backups since I can live without most of the data, but I really want to keep about 1TB of it safe from multiple copies going down simultaneously. I had originally planned on doing a PULL configuration from each target machine with read only access, so if TrueNAS gets compromised it doesn&amp;#39;t matter because it barely knows the target machines exist. If one of the target machines gets compromised, it only has read access so it doesn&amp;#39;t matter much either. But then maintenance and reporting gets a bit squirrely so that&amp;#39;s not great either.... But I think a PUSH configuration on TrueNAS with a locked down user should achieve mostly the same effect. &lt;/p&gt;\n\n&lt;p&gt;The target machines (2?, 3, 4, 7) can run ZFS so in the event TrueNAS becomes compromised it can overwrite the backup folder, but that&amp;#39;ll just fill up the drive while the snapshots hosted on the target machines are still intact. If the target machines become compromised then they don&amp;#39;t have access to TrueNAS at all. I can&amp;#39;t really do snapshots on Storj, but they&amp;#39;re freebie accounts so I wasn&amp;#39;t going to rely on them anyway.&lt;/p&gt;\n\n&lt;p&gt;This seems like a decent compromise between security and ease of use. I can monitor the status of my backups from a central location (TrueNAS&amp;#39;s UI) without SSH&amp;#39;ing into half a dozen machines or figuring out grafana / prometheus. Also if the backup parameters change (what goes where, ignore files X, Y, Z, etc) then I can do that from one location as well. Also I can save TrueNAS&amp;#39;s single configuration easily without worrying about which individual machines have which version of a backup script.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to avoid dedicated backup programs because I don&amp;#39;t trust them or myself to set them up. I&amp;#39;ve been using Kopia which is easily accessible and intuitive, but I&amp;#39;ve also had it silently lose a chunk randomly corrupting the repository with literally no way of knowing from the GUI. There are a bunch of other programs out there like Dupliacy, Duplicati, restic, bacula but they all have their downsides. Some of them don&amp;#39;t have a GUI for windows or are WAY too complicated which is a nogo for an idiot user like me. Or they just don&amp;#39;t have some fairly basic features like backing up from a network location (Veaam windows...) But you know what hasn&amp;#39;t failed me? ZFS. But since I don&amp;#39;t want to replicate entire datasets everywhere and not every location is capable of running ZFS, that rules out ZFS send / recv. The rsync task probably works but I know for sure cloud sync works, so that is probably what I&amp;#39;ll go with.&lt;/p&gt;\n\n&lt;p&gt;So to recap;\n1. TrueNAS -&amp;gt; Cloud Sync Task -&amp;gt; Bunch of storage -&amp;gt; ZFS snapshot\n2. The accounts TrueNAS has access to will have no privleges outside of the backup folder&lt;/p&gt;\n\n&lt;p&gt;Does this make sense? Am I missing something? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1515ivg", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1515ivg/looking_to_redo_some_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1515ivg/looking_to_redo_some_backups/", "subreddit_subscribers": 692909, "created_utc": 1689511908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 12TB (ST12000NM001G) and 2TB (ST2000LX001) HDD in my PC. The 12TB I bought new last October, while the 2TB I bought new around 4 years ago. The 2TB drive was first used for around 3 years in my PS4 for gaming.\r\n\r\nThis 1st July, CrystalDiskInfo is reporting Caution for the 12TB drive. This is because of 100 reallocated sectors. When I searched online about this, I found that this is something that happens, and it is okay to keep using the drive unless the number keeps going up. However, some expressed that anything other than 0 is a risk and the drive must be replaced.\r\n\r\nThe drive is fairly new and is not used constantly. When investigating further I found the following:\r\n\r\nThe 12TB drive has 100 current, 100 worst, 10 threshold, and 000000000038 raw value in CrystalDiskInfo for Reallocated Sectores Count. It is marked as yellow. However, the 2TB drive has 100 current, 100 worst, 36 threshold, and 000000000000 raw value in CrystalDiskInfo for Reallocated Sectores Count and marked as blue. How is the current and worst value the same with a different and all-zero raw value? How is it also not yellow if the other drive has the same current value and yellow?\r\n\r\nThings get even weirder when I check the same values with Samsung Magician. The ECC Error Rate is critical for the 12TB drive in Samsung magician but good in CrystalDiskInfo. The values are 12/2/0 for Current/Worst/Threshold and EE034D raw.\r\n\r\nCan someone help me understand why the CrystalDiskInfo is reporting Good and Caution for the same value and why Samsung Magician has a different opinion to the values?\r\n\r\nThanks", "author_fullname": "t2_2dhcdz13", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interpreting SMART Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15163g5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689513493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 12TB (ST12000NM001G) and 2TB (ST2000LX001) HDD in my PC. The 12TB I bought new last October, while the 2TB I bought new around 4 years ago. The 2TB drive was first used for around 3 years in my PS4 for gaming.&lt;/p&gt;\n\n&lt;p&gt;This 1st July, CrystalDiskInfo is reporting Caution for the 12TB drive. This is because of 100 reallocated sectors. When I searched online about this, I found that this is something that happens, and it is okay to keep using the drive unless the number keeps going up. However, some expressed that anything other than 0 is a risk and the drive must be replaced.&lt;/p&gt;\n\n&lt;p&gt;The drive is fairly new and is not used constantly. When investigating further I found the following:&lt;/p&gt;\n\n&lt;p&gt;The 12TB drive has 100 current, 100 worst, 10 threshold, and 000000000038 raw value in CrystalDiskInfo for Reallocated Sectores Count. It is marked as yellow. However, the 2TB drive has 100 current, 100 worst, 36 threshold, and 000000000000 raw value in CrystalDiskInfo for Reallocated Sectores Count and marked as blue. How is the current and worst value the same with a different and all-zero raw value? How is it also not yellow if the other drive has the same current value and yellow?&lt;/p&gt;\n\n&lt;p&gt;Things get even weirder when I check the same values with Samsung Magician. The ECC Error Rate is critical for the 12TB drive in Samsung magician but good in CrystalDiskInfo. The values are 12/2/0 for Current/Worst/Threshold and EE034D raw.&lt;/p&gt;\n\n&lt;p&gt;Can someone help me understand why the CrystalDiskInfo is reporting Good and Caution for the same value and why Samsung Magician has a different opinion to the values?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15163g5", "is_robot_indexable": true, "report_reasons": null, "author": "ISuckAtNames387", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15163g5/interpreting_smart_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15163g5/interpreting_smart_data/", "subreddit_subscribers": 692909, "created_utc": 1689513493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have too much digital stuff. I am deleting everything I don\u2019t need. I\u2019ve about 10 years worth of stuff to go through, and it will take a long time. My fear of deleting anything first happened when my first computer got a virus and I lost everything + losing stuff due to files becoming corrupt. Even though my \u2018hoarding\u2019 is pretty mild, I don\u2019t like having to keep stuff \u2018just in case.\u2019 Especially if I genuinely don\u2019t need it anymore. \n\nMy digital clutter clutters my mind. If I see something and either don\u2019t need it or don\u2019t see myself reopening the file, I am getting rid of it. It makes me feel uncomfortable just deleting shit, but I have to delete and forget about it. I signed up to lots of accounts and emails as well, which are all in the process of getting deleted.\n\nYou guys might laugh at this but I have only around 2-5 TB worth of stuff, which might seem like nothing around here, but I find it overwhelming and extremely burdensome. Respect to all of you who are able to manage all that, jesus.\n\nEdit: Thanks guys, I didn\u2019t even know my post was approved because it got rejected at first due to my account being new. I really appreciate your comments and advice. It\u2019s great to relate to you all.", "author_fullname": "t2_d0lx00uz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am tired", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150x71e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689521066.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689483892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have too much digital stuff. I am deleting everything I don\u2019t need. I\u2019ve about 10 years worth of stuff to go through, and it will take a long time. My fear of deleting anything first happened when my first computer got a virus and I lost everything + losing stuff due to files becoming corrupt. Even though my \u2018hoarding\u2019 is pretty mild, I don\u2019t like having to keep stuff \u2018just in case.\u2019 Especially if I genuinely don\u2019t need it anymore. &lt;/p&gt;\n\n&lt;p&gt;My digital clutter clutters my mind. If I see something and either don\u2019t need it or don\u2019t see myself reopening the file, I am getting rid of it. It makes me feel uncomfortable just deleting shit, but I have to delete and forget about it. I signed up to lots of accounts and emails as well, which are all in the process of getting deleted.&lt;/p&gt;\n\n&lt;p&gt;You guys might laugh at this but I have only around 2-5 TB worth of stuff, which might seem like nothing around here, but I find it overwhelming and extremely burdensome. Respect to all of you who are able to manage all that, jesus.&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks guys, I didn\u2019t even know my post was approved because it got rejected at first due to my account being new. I really appreciate your comments and advice. It\u2019s great to relate to you all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "150x71e", "is_robot_indexable": true, "report_reasons": null, "author": "OkPoet6127", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150x71e/i_am_tired/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150x71e/i_am_tired/", "subreddit_subscribers": 692909, "created_utc": 1689483892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone! I've recently been looking all over the internet for an xml dump of fandom (formerly known as wikia) wikis from the years 2016 to 2018. I've seen dumps from 2015 and 2020, but the 2018 dump doesn't seem to have the specific wiki I'm looking for. Where would I look for more information?\n\n&amp;#x200B;", "author_fullname": "t2_21qnqkln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to Find Website Dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150qz0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689465126.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I&amp;#39;ve recently been looking all over the internet for an xml dump of fandom (formerly known as wikia) wikis from the years 2016 to 2018. I&amp;#39;ve seen dumps from 2015 and 2020, but the 2018 dump doesn&amp;#39;t seem to have the specific wiki I&amp;#39;m looking for. Where would I look for more information?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150qz0n", "is_robot_indexable": true, "report_reasons": null, "author": "TemporarilyResolute", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150qz0n/trying_to_find_website_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150qz0n/trying_to_find_website_dump/", "subreddit_subscribers": 692909, "created_utc": 1689465126.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently got a Terramaster D4-300 4 bay DAS and some WD red 16TB drives for it. The noise from the HDDs (read/write noises and what I assume is preventive wear leveling based on some google searches) is very loud. I have also had a WD elements 8TB external drive for a few years and I keep it in the same location. You can hear that drive if you get close to it but the new 16 TB drives are substantially louder.\n\nSo I\u2019m wondering, is it normal that they would be so much louder than the 8TB? Or would the issue be more with the DAS? The external drive has plastic housing while this DAS is metal. I\u2019m also wondering if it\u2019s possible I didn\u2019t install them well, or should use screws instead of the plastic piece it comes with to snap them in. \n\nIf there isn\u2019t a good way to reduce the noise I\u2019m considering putting the DAS elsewhere and getting a long USB-C cable. But they don\u2019t seem to make data transfer cables more than a few feet long, so I\u2019m not sure if this is a great idea. \n\nAny advice on this would be appreciated.", "author_fullname": "t2_8fbeb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Excessive noise with new DAS setup.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151d29d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689530881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got a Terramaster D4-300 4 bay DAS and some WD red 16TB drives for it. The noise from the HDDs (read/write noises and what I assume is preventive wear leveling based on some google searches) is very loud. I have also had a WD elements 8TB external drive for a few years and I keep it in the same location. You can hear that drive if you get close to it but the new 16 TB drives are substantially louder.&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m wondering, is it normal that they would be so much louder than the 8TB? Or would the issue be more with the DAS? The external drive has plastic housing while this DAS is metal. I\u2019m also wondering if it\u2019s possible I didn\u2019t install them well, or should use screws instead of the plastic piece it comes with to snap them in. &lt;/p&gt;\n\n&lt;p&gt;If there isn\u2019t a good way to reduce the noise I\u2019m considering putting the DAS elsewhere and getting a long USB-C cable. But they don\u2019t seem to make data transfer cables more than a few feet long, so I\u2019m not sure if this is a great idea. &lt;/p&gt;\n\n&lt;p&gt;Any advice on this would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151d29d", "is_robot_indexable": true, "report_reasons": null, "author": "Zander327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151d29d/excessive_noise_with_new_das_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151d29d/excessive_noise_with_new_das_setup/", "subreddit_subscribers": 692909, "created_utc": 1689530881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a number of laptop backups, very old (oldest ~8-9 yrs back) laptop backups, which were taken at random dates and a lot of the backup just repeats itself (~195 GB total). Basically a copy paste of most of the folders from the laptop to a harddrive at different points of time.\n\nI was going through the backups, and found a lot of photos, which I have never seen. I would love to find all such \"lost\" photos in these backups. And a way to de-dup the duplicate ones (there will be duplicates)\n\nThe other documents in these backups are pretty much useless for me, apart from the photos. Is there a quick way to extract photos from these huge backups?\n\nI tried searching for a software which could do this, but was unable to find one.\n\nI have a feeling that this might be a common use-case and that there must be a solution outside, just that I don't have the proper key-word to search with.\n\nAny help is appreciated!\n\n\n\nPS:\n- I also tried writing a small script which could extract photos (recursively search all files, check if image, move) but that gives me very very huge list of images (I think they are internal laptop/windows images, and are of no use to me). \n- Is there a good way of doing this? I could also filter on the image size, but these backups also contain a lot of small images aswell - whatsapp images, screenshots, etc. which are small in size too (so this only won't work).\n- One more disadvantage is - the script is very slow (or atleast I think that there must be better optimized way of doing this).\n- I would still not know if what I am writing will cover all the photos in the disk. Basically will have to spend a lot of time testing, running, etc.", "author_fullname": "t2_blvc1ovuy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting all photos from a laptop backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15108wx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689496045.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689494176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a number of laptop backups, very old (oldest ~8-9 yrs back) laptop backups, which were taken at random dates and a lot of the backup just repeats itself (~195 GB total). Basically a copy paste of most of the folders from the laptop to a harddrive at different points of time.&lt;/p&gt;\n\n&lt;p&gt;I was going through the backups, and found a lot of photos, which I have never seen. I would love to find all such &amp;quot;lost&amp;quot; photos in these backups. And a way to de-dup the duplicate ones (there will be duplicates)&lt;/p&gt;\n\n&lt;p&gt;The other documents in these backups are pretty much useless for me, apart from the photos. Is there a quick way to extract photos from these huge backups?&lt;/p&gt;\n\n&lt;p&gt;I tried searching for a software which could do this, but was unable to find one.&lt;/p&gt;\n\n&lt;p&gt;I have a feeling that this might be a common use-case and that there must be a solution outside, just that I don&amp;#39;t have the proper key-word to search with.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated!&lt;/p&gt;\n\n&lt;p&gt;PS:\n- I also tried writing a small script which could extract photos (recursively search all files, check if image, move) but that gives me very very huge list of images (I think they are internal laptop/windows images, and are of no use to me). \n- Is there a good way of doing this? I could also filter on the image size, but these backups also contain a lot of small images aswell - whatsapp images, screenshots, etc. which are small in size too (so this only won&amp;#39;t work).\n- One more disadvantage is - the script is very slow (or atleast I think that there must be better optimized way of doing this).\n- I would still not know if what I am writing will cover all the photos in the disk. Basically will have to spend a lot of time testing, running, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15108wx", "is_robot_indexable": true, "report_reasons": null, "author": "MeasurementSad6631", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15108wx/extracting_all_photos_from_a_laptop_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15108wx/extracting_all_photos_from_a_laptop_backup/", "subreddit_subscribers": 692909, "created_utc": 1689494176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently acquired a couple of 12-bay synology NASes loaded with drives, after a family friend passed away. \n\nUnsure if the contents are worth saving, but it appears the first enclosure I started messing with, a DS2415, is suffering from the \u201catom power problem\u201d, with blinking power and alert lights upon trying to startup.  I\u2019ve removed and numbered the drives for now. \n\nIs there any other way for me to view these drives, outside of getting them into a new 12-bay enclosure? Will that even work? I can\u2019t move them into the other enclosure I got from them because it is an expansion unit.", "author_fullname": "t2_ablys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reconstructing/salvaging a 12-drive RAID without the 12-bay enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150x4zh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689483717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently acquired a couple of 12-bay synology NASes loaded with drives, after a family friend passed away. &lt;/p&gt;\n\n&lt;p&gt;Unsure if the contents are worth saving, but it appears the first enclosure I started messing with, a DS2415, is suffering from the \u201catom power problem\u201d, with blinking power and alert lights upon trying to startup.  I\u2019ve removed and numbered the drives for now. &lt;/p&gt;\n\n&lt;p&gt;Is there any other way for me to view these drives, outside of getting them into a new 12-bay enclosure? Will that even work? I can\u2019t move them into the other enclosure I got from them because it is an expansion unit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150x4zh", "is_robot_indexable": true, "report_reasons": null, "author": "JayVeeBee", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150x4zh/reconstructingsalvaging_a_12drive_raid_without/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150x4zh/reconstructingsalvaging_a_12drive_raid_without/", "subreddit_subscribers": 692909, "created_utc": 1689483717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have many pictures and videos of pets of mine who have since passed away and I really don't want the timestamp on each file to be reset when copying the files over; this is especially a problem when transferring from my phone. Is there any way to make the necessary copies while keeping the original time and date intact?", "author_fullname": "t2_t42gx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to keep picture/video EXIF data intact when following the 3-2-1 rule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151gch3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689538600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have many pictures and videos of pets of mine who have since passed away and I really don&amp;#39;t want the timestamp on each file to be reset when copying the files over; this is especially a problem when transferring from my phone. Is there any way to make the necessary copies while keeping the original time and date intact?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151gch3", "is_robot_indexable": true, "report_reasons": null, "author": "sunbr0_7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151gch3/how_to_keep_picturevideo_exif_data_intact_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151gch3/how_to_keep_picturevideo_exif_data_intact_when/", "subreddit_subscribers": 692909, "created_utc": 1689538600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone ever tried looking at Snopes via the WayBackMachine?  Interestingly enough, theres no archives before 2021. I find that  strange as even niche websites usually do. Did they used to go by a  different domain affix than dot com?\n\nI  found the site when I was young around 2009 and wanted to look at the  old layout and collection of articles again. I know I can just look at  the archive, but frankly Im after the old aesthetic. Does anybody have  any ideas how to access the classic Snopes?", "author_fullname": "t2_4r1t92im", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice for archiving old Snopes website data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151dqu9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689532508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever tried looking at Snopes via the WayBackMachine?  Interestingly enough, theres no archives before 2021. I find that  strange as even niche websites usually do. Did they used to go by a  different domain affix than dot com?&lt;/p&gt;\n\n&lt;p&gt;I  found the site when I was young around 2009 and wanted to look at the  old layout and collection of articles again. I know I can just look at  the archive, but frankly Im after the old aesthetic. Does anybody have  any ideas how to access the classic Snopes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151dqu9", "is_robot_indexable": true, "report_reasons": null, "author": "EI_CEO_CFT", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151dqu9/seeking_advice_for_archiving_old_snopes_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151dqu9/seeking_advice_for_archiving_old_snopes_website/", "subreddit_subscribers": 692909, "created_utc": 1689532508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long ago PC only needed one power connector to the motherboard. Then as CPU started getting more powerful, came the need for extra 12v. Some of the higher end motherboard uses 2 of the 8 pin connectors.\n\nLong ago video cards could use power from the slot. Some of the powerful AGP needed extra power and had a 4 pin connector on it. Today's high end video card uses 2 8-pin PCIe power or even 12 pin connector.\n\nSome USB cards also needed extra power for driving external USB 3.x devices at over 2A per port.\n\nI was just reading into the recently released PCIe 5.0 NVME drive, it uses a hefty 12w of power. It got me thinking, could one day we find them with an extra connector such as the old floppy disk drive connector or something to get more power?  I think the max from m.2 port is 25w?", "author_fullname": "t2_y5py1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could NVME drives one day need external power connection?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151a17m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689523596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long ago PC only needed one power connector to the motherboard. Then as CPU started getting more powerful, came the need for extra 12v. Some of the higher end motherboard uses 2 of the 8 pin connectors.&lt;/p&gt;\n\n&lt;p&gt;Long ago video cards could use power from the slot. Some of the powerful AGP needed extra power and had a 4 pin connector on it. Today&amp;#39;s high end video card uses 2 8-pin PCIe power or even 12 pin connector.&lt;/p&gt;\n\n&lt;p&gt;Some USB cards also needed extra power for driving external USB 3.x devices at over 2A per port.&lt;/p&gt;\n\n&lt;p&gt;I was just reading into the recently released PCIe 5.0 NVME drive, it uses a hefty 12w of power. It got me thinking, could one day we find them with an extra connector such as the old floppy disk drive connector or something to get more power?  I think the max from m.2 port is 25w?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "151a17m", "is_robot_indexable": true, "report_reasons": null, "author": "tomytronics", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/151a17m/could_nvme_drives_one_day_need_external_power/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151a17m/could_nvme_drives_one_day_need_external_power/", "subreddit_subscribers": 692909, "created_utc": 1689523596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i currently have 4 external hard drives all backing up various data, all with mismatched backup and stuff (I'll fix it later, this was completely unrelated to the post)\n\nare there any external hard drive bays made to store hard drives to keep them safe from physical damage? i just want a cheap solution to keeping them safe, not a costly one that connects to the internet and provides other knick knacks\n\nI'm searching for a temporary solution to keep my external HDDs safe from physical damage until i start earning enough to create better back ups\n\nfor more details:\n1x2 TB Seagate HDD (&lt;1 year old)\n2x1 TB Seagate HDD (between 1-4 years old)\n1x1.5 TB Seagate HDD (&gt;4 years old)", "author_fullname": "t2_85ypvsyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "safe storage solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1519l3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689522500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i currently have 4 external hard drives all backing up various data, all with mismatched backup and stuff (I&amp;#39;ll fix it later, this was completely unrelated to the post)&lt;/p&gt;\n\n&lt;p&gt;are there any external hard drive bays made to store hard drives to keep them safe from physical damage? i just want a cheap solution to keeping them safe, not a costly one that connects to the internet and provides other knick knacks&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m searching for a temporary solution to keep my external HDDs safe from physical damage until i start earning enough to create better back ups&lt;/p&gt;\n\n&lt;p&gt;for more details:\n1x2 TB Seagate HDD (&amp;lt;1 year old)\n2x1 TB Seagate HDD (between 1-4 years old)\n1x1.5 TB Seagate HDD (&amp;gt;4 years old)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1519l3p", "is_robot_indexable": true, "report_reasons": null, "author": "TriggeredTrigz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1519l3p/safe_storage_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1519l3p/safe_storage_solution/", "subreddit_subscribers": 692909, "created_utc": 1689522500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, Sorry for a newbie question. I did use the search, I found it hard to get past the recent Google restrictions posts, or maybe I really am the first to ask this, but I couldn't find an answer either way. I'm hoping you data storage exists can help me. \n\nI only keep about 1.5-2TB data on the paid for Google workspace thing. I have 2 accounts, one for myself and one for my wife. It's got some work stuff on but it's mostly personal crap and photos etc. I don't want to lose any of it really.\n\nIs there a way of syncing my Google data to say Wasabi or AWS or some other service without changing the way I use it now so that I have a second backup should Google lose it/ban me or whatever bad scenario might happen?\n\nDo I even need to worry about it?\n\nAny thoughts appreciated. I'm not against getting a local NAS or something if that's the best route, I'm also happy to pay another cloud provider. I care more about convenience than cost.", "author_fullname": "t2_4y08k82j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dual cloud backup, gcloud + ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1519ios", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689522332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, Sorry for a newbie question. I did use the search, I found it hard to get past the recent Google restrictions posts, or maybe I really am the first to ask this, but I couldn&amp;#39;t find an answer either way. I&amp;#39;m hoping you data storage exists can help me. &lt;/p&gt;\n\n&lt;p&gt;I only keep about 1.5-2TB data on the paid for Google workspace thing. I have 2 accounts, one for myself and one for my wife. It&amp;#39;s got some work stuff on but it&amp;#39;s mostly personal crap and photos etc. I don&amp;#39;t want to lose any of it really.&lt;/p&gt;\n\n&lt;p&gt;Is there a way of syncing my Google data to say Wasabi or AWS or some other service without changing the way I use it now so that I have a second backup should Google lose it/ban me or whatever bad scenario might happen?&lt;/p&gt;\n\n&lt;p&gt;Do I even need to worry about it?&lt;/p&gt;\n\n&lt;p&gt;Any thoughts appreciated. I&amp;#39;m not against getting a local NAS or something if that&amp;#39;s the best route, I&amp;#39;m also happy to pay another cloud provider. I care more about convenience than cost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1519ios", "is_robot_indexable": true, "report_reasons": null, "author": "jeffreyshran", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1519ios/dual_cloud_backup_gcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1519ios/dual_cloud_backup_gcloud/", "subreddit_subscribers": 692909, "created_utc": 1689522332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Current main NAS setup:\n- 2 x 3TB mirror (WS reds, new drives) for important stuff\n- 2 x 1TB stripe (8y old drives) for plex media/torrents - \n\nBackup NAS setup:\n- 1 x 4TB (WD red, new drive) Backup for the important stuff (the media drives don't have a backup)\n\nOther unused, old drives I have:\n - 3 x 1TB WD blues\n\nNow, as I don't live in the US and drive prices are all over the place I can't just purchase 3 x 16TB drives an put all the stuff on them.\n\nThe main 3TB mirror setup is half full and won't need much expanding for the years to come. \nThe media drives are what concerns me, the stripe pool (2TB in total) is almost full and expanding every month. The safety of this pool is in the negative so to say, so I need to change something.\n\nWhat to do if you want to reuse as many drives as possible?\n- Purchase new 2 x 3TB drives (mirror) just for the media stuff?\n- Purchase two very large drives (mirror), put everything on them and use the 2 x 3TB ones in the backup machine to backup everything, not just the important stuff?\n- Use a different configuration altogether (non mirror)?", "author_fullname": "t2_1hsjxygy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on old/new drives combination for max value and safety", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1514uwo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689509909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current main NAS setup:\n- 2 x 3TB mirror (WS reds, new drives) for important stuff\n- 2 x 1TB stripe (8y old drives) for plex media/torrents - &lt;/p&gt;\n\n&lt;p&gt;Backup NAS setup:\n- 1 x 4TB (WD red, new drive) Backup for the important stuff (the media drives don&amp;#39;t have a backup)&lt;/p&gt;\n\n&lt;p&gt;Other unused, old drives I have:\n - 3 x 1TB WD blues&lt;/p&gt;\n\n&lt;p&gt;Now, as I don&amp;#39;t live in the US and drive prices are all over the place I can&amp;#39;t just purchase 3 x 16TB drives an put all the stuff on them.&lt;/p&gt;\n\n&lt;p&gt;The main 3TB mirror setup is half full and won&amp;#39;t need much expanding for the years to come. \nThe media drives are what concerns me, the stripe pool (2TB in total) is almost full and expanding every month. The safety of this pool is in the negative so to say, so I need to change something.&lt;/p&gt;\n\n&lt;p&gt;What to do if you want to reuse as many drives as possible?\n- Purchase new 2 x 3TB drives (mirror) just for the media stuff?\n- Purchase two very large drives (mirror), put everything on them and use the 2 x 3TB ones in the backup machine to backup everything, not just the important stuff?\n- Use a different configuration altogether (non mirror)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1514uwo", "is_robot_indexable": true, "report_reasons": null, "author": "DAndreyD", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1514uwo/advice_on_oldnew_drives_combination_for_max_value/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1514uwo/advice_on_oldnew_drives_combination_for_max_value/", "subreddit_subscribers": 692909, "created_utc": 1689509909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I seem to have been getting error messages since Friday evening when trying to use my iDrive e2 bucket in Frankfurt?\n\nIs the full region down or just my server?\n\nI have also noticed they have reduced their yearly price to $20 for 1TB (was $40 before)", "author_fullname": "t2_lwumyho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iDrive e2 Frankfurt down since Friday evening?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15136lt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689504451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seem to have been getting error messages since Friday evening when trying to use my iDrive e2 bucket in Frankfurt?&lt;/p&gt;\n\n&lt;p&gt;Is the full region down or just my server?&lt;/p&gt;\n\n&lt;p&gt;I have also noticed they have reduced their yearly price to $20 for 1TB (was $40 before)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15136lt", "is_robot_indexable": true, "report_reasons": null, "author": "TedBob99", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15136lt/idrive_e2_frankfurt_down_since_friday_evening/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15136lt/idrive_e2_frankfurt_down_since_friday_evening/", "subreddit_subscribers": 692909, "created_utc": 1689504451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to download all photos from users albums in original HQ format. I understand there was a project but it was discontinued.\n\nAn example would be: [https://weibo.com/u/5732554220?tabtype=album](https://weibo.com/u/5732554220?tabtype=album) with an almost infinite scroll for several years of professional content.\n\nHopefully with a GUI as I'm big enough to admit I'm not savvy enough to use Gibhub projects.", "author_fullname": "t2_a7s673lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a current Weibo album downloader?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15124wf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689500833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to download all photos from users albums in original HQ format. I understand there was a project but it was discontinued.&lt;/p&gt;\n\n&lt;p&gt;An example would be: &lt;a href=\"https://weibo.com/u/5732554220?tabtype=album\"&gt;https://weibo.com/u/5732554220?tabtype=album&lt;/a&gt; with an almost infinite scroll for several years of professional content.&lt;/p&gt;\n\n&lt;p&gt;Hopefully with a GUI as I&amp;#39;m big enough to admit I&amp;#39;m not savvy enough to use Gibhub projects.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15124wf", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Distance740", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15124wf/is_there_a_current_weibo_album_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15124wf/is_there_a_current_weibo_album_downloader/", "subreddit_subscribers": 692909, "created_utc": 1689500833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My pool is a mix of duplicated and unduplicated files across 7 drives (52.8 TB total). My StableBit Scanner says I have 512 byte bad sector in one of the drives. DrivePool immediately copies unduplicated files on the drive with the bad sector across other drives. After it finishes, it did a remeasure and pool organization is green with \"file distribution not normal\". For some reason, the bad drive shows I have around 400GB unduplicated data, which I really want to backup just in case. My pool statistics shows:\n\n* 21.9 TB unduplicated\n* 24.6 TB duplicated\n* 6.05 TB unusable for duplication\n\nIs 512 byte bad sector something to worry about? Should I keep my drive or replace it? I'm not sure what to do next from here. Thank you in advance.\n\n&amp;#x200B;", "author_fullname": "t2_u50yq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "StableBit DrivePool/Scanner says I have 512 byte bad sector...what to do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15120jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689500416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My pool is a mix of duplicated and unduplicated files across 7 drives (52.8 TB total). My StableBit Scanner says I have 512 byte bad sector in one of the drives. DrivePool immediately copies unduplicated files on the drive with the bad sector across other drives. After it finishes, it did a remeasure and pool organization is green with &amp;quot;file distribution not normal&amp;quot;. For some reason, the bad drive shows I have around 400GB unduplicated data, which I really want to backup just in case. My pool statistics shows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;21.9 TB unduplicated&lt;/li&gt;\n&lt;li&gt;24.6 TB duplicated&lt;/li&gt;\n&lt;li&gt;6.05 TB unusable for duplication&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is 512 byte bad sector something to worry about? Should I keep my drive or replace it? I&amp;#39;m not sure what to do next from here. Thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15120jl", "is_robot_indexable": true, "report_reasons": null, "author": "neohanime", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15120jl/stablebit_drivepoolscanner_says_i_have_512_byte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15120jl/stablebit_drivepoolscanner_says_i_have_512_byte/", "subreddit_subscribers": 692909, "created_utc": 1689500416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm having trouble **only** wgetting this URL : [https://forum.spacehey.com/topic?id=3959](https://forum.spacehey.com/topic?id=3959) and all the links that branch out of it, only 1 level away from the link.\n\n    When I run wget -e robots=off --recursive -np -k --html-extension \"https://forum.spacehey.com/topic?id=3959\"\n\nit starts downloading all of the forum pages and all the user profiles, even those that did not participate in that specific thread. I just want \\`topic?id=3959  \n to be downloaded, plus the user profiles or links that might branch out from the link, but no deeper than 1 level. Not sure if I'm explaining myself correctly.", "author_fullname": "t2_2rewrxo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150q5z7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689462973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having trouble &lt;strong&gt;only&lt;/strong&gt; wgetting this URL : &lt;a href=\"https://forum.spacehey.com/topic?id=3959\"&gt;https://forum.spacehey.com/topic?id=3959&lt;/a&gt; and all the links that branch out of it, only 1 level away from the link.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;When I run wget -e robots=off --recursive -np -k --html-extension &amp;quot;https://forum.spacehey.com/topic?id=3959&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;it starts downloading all of the forum pages and all the user profiles, even those that did not participate in that specific thread. I just want `topic?id=3959&lt;br/&gt;\n to be downloaded, plus the user profiles or links that might branch out from the link, but no deeper than 1 level. Not sure if I&amp;#39;m explaining myself correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?auto=webp&amp;s=3857ed03a465a31599b65c323030194450e8bc09", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=722e416e8ee490729a17976e9f4bbe7c3e078871", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f35418a5521952a5db5b0452b454e3a32b03a46", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d16b9e2c1149bb5979da229871a45ec1d23d7dfc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=536746646c1dded870ef8ae904c9414d295e6926", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=acee12e384beec774f373142492cf49d80ee2300", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0y8CB3eptKH-IvGYyo1L3CsMuqXRNLabvt1qH36-QxU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8453b2a3662b55ed0db129bc72c0d660403065ee", "width": 1080, "height": 540}], "variants": {}, "id": "ztoUwoXJpe58a61I-q3Dg8eJzueXuf-NZZ1WQ20Kpgo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150q5z7", "is_robot_indexable": true, "report_reasons": null, "author": "iPodClassic7", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150q5z7/need_help_with_wget/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150q5z7/need_help_with_wget/", "subreddit_subscribers": 692909, "created_utc": 1689462973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Why does the TerraMaster D4-300 consume more power than that of a low-powered NAS solution?\n\nFor example:\n\nThe TerraMaster D4-300 is a simple DAS (no CPU -- it's just a simple HDD docking station) has a power draw of around 45.6W. Whereas, a low-powered ARM-based NAS, like the Synology DS463 power draw ranges from  4.97W to 32.41W. Heck, even the DS463+ (which features an x86-based CPU) seems to use less power than the DAS with a range of 8W to 28W.\n\n&gt; **NOTE:** All of these devices support up to four (4) 3.5\" HDDs within their enclusures.", "author_fullname": "t2_429x3s08", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple DAS consumes more power than a NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151cyko", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689530643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why does the TerraMaster D4-300 consume more power than that of a low-powered NAS solution?&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;p&gt;The TerraMaster D4-300 is a simple DAS (no CPU -- it&amp;#39;s just a simple HDD docking station) has a power draw of around 45.6W. Whereas, a low-powered ARM-based NAS, like the Synology DS463 power draw ranges from  4.97W to 32.41W. Heck, even the DS463+ (which features an x86-based CPU) seems to use less power than the DAS with a range of 8W to 28W.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; All of these devices support up to four (4) 3.5&amp;quot; HDDs within their enclusures.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151cyko", "is_robot_indexable": true, "report_reasons": null, "author": "etho201", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151cyko/simple_das_consumes_more_power_than_a_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151cyko/simple_das_consumes_more_power_than_a_nas/", "subreddit_subscribers": 692909, "created_utc": 1689530643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_eqim9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A small tool I wrote to rename files and keep them under a specified byte length", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_151c8dq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/u876Jb36vi1KZ77Etm_0g8WqUGWAf_dUHLOhVfZ_NMw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689528934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/mashariqk/rnx", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?auto=webp&amp;s=434fd8e1ace017eb9306385ad3211773f41971eb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f44ec9a94b7307cabbcd70ac9967bb1d3da0ae82", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=096b0a36ce29820cc1811886e81d20b98c17c2f5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b85613a33e7e4de4b0a83a93e589ef6804586225", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=05064e18ac978f43eb11966a41f57d5e948e21e6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0dd4870ddc4b84008b4d21143fbb6827f4f63d5b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QZIiwiSyJ6pmZjqVK9nX7DeR617lvb5eEFVxD_7gA2s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=572f6fed2dcd592fb257cd587cf16e0ec5ae970c", "width": 1080, "height": 540}], "variants": {}, "id": "N5KjDDYIOFMLeNApx-JOg7PfTvgd3LNwev2zj9E40_Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151c8dq", "is_robot_indexable": true, "report_reasons": null, "author": "mashariq", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151c8dq/a_small_tool_i_wrote_to_rename_files_and_keep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/mashariqk/rnx", "subreddit_subscribers": 692909, "created_utc": 1689528934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m looking for recommendations for 2TB, 4TB and 8TB SATA SSD\u2019s due to the whole Samsung SSD failure issues.", "author_fullname": "t2_15jrzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best SATA ssd to buy at the moment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151bxoi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689528215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for recommendations for 2TB, 4TB and 8TB SATA SSD\u2019s due to the whole Samsung SSD failure issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151bxoi", "is_robot_indexable": true, "report_reasons": null, "author": "xkcx123", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151bxoi/what_is_the_best_sata_ssd_to_buy_at_the_moment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151bxoi/what_is_the_best_sata_ssd_to_buy_at_the_moment/", "subreddit_subscribers": 692909, "created_utc": 1689528215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I did a quick search of /r/datahoarder and couldn\u2019t find any mention of this horrible restriction. \n\nReddit app used to save the full resolution image. Now, clicking \u201cdownload\u201d in the mobile app: \n\n* saves a JPEG to the camera roll with a maximum of 1164 px on the long edge\n* adds a huge title margin, ~100 px: \u201cFrom X community on Reddit\u201d\n* adds large ~50 px side and bottom margin\n* adds a large, orange Reddit logo (size is at least ~100 px, and transparency at least 50%)\n\nIs this why they\u2019re killing the third party apps? To foist this nonsense on us? \n\n\n*EDIT: wasn\u2019t sure entirely where to post this, but I figured it\u2019s undoubtedly an important topic to my fellow hoarders.*", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving images from the Reddit mobile app: Reddit is downscaling images to 1164 px on the long edge, AND adding destructive watermarking. WTAF!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151bv0h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689528339.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689528041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did a quick search of &lt;a href=\"/r/datahoarder\"&gt;/r/datahoarder&lt;/a&gt; and couldn\u2019t find any mention of this horrible restriction. &lt;/p&gt;\n\n&lt;p&gt;Reddit app used to save the full resolution image. Now, clicking \u201cdownload\u201d in the mobile app: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;saves a JPEG to the camera roll with a maximum of 1164 px on the long edge&lt;/li&gt;\n&lt;li&gt;adds a huge title margin, ~100 px: \u201cFrom X community on Reddit\u201d&lt;/li&gt;\n&lt;li&gt;adds large ~50 px side and bottom margin&lt;/li&gt;\n&lt;li&gt;adds a large, orange Reddit logo (size is at least ~100 px, and transparency at least 50%)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is this why they\u2019re killing the third party apps? To foist this nonsense on us? &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EDIT: wasn\u2019t sure entirely where to post this, but I figured it\u2019s undoubtedly an important topic to my fellow hoarders.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151bv0h", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151bv0h/saving_images_from_the_reddit_mobile_app_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151bv0h/saving_images_from_the_reddit_mobile_app_reddit/", "subreddit_subscribers": 692909, "created_utc": 1689528041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an 6 yrs old WD 2TB HDD, dropped it by accident , still spinning no signs of head crash or any physical damage but does not show up . I was looking if it could be fixed and how much would it take . The data is really important but I cant really cannot spend too much for data extraction ", "author_fullname": "t2_brtfbspo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with a crashed HDD, No head crash or physical damage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_151avoh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689525683.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an 6 yrs old WD 2TB HDD, dropped it by accident , still spinning no signs of head crash or any physical damage but does not show up . I was looking if it could be fixed and how much would it take . The data is really important but I cant really cannot spend too much for data extraction &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "151avoh", "is_robot_indexable": true, "report_reasons": null, "author": "Top_Top_315", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/151avoh/need_help_with_a_crashed_hdd_no_head_crash_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/151avoh/need_help_with_a_crashed_hdd_no_head_crash_or/", "subreddit_subscribers": 692909, "created_utc": 1689525683.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using Storage Spaces for years to store relatively unimportant data and it's worked fine with two 18TB internal drives and two 8TB external drives. I was finally running out of room so I decided to add two new 18TB drives attached via a USB Docking Station. Got the new drives/docking station, started to optimize, and immediately started getting errors that one of the old drives had failed. \n\nFast-forward a bit and I have to stop optimization, and after stopping it and reconnecting all drives, all the drives  read as \"OK.\" So I start trying to optimize again. This time things go even worse. One of the old drives read as \"failed\" and, for some reason, Storage Spaces automatically prepares it to remove even though I didn't tell it to, and I get a \"warning\" on one of the new drives. So optimization completes, and I try unplugging/replugging everything again, but this time it doesn't work. The old drive will only read \"ready to remove\" and new drives now both have Warning marks. So I remove the old drive and eventually reconnnect it using a different USB port. Now getting the message \"Error Unrecognized Configuration Reset Drive\" which, according to a Google search, will wipe the drive. \n\nMy Storage Spaces volume is still accessible but a lot of data is missing/inaccessible. Most of it I could get back if I wanted to put in the time/effort, but I'd be willing to pay some if I could recover it without having to manually find/download everything I'd saved, but I don't know what my options are. Any advice/recommendations? I know of, eg, ReClaime, but it's like $300 and I have no idea how successful it would be. I've got to think the data is recoverable given that none of the drives have actually failed and the data is still there. ", "author_fullname": "t2_b04kmo9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces Fails to Optimize: Data Lost? Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150zkpk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689491866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Storage Spaces for years to store relatively unimportant data and it&amp;#39;s worked fine with two 18TB internal drives and two 8TB external drives. I was finally running out of room so I decided to add two new 18TB drives attached via a USB Docking Station. Got the new drives/docking station, started to optimize, and immediately started getting errors that one of the old drives had failed. &lt;/p&gt;\n\n&lt;p&gt;Fast-forward a bit and I have to stop optimization, and after stopping it and reconnecting all drives, all the drives  read as &amp;quot;OK.&amp;quot; So I start trying to optimize again. This time things go even worse. One of the old drives read as &amp;quot;failed&amp;quot; and, for some reason, Storage Spaces automatically prepares it to remove even though I didn&amp;#39;t tell it to, and I get a &amp;quot;warning&amp;quot; on one of the new drives. So optimization completes, and I try unplugging/replugging everything again, but this time it doesn&amp;#39;t work. The old drive will only read &amp;quot;ready to remove&amp;quot; and new drives now both have Warning marks. So I remove the old drive and eventually reconnnect it using a different USB port. Now getting the message &amp;quot;Error Unrecognized Configuration Reset Drive&amp;quot; which, according to a Google search, will wipe the drive. &lt;/p&gt;\n\n&lt;p&gt;My Storage Spaces volume is still accessible but a lot of data is missing/inaccessible. Most of it I could get back if I wanted to put in the time/effort, but I&amp;#39;d be willing to pay some if I could recover it without having to manually find/download everything I&amp;#39;d saved, but I don&amp;#39;t know what my options are. Any advice/recommendations? I know of, eg, ReClaime, but it&amp;#39;s like $300 and I have no idea how successful it would be. I&amp;#39;ve got to think the data is recoverable given that none of the drives have actually failed and the data is still there. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "150zkpk", "is_robot_indexable": true, "report_reasons": null, "author": "Suspicious_War5435", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/150zkpk/storage_spaces_fails_to_optimize_data_lost_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/150zkpk/storage_spaces_fails_to_optimize_data_lost_help/", "subreddit_subscribers": 692909, "created_utc": 1689491866.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}