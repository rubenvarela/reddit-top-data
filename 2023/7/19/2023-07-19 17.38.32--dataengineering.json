{"kind": "Listing", "data": {"after": "t3_1537lly", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uu62zwts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fact", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_153o48v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 113, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 113, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kn2VeRL7dKE02ma838cuSRPnLHQrVqVdvja0DMkus7Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689752236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/glkf1eivkvcb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/glkf1eivkvcb1.jpg?auto=webp&amp;s=e99447cdb12d56a8fcb22529774b09d0e98e9a28", "width": 552, "height": 414}, "resolutions": [{"url": "https://preview.redd.it/glkf1eivkvcb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6916d3d15394f881e4d3c9973b868d60ee73f5e1", "width": 108, "height": 81}, {"url": "https://preview.redd.it/glkf1eivkvcb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f5d589dbfbac864bff82915ab383a8ac0b23546", "width": 216, "height": 162}, {"url": "https://preview.redd.it/glkf1eivkvcb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4b63873775811f460e2ddc4aa21baadabb2fd36", "width": 320, "height": 240}], "variants": {}, "id": "57DxVeWB5qdgJSeyFIqARGyquEhznuPKWtiu27RO7ek"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "153o48v", "is_robot_indexable": true, "report_reasons": null, "author": "Sailja_Jain", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153o48v/fact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/glkf1eivkvcb1.jpg", "subreddit_subscribers": 116782, "created_utc": 1689752236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR - you mini-interview, we donate: [VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) \n\nI'm a founder at a small data startup, and we are looking for input from Data Professionals related to the problem our tool is solving; I am a DE myself and am too close to our solution to be objective. \n\n**We decided instead of paying a marketing research group, we would take a shot with the communities we are part of (like this one) and donate our small research budget to good causes instead.** (mini-interview Link is above, here it is again):\n\n[VideoAsk Mini-Interview](https://www.videoask.com/fisb43vgs) \n\nFor every completed (applicable) mini-interview we'll donate $5 to either [code.org](https://code.org) or [phillyPAWS](https://phillypaws.org/) (our local rescue here in Philadelphia) until we run out of $$ (we will shut the survey down before then, no free lunches on our part).   \nby \"applicable\" we mean the respondent is either a data or software professional or works directly with data/software professionals.", "author_fullname": "t2_dilsfh4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For every Data Professional mini-interview we'll donate $5 to dogs or code.org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537puk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689731948.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689708241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR - you mini-interview, we donate: &lt;a href=\"https://www.videoask.com/fisb43vgs\"&gt;VideoAsk Mini-Interview&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a founder at a small data startup, and we are looking for input from Data Professionals related to the problem our tool is solving; I am a DE myself and am too close to our solution to be objective. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We decided instead of paying a marketing research group, we would take a shot with the communities we are part of (like this one) and donate our small research budget to good causes instead.&lt;/strong&gt; (mini-interview Link is above, here it is again):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.videoask.com/fisb43vgs\"&gt;VideoAsk Mini-Interview&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;For every completed (applicable) mini-interview we&amp;#39;ll donate $5 to either &lt;a href=\"https://code.org\"&gt;code.org&lt;/a&gt; or &lt;a href=\"https://phillypaws.org/\"&gt;phillyPAWS&lt;/a&gt; (our local rescue here in Philadelphia) until we run out of $$ (we will shut the survey down before then, no free lunches on our part).&lt;br/&gt;\nby &amp;quot;applicable&amp;quot; we mean the respondent is either a data or software professional or works directly with data/software professionals.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?auto=webp&amp;s=54eb457a178d1a2f8d5b8ec0c59601357ed9c5df", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47391c61e0302e14c4c7a003ea94a70c5327fc17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=885c32075812ee41c63f825a070b6967f1e016a0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf00bd66185d4fc18e7211516a5619f9c01e1fd8", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac037a629514a6c2f4b6b4fde66a556edb2db7d3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44882261d6778024f5348d95f10764cda7f9b0bb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/vAgrqUzBXR6rK9eslleCs9MhwVsp2LtVbgk5LNAGvf8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aca94bf69b7fd3c42ff2d3307257d37643e08a74", "width": 1080, "height": 567}], "variants": {}, "id": "_eefME0VHeiJVwdc9ToiHwgOJ9nypQUidjH9mzYLfys"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1537puk", "is_robot_indexable": true, "report_reasons": null, "author": "sprintymcsprintface", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537puk/for_every_data_professional_miniinterview_well/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537puk/for_every_data_professional_miniinterview_well/", "subreddit_subscribers": 116782, "created_utc": 1689708241.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bridging the gap between IaC and Schema Management | Atlas | Open-source database schema management tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_153w8gb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jZyQUE5tMBKtFY7M9aXlkkguJO444ur05EqfqatbPEk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689776056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/07/19/bridging-the-gap-between-iac-and-schema-management", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?auto=webp&amp;s=d799bf3fbbae4d3e7bc4d96e463d8e63e27e64ca", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=821b930b0430b20ba6a62db39d4d179c6775190c", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9fbbcb84119df711bf3c224456043f49e0754c7", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d40963da30fab9e04cd5211a505f5d369ac86816", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11896f760a027c746ccdf72422c45e0e015d904e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=db05a694a3196fce042cf43d00949cad2c718748", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/LOWDQxtZoqPT6tKFrPYjlEmKFKLCXTrKMLW77pNWhhY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2495a9f7e1ab3bdaa6d05269c4982331f873e518", "width": 1080, "height": 607}], "variants": {}, "id": "_O7gzcuYqhIrhryNdtPkpLoNou7--_Y8M3r_4gmAu2M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "153w8gb", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153w8gb/bridging_the_gap_between_iac_and_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/07/19/bridging-the-gap-between-iac-and-schema-management", "subreddit_subscribers": 116782, "created_utc": 1689776056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ummm weird question. At this point I dream in SQL sometimes and think I might think in it as much as I think in English. I think most lead data engineers know it super well.", "author_fullname": "t2_57mn29f0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job application for a lead role \u201cPlease expand more on your experience with SQL\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153llcj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689744004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ummm weird question. At this point I dream in SQL sometimes and think I might think in it as much as I think in English. I think most lead data engineers know it super well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "153llcj", "is_robot_indexable": true, "report_reasons": null, "author": "k_dani_b", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153llcj/job_application_for_a_lead_role_please_expand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153llcj/job_application_for_a_lead_role_please_expand/", "subreddit_subscribers": 116782, "created_utc": 1689744004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparing Data Parallel, Task Parallel, and Agent Actor Architectures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1539nyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TyLkTvEB3pDv-Glcl8NxhzIoTyDydo0-t0adN9VrevQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689712664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytewax.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?auto=webp&amp;s=1ab273c790a3deab79b9a96b2a8963b849d81e2d", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=989bfe391b25edca2ffd8058f2818e412bd14ecf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ef71b43c5a4c3ed4df39f3097636a5395e92277", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebf82c1b4d733db4980b7d2a96d70bdb93348989", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=280e99d12ebe57c09258c29198d5118563158406", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e6bcdd642fb93b12d25cb210e0ead6a96438c68", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/D_vY5hbDPwfqVuDh6ctWR2CwNmMk5ddz-mgtgJqdnFM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fa4f2ed27dbc2e064f10e1af673ea9e97b8f1d0", "width": 1080, "height": 564}], "variants": {}, "id": "_GAQl_RTzqMiEFZm4ZiVzQqqGTrkugbg6LCg3kt5mlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1539nyl", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1539nyl/comparing_data_parallel_task_parallel_and_agent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bytewax.io/blog/data-parallel-task-parallel-and-agent-actor-architectures", "subreddit_subscribers": 116782, "created_utc": 1689712664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm currently managing a data swamp, and working to make it clean and reliable.\n\nWe have a lot of raw data as JSONs, that we need to store and be able to manipulate on our Cloudera Data Platform (on top of HDFS).\n\nWhat would be the best way to store them on our cluster, in order to access them afterwards in our data preparation ?\n\nAtm, we are inserting them transformed in Apache Hive, but we really want to store them raw and manipulate them from a more appropriate DBMS / Document Manager", "author_fullname": "t2_7odlp6k1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing JSONs as raw data in CDP data lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15396op", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689711563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently managing a data swamp, and working to make it clean and reliable.&lt;/p&gt;\n\n&lt;p&gt;We have a lot of raw data as JSONs, that we need to store and be able to manipulate on our Cloudera Data Platform (on top of HDFS).&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to store them on our cluster, in order to access them afterwards in our data preparation ?&lt;/p&gt;\n\n&lt;p&gt;Atm, we are inserting them transformed in Apache Hive, but we really want to store them raw and manipulate them from a more appropriate DBMS / Document Manager&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15396op", "is_robot_indexable": true, "report_reasons": null, "author": "TheSoulHokib", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15396op/managing_jsons_as_raw_data_in_cdp_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15396op/managing_jsons_as_raw_data_in_cdp_data_lake/", "subreddit_subscribers": 116782, "created_utc": 1689711563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In your opinion - what' s the threshold of diminishing returns as far as postgres/database knowledge goes?\n\nFor context Im currently working as a data engineer and I've ended up doing most of the work on the database (it's a startup...). This is giving me experience with data modelling, schema design, writing methods on the database, performance monitoring and so on. \n\nI'm on the fence as to how useful this experience is, on the one hand I feel it's pretty valuable and can be hard to get  (most people _probably_ wouldn't have wanted me managing their prod db...). On the other - I'm _not_ doing things such as dataflows, pyspark and so on. My python is fairly solid mid level (package publishing, reasonable code/data patterns/structures, testing etc), so I'm not lacking ability to write python - I just haven't been working with cloud / distributed stuff.  \n\nObviously data engineering is a very broad brush - but I'm wondering what sort of database experience / knowledge people typically have here _beyond_ querying effectively and how valuable they feel it is. Do you have experience setting up postgres on a server from scratch, create and test backup&amp;restore, handle load balancing, bastion server etc etc... if not, do you care?\n\nSo - In your opinion - what' s the threshold of diminishing returns as far as postgres/databases goes?  \n\nThanks! :)", "author_fullname": "t2_tczfts4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How important is database knowledge beyond querying? At what point are returns diminishing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153qe9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689762841.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689760055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In your opinion - what&amp;#39; s the threshold of diminishing returns as far as postgres/database knowledge goes?&lt;/p&gt;\n\n&lt;p&gt;For context Im currently working as a data engineer and I&amp;#39;ve ended up doing most of the work on the database (it&amp;#39;s a startup...). This is giving me experience with data modelling, schema design, writing methods on the database, performance monitoring and so on. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on the fence as to how useful this experience is, on the one hand I feel it&amp;#39;s pretty valuable and can be hard to get  (most people &lt;em&gt;probably&lt;/em&gt; wouldn&amp;#39;t have wanted me managing their prod db...). On the other - I&amp;#39;m &lt;em&gt;not&lt;/em&gt; doing things such as dataflows, pyspark and so on. My python is fairly solid mid level (package publishing, reasonable code/data patterns/structures, testing etc), so I&amp;#39;m not lacking ability to write python - I just haven&amp;#39;t been working with cloud / distributed stuff.  &lt;/p&gt;\n\n&lt;p&gt;Obviously data engineering is a very broad brush - but I&amp;#39;m wondering what sort of database experience / knowledge people typically have here &lt;em&gt;beyond&lt;/em&gt; querying effectively and how valuable they feel it is. Do you have experience setting up postgres on a server from scratch, create and test backup&amp;amp;restore, handle load balancing, bastion server etc etc... if not, do you care?&lt;/p&gt;\n\n&lt;p&gt;So - In your opinion - what&amp;#39; s the threshold of diminishing returns as far as postgres/databases goes?  &lt;/p&gt;\n\n&lt;p&gt;Thanks! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153qe9h", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Fix2471", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153qe9h/how_important_is_database_knowledge_beyond/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153qe9h/how_important_is_database_knowledge_beyond/", "subreddit_subscribers": 116782, "created_utc": 1689760055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How are you solving data analytics and data management considering a decentralized microservices ecosystem for operational data\n\nAre you using any tool for centralized data governance on the operational side?", "author_fullname": "t2_hpfuccl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microservices and DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153clpb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689719449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are you solving data analytics and data management considering a decentralized microservices ecosystem for operational data&lt;/p&gt;\n\n&lt;p&gt;Are you using any tool for centralized data governance on the operational side?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153clpb", "is_robot_indexable": true, "report_reasons": null, "author": "ubiquae", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153clpb/microservices_and_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153clpb/microservices_and_de/", "subreddit_subscribers": 116782, "created_utc": 1689719449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.  \nMergers and acquisitions (M&amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.\n\n  \nMergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;A, and provide tools for addressing them effectively.  \nDuring and after M&amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:\n\n# 1. Integrating disparate systems and platforms\n\nIntegrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.\n\nHere are some of the key issues that arise during this integration process:\n\n* Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;\n* Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;\n* Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;\n* Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;\n* User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;\n* System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.\n\n# 2. Reconciling different data formats and standards\n\nWhen two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.\n\n# 3. Significant differences in the technological maturity between the two companies\n\nIf one of the companies has a significant technological advantage over the other, several issues may arise:\n\n* Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;\n* Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;\n* Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.\n\n# 4. Considering the human element\n\nThis is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;A.  \nThe problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.  \nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.\n\n  \nMerging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.\n\n  \nIn fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:\n\n1. Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;\n2. Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;\n3. Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;\n4. Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.\n\nOther issues can be solved via the right tool that addresses every issue mentioned.\n\n# How to effectively address data-related M&amp;A challenges?\n\nTo effectively address the challenges posed by M&amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.  \nA central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.  \nBy transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.  \nMoreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.\n\n# No-Code Data Solutions\n\nNo-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;A right away.\n\nFurthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.\n\nIn addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;A process.\n\nThe rest of my research can be found here: [https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm\\_source=linkedin&amp;utm\\_medium=social&amp;utm\\_campaign=statistics&amp;utm\\_content=migrations\\_acquistions&amp;utm\\_term=ITarchitecture](https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=statistics&amp;utm_content=migrations_acquistions&amp;utm_term=ITarchitecture)", "author_fullname": "t2_ct09rz3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data management challenges in M&amp;A", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1539422", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689711385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has recently gone through a merger. My team and I, an executive of the company, faced many issues that took too much time to solve. That got me thinking: how many other executives and team leads out there will have to struggle through the same issues, and how can I help them prepare? This is why I gathered some useful information that I believe could be useful for many people.&lt;br/&gt;\nMergers and acquisitions (M&amp;amp;A) bring about significant shifts in operations and culture for employees and the impact of data-related challenges on these aspects is frequently overlooked. Ensure you or your company are not making the same mistake.&lt;/p&gt;\n\n&lt;p&gt;Mergers and acquisitions often trigger substantial organizational uncertainty about what lies ahead: typically, the operational model and culture undergo significant transformations for one or both of the merging entities. These alterations extend beyond just a new name and executive leadership; first and foremost, M&amp;amp;A change the way IT teams work. Anticipating and addressing these IT challenges can pave the way for smooth and efficient integration. Conversely, failure to foresee and manage these issues can result in poor business performance, attrition of key talent, and potential data breaches. In this article, experts from AINSYS delve into the IT complications that can emerge during and post M&amp;amp;A, and provide tools for addressing them effectively.&lt;br/&gt;\nDuring and after M&amp;amp;A, two companies\u2019 tech stacks essentially collide, and IT management has to deal with a number of challenges:&lt;/p&gt;\n\n&lt;h1&gt;1. Integrating disparate systems and platforms&lt;/h1&gt;\n\n&lt;p&gt;Integrating disparate systems and platforms is one of the most significant challenges during mergers and acquisitions. When two companies come together, they often bring with them different IT systems and platforms that have been tailored to their specific business processes and needs. These systems can range from customer relationship management (CRM) and enterprise resource planning (ERP) systems, to data management platforms, financial systems, and more.&lt;/p&gt;\n\n&lt;p&gt;Here are some of the key issues that arise during this integration process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Compatibility: the systems used by the merging companies may not be compatible. They may be built on different architectures, use different data formats, or be based on different technologies. This can make integration a complex and time-consuming process;&lt;/li&gt;\n&lt;li&gt;Data Consistency: each system may have its own way of storing and structuring data. Ensuring consistency in data definitions, formats, and structures across all systems is crucial to avoid confusion and errors in data analysis and decision-making;&lt;/li&gt;\n&lt;li&gt;Redundancy: there may be significant overlap in the functionality of the systems used by the two companies, leading to redundancy. Identifying and eliminating these redundancies is important to avoid unnecessary costs and complexity;&lt;/li&gt;\n&lt;li&gt;Security and Compliance: each system will have its own security measures and compliance requirements. Ensuring that the integrated system meets all necessary security standards and regulatory requirements is crucial;&lt;/li&gt;\n&lt;li&gt;User Training: Employees from each company will be familiar with their own systems, but may need training to use the new integrated system effectively;&lt;/li&gt;\n&lt;li&gt;System Performance: Integrating two systems can put a strain on the performance of the IT infrastructure. Ensuring the integrated system performs effectively without causing downtime or delays is important.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;2. Reconciling different data formats and standards&lt;/h1&gt;\n\n&lt;p&gt;When two companies merge, they bring with them different data systems that have been tailored to their specific needs and processes. These systems may use different data formats and adhere to different data standards, which can create several issues during integration, including data inconsistency, data loss (if one system uses a data format that the other system does not support, some data may be lost during the conversion process, leading to incomplete or inaccurate data in the integrated system), data quality (one system might allow for missing values in certain fields, while the other does not), and compliance issues.&lt;/p&gt;\n\n&lt;h1&gt;3. Significant differences in the technological maturity between the two companies&lt;/h1&gt;\n\n&lt;p&gt;If one of the companies has a significant technological advantage over the other, several issues may arise:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Integration Complexity: if one company uses modern, cloud-based systems while the other still relies on legacy systems, integrating the two can be complex and time-consuming. It may require significant resources to upgrade or replace outdated systems;&lt;/li&gt;\n&lt;li&gt;Operational Disruptions: if one company\u2019s systems are automated and the other\u2019s are manual, it could disrupt business processes and workflows until the less advanced systems are upgraded;&lt;/li&gt;\n&lt;li&gt;Security Risks: Older, less advanced systems may have more vulnerabilities, increasing the risk of data breaches or cyberattacks. It\u2019s crucial to assess and address these risks as part of the integration process.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;4. Considering the human element&lt;/h1&gt;\n\n&lt;p&gt;This is perhaps the most complex and difficult issue to solve, seeing as no specialist is alike and supporting employees through change in M&amp;amp;A is the most important matter you will need to solve. Why is that? Well, the reason is that the success of any merger or acquisition is not just about integrating systems and processes, but also about bringing together people from different organizational cultures. Employees are the backbone of any organization, and their acceptance and adaptation to change can significantly impact the outcome of the M&amp;amp;A.&lt;br/&gt;\nThe problem is, employees are conservative and don\u2019t want to switch from systems they have been using for years, having to learn new skills to actually use them. Additionally, companies often have to hire additional employees or IT consultants in order to support them through the process.&lt;br/&gt;\nAnd it is not like executives can allow their employees to take their sweet time to figure out a whole new way of doing things \u2013 they need to start profiting from the merger or acquisition as soon as possible.&lt;/p&gt;\n\n&lt;p&gt;Merging or acquiring companies must shift the day-to-day behavior and mind-sets of their employees to protect a deal\u2019s sources of value, both financial and organizational, and to make changes sustainable. Yet when McKinsey asked 3,199 leaders if they regarded the change programs at their own companies as successful, only one-third did.&lt;/p&gt;\n\n&lt;p&gt;In fact, comprehensive change management strategies are vital in easing transitions during mergers and acquisitions (M&amp;amp;As), as they help increase acceptance among employees and minimize potential disruption. Here\u2019s how these strategies can be applied:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Implement Surveys for In-depth Understanding: confidential surveys can encourage employees to express concerns they might not voice in person. These surveys should be designed to address specific issues identified during the interviews, such as opportunities for promotion. The responses can provide a more detailed understanding of the company\u2019s situation;&lt;/li&gt;\n&lt;li&gt;Compile and Present a Detailed Report to Stakeholders: the findings from the interviews and surveys should be compiled into a comprehensive report. This report should highlight both the positive aspects and potential issues within the company, such as knowledge gaps or cultural problems. The report should be presented to stakeholders in a clear and concise manner;&lt;/li&gt;\n&lt;li&gt;Include Personalized Action Plans in the Report: the report should also contain personalized action plans for key tasks. This assigns responsibility and accountability, ensuring that important tasks are not overlooked. A well-implemented action plan can drive value-generating changes in the company;&lt;/li&gt;\n&lt;li&gt;Perform a Potential Resistance Analysis: a potential resistance analysis can identify potential obstacles within both organizations involved in the acquisition. This step should avoid creating a divisive \u2018us versus them\u2019 mentality. Instead, it should focus on understanding employee concerns, perceived negatives of the deal, and their vision of their future roles in the company. This analysis can help drive the acquisition forward in a positive and inclusive manner.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Other issues can be solved via the right tool that addresses every issue mentioned.&lt;/p&gt;\n\n&lt;h1&gt;How to effectively address data-related M&amp;amp;A challenges?&lt;/h1&gt;\n\n&lt;p&gt;To effectively address the challenges posed by M&amp;amp;A, a comprehensive approach is required that addresses not only the technical aspects but also the human element. A no-code data sync and integration solution that serves as a central source of truth can be the key to this approach. This solution would transform data into a uniform format and securely store it in a single data warehouse, simplifying the integration process and ensuring data consistency and security.&lt;br/&gt;\nA central source of truth (CSOT) is a concept in data management that refers to a single, authoritative source of data that everyone in the organization agrees is the real, trusted number. In the context of M&amp;amp;A, a CSOT can help eliminate data inconsistencies and redundancies, streamline data management, and ensure all employees have access to the same, accurate data.&lt;br/&gt;\nBy transforming all data into the same format, a CSOT can help to overcome the challenges of integrating disparate systems and reconciling different data formats and standards. No code aspect of the tool can significantly reduce the complexity and time required for data integration by involving business in the process. This ensures that all data is consistent and reliable.&lt;br/&gt;\nMoreover, by storing all data securely in one data warehouse, a CSOT can help to maintain data privacy and security during and after the M&amp;amp;A process. This can help to prevent data breaches and ensure compliance with all relevant regulations.&lt;/p&gt;\n\n&lt;h1&gt;No-Code Data Solutions&lt;/h1&gt;\n\n&lt;p&gt;No-code data solutions can play a crucial role in integrating two systems. These solutions allow users to manage and manipulate data without needing to write any code, making them accessible to both IT specialists and employees with no coding experience. This can help to speed up the integration process and enable companies to start profiting from M&amp;amp;A right away.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, by enabling non-technical employees to work with data, no-code solutions can help to address the human element of M&amp;amp;A. They can get more employees on board with adopting new systems and for them to adapt to new workflows. This can help to reduce resistance to change and increase employee engagement and productivity.&lt;/p&gt;\n\n&lt;p&gt;In addition, no-code solutions can reduce the need to hire IT consultants or spend time figuring out the best IT architecture option. This can result in significant cost savings and allow companies to focus their resources on other aspects of the M&amp;amp;A process.&lt;/p&gt;\n\n&lt;p&gt;The rest of my research can be found here: &lt;a href=\"https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=statistics&amp;amp;utm_content=migrations_acquistions&amp;amp;utm_term=ITarchitecture\"&gt;https://ainsys.com/blog/2023/06/12/data-management-challenges-in-ma/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=statistics&amp;amp;utm_content=migrations_acquistions&amp;amp;utm_term=ITarchitecture&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I3xJj8ELsQOmy2H-NeHKFmGnh2tKVfHX1i7SJ1O1qwE.jpg?auto=webp&amp;s=8a29f85fd320507a70ec9694b971f494352d2655", "width": 79, "height": 86}, "resolutions": [], "variants": {}, "id": "fKkAw45B6Aa1K9gfC0CvmXNzCjb0F-J2wvKUvaKf1Vk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1539422", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Speech36", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1539422/data_management_challenges_in_ma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1539422/data_management_challenges_in_ma/", "subreddit_subscribers": 116782, "created_utc": 1689711385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've done 3 different personal projects this month, and each one I run into thousands of problems with AWS, Azure, or GCP. I'm not trying to be a DevOps person but with how many cloud issues I've resolved, maybe I should be. Does anyone else feel these limitations? I just wanna get to the point on projects where I can utilize my SQL or Python without wading through 4 hours of cloud initialization\n\nIt feels like on all these projects, I spend 95% of the time working out issues with the cloud development and getting that running, and 5% on the actual transformations and orchestrations between the services once they're running.", "author_fullname": "t2_l3oy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Data Engineering ever stop running into issues with the cloud service provider?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153i3me", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689734057.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689733696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve done 3 different personal projects this month, and each one I run into thousands of problems with AWS, Azure, or GCP. I&amp;#39;m not trying to be a DevOps person but with how many cloud issues I&amp;#39;ve resolved, maybe I should be. Does anyone else feel these limitations? I just wanna get to the point on projects where I can utilize my SQL or Python without wading through 4 hours of cloud initialization&lt;/p&gt;\n\n&lt;p&gt;It feels like on all these projects, I spend 95% of the time working out issues with the cloud development and getting that running, and 5% on the actual transformations and orchestrations between the services once they&amp;#39;re running.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153i3me", "is_robot_indexable": true, "report_reasons": null, "author": "LeftShark", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153i3me/does_data_engineering_ever_stop_running_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153i3me/does_data_engineering_ever_stop_running_into/", "subreddit_subscribers": 116782, "created_utc": 1689733696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What do you all use to monitor your ingestion pipelines in databricks?\n\nWe currently use Azure Log Analytics/Azure monitor but the SparkListener library for Log Analytics sends Spark performance metrics. \n\nBeen using databricks notebooks that do some aggregations and push data through to Log Analytics as a custom SDK rig using log analytics and data collector API's. \n\nNot great, logs are slow to arrive in LA and the authorization headers and JSON responses get fiddly when data structures differ.", "author_fullname": "t2_r7bmwiee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Monitoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537ll0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689707981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you all use to monitor your ingestion pipelines in databricks?&lt;/p&gt;\n\n&lt;p&gt;We currently use Azure Log Analytics/Azure monitor but the SparkListener library for Log Analytics sends Spark performance metrics. &lt;/p&gt;\n\n&lt;p&gt;Been using databricks notebooks that do some aggregations and push data through to Log Analytics as a custom SDK rig using log analytics and data collector API&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;Not great, logs are slow to arrive in LA and the authorization headers and JSON responses get fiddly when data structures differ.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1537ll0", "is_robot_indexable": true, "report_reasons": null, "author": "va1kyrja-kara", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537ll0/databricks_monitoring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537ll0/databricks_monitoring/", "subreddit_subscribers": 116782, "created_utc": 1689707981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello to all Data Engineers !\n\nI'm diving into the exciting world of Apache Spark and I'm looking for the perfect video tutorial that provides a comprehensive understanding of Spark's architecture, concepts, and practical usage. With so many options out there, both free and paid, I thought it would be great to tap into the knowledge and experiences of this community.\n\nSo, my question is: Which video tutorial would you recommend as the best resource for learning Apache Spark? It could be a free tutorial available on YouTube or other platforms, or even a paid course that you found to be incredibly valuable.\n\nI'm particularly interested in a tutorial that covers the following aspects:\n\n* Spark architecture and core concepts\n* Hands-on examples and real-world use cases\n* Detailed explanations of Spark transformations and actions\n* Spark SQL, data frames, and dataset APIs\n* Integration with popular big data tools and frameworks\n\nYour insights and recommendations would be highly appreciated. Please feel free to share any personal experiences, tutorial names, links, or even the instructors who made a significant impact on your Spark learning journey.\n\nLooking forward to your valuable suggestions and thank you in advance for your help!", "author_fullname": "t2_hvt2pp4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking the Best Apache Spark Video Tutorial - Recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1535mkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689703458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all Data Engineers !&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m diving into the exciting world of Apache Spark and I&amp;#39;m looking for the perfect video tutorial that provides a comprehensive understanding of Spark&amp;#39;s architecture, concepts, and practical usage. With so many options out there, both free and paid, I thought it would be great to tap into the knowledge and experiences of this community.&lt;/p&gt;\n\n&lt;p&gt;So, my question is: Which video tutorial would you recommend as the best resource for learning Apache Spark? It could be a free tutorial available on YouTube or other platforms, or even a paid course that you found to be incredibly valuable.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m particularly interested in a tutorial that covers the following aspects:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark architecture and core concepts&lt;/li&gt;\n&lt;li&gt;Hands-on examples and real-world use cases&lt;/li&gt;\n&lt;li&gt;Detailed explanations of Spark transformations and actions&lt;/li&gt;\n&lt;li&gt;Spark SQL, data frames, and dataset APIs&lt;/li&gt;\n&lt;li&gt;Integration with popular big data tools and frameworks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Your insights and recommendations would be highly appreciated. Please feel free to share any personal experiences, tutorial names, links, or even the instructors who made a significant impact on your Spark learning journey.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your valuable suggestions and thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1535mkd", "is_robot_indexable": true, "report_reasons": null, "author": "Loki_029", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1535mkd/seeking_the_best_apache_spark_video_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1535mkd/seeking_the_best_apache_spark_video_tutorial/", "subreddit_subscribers": 116782, "created_utc": 1689703458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to understand how to think about and architect the schema (ddl) and data migration strategy for some fairly complex data pipelines in databricks. i\u2019m really puzzled because i\u2019m coming from an applications (backend) background, largely with postgres/mysql. so for these, database migration is pretty mature and used everywhere. but when i search this up for delta lake/databricks, there\u2019s basically nothing. \n\nso maybe i\u2019m just not thinking about this in the data engineering way, or maybe there\u2019s some other terms i need to be using? how should i think about supporting structured, version-controlled changes to the table structures and the data they contain? \n\nam i way out of line to be thinking of implementing sqlalchemy alembic for delta lake? is there _any_ tool or even just discussion of how this is done? \n\ni can\u2019t possibly be the first person to have this this thought, but when i asked my databricks rep, i spent half an hour trying to explain what a schema migration even was, so clearly there\u2019s a disconnect somewhere.", "author_fullname": "t2_flpxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema Migration for Delta Lake on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153xtyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689779717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to understand how to think about and architect the schema (ddl) and data migration strategy for some fairly complex data pipelines in databricks. i\u2019m really puzzled because i\u2019m coming from an applications (backend) background, largely with postgres/mysql. so for these, database migration is pretty mature and used everywhere. but when i search this up for delta lake/databricks, there\u2019s basically nothing. &lt;/p&gt;\n\n&lt;p&gt;so maybe i\u2019m just not thinking about this in the data engineering way, or maybe there\u2019s some other terms i need to be using? how should i think about supporting structured, version-controlled changes to the table structures and the data they contain? &lt;/p&gt;\n\n&lt;p&gt;am i way out of line to be thinking of implementing sqlalchemy alembic for delta lake? is there &lt;em&gt;any&lt;/em&gt; tool or even just discussion of how this is done? &lt;/p&gt;\n\n&lt;p&gt;i can\u2019t possibly be the first person to have this this thought, but when i asked my databricks rep, i spent half an hour trying to explain what a schema migration even was, so clearly there\u2019s a disconnect somewhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153xtyq", "is_robot_indexable": true, "report_reasons": null, "author": "geeeffwhy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153xtyq/schema_migration_for_delta_lake_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153xtyq/schema_migration_for_delta_lake_on_databricks/", "subreddit_subscribers": 116782, "created_utc": 1689779717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone,\n\nI am a recent graduate with a passion for working in the field of data engineering. After earning a Master's degree in Data Analytics Engineering and a Bachelor's in Management in Information Systems, I've managed to gain experience and develop key skills in this domain.\n\nMy proficiency lies in designing scalable data solutions, optimizing data infrastructure and pipelines, and managing large datasets. I am proficient in Python, R, SQL, C++, and Java, and have considerable experience with big data technologies such as Apache Spark, Apache Hadoop, Kafka, PySpark, and Snowflake. I've gained a good understanding of cloud tech involving AWS, DB2, and Oracle SQL. \n\nOver the past few years, I have held positions (as an intern) Senior backend developer in a renowned tech company and as a data engineer at a startup. My roles included developing real-time data ingestion pipelines, managing data warehouses, optimizing SQL queries, integrating RESTful APIs into database solutions, and ensuring data integrity, security, and compliance across all operations.\n\nDespite my experience, navigating the job search post-graduation has proven to be quite a challenge. I am reaching out to this community seeking advice:\n\n* What companies should a fresh graduate in data engineering consider for a good start?\n* Do you have any suggestions on how to stand out among a pool of candidates?\n* How do I strategize my job search? - Up until now, I've usually relied on LinkedIn, so should I broaden my search to other Job Portals?   \n\n\nI sincerely appreciate any advice or insights this community can offer. Here's to a shared passion for data engineering and the exciting journey it entails!\n\nBest Regards,   \nA Passionate Data Engineer", "author_fullname": "t2_5u73tpsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice: Navigating the Field of Data Engineering after the completion of my Master's Degree", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153ds3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689722224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a recent graduate with a passion for working in the field of data engineering. After earning a Master&amp;#39;s degree in Data Analytics Engineering and a Bachelor&amp;#39;s in Management in Information Systems, I&amp;#39;ve managed to gain experience and develop key skills in this domain.&lt;/p&gt;\n\n&lt;p&gt;My proficiency lies in designing scalable data solutions, optimizing data infrastructure and pipelines, and managing large datasets. I am proficient in Python, R, SQL, C++, and Java, and have considerable experience with big data technologies such as Apache Spark, Apache Hadoop, Kafka, PySpark, and Snowflake. I&amp;#39;ve gained a good understanding of cloud tech involving AWS, DB2, and Oracle SQL. &lt;/p&gt;\n\n&lt;p&gt;Over the past few years, I have held positions (as an intern) Senior backend developer in a renowned tech company and as a data engineer at a startup. My roles included developing real-time data ingestion pipelines, managing data warehouses, optimizing SQL queries, integrating RESTful APIs into database solutions, and ensuring data integrity, security, and compliance across all operations.&lt;/p&gt;\n\n&lt;p&gt;Despite my experience, navigating the job search post-graduation has proven to be quite a challenge. I am reaching out to this community seeking advice:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What companies should a fresh graduate in data engineering consider for a good start?&lt;/li&gt;\n&lt;li&gt;Do you have any suggestions on how to stand out among a pool of candidates?&lt;/li&gt;\n&lt;li&gt;How do I strategize my job search? - Up until now, I&amp;#39;ve usually relied on LinkedIn, so should I broaden my search to other Job Portals?&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I sincerely appreciate any advice or insights this community can offer. Here&amp;#39;s to a shared passion for data engineering and the exciting journey it entails!&lt;/p&gt;\n\n&lt;p&gt;Best Regards,&lt;br/&gt;\nA Passionate Data Engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "153ds3e", "is_robot_indexable": true, "report_reasons": null, "author": "arcofiero", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153ds3e/seeking_advice_navigating_the_field_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153ds3e/seeking_advice_navigating_the_field_of_data/", "subreddit_subscribers": 116782, "created_utc": 1689722224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have several parquet files (around 100 files), all have the same format, the only difference is that each file is the historical data of an specific date. Currently I have all the files stored in AWS S3, but i need to clean, add columns, and manipulate some columns. After that, i will need to use these data for analytics and to train a ML model.  WHAT WOULD BE THE BEST APPROACH FOR ALL OF THIS? Should i merge all the files into a database (all files have the same format and columns) and that would be easier to use and would increase the performance of the data cleaning and the analytics? HOW CAN I DO IT? WHICH SERVICES DO YOU RECOMMEND (redshift, glue databrew, etc??)?\n\nBtw, Im new on this stuff. Thanks\n\n&amp;#x200B;", "author_fullname": "t2_6bq784p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merge multiple parquet files into a single table in database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153arfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689715203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have several parquet files (around 100 files), all have the same format, the only difference is that each file is the historical data of an specific date. Currently I have all the files stored in AWS S3, but i need to clean, add columns, and manipulate some columns. After that, i will need to use these data for analytics and to train a ML model.  WHAT WOULD BE THE BEST APPROACH FOR ALL OF THIS? Should i merge all the files into a database (all files have the same format and columns) and that would be easier to use and would increase the performance of the data cleaning and the analytics? HOW CAN I DO IT? WHICH SERVICES DO YOU RECOMMEND (redshift, glue databrew, etc??)?&lt;/p&gt;\n\n&lt;p&gt;Btw, Im new on this stuff. Thanks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "153arfs", "is_robot_indexable": true, "report_reasons": null, "author": "pussydestroyerSPY", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153arfs/merge_multiple_parquet_files_into_a_single_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153arfs/merge_multiple_parquet_files_into_a_single_table/", "subreddit_subscribers": 116782, "created_utc": 1689715203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we have a data lake on AWS, data goes with its original format (or csv if extracted from a ddbb) into the landing zone, then gets converted into parquet and carried through the next zones.\n\nThe DA at my team came up with this idea of creating a reference table that would work as an index.\n\nSo for example, given a users table, create a reference table with only the id and the relevant timestamp column (for eg.: created\\_at, updated\\_at), add some fields like 'valid\\_from', 'valid\\_to' and 'is\\_current\\_row' and then join this reference table with the main one.On the next executions, the new records should be inserted into the main table and then, both the main and the reference tables should update values for 'valid\\*' and 'is\\_current\\_row'.\n\nSo the purpose of this is to use the index table for speeding up some queries since it holds less columns, and also be able to tell in which point in time the row was inserted/updated.\n\nFinally, all this data will be loaded into redshift.\n\nMy questions are, is this index table necessary since parquet is columnar and supports page-level indexes? Wouldn't be more convenient to use apache iceberg for handling the 'time travel'? It also provides more features like compacting files that otherwise would have to be done with sepparate script.", "author_fullname": "t2_dpj60sgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create index (reference) table for parquet tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_153zndo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689783825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we have a data lake on AWS, data goes with its original format (or csv if extracted from a ddbb) into the landing zone, then gets converted into parquet and carried through the next zones.&lt;/p&gt;\n\n&lt;p&gt;The DA at my team came up with this idea of creating a reference table that would work as an index.&lt;/p&gt;\n\n&lt;p&gt;So for example, given a users table, create a reference table with only the id and the relevant timestamp column (for eg.: created_at, updated_at), add some fields like &amp;#39;valid_from&amp;#39;, &amp;#39;valid_to&amp;#39; and &amp;#39;is_current_row&amp;#39; and then join this reference table with the main one.On the next executions, the new records should be inserted into the main table and then, both the main and the reference tables should update values for &amp;#39;valid*&amp;#39; and &amp;#39;is_current_row&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;So the purpose of this is to use the index table for speeding up some queries since it holds less columns, and also be able to tell in which point in time the row was inserted/updated.&lt;/p&gt;\n\n&lt;p&gt;Finally, all this data will be loaded into redshift.&lt;/p&gt;\n\n&lt;p&gt;My questions are, is this index table necessary since parquet is columnar and supports page-level indexes? Wouldn&amp;#39;t be more convenient to use apache iceberg for handling the &amp;#39;time travel&amp;#39;? It also provides more features like compacting files that otherwise would have to be done with sepparate script.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "153zndo", "is_robot_indexable": true, "report_reasons": null, "author": "_unwin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153zndo/create_index_reference_table_for_parquet_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153zndo/create_index_reference_table_for_parquet_tables/", "subreddit_subscribers": 116782, "created_utc": 1689783825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy (quant) teammate (Data Visualization Specialist) is working with a couple of proficient Data Engineers and a DBA. We're in the middle of transitioning from a solely SQL Server and Power BI environment to incorporating Azure Data Factory (for data ingestion), Data Bricks (for storage and transformation), and maintaining Power BI for self-service reporting. The data sources are scattered, and the data literacy is in the basement. \"We're pretty much burning it to the ground and starting over\" as my teammate (DE) puts it.\n\nAs we gear up for this bonfire, a few critical **questions pertaining to best practices** are surfacing, particularly surrounding the housing and management of our data models. I am hoping to tap into this community's wisdom and gain some insights into the following:\n\n1. **Location of Data Models:** What is the best practice in terms of where to place our data models? Is Azure Analysis Services (AAS) a feasible choice to hold enterprise-grade data models in the cloud, or should we lean towards Power BI's built-in datasets?\n2. **Exposing Data Models to Users:** What are the most effective ways to expose our data models to the business users who need them? We plan to leverage Power BI's roles and permissions features but are there other recommended strategies? We've set up security groups and spoken about the level of access during each stage prior to publishing.\n3. **Model Complexity and Size:** How can we accommodate larger datasets in the future, considering the model size limitations in Power BI? Does Azure Analysis Services or another platform provide a better solution for larger data models?\n\nOur goal is to provide each business group a dedicated PBI Workspace (in addition to the default individual workspaces). As expected, all reporting that comes out of these distributed development spaces will go through a pipeline from Development to Testing to Production, overseen by the Data Team before reaching the production space. \n\nThis business has grown by 400% in the last 5 years and has no real experience with proper data governance or infrastructure. Thankfully, we don't have a ton of data, only about 400GB. Help has arrived, but your insights and experiences regarding these questions would be greatly appreciated and would go a long way in helping us make informed decisions quickly (obviously we'll research and validate any advice given).\n\nLastly, if it isn't apparent, I'm unfamiliar with many of these matters (fortunately not as unfamiliar as most of our users), so please be as explicit as is convenient. Thank you in advance for your help, and I look forward to reading your responses.", "author_fullname": "t2_c9t8mf2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Infrastructure/Modeling Questions about Best Practices.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_153z4uf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689782654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My (quant) teammate (Data Visualization Specialist) is working with a couple of proficient Data Engineers and a DBA. We&amp;#39;re in the middle of transitioning from a solely SQL Server and Power BI environment to incorporating Azure Data Factory (for data ingestion), Data Bricks (for storage and transformation), and maintaining Power BI for self-service reporting. The data sources are scattered, and the data literacy is in the basement. &amp;quot;We&amp;#39;re pretty much burning it to the ground and starting over&amp;quot; as my teammate (DE) puts it.&lt;/p&gt;\n\n&lt;p&gt;As we gear up for this bonfire, a few critical &lt;strong&gt;questions pertaining to best practices&lt;/strong&gt; are surfacing, particularly surrounding the housing and management of our data models. I am hoping to tap into this community&amp;#39;s wisdom and gain some insights into the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Location of Data Models:&lt;/strong&gt; What is the best practice in terms of where to place our data models? Is Azure Analysis Services (AAS) a feasible choice to hold enterprise-grade data models in the cloud, or should we lean towards Power BI&amp;#39;s built-in datasets?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Exposing Data Models to Users:&lt;/strong&gt; What are the most effective ways to expose our data models to the business users who need them? We plan to leverage Power BI&amp;#39;s roles and permissions features but are there other recommended strategies? We&amp;#39;ve set up security groups and spoken about the level of access during each stage prior to publishing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model Complexity and Size:&lt;/strong&gt; How can we accommodate larger datasets in the future, considering the model size limitations in Power BI? Does Azure Analysis Services or another platform provide a better solution for larger data models?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Our goal is to provide each business group a dedicated PBI Workspace (in addition to the default individual workspaces). As expected, all reporting that comes out of these distributed development spaces will go through a pipeline from Development to Testing to Production, overseen by the Data Team before reaching the production space. &lt;/p&gt;\n\n&lt;p&gt;This business has grown by 400% in the last 5 years and has no real experience with proper data governance or infrastructure. Thankfully, we don&amp;#39;t have a ton of data, only about 400GB. Help has arrived, but your insights and experiences regarding these questions would be greatly appreciated and would go a long way in helping us make informed decisions quickly (obviously we&amp;#39;ll research and validate any advice given).&lt;/p&gt;\n\n&lt;p&gt;Lastly, if it isn&amp;#39;t apparent, I&amp;#39;m unfamiliar with many of these matters (fortunately not as unfamiliar as most of our users), so please be as explicit as is convenient. Thank you in advance for your help, and I look forward to reading your responses.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "153z4uf", "is_robot_indexable": true, "report_reasons": null, "author": "GreenMellowphant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153z4uf/infrastructuremodeling_questions_about_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153z4uf/infrastructuremodeling_questions_about_best/", "subreddit_subscribers": 116782, "created_utc": 1689782654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work as a Data Engineer with 5 months experience in AWS. I have proficient knowledge in Python and SQL.\n\nI want to take the exam to learn about Microsoft Azure and all cloud related topics in Data Engineering.  I have never used Microsoft Azure and have no idea even where to start to learn. I am familiar with some of AWS tools like Glue, Athena, S3, Lamda, EC2. \n\nI am planning to take a week off work to revise and 2 hours a day for another week. \n\nMy question is,\n\n1. Can I pass this exam by learning from [Azure Data Engineer Associate](https://learn.microsoft.com/en-us/certifications/azure-data-engineer/?tab=tab-learning-paths#certification-exams)\n2. Is there any other website that can help prepare me for the exam. \n3. Is there a course or free learning that I can follow with hands on experience. ( I am guessing Azure has a free trial like AWS but I would need a guide to help me through) \n\nThanks  ", "author_fullname": "t2_e793a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preparing Advice for DP-203: Data Engineering on Microsoft Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153va24", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689773784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work as a Data Engineer with 5 months experience in AWS. I have proficient knowledge in Python and SQL.&lt;/p&gt;\n\n&lt;p&gt;I want to take the exam to learn about Microsoft Azure and all cloud related topics in Data Engineering.  I have never used Microsoft Azure and have no idea even where to start to learn. I am familiar with some of AWS tools like Glue, Athena, S3, Lamda, EC2. &lt;/p&gt;\n\n&lt;p&gt;I am planning to take a week off work to revise and 2 hours a day for another week. &lt;/p&gt;\n\n&lt;p&gt;My question is,&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Can I pass this exam by learning from &lt;a href=\"https://learn.microsoft.com/en-us/certifications/azure-data-engineer/?tab=tab-learning-paths#certification-exams\"&gt;Azure Data Engineer Associate&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Is there any other website that can help prepare me for the exam. &lt;/li&gt;\n&lt;li&gt;Is there a course or free learning that I can follow with hands on experience. ( I am guessing Azure has a free trial like AWS but I would need a guide to help me through) &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?auto=webp&amp;s=e62264227377a9581e2e2946169864d130fa3217", "width": 400, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b9526a51504048891d5e64783519fd5dc3cd83f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0150bc3ab1c6838c35ff951d69578f3d19ae4ed3", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YH8gNap4KoGcFgyFYrNZ86fXmYfRn6pa7uuwIlZkjEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1e830770227ae4802c5776d22f63d0f6aa71b15", "width": 320, "height": 320}], "variants": {}, "id": "LVmzWMJU1UZwRubzQYJZSar-z-Rq8ntUH65yhQyfxB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "153va24", "is_robot_indexable": true, "report_reasons": null, "author": "keyboard1ish", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153va24/preparing_advice_for_dp203_data_engineering_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153va24/preparing_advice_for_dp203_data_engineering_on/", "subreddit_subscribers": 116782, "created_utc": 1689773784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Started researching these open source data warehouses. One thing I'm confused about is, if using this type of tech, where do you do your transformation jobs? If I have a bunch of csv or parquet files in blob storage, if I load them into Doris/Starrocks, does it support transformation or should your data be cleaned and ready to go before loading it in these types of warehouses?  \n\nIf you're using either of these, what's your overall architecture look like?", "author_fullname": "t2_ffzuzn1vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Doris/Starrocks Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153u367", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689770815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Started researching these open source data warehouses. One thing I&amp;#39;m confused about is, if using this type of tech, where do you do your transformation jobs? If I have a bunch of csv or parquet files in blob storage, if I load them into Doris/Starrocks, does it support transformation or should your data be cleaned and ready to go before loading it in these types of warehouses?  &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re using either of these, what&amp;#39;s your overall architecture look like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153u367", "is_robot_indexable": true, "report_reasons": null, "author": "Public_Fart42069", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153u367/apache_dorisstarrocks_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153u367/apache_dorisstarrocks_architecture/", "subreddit_subscribers": 116782, "created_utc": 1689770815.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jboiz9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Data Processing Pipeline With MongoDB, Kafka, Debezium And RisingWave", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_153m5jl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZN9e_WGYPBu8pHZr_Al8mfSxgeTvcLoaEBQ2sBh01xY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689745793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "boburadvocate.hashnode.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://boburadvocate.hashnode.dev/real-time-data-processing-pipeline-with-mongodb-kafka-debezium-and-risingwave", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?auto=webp&amp;s=d638cff8e9a623eeded7fc57f19d0114edef5e72", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=103ee0c9d51930d798e8e2cf41ebadc311ea08f7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a567d5f2f41e47494f2d35a8581843219910c62a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=082f778886e3d01ad234fa144e05344c2ee67ef0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6a6facecb4118d42af2f91fd15d306d2aaa5088", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eecadc0b4a26d7f0a2faa3cd1f2d98dbfecd5cc9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/x7HeQHn6QdCHHO1_E2YIvoYd5vCGzdIuMi7dLA4bYVI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f3e8e3e746d4530490228d21b8c84792e0cb115", "width": 1080, "height": 567}], "variants": {}, "id": "dyHGhC0uRgESlyQnmZZhi1OWOiKJBYUTlj8UbWtRiwk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "153m5jl", "is_robot_indexable": true, "report_reasons": null, "author": "bumurzokov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153m5jl/realtime_data_processing_pipeline_with_mongodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://boburadvocate.hashnode.dev/real-time-data-processing-pipeline-with-mongodb-kafka-debezium-and-risingwave", "subreddit_subscribers": 116782, "created_utc": 1689745793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/@stefentaime\\_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99](https://medium.com/@stefentaime_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6v00mbnrducb1.png?width=1767&amp;format=png&amp;auto=webp&amp;s=45eff0b11287df6931a7b80c55859fa36b6ebc32", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free, Real-time Flight Status Pipeline with Kafka, Schemas Registry, Avro, GraphQL, Postgres, and React", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 65, "top_awarded_type": null, "hide_score": false, "media_metadata": {"6v00mbnrducb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=48b75a26e27698c23629f4c331f41b18e5221dda"}, {"y": 88, "x": 216, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc33223e13af1236b2570a36269c9dbf1fa7244a"}, {"y": 130, "x": 320, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b54f75689e6c391d97c91b5448745e16f4741cf"}, {"y": 260, "x": 640, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ca137508497c723631431140446b24c86a08292"}, {"y": 391, "x": 960, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d8364010d42d672089bc2fa3db15ee03250a99f4"}, {"y": 440, "x": 1080, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9be6389be23461dd7e11400e7be3a3b89ee4259"}], "s": {"y": 720, "x": 1767, "u": "https://preview.redd.it/6v00mbnrducb1.png?width=1767&amp;format=png&amp;auto=webp&amp;s=45eff0b11287df6931a7b80c55859fa36b6ebc32"}, "id": "6v00mbnrducb1"}}, "name": "t3_153jimw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/hYWnhj_sQpmown0aaFyYZbrqFFxLmlQ820Oz5GeLke8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1689737731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99\"&gt;https://medium.com/@stefentaime_10958/free-real-time-flight-status-pipeline-with-kafka-schemas-registry-avro-graphql-postgres-and-7ac59b63ea99&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6v00mbnrducb1.png?width=1767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45eff0b11287df6931a7b80c55859fa36b6ebc32\"&gt;https://preview.redd.it/6v00mbnrducb1.png?width=1767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45eff0b11287df6931a7b80c55859fa36b6ebc32&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?auto=webp&amp;s=66627349895c28e6634d65d75fcfd42fd49486a6", "width": 1200, "height": 565}, "resolutions": [{"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4555d40ee64100ccf805363527d7f5197955c25d", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7e35b71ac63cddd0e2e77a4072c02201fc91b9b", "width": 216, "height": 101}, {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d7d0b8bc4a2545e39bc3039a5a0eaf1804a9820", "width": 320, "height": 150}, {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22f7045baaa0f3976656490469811f482d7d5086", "width": 640, "height": 301}, {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc09743ab46d3a08537c8aac34aa665a82b9c4fa", "width": 960, "height": 452}, {"url": "https://external-preview.redd.it/vljW1tlHbHwl74xdLalcCzjCf846TAfkQpKL0CkdrwA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=abbc3e35dc236fe4369afb034cf5cef24ce31396", "width": 1080, "height": 508}], "variants": {}, "id": "-X-TpCor12_5vrjoVNLpOgJN1TPsQ4em-wmUcKvTC6g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "153jimw", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153jimw/free_realtime_flight_status_pipeline_with_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153jimw/free_realtime_flight_status_pipeline_with_kafka/", "subreddit_subscribers": 116782, "created_utc": 1689737731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious how often other engineers see any results of the effort and money sunk into your or your customer\u2019s company in the developing of their data estate. Pipelines are built, infrastructure stood up, dev ops incorporated, warehousing, reports, etc\u2026.what is the benefit? I love engineering but sometimes I feel bad for the those flipping the bill. There is this idea(on every major data products website) that there will be n-zettabytes of data worldwide by 202X and we can unlock all \u201cthis\u201d insight from the data, etc., etc. How often is a tangible report delivered that actually produces insight or generates ROI? I see very basic deliverables, but rarely a payoff that equates to the amount of resources sunk into the endeavor.", "author_fullname": "t2_74fehzbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a ROI for the data engineering endeavor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153ioma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689735343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how often other engineers see any results of the effort and money sunk into your or your customer\u2019s company in the developing of their data estate. Pipelines are built, infrastructure stood up, dev ops incorporated, warehousing, reports, etc\u2026.what is the benefit? I love engineering but sometimes I feel bad for the those flipping the bill. There is this idea(on every major data products website) that there will be n-zettabytes of data worldwide by 202X and we can unlock all \u201cthis\u201d insight from the data, etc., etc. How often is a tangible report delivered that actually produces insight or generates ROI? I see very basic deliverables, but rarely a payoff that equates to the amount of resources sunk into the endeavor.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153ioma", "is_robot_indexable": true, "report_reasons": null, "author": "afivegallonbucket", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153ioma/is_there_a_roi_for_the_data_engineering_endeavor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153ioma/is_there_a_roi_for_the_data_engineering_endeavor/", "subreddit_subscribers": 116782, "created_utc": 1689735343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working to stand up my company\u2019s first enterprise data warehouse, and the biggest and most valuable source system is our flagship product: a CRM. This CRM has an RDBMS back-end on top of which sits an API layer with a ton of business logic. None of that logic is consistently documented, so when I think of integrating that database into our EDW, I\u2019m not sure if there\u2019s any non time-consuming way to discover that logic from the code and incorporate it in our data pipelines. \n\nAm I missing something here? I don\u2019t think the answer can be \u201cgo through the API layer,\u201d can it? \n\nThanks, braintrust.", "author_fullname": "t2_52cbaf2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading Data from a Source with a lot of Business Logic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153d0bs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689720406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working to stand up my company\u2019s first enterprise data warehouse, and the biggest and most valuable source system is our flagship product: a CRM. This CRM has an RDBMS back-end on top of which sits an API layer with a ton of business logic. None of that logic is consistently documented, so when I think of integrating that database into our EDW, I\u2019m not sure if there\u2019s any non time-consuming way to discover that logic from the code and incorporate it in our data pipelines. &lt;/p&gt;\n\n&lt;p&gt;Am I missing something here? I don\u2019t think the answer can be \u201cgo through the API layer,\u201d can it? &lt;/p&gt;\n\n&lt;p&gt;Thanks, braintrust.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "153d0bs", "is_robot_indexable": true, "report_reasons": null, "author": "yoquierodata", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153d0bs/loading_data_from_a_source_with_a_lot_of_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153d0bs/loading_data_from_a_source_with_a_lot_of_business/", "subreddit_subscribers": 116782, "created_utc": 1689720406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TLDR**: I created [this package](https://github.com/Elsayed91/easy_ge) so you can use Great Expectations without knowing much about it or if you have simple use cases and would rather an option with better readability and easier implementation.\n\nIf you'd rather watch than read, you can check [this low-budget demo](https://www.youtube.com/watch?v=9v8mlDb2oRo). a mm a mmm a. \n\nThis is my first package ever. I called it \"Easy G.E\", pretty cringe, but at least it rhymes. Now I am going to present it as if I'm selling you a real enterprise-grade solution.\n\n**Features**:\n\n1. Provides a low-code approach for validating files stored on various filesystems (local, GCS, S3) or in-memory dataframes.\n2. Works with both Pandas and Spark engines.\n3. Reduces configuration complexity to just four schema-enforced fields in a YAML file.\n4. Allows dynamic variable definition/substitution at runtime, eliminating the need for any hardcoded values in the configuration YAML and facilitating the utilization as part of dynamic data pipelines.\n\n**How it works**:\n\n1. Create an expectation suite for your data that you want to validate. This is straightforward, and a guide is available in the repository.\n2. Fill out a `YAML` file with the dataframe name/file path, destination to save results, and the expectation suite file name.\n3. Place the expectation suite in a folder named \"expectations\" in the destination you've defined in the YAML file.\n4. Import the package and execute `results = easy_validation('/path/to/yaml_config.yaml')`   \nThat's it! You're done!   \nAlternatively, you can use the docker image provided to validate files on the fly. It also integrates well with  `Kubernetes` and `Airflow` when using Kubernetes Jobs for tasks.\n\n**Motivation**:\n\n* When I first tried using Great Expectations, I became so frustrated that I decided to include DBT in my project just to leverage the easier-to-implement DBT Expectations.\n* When you are not using `great_expectations.yaml` you might have to write \\~ 100 lines of cryptic code to run the validation.\n* The documentation is hard to navigate, at times difficult to understand, undergoes significant changes between major releases. Moreover, the examples are riddled with [irrelevant context ](https://github.com/great-expectations/great_expectations/blob/develop/tests/integration/docusaurus/connecting_to_your_data/cloud/gcs/pandas/inferred_and_runtime_python_example.py)for users, mainly because the examples serve as tests.\n* Another technology to add to the learning list.\n\n**Target Users**:\n\n|Who Could Benefit from the package?|Problems Addressed by the package:|\n|:-|:-|\n|Individuals who are unfamiliar with `Great Expectations`  but wish to implement data quality checks and documentation.|Overcoming Great Expectation's learning curve required to run basic use-cases.|\n|Individuals experienced with `Great Expectations`|Providing a low-code solution without the need for extensive module creation, documentation, and unit testing.|\n|Users seeking a solution that is easily readable by peers unfamiliar with the tool.|Enhancing readability for individuals who are unfamiliar with Great Expectations, ensuring ease of understanding and collaboration.|\n\n&amp;#x200B;\n\nDisclaimer #1:  *It ain't much but it is honest work.*\n\nThis package is rudimentary but will carry out the advertised functionalities. However, as I'm no Great Expectations power user, I've only implemented what I believe to be common use cases. Nevertheless, I've put in my best effort to design the package to be extensible.\n\nDisclaimer #2\n\nI have approximately 6 months of data engineering learning under my belt, and a total of 10 months in the programming ecosystem. Zero real world experience, which makes me feel insanely embarrassed posting this here (imposter syndrome intensifies), but YOLO. Receiving feedback could greatly benefit my ongoing job hunt and help me grow. Also considering how useful I'd have found this package myself some months back, it could be of some use to others, if the whole concept is not a disaster that I am unaware of, that is.", "author_fullname": "t2_jwq2blxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a great expectations wrapper so that even my mum could validate her data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_153bm5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689717195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: I created &lt;a href=\"https://github.com/Elsayed91/easy_ge\"&gt;this package&lt;/a&gt; so you can use Great Expectations without knowing much about it or if you have simple use cases and would rather an option with better readability and easier implementation.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;d rather watch than read, you can check &lt;a href=\"https://www.youtube.com/watch?v=9v8mlDb2oRo\"&gt;this low-budget demo&lt;/a&gt;. a mm a mmm a. &lt;/p&gt;\n\n&lt;p&gt;This is my first package ever. I called it &amp;quot;Easy G.E&amp;quot;, pretty cringe, but at least it rhymes. Now I am going to present it as if I&amp;#39;m selling you a real enterprise-grade solution.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Provides a low-code approach for validating files stored on various filesystems (local, GCS, S3) or in-memory dataframes.&lt;/li&gt;\n&lt;li&gt;Works with both Pandas and Spark engines.&lt;/li&gt;\n&lt;li&gt;Reduces configuration complexity to just four schema-enforced fields in a YAML file.&lt;/li&gt;\n&lt;li&gt;Allows dynamic variable definition/substitution at runtime, eliminating the need for any hardcoded values in the configuration YAML and facilitating the utilization as part of dynamic data pipelines.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create an expectation suite for your data that you want to validate. This is straightforward, and a guide is available in the repository.&lt;/li&gt;\n&lt;li&gt;Fill out a &lt;code&gt;YAML&lt;/code&gt; file with the dataframe name/file path, destination to save results, and the expectation suite file name.&lt;/li&gt;\n&lt;li&gt;Place the expectation suite in a folder named &amp;quot;expectations&amp;quot; in the destination you&amp;#39;ve defined in the YAML file.&lt;/li&gt;\n&lt;li&gt;Import the package and execute &lt;code&gt;results = easy_validation(&amp;#39;/path/to/yaml_config.yaml&amp;#39;)&lt;/code&gt;&lt;br/&gt;\nThat&amp;#39;s it! You&amp;#39;re done!&lt;br/&gt;\nAlternatively, you can use the docker image provided to validate files on the fly. It also integrates well with  &lt;code&gt;Kubernetes&lt;/code&gt; and &lt;code&gt;Airflow&lt;/code&gt; when using Kubernetes Jobs for tasks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When I first tried using Great Expectations, I became so frustrated that I decided to include DBT in my project just to leverage the easier-to-implement DBT Expectations.&lt;/li&gt;\n&lt;li&gt;When you are not using &lt;code&gt;great_expectations.yaml&lt;/code&gt; you might have to write ~ 100 lines of cryptic code to run the validation.&lt;/li&gt;\n&lt;li&gt;The documentation is hard to navigate, at times difficult to understand, undergoes significant changes between major releases. Moreover, the examples are riddled with &lt;a href=\"https://github.com/great-expectations/great_expectations/blob/develop/tests/integration/docusaurus/connecting_to_your_data/cloud/gcs/pandas/inferred_and_runtime_python_example.py\"&gt;irrelevant context &lt;/a&gt;for users, mainly because the examples serve as tests.&lt;/li&gt;\n&lt;li&gt;Another technology to add to the learning list.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Target Users&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Who Could Benefit from the package?&lt;/th&gt;\n&lt;th align=\"left\"&gt;Problems Addressed by the package:&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Individuals who are unfamiliar with &lt;code&gt;Great Expectations&lt;/code&gt;  but wish to implement data quality checks and documentation.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Overcoming Great Expectation&amp;#39;s learning curve required to run basic use-cases.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Individuals experienced with &lt;code&gt;Great Expectations&lt;/code&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Providing a low-code solution without the need for extensive module creation, documentation, and unit testing.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Users seeking a solution that is easily readable by peers unfamiliar with the tool.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Enhancing readability for individuals who are unfamiliar with Great Expectations, ensuring ease of understanding and collaboration.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer #1:  &lt;em&gt;It ain&amp;#39;t much but it is honest work.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This package is rudimentary but will carry out the advertised functionalities. However, as I&amp;#39;m no Great Expectations power user, I&amp;#39;ve only implemented what I believe to be common use cases. Nevertheless, I&amp;#39;ve put in my best effort to design the package to be extensible.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer #2&lt;/p&gt;\n\n&lt;p&gt;I have approximately 6 months of data engineering learning under my belt, and a total of 10 months in the programming ecosystem. Zero real world experience, which makes me feel insanely embarrassed posting this here (imposter syndrome intensifies), but YOLO. Receiving feedback could greatly benefit my ongoing job hunt and help me grow. Also considering how useful I&amp;#39;d have found this package myself some months back, it could be of some use to others, if the whole concept is not a disaster that I am unaware of, that is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?auto=webp&amp;s=7423cb1aba2e435f016e105029d3c10b47077bc5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=153d78c5238578c181f0639d713cfddc046ea1f3", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbde46af15c6568fdb673d17fd97c6cfa17a5a81", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=986b8f948c8a4caa3c1371de460c227fb994d955", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=117ae1527a69c6d19e673493c26820f8171c19b1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a4b89ba266d4855fdf73dfc6c4e70c7a114479b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fFQbLhXPFWNb14lodfcmEogdH9goZg4S6z-14YyhcyQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=32849eaa6edc4791ae9dda074ede8bbaeab6a59b", "width": 1080, "height": 540}], "variants": {}, "id": "VbnN3cGyjwsaRntjS52g6SlwMjoYtHezmOYEburSEyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "153bm5a", "is_robot_indexable": true, "report_reasons": null, "author": "whatisthisdataman", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/153bm5a/i_created_a_great_expectations_wrapper_so_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/153bm5a/i_created_a_great_expectations_wrapper_so_that/", "subreddit_subscribers": 116782, "created_utc": 1689717195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are just about to build the gold layer in our data lake but unsure of how to organize the directories. We aim to have a kimball model for each project, but unsure where to store the dimension tables (are in delta format). Should we create directories inside the gold container for each model and duplicate shared dimension tables? How do you guys set this up in your orgs? We have the facts and dims in their respective delta tables.", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building the gold container in the medallion architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1537lly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689707982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are just about to build the gold layer in our data lake but unsure of how to organize the directories. We aim to have a kimball model for each project, but unsure where to store the dimension tables (are in delta format). Should we create directories inside the gold container for each model and duplicate shared dimension tables? How do you guys set this up in your orgs? We have the facts and dims in their respective delta tables.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1537lly", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1537lly/building_the_gold_container_in_the_medallion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1537lly/building_the_gold_container_in_the_medallion/", "subreddit_subscribers": 116782, "created_utc": 1689707982.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}