{"kind": "Listing", "data": {"after": "t3_14xq53v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So many people think data engineers are only responsible for building data pipelines.\n\nBut in reality, if you are doing a data lake project, you may also need to understand the cloud infra (VPC, IP, DBA infra, Terraform, K8s).\n\nAs a data engineer, I think being a cloud engineer is better than being a data engineer.", "author_fullname": "t2_txqvauef", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer isn\u2019t really just data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wv6f6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689090964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many people think data engineers are only responsible for building data pipelines.&lt;/p&gt;\n\n&lt;p&gt;But in reality, if you are doing a data lake project, you may also need to understand the cloud infra (VPC, IP, DBA infra, Terraform, K8s).&lt;/p&gt;\n\n&lt;p&gt;As a data engineer, I think being a cloud engineer is better than being a data engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14wv6f6", "is_robot_indexable": true, "report_reasons": null, "author": "Dice__R", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wv6f6/data_engineer_isnt_really_just_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wv6f6/data_engineer_isnt_really_just_data_engineering/", "subreddit_subscribers": 115394, "created_utc": 1689090964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got into a data engineering role, it's my first job as a DE. And i am feeling absolutely lost, i don't understand what's happening, everything is everywhere, my team mates are very busy so no one properly explains what's happening and some structural change is happening in the whole section of DE teams. And I feel absolutely overwhelmed.\nHow do you tackle this?", "author_fullname": "t2_vn1meuc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal to feel completely lost during initial months of your data engineering job ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xhi13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689147891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got into a data engineering role, it&amp;#39;s my first job as a DE. And i am feeling absolutely lost, i don&amp;#39;t understand what&amp;#39;s happening, everything is everywhere, my team mates are very busy so no one properly explains what&amp;#39;s happening and some structural change is happening in the whole section of DE teams. And I feel absolutely overwhelmed.\nHow do you tackle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14xhi13", "is_robot_indexable": true, "report_reasons": null, "author": "jojobaoil68", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xhi13/is_it_normal_to_feel_completely_lost_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xhi13/is_it_normal_to_feel_completely_lost_during/", "subreddit_subscribers": 115394, "created_utc": 1689147891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone - for context I'm a DE with \\~3 YoE that I spent working in a small startup where I was, *for the most part*, the sole DE.  \nI was responsible for not only implementing an end-to-end data pipeline that handles data from ingestion to analytics dashboards but also built the websockets and APIs needed to interact with different parts of the pipeline - which I really liked.\n\nAround a month ago I got the opportunity to join a big company as a DE, both the job listing and the interview had big emphasis on experience with end-to-end data pipelines and good SWE practices.\n\nI figured this would be a great opportunity to work with more experienced engineers on a project with significantly more data than what I'm used to.\n\nThe data stack is: Airflow - dbt (core) - AWS S3 (data lake) - AWS Redshift (dw) - Quicksight.\n\nMost of the tasks I pick up involve building dbt models, solving merge conflicts, configuring Airflow DAGs or sometimes assisting the analytics team.\n\nThe good thing is that the company has a great data culture and big emphasis on CI/CD and good SWE practices - they also insist on having DEs handle the building of dbt models and query optimization which is a good thing imo.\n\nMy major concern however is that the role doesn't feel as technically challenging as I was hoping for it to be, so do you guys think this is a good position to progress my career long term *from a technical standpoint* or should I keep an eye on other opportunities ?", "author_fullname": "t2_pna4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joined a new company as a DE for a big project and I'm feeling a bit underwhelmed - Would like to hear a second opinion.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xkfjd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689157100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone - for context I&amp;#39;m a DE with ~3 YoE that I spent working in a small startup where I was, &lt;em&gt;for the most part&lt;/em&gt;, the sole DE.&lt;br/&gt;\nI was responsible for not only implementing an end-to-end data pipeline that handles data from ingestion to analytics dashboards but also built the websockets and APIs needed to interact with different parts of the pipeline - which I really liked.&lt;/p&gt;\n\n&lt;p&gt;Around a month ago I got the opportunity to join a big company as a DE, both the job listing and the interview had big emphasis on experience with end-to-end data pipelines and good SWE practices.&lt;/p&gt;\n\n&lt;p&gt;I figured this would be a great opportunity to work with more experienced engineers on a project with significantly more data than what I&amp;#39;m used to.&lt;/p&gt;\n\n&lt;p&gt;The data stack is: Airflow - dbt (core) - AWS S3 (data lake) - AWS Redshift (dw) - Quicksight.&lt;/p&gt;\n\n&lt;p&gt;Most of the tasks I pick up involve building dbt models, solving merge conflicts, configuring Airflow DAGs or sometimes assisting the analytics team.&lt;/p&gt;\n\n&lt;p&gt;The good thing is that the company has a great data culture and big emphasis on CI/CD and good SWE practices - they also insist on having DEs handle the building of dbt models and query optimization which is a good thing imo.&lt;/p&gt;\n\n&lt;p&gt;My major concern however is that the role doesn&amp;#39;t feel as technically challenging as I was hoping for it to be, so do you guys think this is a good position to progress my career long term &lt;em&gt;from a technical standpoint&lt;/em&gt; or should I keep an eye on other opportunities ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14xkfjd", "is_robot_indexable": true, "report_reasons": null, "author": "King_TN", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xkfjd/joined_a_new_company_as_a_de_for_a_big_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xkfjd/joined_a_new_company_as_a_de_for_a_big_project/", "subreddit_subscribers": 115394, "created_utc": 1689157100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Preview of Stream Processing Performance Report: Apache Flink and RisingWave Comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14wyfs8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HxoROitqUJU4SYhu2S-EC6aowSdO-lrcU7aINgUyO_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689098237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/the-preview-of-stream-processing-performance-report-apache-flink-and-risingwave-comparison/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?auto=webp&amp;s=4deee54975213003aec428e9b2faf715ceeb8a9e", "width": 3533, "height": 4416}, "resolutions": [{"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=401403de7e9c1b96af97472763bfd0882fa727e0", "width": 108, "height": 134}, {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec792cc44c027d23d038ebcfdea53bc3fcc9ff0f", "width": 216, "height": 269}, {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=615a86f8745b609a69ff1c2242acbf844fd1390a", "width": 320, "height": 399}, {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73f1f340d3347c1f01097bdb4d9fdd0896d68c86", "width": 640, "height": 799}, {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bdf6a920792de1f278c6a275f94095917980d06", "width": 960, "height": 1199}, {"url": "https://external-preview.redd.it/-7HvcLxR-vh5bcGn9Bmgg0lFlcDGq7SXGm3BpZSLSTs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f03e2ead490eea564f8a4dde10254d893682d1c9", "width": 1080, "height": 1349}], "variants": {}, "id": "x9k2v5bDNTokkZtW3CyOE1b1tJfRb5BBOgn4eEE4Rso"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "14wyfs8", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wyfs8/the_preview_of_stream_processing_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/the-preview-of-stream-processing-performance-report-apache-flink-and-risingwave-comparison/", "subreddit_subscribers": 115394, "created_utc": 1689098237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Backdrop:\nI am using pyspark on AWS EMR (22G Mem available, 8 VCores). Trying to process 12 Billion Rows. \n( I am using default settings for memory and executors, new to spark and for lack of better words, dont know how to fix them optimally)\n\nWhat my job does:\nReads two dataframes, crossjoins and writes the output. ( Yes, crossjoin is costly but thats what my requirement is, map every row to everyother row)\n\nProblem:\nSpark creates 200 tasks at Stage 1, 199 get executed instantly and the remaining task doesn't even get executed in another hour. The issue being, 1 task is being run on a single executor so the second executor is being killed by the driver for sitting idle. ( I want both executors, since there are some stages that will come after this particular stage is executed)\n\nWhats already tried:\n1) No, I cant use a broadcast join(both dataset have equal rows)\n2) Data skew seems to be an issue but I repartition the data before and after the join ( among the 400 partition, data varies from habing 28 million to 32 million records, I guess it isnt that skewed afterall)\n\n\nPlease help me get over this menace, been stuck here for the past 2 days. My managers been poking me lol.", "author_fullname": "t2_t9iw9tl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help from Spark Gurus", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wwzs9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689094975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Backdrop:\nI am using pyspark on AWS EMR (22G Mem available, 8 VCores). Trying to process 12 Billion Rows. \n( I am using default settings for memory and executors, new to spark and for lack of better words, dont know how to fix them optimally)&lt;/p&gt;\n\n&lt;p&gt;What my job does:\nReads two dataframes, crossjoins and writes the output. ( Yes, crossjoin is costly but thats what my requirement is, map every row to everyother row)&lt;/p&gt;\n\n&lt;p&gt;Problem:\nSpark creates 200 tasks at Stage 1, 199 get executed instantly and the remaining task doesn&amp;#39;t even get executed in another hour. The issue being, 1 task is being run on a single executor so the second executor is being killed by the driver for sitting idle. ( I want both executors, since there are some stages that will come after this particular stage is executed)&lt;/p&gt;\n\n&lt;p&gt;Whats already tried:\n1) No, I cant use a broadcast join(both dataset have equal rows)\n2) Data skew seems to be an issue but I repartition the data before and after the join ( among the 400 partition, data varies from habing 28 million to 32 million records, I guess it isnt that skewed afterall)&lt;/p&gt;\n\n&lt;p&gt;Please help me get over this menace, been stuck here for the past 2 days. My managers been poking me lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14wwzs9", "is_robot_indexable": true, "report_reasons": null, "author": "Straight-End4310", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wwzs9/need_help_from_spark_gurus/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wwzs9/need_help_from_spark_gurus/", "subreddit_subscribers": 115394, "created_utc": 1689094975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team is looking at writing a simple framework/platform based on Cloud that'd allow various users to define batch based pipelines. Currently focusing on AWS, there are multiple modules which setup a set of services, dynamic infrastructure, orchestration, roles, notifications for the users as part of the platform. \n\nAs part of orchestration, I started looking at Dagster and one of the thing I immediately realized is that the actual data processing and orchestration is intertwined and deliberately kept together in most of the examples I saw and also a forte of Dagster as I understand compared to the traditional orhestration/workflow frameworks where they are loosely coupled and a DAG is defined.   \n\n\nWhen I look at any orchestrator, the main things I look for is that - orchestration, meaning given a set of tasks, it should be able to execute, report on, track, kill at will, rollback, fanout, do reporting and maintain metadata for it. Combining this with the actual data being processed as part of the actions/tasks is a great thing for small scale pipelines but quickly becomes a scaling/deployment problem if I want to make it available as a service (not looking for Dagster cloud yet) where users can submit many such jobs.   \n\n\nI understand and yet to explore the constructs such as Assets, Operators that allow me integrate with external services (in my case the cloud services which I'd integrate with a good wrapper over common AWS managed services and compute) but before I proceed wanted to check if I am not setting myself on a wrong path just because its supports but is indeed an antipattern.   \n\n\nWhy separate actual compute from orchestration ?   \n\\- I want to fully exploit and use existing AWS services where given a script (say the script/python code utilizing say polars,pandas, duckdb etc), I have a freedom to pack and run it as a docker image on ECS, Fargate, or lamda   \n\\- For a very big workloads, which don't fit into services such as AWS Glue/Redshift for ELT/ETL purpose, be able to spawn an appliance say through spot and get it executed in EC2 and so on..  \n\n\n.. and still use Dagster as a pure orchestrator where it can have a shim layer written that not only launches but tracks these jobs very well, helps with retries, rollbacks etc.", "author_fullname": "t2_qg4yidm6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster - Separating compute and orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xodd2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689167886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team is looking at writing a simple framework/platform based on Cloud that&amp;#39;d allow various users to define batch based pipelines. Currently focusing on AWS, there are multiple modules which setup a set of services, dynamic infrastructure, orchestration, roles, notifications for the users as part of the platform. &lt;/p&gt;\n\n&lt;p&gt;As part of orchestration, I started looking at Dagster and one of the thing I immediately realized is that the actual data processing and orchestration is intertwined and deliberately kept together in most of the examples I saw and also a forte of Dagster as I understand compared to the traditional orhestration/workflow frameworks where they are loosely coupled and a DAG is defined.   &lt;/p&gt;\n\n&lt;p&gt;When I look at any orchestrator, the main things I look for is that - orchestration, meaning given a set of tasks, it should be able to execute, report on, track, kill at will, rollback, fanout, do reporting and maintain metadata for it. Combining this with the actual data being processed as part of the actions/tasks is a great thing for small scale pipelines but quickly becomes a scaling/deployment problem if I want to make it available as a service (not looking for Dagster cloud yet) where users can submit many such jobs.   &lt;/p&gt;\n\n&lt;p&gt;I understand and yet to explore the constructs such as Assets, Operators that allow me integrate with external services (in my case the cloud services which I&amp;#39;d integrate with a good wrapper over common AWS managed services and compute) but before I proceed wanted to check if I am not setting myself on a wrong path just because its supports but is indeed an antipattern.   &lt;/p&gt;\n\n&lt;p&gt;Why separate actual compute from orchestration ?&lt;br/&gt;\n- I want to fully exploit and use existing AWS services where given a script (say the script/python code utilizing say polars,pandas, duckdb etc), I have a freedom to pack and run it as a docker image on ECS, Fargate, or lamda&lt;br/&gt;\n- For a very big workloads, which don&amp;#39;t fit into services such as AWS Glue/Redshift for ELT/ETL purpose, be able to spawn an appliance say through spot and get it executed in EC2 and so on..  &lt;/p&gt;\n\n&lt;p&gt;.. and still use Dagster as a pure orchestrator where it can have a shim layer written that not only launches but tracks these jobs very well, helps with retries, rollbacks etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14xodd2", "is_robot_indexable": true, "report_reasons": null, "author": "Other_Cartoonist7071", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xodd2/dagster_separating_compute_and_orchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xodd2/dagster_separating_compute_and_orchestration/", "subreddit_subscribers": 115394, "created_utc": 1689167886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any easy way to calculate the minimum number of workers, drivers and memory needed in databricks? A website that does the calculation if any?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any websites to calculate Databricks drivers and workers required?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xgcwi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689144002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any easy way to calculate the minimum number of workers, drivers and memory needed in databricks? A website that does the calculation if any?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14xgcwi", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xgcwi/any_websites_to_calculate_databricks_drivers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xgcwi/any_websites_to_calculate_databricks_drivers_and/", "subreddit_subscribers": 115394, "created_utc": 1689144002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am learning Apache Kafka, and I have completed the \"Apache Kafka Series - Learn Apache Kafka for Beginners v3\" course by Stephane Maarek. What do you think should be my next step?", "author_fullname": "t2_feara4tb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14x6c6f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689116033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am learning Apache Kafka, and I have completed the &amp;quot;Apache Kafka Series - Learn Apache Kafka for Beginners v3&amp;quot; course by Stephane Maarek. What do you think should be my next step?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14x6c6f", "is_robot_indexable": true, "report_reasons": null, "author": "Kratos_1412", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14x6c6f/apache_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14x6c6f/apache_kafka/", "subreddit_subscribers": 115394, "created_utc": 1689116033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everybody,  I have been trying to look at systems that process streaming data (downstream from Kafka).  One of the inputs to our decision-making process is to have a good understanding of the throughputs these systems/platforms can handle.  I was hoping that we don't have to set up all these environments and run a side-by-side comparison.  Do you have some suggestions on where I can find this information? ", "author_fullname": "t2_48nrgvyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benchmarks for stream processing systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14x3zux", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689110618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,  I have been trying to look at systems that process streaming data (downstream from Kafka).  One of the inputs to our decision-making process is to have a good understanding of the throughputs these systems/platforms can handle.  I was hoping that we don&amp;#39;t have to set up all these environments and run a side-by-side comparison.  Do you have some suggestions on where I can find this information? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14x3zux", "is_robot_indexable": true, "report_reasons": null, "author": "anupsurendran", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14x3zux/benchmarks_for_stream_processing_systems/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14x3zux/benchmarks_for_stream_processing_systems/", "subreddit_subscribers": 115394, "created_utc": 1689110618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am investigating it  and I see that it has 2 limitations:\n\n1.  All tables created and updated by Delta Live Tables are Delta tables \n2. i can\u2019t customize their target delta table file paths to their desired location for bronze, silver or gold tables.\n\nin my organization we load data in bronze with its original format(could be json, parquet, etc) so we can not use DLT for this  if I am not mistaken and we can't use it either for a workflow with multiple notebooks that write in bronze , silver a gold successively.\n\nhow do you use DLT? which are the remaining use cases?\n\nthanks", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how do you use delta live tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14x2omf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689107698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am investigating it  and I see that it has 2 limitations:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; All tables created and updated by Delta Live Tables are Delta tables &lt;/li&gt;\n&lt;li&gt;i can\u2019t customize their target delta table file paths to their desired location for bronze, silver or gold tables.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;in my organization we load data in bronze with its original format(could be json, parquet, etc) so we can not use DLT for this  if I am not mistaken and we can&amp;#39;t use it either for a workflow with multiple notebooks that write in bronze , silver a gold successively.&lt;/p&gt;\n\n&lt;p&gt;how do you use DLT? which are the remaining use cases?&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14x2omf", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14x2omf/how_do_you_use_delta_live_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14x2omf/how_do_you_use_delta_live_tables/", "subreddit_subscribers": 115394, "created_utc": 1689107698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as the title suggests, I'm currently working as a DBA mainly specializing in performance optimization, and I want to make a career shift towards Data Engineering but I'm confused of the roadmap or what to learn.\n\nCurrently I have experience and skills in SQL, Teradata, and a beginner Python (projects made using Pandas, Tkinter, and Data scraping libraries)\nAdditionally I have done some visualization on Power BI and I'm decent at it and DAX.\n\nThe country where I'm looking to apply is mostly dominant in Azure Cloud technologies for data engineering roles,  but most of the job postings also require experience in Python and ETL. \n\nMy initial goal was to do the DP 203 Azure Data Engineer Associate certification but now I'm thinking of learning ETL, Data orchestration and Data modelling before I jump to cloud.\n\nMy plan is to learn spark with databricks, pyspark for ETL. Maybe some Airflow for data orchestration and Kimball for data modelling\n\nShould I go for the DP 203 certification and then focus on the spark, pyspark and Airflow or do this in reverse or is there another better roadmap which could cover ETL, spark and cloud?", "author_fullname": "t2_k9utu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From DBA to Data Engineer, what learning path to take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14ww6ky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689093184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title suggests, I&amp;#39;m currently working as a DBA mainly specializing in performance optimization, and I want to make a career shift towards Data Engineering but I&amp;#39;m confused of the roadmap or what to learn.&lt;/p&gt;\n\n&lt;p&gt;Currently I have experience and skills in SQL, Teradata, and a beginner Python (projects made using Pandas, Tkinter, and Data scraping libraries)\nAdditionally I have done some visualization on Power BI and I&amp;#39;m decent at it and DAX.&lt;/p&gt;\n\n&lt;p&gt;The country where I&amp;#39;m looking to apply is mostly dominant in Azure Cloud technologies for data engineering roles,  but most of the job postings also require experience in Python and ETL. &lt;/p&gt;\n\n&lt;p&gt;My initial goal was to do the DP 203 Azure Data Engineer Associate certification but now I&amp;#39;m thinking of learning ETL, Data orchestration and Data modelling before I jump to cloud.&lt;/p&gt;\n\n&lt;p&gt;My plan is to learn spark with databricks, pyspark for ETL. Maybe some Airflow for data orchestration and Kimball for data modelling&lt;/p&gt;\n\n&lt;p&gt;Should I go for the DP 203 certification and then focus on the spark, pyspark and Airflow or do this in reverse or is there another better roadmap which could cover ETL, spark and cloud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14ww6ky", "is_robot_indexable": true, "report_reasons": null, "author": "irtaza23", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ww6ky/from_dba_to_data_engineer_what_learning_path_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14ww6ky/from_dba_to_data_engineer_what_learning_path_to/", "subreddit_subscribers": 115394, "created_utc": 1689093184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Am new to working in Azure Databricks, am trying to run a job in the workflow, that creates a job cluster on the run that has one driver and 3 workers each with 56G memory and 8 cores. My question is do we need to set number of executors, memory and cores. \nWhat happens when we set number of executors and what happens when we don't", "author_fullname": "t2_ouayfdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "So do we need to set executor memory and core in Azure Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14x02a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689101877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Am new to working in Azure Databricks, am trying to run a job in the workflow, that creates a job cluster on the run that has one driver and 3 workers each with 56G memory and 8 cores. My question is do we need to set number of executors, memory and cores. \nWhat happens when we set number of executors and what happens when we don&amp;#39;t&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14x02a8", "is_robot_indexable": true, "report_reasons": null, "author": "am_oldmonk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14x02a8/so_do_we_need_to_set_executor_memory_and_core_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14x02a8/so_do_we_need_to_set_executor_memory_and_core_in/", "subreddit_subscribers": 115394, "created_utc": 1689101877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I really like Redshift Spectrum and find it extremely useful when I have a few big files.\n\nThe problem I have is when I have event-based data e.g. output from Firehose and there are lots of smaller files in S3 (millions). I have this scenario where there are a lot of files and the file sizes are not big but it still takes a very long time to read with Redshift Spectrum. The data is partitioned and indexed in Glue based on year, month and day.\n\nAm I doing it wrong? Where is the bottleneck? Is it the S3 Get Requests?\n\nThank you", "author_fullname": "t2_52cc7hzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I Using Redshift Spectrum Wrong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wzd3l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689100292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like Redshift Spectrum and find it extremely useful when I have a few big files.&lt;/p&gt;\n\n&lt;p&gt;The problem I have is when I have event-based data e.g. output from Firehose and there are lots of smaller files in S3 (millions). I have this scenario where there are a lot of files and the file sizes are not big but it still takes a very long time to read with Redshift Spectrum. The data is partitioned and indexed in Glue based on year, month and day.&lt;/p&gt;\n\n&lt;p&gt;Am I doing it wrong? Where is the bottleneck? Is it the S3 Get Requests?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14wzd3l", "is_robot_indexable": true, "report_reasons": null, "author": "BananaSpears262", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wzd3l/am_i_using_redshift_spectrum_wrong/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wzd3l/am_i_using_redshift_spectrum_wrong/", "subreddit_subscribers": 115394, "created_utc": 1689100292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nWe are currently storing images, and text in AWS s3, and small percentage of data comes with annotations. Every week we should remove most of the annotated data and keep only data that are relevant for further training the models which we are currently doing using airflow and sampling techniques.\n\nWe want to migrate fully to the cloud and we are rewriting ML pipelines to Azure stack using Azure ML and storing data in Azure blop storage.\n\nWe want to include sampling pipeline, which basically downloads the data from azure blop storage, do some sampling techniques and moves or copies/deletes data to a new location in azure blop storage and creates new data assets.\n\nWould it make sense to include sampling pipeline in Azure ML pipelines, or rather Azure data factory pipelines? Are there any advantages/disadvantages in using either of those?\n\n ", "author_fullname": "t2_7vjhs5w4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure ML or Azure Data Factory pipelines for this specific case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wwl54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689094091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently storing images, and text in AWS s3, and small percentage of data comes with annotations. Every week we should remove most of the annotated data and keep only data that are relevant for further training the models which we are currently doing using airflow and sampling techniques.&lt;/p&gt;\n\n&lt;p&gt;We want to migrate fully to the cloud and we are rewriting ML pipelines to Azure stack using Azure ML and storing data in Azure blop storage.&lt;/p&gt;\n\n&lt;p&gt;We want to include sampling pipeline, which basically downloads the data from azure blop storage, do some sampling techniques and moves or copies/deletes data to a new location in azure blop storage and creates new data assets.&lt;/p&gt;\n\n&lt;p&gt;Would it make sense to include sampling pipeline in Azure ML pipelines, or rather Azure data factory pipelines? Are there any advantages/disadvantages in using either of those?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14wwl54", "is_robot_indexable": true, "report_reasons": null, "author": "Desperate-Hat268", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wwl54/azure_ml_or_azure_data_factory_pipelines_for_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wwl54/azure_ml_or_azure_data_factory_pipelines_for_this/", "subreddit_subscribers": 115394, "created_utc": 1689094091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How is data infrastructure managed in large chains such as grocery stores? \n\nWhen I visit grocery store job postings website, I don't often come across data-related job postings. Do they typically hire third-party companies to handle their data infrastructure and analytics? \n\nHow to search for these 3ed party companies. Do they have a specific term?", "author_fullname": "t2_upc5lnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who manages data infrastructure in large chains", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wvrn9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689092269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is data infrastructure managed in large chains such as grocery stores? &lt;/p&gt;\n\n&lt;p&gt;When I visit grocery store job postings website, I don&amp;#39;t often come across data-related job postings. Do they typically hire third-party companies to handle their data infrastructure and analytics? &lt;/p&gt;\n\n&lt;p&gt;How to search for these 3ed party companies. Do they have a specific term?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14wvrn9", "is_robot_indexable": true, "report_reasons": null, "author": "eliamartali", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wvrn9/who_manages_data_infrastructure_in_large_chains/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wvrn9/who_manages_data_infrastructure_in_large_chains/", "subreddit_subscribers": 115394, "created_utc": 1689092269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I need some advice on how to create a data processing pipeline.  \nI've seen a few tools (airflow, dagster, prefect and others) but I'm not sure if they meet my needs.\n\nIt needs to be deployed inside k8s and can use Azure tools.  \n\n\nHere's the input data:\n\n* A batch of files (different extensions) linked to a client and a project that can arrive at any time.\n* The files are stored in a MinIO storage space. \n\n&amp;#x200B;\n\nI want to process the files one by one, preferably using an API to trigger the job. \n\nEach file will pass through the pipeline, but not through the same jobs. Some extension types will require different processing (or can be excluded). \n\nI also need a dashboard:\n\n* I need to be able to filter by client and project. \n* I need to see pipeline progress (by file) \n* Track the performance of each task. \n* Can be added to client app\n\nRe-run jobs/tasks: \n\nIf a file is made up of the following jobs: \\[A -&gt; B -&gt; \\[C, D\\]\\].  \nI'd like to be able to re-run from step B, which will trigger the re-processing of C and D.\n\n  \nI feel like I have to combine different tools to get the pipeline + the dashboard.\n\n  \n\n\nWe welcome any contributions or tool recommendations!\n\nThank you very much!", "author_fullname": "t2_a1lni9541", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice : Ingestion pipeline tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wuip0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689089488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I need some advice on how to create a data processing pipeline.&lt;br/&gt;\nI&amp;#39;ve seen a few tools (airflow, dagster, prefect and others) but I&amp;#39;m not sure if they meet my needs.&lt;/p&gt;\n\n&lt;p&gt;It needs to be deployed inside k8s and can use Azure tools.  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the input data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A batch of files (different extensions) linked to a client and a project that can arrive at any time.&lt;/li&gt;\n&lt;li&gt;The files are stored in a MinIO storage space. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to process the files one by one, preferably using an API to trigger the job. &lt;/p&gt;\n\n&lt;p&gt;Each file will pass through the pipeline, but not through the same jobs. Some extension types will require different processing (or can be excluded). &lt;/p&gt;\n\n&lt;p&gt;I also need a dashboard:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need to be able to filter by client and project. &lt;/li&gt;\n&lt;li&gt;I need to see pipeline progress (by file) &lt;/li&gt;\n&lt;li&gt;Track the performance of each task. &lt;/li&gt;\n&lt;li&gt;Can be added to client app&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Re-run jobs/tasks: &lt;/p&gt;\n\n&lt;p&gt;If a file is made up of the following jobs: [A -&amp;gt; B -&amp;gt; [C, D]].&lt;br/&gt;\nI&amp;#39;d like to be able to re-run from step B, which will trigger the re-processing of C and D.&lt;/p&gt;\n\n&lt;p&gt;I feel like I have to combine different tools to get the pipeline + the dashboard.&lt;/p&gt;\n\n&lt;p&gt;We welcome any contributions or tool recommendations!&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14wuip0", "is_robot_indexable": true, "report_reasons": null, "author": "derekoms_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14wuip0/need_advice_ingestion_pipeline_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14wuip0/need_advice_ingestion_pipeline_tool/", "subreddit_subscribers": 115394, "created_utc": 1689089488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title states. I signed an offer letter with a company but have received a much better offer, with higher  salary and added benefits and I feel like I have much more things to learn in this new job technology wise. \n\nI am in Canada (Ontario), Not sure how to go about this?   The signed offer letter states 4 weeks notice period.  Anyone rejected after accepting an offer before start date, I want to do everything legally.  I usually follow this subreddit, not sure where to post!  \n\nThanks for your help ", "author_fullname": "t2_7my53me4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rejecting a signed offer for Data engineer before start date", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14xq76j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689172274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states. I signed an offer letter with a company but have received a much better offer, with higher  salary and added benefits and I feel like I have much more things to learn in this new job technology wise. &lt;/p&gt;\n\n&lt;p&gt;I am in Canada (Ontario), Not sure how to go about this?   The signed offer letter states 4 weeks notice period.  Anyone rejected after accepting an offer before start date, I want to do everything legally.  I usually follow this subreddit, not sure where to post!  &lt;/p&gt;\n\n&lt;p&gt;Thanks for your help &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "14xq76j", "is_robot_indexable": true, "report_reasons": null, "author": "alliswelll2020", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xq76j/rejecting_a_signed_offer_for_data_engineer_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xq76j/rejecting_a_signed_offer_for_data_engineer_before/", "subreddit_subscribers": 115394, "created_utc": 1689172274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm actually going to make this, so I want to make sure I don't leave out any great jokes. What should the office look like? What struggles should I include? What would make you laugh? ", "author_fullname": "t2_68z0an9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If there was a nature documentary about the \"datus engineerius\" and it's life inside of the corporate habitat, what kinds of things would for sure be pointed out?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xoc20", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689167792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m actually going to make this, so I want to make sure I don&amp;#39;t leave out any great jokes. What should the office look like? What struggles should I include? What would make you laugh? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14xoc20", "is_robot_indexable": true, "report_reasons": null, "author": "3spelledout", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xoc20/if_there_was_a_nature_documentary_about_the_datus/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xoc20/if_there_was_a_nature_documentary_about_the_datus/", "subreddit_subscribers": 115394, "created_utc": 1689167792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Follow up post: [Previous post](https://www.reddit.com/r/dataengineering/comments/14o8h6h/data_engineer_looking_to_study_mechanical/?utm_source=share&amp;utm_medium=web2x&amp;context=3)\n\nHello, So as you know from last time, I was working on a GKE project, where I deployed a docker image that retrieves data from ADH and inserts it into BigQuery, Thank you everyone for your recommendation and your advices, I finally managed to finish the project, I scheduled, everything works perfectly!! My question is: how can I further improve the project by automating the deployment process? I was thinking of terraform and ansible, but I'm here once again to ask about your opinion, and if you have any advice!! \n\nEdit: I used Makefile but I think it's a bit clumsy \n\nTL;DR I want to automate the deployment process of pushing an image to artifact registry on google + apply workload cronjobs using ansible/terraform, Any advices? :D", "author_fullname": "t2_5u3c1gj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate GKE deployment process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xj1mi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689152860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Follow up post: &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/14o8h6h/data_engineer_looking_to_study_mechanical/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;Previous post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hello, So as you know from last time, I was working on a GKE project, where I deployed a docker image that retrieves data from ADH and inserts it into BigQuery, Thank you everyone for your recommendation and your advices, I finally managed to finish the project, I scheduled, everything works perfectly!! My question is: how can I further improve the project by automating the deployment process? I was thinking of terraform and ansible, but I&amp;#39;m here once again to ask about your opinion, and if you have any advice!! &lt;/p&gt;\n\n&lt;p&gt;Edit: I used Makefile but I think it&amp;#39;s a bit clumsy &lt;/p&gt;\n\n&lt;p&gt;TL;DR I want to automate the deployment process of pushing an image to artifact registry on google + apply workload cronjobs using ansible/terraform, Any advices? :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14xj1mi", "is_robot_indexable": true, "report_reasons": null, "author": "iGodFather302", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xj1mi/automate_gke_deployment_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xj1mi/automate_gke_deployment_process/", "subreddit_subscribers": 115394, "created_utc": 1689152860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\nI have platform basing on Kafka. A lot of events go through various topics. I am writting streaming app to process events in real time(join on streams are possible). I consider Kafka Streams and Flink as a streaming technologies. My first thought was to use Kafka Streams but I heard that there is some problem with inner Kafka topics during join operations. Could uou gove me some advice what would u use?", "author_fullname": "t2_omva4fi3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flink or Kafka Streams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14xhrhj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689148809.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nI have platform basing on Kafka. A lot of events go through various topics. I am writting streaming app to process events in real time(join on streams are possible). I consider Kafka Streams and Flink as a streaming technologies. My first thought was to use Kafka Streams but I heard that there is some problem with inner Kafka topics during join operations. Could uou gove me some advice what would u use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14xhrhj", "is_robot_indexable": true, "report_reasons": null, "author": "BigDataMax", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xhrhj/flink_or_kafka_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xhrhj/flink_or_kafka_streams/", "subreddit_subscribers": 115394, "created_utc": 1689148809.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI'm on a relatively small and new Data Science team and currently trying to figure out the best tech stack/practices for deploying ML models. Right now we use BigQuery for our data base, DBT for the SQL pipeline (am I even referring to it properly?), and VertexAI workbench where the modeling and experimentation is done, and then a BitBucket repo. There is Airflow usage among the data engineers in other teams so there is a precedent there as far as orchestration goes.\n\nGiven that most of my past experience has focused on ad-hoc experimentation and not actual deployment, was hoping to get some general advice on how to set certain pipelines. \n\nUltimately, going forward the current suite of models will have to be retrained on a weekly basis, and the results will have to be uploaded to an environment where analysts can interface with the results.\n\n Currently trying to figure out best practices for building a robust pipeline given the tools we have above and was hoping to get some community feedback from more seasoned engineers.\n\nRight now most of the models are retrained manually.\n\n&amp;#x200B;\n\nGeneral outline of a pipeline that is currently in use that I feel needs to be refactored. \n\n1.)  SQL queries to pull data into python environment, where feature engineering in performed. (Some of the features are generated in the SQL queries where possible). Features are created using functions and the table with the target variable and features are written to a new table in GCP.\n\n2.)  In a separate script, read in the feature table, and then proceed to do some training (train-test splits / Cross Valdiation / Hyperparameter tuning) -&gt; hyper parameters are saved in a pickle file. Final model is trained using the found hyper paramters -&gt; final model saved in pickle file. \n\n3.) In separate script, the feature table is again read into python and the model is loaded from pickle and used to generate the output we need. The predicted values are then read into a final big query table which is referenced by analysts/data engineers for client facing platform.\n\n&amp;#x200B;\n\nWhat would be some better ways of structuring the pipeline above, or are there github ML pipeline examples that exist that I can learn from?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_bf43tms01", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying Data Engineering best practices to ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14x1r3k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689105657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a relatively small and new Data Science team and currently trying to figure out the best tech stack/practices for deploying ML models. Right now we use BigQuery for our data base, DBT for the SQL pipeline (am I even referring to it properly?), and VertexAI workbench where the modeling and experimentation is done, and then a BitBucket repo. There is Airflow usage among the data engineers in other teams so there is a precedent there as far as orchestration goes.&lt;/p&gt;\n\n&lt;p&gt;Given that most of my past experience has focused on ad-hoc experimentation and not actual deployment, was hoping to get some general advice on how to set certain pipelines. &lt;/p&gt;\n\n&lt;p&gt;Ultimately, going forward the current suite of models will have to be retrained on a weekly basis, and the results will have to be uploaded to an environment where analysts can interface with the results.&lt;/p&gt;\n\n&lt;p&gt;Currently trying to figure out best practices for building a robust pipeline given the tools we have above and was hoping to get some community feedback from more seasoned engineers.&lt;/p&gt;\n\n&lt;p&gt;Right now most of the models are retrained manually.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;General outline of a pipeline that is currently in use that I feel needs to be refactored. &lt;/p&gt;\n\n&lt;p&gt;1.)  SQL queries to pull data into python environment, where feature engineering in performed. (Some of the features are generated in the SQL queries where possible). Features are created using functions and the table with the target variable and features are written to a new table in GCP.&lt;/p&gt;\n\n&lt;p&gt;2.)  In a separate script, read in the feature table, and then proceed to do some training (train-test splits / Cross Valdiation / Hyperparameter tuning) -&amp;gt; hyper parameters are saved in a pickle file. Final model is trained using the found hyper paramters -&amp;gt; final model saved in pickle file. &lt;/p&gt;\n\n&lt;p&gt;3.) In separate script, the feature table is again read into python and the model is loaded from pickle and used to generate the output we need. The predicted values are then read into a final big query table which is referenced by analysts/data engineers for client facing platform.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What would be some better ways of structuring the pipeline above, or are there github ML pipeline examples that exist that I can learn from?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14x1r3k", "is_robot_indexable": true, "report_reasons": null, "author": "lyrical_empirical", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14x1r3k/applying_data_engineering_best_practices_to_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14x1r3k/applying_data_engineering_best_practices_to_ml/", "subreddit_subscribers": 115394, "created_utc": 1689105657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi team,\n\nI have ubuntu box where we run our python scripts of off.\n\nOne of my python scripts uses aws cli to extract data from....\n\nwhen i execute python script manually it will run.\n\nwhen i add this python script to run on crontab it starts...but doesnt go past 'aws cli command' executed almost at the very begining of python script.\n\nthe python script with aws CLI:\n\n \n\nsubprocess.run(\\[\"aws\", \"s3\", \"cp\", \"link\\_to\\_aws\\_stage\", \"download\\_to\\_path, \"--recursive\",\"--profile\",\"ABC\"\\])\n\n&amp;#x200B;\n\n**Question:**\n\nhow to workaround that?\n\nhow can i set which ubuntu users can access CLI ?\n\ndo you have any better idea\n\nthanks!\n\n&amp;#x200B;", "author_fullname": "t2_do9wxbfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help with aws cli", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14xqpch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689173421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi team,&lt;/p&gt;\n\n&lt;p&gt;I have ubuntu box where we run our python scripts of off.&lt;/p&gt;\n\n&lt;p&gt;One of my python scripts uses aws cli to extract data from....&lt;/p&gt;\n\n&lt;p&gt;when i execute python script manually it will run.&lt;/p&gt;\n\n&lt;p&gt;when i add this python script to run on crontab it starts...but doesnt go past &amp;#39;aws cli command&amp;#39; executed almost at the very begining of python script.&lt;/p&gt;\n\n&lt;p&gt;the python script with aws CLI:&lt;/p&gt;\n\n&lt;p&gt;subprocess.run([&amp;quot;aws&amp;quot;, &amp;quot;s3&amp;quot;, &amp;quot;cp&amp;quot;, &amp;quot;link_to_aws_stage&amp;quot;, &amp;quot;download_to_path, &amp;quot;--recursive&amp;quot;,&amp;quot;--profile&amp;quot;,&amp;quot;ABC&amp;quot;])&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;how to workaround that?&lt;/p&gt;\n\n&lt;p&gt;how can i set which ubuntu users can access CLI ?&lt;/p&gt;\n\n&lt;p&gt;do you have any better idea&lt;/p&gt;\n\n&lt;p&gt;thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14xqpch", "is_robot_indexable": true, "report_reasons": null, "author": "87keicam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xqpch/help_with_aws_cli/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xqpch/help_with_aws_cli/", "subreddit_subscribers": 115394, "created_utc": 1689173421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background on myself. I recently transitioned into data engineering after 5 years as a data analyst and have been working at a startup since the beginning of the year on a contract basis. I'm practically the sole data engineer so I'm building end-to-end pipelines and I get to work with Python, SQL, DBT, Fivetran, Airflow, Postgres, MongoDB, Elasticsearch. I also do some Devops stuff - DigitalOcean is our main cloud service, but I also work with GCP and Azure (albeit at a surface level). I love the tech stack, but at the same time I'm worried I'm developing bad habits as an engineer since I'm working mostly independently and there's no proper CI/CD, at least for now. There's also no guarantee I'm gonna have a job in around 6 months since there's only a certain amount of money that's been reserved to fund my position, though there are efforts to get more funding.\n\nI recently received a job offer at a FinTech company where the core tech stack is DBT, SQL, Snowflake, and Airflow. Certain things that concern me:\n\n* Python seems to be a small part of the role. In my interview, they mentioned that Python would mostly be used to write up Airflow jobs. The job description also emphasizes more on SQL for deploying analytics code.\n* No GCP, AWS or Azure.\n* I've seen multiple job titles for the role, mainly \"Data Engineer\" and \"Analytics Engineer\". I asked them about this and they said that data and analytics engineers are one and the same to them, and they had put up the role under multiple titles to attract as many applicants as possible.\n\nSome positives I see for the role:\n\n* It's in a bigger proper engineering team, with a scrum master, other DevOps engineers, QA, etc. and CI/CD pipelines in place.\n* I got along well with the hiring manager, who really seems to know his stuff.\n* The team is rapidly expanding, which to me seems like a good sign.\n\nI've been applying to jobs for a couple months now, and I frequently see GCP/AWS/Azure experience as a requirement. Python is easily the most enjoyable part of my job, and I want to be viewed more as programmer or a SWE-focused DE in the long term rather than an analytics engineer or a SQL developer (no hate against these types of roles, they're just not what I'm looking for).\n\nAny advice would be greatly appreciated!", "author_fullname": "t2_3bhqq33p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take this job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14xqg8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689172853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background on myself. I recently transitioned into data engineering after 5 years as a data analyst and have been working at a startup since the beginning of the year on a contract basis. I&amp;#39;m practically the sole data engineer so I&amp;#39;m building end-to-end pipelines and I get to work with Python, SQL, DBT, Fivetran, Airflow, Postgres, MongoDB, Elasticsearch. I also do some Devops stuff - DigitalOcean is our main cloud service, but I also work with GCP and Azure (albeit at a surface level). I love the tech stack, but at the same time I&amp;#39;m worried I&amp;#39;m developing bad habits as an engineer since I&amp;#39;m working mostly independently and there&amp;#39;s no proper CI/CD, at least for now. There&amp;#39;s also no guarantee I&amp;#39;m gonna have a job in around 6 months since there&amp;#39;s only a certain amount of money that&amp;#39;s been reserved to fund my position, though there are efforts to get more funding.&lt;/p&gt;\n\n&lt;p&gt;I recently received a job offer at a FinTech company where the core tech stack is DBT, SQL, Snowflake, and Airflow. Certain things that concern me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python seems to be a small part of the role. In my interview, they mentioned that Python would mostly be used to write up Airflow jobs. The job description also emphasizes more on SQL for deploying analytics code.&lt;/li&gt;\n&lt;li&gt;No GCP, AWS or Azure.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve seen multiple job titles for the role, mainly &amp;quot;Data Engineer&amp;quot; and &amp;quot;Analytics Engineer&amp;quot;. I asked them about this and they said that data and analytics engineers are one and the same to them, and they had put up the role under multiple titles to attract as many applicants as possible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Some positives I see for the role:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s in a bigger proper engineering team, with a scrum master, other DevOps engineers, QA, etc. and CI/CD pipelines in place.&lt;/li&gt;\n&lt;li&gt;I got along well with the hiring manager, who really seems to know his stuff.&lt;/li&gt;\n&lt;li&gt;The team is rapidly expanding, which to me seems like a good sign.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve been applying to jobs for a couple months now, and I frequently see GCP/AWS/Azure experience as a requirement. Python is easily the most enjoyable part of my job, and I want to be viewed more as programmer or a SWE-focused DE in the long term rather than an analytics engineer or a SQL developer (no hate against these types of roles, they&amp;#39;re just not what I&amp;#39;m looking for).&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14xqg8d", "is_robot_indexable": true, "report_reasons": null, "author": "timbaktubear", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xqg8d/should_i_take_this_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xqg8d/should_i_take_this_job/", "subreddit_subscribers": 115394, "created_utc": 1689172853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone ,\n\nI have a lot of HTML files saved in txt on my raw container, and I have to extract only the necessary information from them.Each html file could become a JSON, because they have some levels of nested information.\n\nWhat transformation would be the best in that case to make the data more structured to silver/structured layer?\n\nCreate a JSON?\n\nCreate a parquet file for each html, but duplicate data from the first nests?\n\nCreate a parquet file for each nest with a key to join them in the database?\n\nOr any other path that u guys have in mind?  \n\n\n&amp;#x200B;", "author_fullname": "t2_2t4hdsut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with HTML files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14xqces", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689172615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone ,&lt;/p&gt;\n\n&lt;p&gt;I have a lot of HTML files saved in txt on my raw container, and I have to extract only the necessary information from them.Each html file could become a JSON, because they have some levels of nested information.&lt;/p&gt;\n\n&lt;p&gt;What transformation would be the best in that case to make the data more structured to silver/structured layer?&lt;/p&gt;\n\n&lt;p&gt;Create a JSON?&lt;/p&gt;\n\n&lt;p&gt;Create a parquet file for each html, but duplicate data from the first nests?&lt;/p&gt;\n\n&lt;p&gt;Create a parquet file for each nest with a key to join them in the database?&lt;/p&gt;\n\n&lt;p&gt;Or any other path that u guys have in mind?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14xqces", "is_robot_indexable": true, "report_reasons": null, "author": "ltofanelli", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xqces/working_with_html_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xqces/working_with_html_files/", "subreddit_subscribers": 115394, "created_utc": 1689172615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have been a lurker forever in this sub and today i have decided to know your opinions in some points regarding the implementation of functional programming in Data Engineering (DE).\n\nAfter reading around this paradigm (most articles i have found converge to [Maxime Beauchemin article](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)), there are two points that makes my head scratch a bit:\n\n&amp;#x200B;\n\n* **Dimensional snapshots**:  The fact that this can create a lot of redundancy in dimensional tables, this really does not sound like a problem to me. I actually like the fact that with this approach avoids upserts and SCD's by just re-processing the data in a given time period. What i would like to understand is, for those who already implemented dimension snapshots, if the redundancy on the tables took a toll on the visualization tools performance big enough for you to get back to SCDs or something hybrid.\n* **Late arriving facts**: this is the point that really makes me avoid functional programming. Knowing that to we have to center our focus based on the event reception or processing time and not on the event time, it means in my opinion that the dimension partition scheme should be based on the event processing time too. However, the analysts surely do their work based on the event time which it will not benefit from the partition pruning. That could make any query based on the event time be way more costly.\n\nTake in note that i have been working more recently with BigQuery, in which the storage capacity is not a problem and you can partition a table with one column only (aside from clustering). Even in platforms that allows tables with multiple partitions i still think the second point potential costs and performance can be heavily affected.\n\nTake in account these two points, you think functional programming still can be something worth to implement in DE context?\n\nI hope my doubts were clear enough for you to share your take. Best regards!\n\n&amp;#x200B;", "author_fullname": "t2_4errm1ck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Funtional Programming in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14xq53v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689172144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have been a lurker forever in this sub and today i have decided to know your opinions in some points regarding the implementation of functional programming in Data Engineering (DE).&lt;/p&gt;\n\n&lt;p&gt;After reading around this paradigm (most articles i have found converge to &lt;a href=\"https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a\"&gt;Maxime Beauchemin article&lt;/a&gt;), there are two points that makes my head scratch a bit:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Dimensional snapshots&lt;/strong&gt;:  The fact that this can create a lot of redundancy in dimensional tables, this really does not sound like a problem to me. I actually like the fact that with this approach avoids upserts and SCD&amp;#39;s by just re-processing the data in a given time period. What i would like to understand is, for those who already implemented dimension snapshots, if the redundancy on the tables took a toll on the visualization tools performance big enough for you to get back to SCDs or something hybrid.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Late arriving facts&lt;/strong&gt;: this is the point that really makes me avoid functional programming. Knowing that to we have to center our focus based on the event reception or processing time and not on the event time, it means in my opinion that the dimension partition scheme should be based on the event processing time too. However, the analysts surely do their work based on the event time which it will not benefit from the partition pruning. That could make any query based on the event time be way more costly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Take in note that i have been working more recently with BigQuery, in which the storage capacity is not a problem and you can partition a table with one column only (aside from clustering). Even in platforms that allows tables with multiple partitions i still think the second point potential costs and performance can be heavily affected.&lt;/p&gt;\n\n&lt;p&gt;Take in account these two points, you think functional programming still can be something worth to implement in DE context?&lt;/p&gt;\n\n&lt;p&gt;I hope my doubts were clear enough for you to share your take. Best regards!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?auto=webp&amp;s=cf073d9e898ab6b6aac9cbd3d617eb1a8d77093f", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6ca8361de7a95cb6c47013c1938266c06cf1bff", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fae9102fa6d32f067e38cc3b24cc65481bb6ed33", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3efad0c05c8737f8d5c4ac99e42e7579dd97057", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4e73a5bddad916ba74ea62469de010438a9a993", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fada96526d687a1d1efd7a1d8d1e5d7d033d9c8", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/XW0ef_VBf4RJSfDgg9pqQAOBt7SyX-VVlLvDbPwGbFE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dd93c47a0b2c95deabadee9d4fe6e3cc05529c5e", "width": 1080, "height": 720}], "variants": {}, "id": "QnHDt9HlR913WRkisNYJkFlw9Lr3Bt7UcxRxAQAkPdY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14xq53v", "is_robot_indexable": true, "report_reasons": null, "author": "lou1uol", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14xq53v/funtional_programming_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14xq53v/funtional_programming_in_data_engineering/", "subreddit_subscribers": 115394, "created_utc": 1689172144.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}