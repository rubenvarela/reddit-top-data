{"kind": "Listing", "data": {"after": "t3_14s3rtu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Article found here!", "author_fullname": "t2_dz0l9v69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate launching 32TB drives this year!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_14s8yao", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 284, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 284, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oOsV7fAIKarJV9sj2zeQrfSXBf7NOzSCVBMSYjkHU-Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688649390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ghacks.net", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article found here!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.ghacks.net/2023/06/11/seagate-launching-32-tb-hard-drives-later-this-year-and-50-tb-on-the-horizon/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?auto=webp&amp;v=enabled&amp;s=c424091ee8ae331dea0784b65640253c50ed6e96", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b9fab635b64e68701862bf61b09ab5681ef32b3", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5743f14db31fda541efc9e35c567d828a9a4a26", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c7390dcbe62f92fbbf30178ffd66f0d017e78bd", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98be2ed03ff661598f7bff56210948a0690a9e61", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=475c92607a0e145542fa4b9fe11975967f7eb7d3", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/dFsn00mQdT8Z_Ep7aSuot6gikND-fLNJP5W4eoTTfn4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ad863cde071f77a692f7254685ea134c868f8c8", "width": 1080, "height": 720}], "variants": {}, "id": "IabfKLadeUPQXSUu6-Q9KojvopYnJLkZqaXbitGfLbI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s8yao", "is_robot_indexable": true, "report_reasons": null, "author": "stereojorge", "discussion_type": null, "num_comments": 94, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14s8yao/seagate_launching_32tb_drives_this_year/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.ghacks.net/2023/06/11/seagate-launching-32-tb-hard-drives-later-this-year-and-50-tb-on-the-horizon/", "subreddit_subscribers": 691463, "created_utc": 1688649390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am sure others had a thought of a way to use old computers to navigate the old internet; although that assume you need a server and everything else related to it, to simulate the network as it was.\n\nI am looking at something more simpler: a way to surf internet locally, on a old pentium 2 as if I was online in 1994. Not looking to simulate protocols or what not; just a big dump of all the pages as they were in 1994, so I can put everything on a SD card (256 GB or 512 for example), and use data on it.\n\nI thought I could crawl something like wayback machine, but the amount of technical involvement to remove the extra stuff would be impossible for my limited free time pool; so I was hoping someone made something like an archive, that froze in time websites as they were, and make them available offline. Like a downloadable archive/image I can unpack on my old machine, and that let me use that computer as if it was 1994. \n\nIf there is nothing pre-made, what would you suggest to do, to achieve this? I know I can rely on internet archive and wayback machine for most part, but they are not offline and my concern is that they may eventually disappear; and I don't believe that majority of old internet (not really interested in adult websites and similar, which was a big chunk of what made internet big at that time) would not fit in 5-10 TB at most; so a subset including things like yahoo, geocities, delphi and things like that would fit on a 512 GB card or so.\n\nThoughts?", "author_fullname": "t2_18wupfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offline archives to replicate how internet was in 1994?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sgi0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688665716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure others had a thought of a way to use old computers to navigate the old internet; although that assume you need a server and everything else related to it, to simulate the network as it was.&lt;/p&gt;\n\n&lt;p&gt;I am looking at something more simpler: a way to surf internet locally, on a old pentium 2 as if I was online in 1994. Not looking to simulate protocols or what not; just a big dump of all the pages as they were in 1994, so I can put everything on a SD card (256 GB or 512 for example), and use data on it.&lt;/p&gt;\n\n&lt;p&gt;I thought I could crawl something like wayback machine, but the amount of technical involvement to remove the extra stuff would be impossible for my limited free time pool; so I was hoping someone made something like an archive, that froze in time websites as they were, and make them available offline. Like a downloadable archive/image I can unpack on my old machine, and that let me use that computer as if it was 1994. &lt;/p&gt;\n\n&lt;p&gt;If there is nothing pre-made, what would you suggest to do, to achieve this? I know I can rely on internet archive and wayback machine for most part, but they are not offline and my concern is that they may eventually disappear; and I don&amp;#39;t believe that majority of old internet (not really interested in adult websites and similar, which was a big chunk of what made internet big at that time) would not fit in 5-10 TB at most; so a subset including things like yahoo, geocities, delphi and things like that would fit on a 512 GB card or so.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14sgi0i", "is_robot_indexable": true, "report_reasons": null, "author": "fttklr", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sgi0i/offline_archives_to_replicate_how_internet_was_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sgi0i/offline_archives_to_replicate_how_internet_was_in/", "subreddit_subscribers": 691463, "created_utc": 1688665716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whenever I edit videos and render them, I output them into a folder called \"Rendered Videos\". This folder is on an NVME SSD and currently has 135 files totaling 22.1 GB, so not even that big. However, opening this folder takes 10-20 seconds. I have no idea why this happens, and I assumed it would be fixed once I replaced my HDD with an NVME drive, but that did nothing. I assume it's something to do with generating thumbnails, but has anyone else experienced this and do you have a fix? The folder optimization is already set to \"Videos\".", "author_fullname": "t2_7xw9f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have a videos folder that takes forever to load?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14rvx0e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688612429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whenever I edit videos and render them, I output them into a folder called &amp;quot;Rendered Videos&amp;quot;. This folder is on an NVME SSD and currently has 135 files totaling 22.1 GB, so not even that big. However, opening this folder takes 10-20 seconds. I have no idea why this happens, and I assumed it would be fixed once I replaced my HDD with an NVME drive, but that did nothing. I assume it&amp;#39;s something to do with generating thumbnails, but has anyone else experienced this and do you have a fix? The folder optimization is already set to &amp;quot;Videos&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14rvx0e", "is_robot_indexable": true, "report_reasons": null, "author": "Zarrex", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14rvx0e/anyone_have_a_videos_folder_that_takes_forever_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14rvx0e/anyone_have_a_videos_folder_that_takes_forever_to/", "subreddit_subscribers": 691463, "created_utc": 1688612429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have one 2TB external drive hooked up to a raspberry pi. It\u2019s going to fill up soon, and while I do have more room to expand with better drives I\u2019m wondering if it\u2019s worth it to get sata drives and connect those to the pi through the methods available.", "author_fullname": "t2_wr2c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do I go next?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sgbb6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688665317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have one 2TB external drive hooked up to a raspberry pi. It\u2019s going to fill up soon, and while I do have more room to expand with better drives I\u2019m wondering if it\u2019s worth it to get sata drives and connect those to the pi through the methods available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sgbb6", "is_robot_indexable": true, "report_reasons": null, "author": "blud97", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sgbb6/where_do_i_go_next/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sgbb6/where_do_i_go_next/", "subreddit_subscribers": 691463, "created_utc": 1688665317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Setup is a 80tb Truenas, full of emails/attachments, documents, pdf's, machine calibrations, images, videos, basically all the stuff. \n\nright now its a haystack search for anything that is older than maybe year. Im looking for a way to metatag each file, based on content and create a database of file locations. \n\nIve looked into OpenKM, and it has some potential but the open codebase is lets just say, ancient. like Java 1.8 ancient. \n\nbut its striking along the right direction. Additionally, with the documents/emails, we would like to embed them with openai to make contextual searches easier.  this will likely take some coding as I don't know of any off the shelf solution here. \n\n\nSo other than OpenKM anyone know of a decent document manager?", "author_fullname": "t2_ax716", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a better way to organize documents in massive nas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sgoan", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688666767.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688666106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Setup is a 80tb Truenas, full of emails/attachments, documents, pdf&amp;#39;s, machine calibrations, images, videos, basically all the stuff. &lt;/p&gt;\n\n&lt;p&gt;right now its a haystack search for anything that is older than maybe year. Im looking for a way to metatag each file, based on content and create a database of file locations. &lt;/p&gt;\n\n&lt;p&gt;Ive looked into OpenKM, and it has some potential but the open codebase is lets just say, ancient. like Java 1.8 ancient. &lt;/p&gt;\n\n&lt;p&gt;but its striking along the right direction. Additionally, with the documents/emails, we would like to embed them with openai to make contextual searches easier.  this will likely take some coding as I don&amp;#39;t know of any off the shelf solution here. &lt;/p&gt;\n\n&lt;p&gt;So other than OpenKM anyone know of a decent document manager?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sgoan", "is_robot_indexable": true, "report_reasons": null, "author": "bargaindownhill", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sgoan/need_a_better_way_to_organize_documents_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sgoan/need_a_better_way_to_organize_documents_in/", "subreddit_subscribers": 691463, "created_utc": 1688666106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm about to archive a lot of stuff on Blu Ray disks as cold backup and while it's nothing top secret I would like to prevent anyone who grabs the disk to be able to read from them.  \nMy first idea was to use a password protected zip folder which I then burn on the disk but from what I read this makes it more vulnerable to bit rot/read errors so I wanted to ask if there is a better way.\n\nThanks in advance ", "author_fullname": "t2_ab9muaji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Password protected Blu Ray Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s21z5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688630296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to archive a lot of stuff on Blu Ray disks as cold backup and while it&amp;#39;s nothing top secret I would like to prevent anyone who grabs the disk to be able to read from them.&lt;br/&gt;\nMy first idea was to use a password protected zip folder which I then burn on the disk but from what I read this makes it more vulnerable to bit rot/read errors so I wanted to ask if there is a better way.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s21z5", "is_robot_indexable": true, "report_reasons": null, "author": "Atomfried_Fallout", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14s21z5/password_protected_blu_ray_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s21z5/password_protected_blu_ray_archive/", "subreddit_subscribers": 691463, "created_utc": 1688630296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Everything you need/want to know is in the readme, but the new/important bits are at the link below, demonstrating how to use a special keyword to trigger the script to scan all subreddits automatically that your account is joined to:\n\n&lt;https://github.com/michealespinola/reddit.wikidownloader#example-command-syntax&gt;\n\nYou can still manually supply a comma-delimited list of subreddits to target for anything you are not subscribed to or if you don't want to try to download everything you are joined/subscribed to.\n\nThere have also been some expected bug fixes as well (particularly a directory creation issue). There is still no modified/exists type of check, so this is a blind downloader that always overwrites existing.", "author_fullname": "t2_fne37", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reddit.wikidownloader v2 - now download all wikis of all account-joined subreddits automatically", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14sszo3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688694602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everything you need/want to know is in the readme, but the new/important bits are at the link below, demonstrating how to use a special keyword to trigger the script to scan all subreddits automatically that your account is joined to:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/michealespinola/reddit.wikidownloader#example-command-syntax\"&gt;https://github.com/michealespinola/reddit.wikidownloader#example-command-syntax&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can still manually supply a comma-delimited list of subreddits to target for anything you are not subscribed to or if you don&amp;#39;t want to try to download everything you are joined/subscribed to.&lt;/p&gt;\n\n&lt;p&gt;There have also been some expected bug fixes as well (particularly a directory creation issue). There is still no modified/exists type of check, so this is a blind downloader that always overwrites existing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?auto=webp&amp;v=enabled&amp;s=d6734c85de466e76ee47b259518c2c76b75b3c20", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=694216249983ca0c208b234185464e8be19cd4b4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44df5c4c3cbb02c1c81ea9d5d33cf49ce4818c73", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a590155976cffd637022e7f38195b79f1c2717e4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b5efa7f72c343126e56c6f01ed458039c1f3c56", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c60e372091f83b45751ce3f3a905dc619d9f4fe", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/XOIImI52Z6QrkdgO7BmlG4hbdv3QRF8aeuCVtiQQHRE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8e9b52a3d7bca7e74b5c83ab08daad12b88a778", "width": 1080, "height": 540}], "variants": {}, "id": "_YaNy8JBy_3iEAvaBYoFCpxQWGPY605Bt_mhQ3s5J1k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB+8TB+4TB (RAID 1)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sszo3", "is_robot_indexable": true, "report_reasons": null, "author": "Empyrealist", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14sszo3/redditwikidownloader_v2_now_download_all_wikis_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sszo3/redditwikidownloader_v2_now_download_all_wikis_of/", "subreddit_subscribers": 691463, "created_utc": 1688694602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Talks Machina, Undeadwood, and Between the Sheets. \n\nThese shows are being scrubbed from the web after the news broke about the show\u2019s host, Brian Foster, being abusive / nearly homicidal to a crew mate. Just made private on YouTube and deleted from Spotify today.\n\nRequesting if anyone has a backups of the shows", "author_fullname": "t2_5pit6tsn5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Critical Role - Three Series Delisted Today", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14srh67", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688690676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Talks Machina, Undeadwood, and Between the Sheets. &lt;/p&gt;\n\n&lt;p&gt;These shows are being scrubbed from the web after the news broke about the show\u2019s host, Brian Foster, being abusive / nearly homicidal to a crew mate. Just made private on YouTube and deleted from Spotify today.&lt;/p&gt;\n\n&lt;p&gt;Requesting if anyone has a backups of the shows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14srh67", "is_robot_indexable": true, "report_reasons": null, "author": "texas_bacchus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14srh67/critical_role_three_series_delisted_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14srh67/critical_role_three_series_delisted_today/", "subreddit_subscribers": 691463, "created_utc": 1688690676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to access this part of the site herculistas.gr but it says it has not been archives, is there trully no way to access it or is there some sort of trick maybe?\n\nThe link i want to access is:\n(http://www.herculistas.gr/index.php?option=com_content&amp;task=view&amp;id=223&amp;Itemid=40)", "author_fullname": "t2_bgwg2bbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive wayback machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_14snpng", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/M-h8TKUkCozEEPNyXaJvMw_SKXewiHDKp8DULNHGEaw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688681768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to access this part of the site herculistas.gr but it says it has not been archives, is there trully no way to access it or is there some sort of trick maybe?&lt;/p&gt;\n\n&lt;p&gt;The link i want to access is:\n(&lt;a href=\"http://www.herculistas.gr/index.php?option=com_content&amp;amp;task=view&amp;amp;id=223&amp;amp;Itemid=40\"&gt;http://www.herculistas.gr/index.php?option=com_content&amp;amp;task=view&amp;amp;id=223&amp;amp;Itemid=40&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3t0qocet5fab1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?auto=webp&amp;v=enabled&amp;s=38bb0abfaeb5ade433ef78a9d78791ede1ad8cb0", "width": 1080, "height": 696}, "resolutions": [{"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1749cfec64ee912fa5cb16d890d71c539094bf41", "width": 108, "height": 69}, {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a64ed26759b64c62ec2d1c60be3e83efd38f29", "width": 216, "height": 139}, {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dd2442c010be74bff9ea0c110e9ad841d068adf", "width": 320, "height": 206}, {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8309f00683a523b1098f53dc14282010fccf7de", "width": 640, "height": 412}, {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc17f83e059f3fefb1416636bc511ab747cf890c", "width": 960, "height": 618}, {"url": "https://preview.redd.it/3t0qocet5fab1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b444905f6bb66d9662f1812dbb17b61fa4c7a56a", "width": 1080, "height": 696}], "variants": {}, "id": "yho1wb18Nfta0CF0b5JeKQPKC8bAmSU3PRx_z7q5Gm0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14snpng", "is_robot_indexable": true, "report_reasons": null, "author": "D4TA27", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14snpng/archive_wayback_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3t0qocet5fab1.jpg", "subreddit_subscribers": 691463, "created_utc": 1688681768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks, new to data storage and analog tape conversions, but I'm picking up a project to convert an archive of old Hi8 tapes to digital; in my research, I've found a ton of great articles and posts both on this subreddit and over at [digitalfaq.com](https://digitalfaq.com). Seems like many posts on that forum are written by site staff LordSmurf, who is well-known even on this platform and [videohelp.com](https://videohelp.com), and his advice is often referred to in Reddit posts on the topic. I think I'm ready to begin the process and start procuring equipment for my project, but it seems like a lot of the recommended equipment is only available secondhand either from eBay (hit or miss) or from forum members such as LordSmurf himself, who in particular refurbs a lot of these old units and sells them.\n\n&amp;#x200B;\n\nFrom what I gather, a lot of his advice is sound, and he has an incredible amount of experience with analog AV equipment and doing conversions, but I am a little hesitant to drop upwards of $2k on equipment I know little about. I'm not new to buying/selling on forums, but most of the gear that he has listed doesn't have any info turn up on Google, so I feel like I can't do my due diligence in researching what I'm buying. No company page, product page, or even other sellers of such units on eBay or other marketplaces. Now, I know that many of these units are so old and had such a niche market that webpages for them can't be expected, and that the advice preached on that forum is to buy, use, and resell and consider any incurred losses a rental fee. Normally I'd feel comfortable enough with that; however, I haven't found any other way to resell the equipment when I'm finished (other than digitalFAQ itself), so I really don't want to be stuck with such expensive, antiquated equipment that can't be easily resold.\n\n&amp;#x200B;\n\nFurther, I saw a [thread here on this very subreddit](https://www.reddit.com/r/DataHoarder/comments/z9rv0z/word_of_caution_for_anyone_looking_at_using/) a few months back that detailed a very poor experience with LordSmurf. It certainly doesn't bode well, but to be fair, it's the only instance of a negative interaction with him that I've found, and it was for a tape conversion service. I'm only looking to buy hardware from him, and it seems most people who have transacted with him are happy with their purchase. I'm also aware that he is somewhat of a polarizing figure, with some users here deriding his rather gatekeep-y, \"my way or the highway\" attitude. And yet, some of the most detailed how-to guides, equipment recs and reviews, and general advice I can find online about this rather esoteric and nearly extinct art of analog tape conversions are written by him.\n\n&amp;#x200B;\n\nSo, anyone have experiences on buying used and refurbished gear from this particular individual on [digitalFAQ.com](https://digitalFAQ.com)? Do they really retain their resale value that well, and were you able to offload your high dollar equipment easily after using it?\n\n&amp;#x200B;\n\nThanks in advance, and please let me know if there's better subreddits to post this in!", "author_fullname": "t2_9cu5q1yg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitalfaq.com, Refurb Gear from LordSmurf Trustworthy? Resale value hold up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14si54q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688669399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, new to data storage and analog tape conversions, but I&amp;#39;m picking up a project to convert an archive of old Hi8 tapes to digital; in my research, I&amp;#39;ve found a ton of great articles and posts both on this subreddit and over at &lt;a href=\"https://digitalfaq.com\"&gt;digitalfaq.com&lt;/a&gt;. Seems like many posts on that forum are written by site staff LordSmurf, who is well-known even on this platform and &lt;a href=\"https://videohelp.com\"&gt;videohelp.com&lt;/a&gt;, and his advice is often referred to in Reddit posts on the topic. I think I&amp;#39;m ready to begin the process and start procuring equipment for my project, but it seems like a lot of the recommended equipment is only available secondhand either from eBay (hit or miss) or from forum members such as LordSmurf himself, who in particular refurbs a lot of these old units and sells them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;From what I gather, a lot of his advice is sound, and he has an incredible amount of experience with analog AV equipment and doing conversions, but I am a little hesitant to drop upwards of $2k on equipment I know little about. I&amp;#39;m not new to buying/selling on forums, but most of the gear that he has listed doesn&amp;#39;t have any info turn up on Google, so I feel like I can&amp;#39;t do my due diligence in researching what I&amp;#39;m buying. No company page, product page, or even other sellers of such units on eBay or other marketplaces. Now, I know that many of these units are so old and had such a niche market that webpages for them can&amp;#39;t be expected, and that the advice preached on that forum is to buy, use, and resell and consider any incurred losses a rental fee. Normally I&amp;#39;d feel comfortable enough with that; however, I haven&amp;#39;t found any other way to resell the equipment when I&amp;#39;m finished (other than digitalFAQ itself), so I really don&amp;#39;t want to be stuck with such expensive, antiquated equipment that can&amp;#39;t be easily resold.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Further, I saw a &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/z9rv0z/word_of_caution_for_anyone_looking_at_using/\"&gt;thread here on this very subreddit&lt;/a&gt; a few months back that detailed a very poor experience with LordSmurf. It certainly doesn&amp;#39;t bode well, but to be fair, it&amp;#39;s the only instance of a negative interaction with him that I&amp;#39;ve found, and it was for a tape conversion service. I&amp;#39;m only looking to buy hardware from him, and it seems most people who have transacted with him are happy with their purchase. I&amp;#39;m also aware that he is somewhat of a polarizing figure, with some users here deriding his rather gatekeep-y, &amp;quot;my way or the highway&amp;quot; attitude. And yet, some of the most detailed how-to guides, equipment recs and reviews, and general advice I can find online about this rather esoteric and nearly extinct art of analog tape conversions are written by him.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So, anyone have experiences on buying used and refurbished gear from this particular individual on &lt;a href=\"https://digitalFAQ.com\"&gt;digitalFAQ.com&lt;/a&gt;? Do they really retain their resale value that well, and were you able to offload your high dollar equipment easily after using it?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance, and please let me know if there&amp;#39;s better subreddits to post this in!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bmmH_g4z-FVRqpGv-GJa1tq49F3UBGXFuzmmXykwmrI.jpg?auto=webp&amp;v=enabled&amp;s=527dcb0135a05ee6745b590998ef4629a938d8c3", "width": 73, "height": 73}, "resolutions": [], "variants": {}, "id": "Q40bQkiTOYW4hkF-ZLWpRnee2w3QwOGF9g-IeYJ8RUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14si54q", "is_robot_indexable": true, "report_reasons": null, "author": "twopeaksmall", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14si54q/digitalfaqcom_refurb_gear_from_lordsmurf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14si54q/digitalfaqcom_refurb_gear_from_lordsmurf/", "subreddit_subscribers": 691463, "created_utc": 1688669399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm think of either two products for the internal HDD:  \n\\- Seagate Barracuda ST6000DM003   \nor  \n\\- Western Digital  WD60EZAZ   \n\n\nPurpose : Basic storage for documents , images, MP3 audio, a few videos maybe  \n\\*The OS, games and other stuff will be sent to the M.2 SSD\\*  \n\n\nWhich of these two should I choose?  \nOr it doesn't matter?", "author_fullname": "t2_9h4xh041", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "6TB internal desktop drive (for home use) : Seagate Barracuda or Western Digital Blue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14scem1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688657006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m think of either two products for the internal HDD:&lt;br/&gt;\n- Seagate Barracuda ST6000DM003&lt;br/&gt;\nor&lt;br/&gt;\n- Western Digital  WD60EZAZ   &lt;/p&gt;\n\n&lt;p&gt;Purpose : Basic storage for documents , images, MP3 audio, a few videos maybe&lt;br/&gt;\n*The OS, games and other stuff will be sent to the M.2 SSD*  &lt;/p&gt;\n\n&lt;p&gt;Which of these two should I choose?&lt;br/&gt;\nOr it doesn&amp;#39;t matter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14scem1", "is_robot_indexable": true, "report_reasons": null, "author": "blackcyborg009", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14scem1/6tb_internal_desktop_drive_for_home_use_seagate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14scem1/6tb_internal_desktop_drive_for_home_use_seagate/", "subreddit_subscribers": 691463, "created_utc": 1688657006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Help friends --\n\nI've had \"fight club\" unlimited Google Drive at $12/month since 2016. I have about 60 TB of data up on my legacy Google Drive. Now, I can't upload anything -- getting the dreaded \"you're out of storage space\" notifications. \n\nI'm a filmmaker shooting 4k+ for personal and commercial film projects. This data is my whole life. It exists (in theory) on little hard drives all around my office, but who knows when they may fail. So essentially this cloud storage is a repository of my whole career.\n\nDetails: I have access to a gbps ethernet cable and I'm working off of Mac OS. I'm thinking of ordering 20+TB drives and downloading everything as it is off of Google Drive, to then reupload to another cloud solution. However, I'm leaving on Tuesday for 3 weeks on a shoot in Europe. Even if I downloaded Parallels Access and left my computer open for 3 weeks and remotely checked on the status of these transfers, something will still probably go wrong with all the data I have to download and/or transfer.  \n\nI've also been experiencing [this issue](https://support.google.com/drive/thread/125012643/fix-google-drive-for-desktop-mac-big-sur-server-connections-interrupted-message-on-finder?hl=en) from Google Drive where I go to transfer lots of data from Google Drive to a destination, and within a few minutes it says \"server connection interrupted.\" So manually dragging and dropping and letting it cook is causing a ton of problems for me.\n\nWoof.\n\nMy questions, in order:  \n1) Any recommendations on cloud storage with unlimited data? I'm tempted by [Sync.com](https://Sync.com), but hearing that they only offer storage. I honestly dgaf, as long as it's accessible. Should I bite? Any other unlimited options that I should consider? I would like to only do this big migration once. \n\n2) Would it be terribly outside the order of operations to transfer this entirely to another cloud solution, and then download to a NAS? I just don't have the time to get the hardware purchased, installed, and set up before I leave for this shoot in a few days.\n\n3) Any thoughts on when Google might start deleting my data? \n\n4) What are the best cloud storage migration solutions? I used to use CloudHQ for this, but it looks like they don't transfer to sync.com.\n\n5) What questions have I not asked that I should be considering here?\n\n&amp;#x200B;\n\n//\n\n&amp;#x200B;\n\nThanks in advance for your kindness and advice at this ridiculous and potentially painful juncture.", "author_fullname": "t2_kzy98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filmmaker with 54 TB of data on Google Drive Unlimited now being throttled -- what next?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sbblz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688654756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Help friends --&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had &amp;quot;fight club&amp;quot; unlimited Google Drive at $12/month since 2016. I have about 60 TB of data up on my legacy Google Drive. Now, I can&amp;#39;t upload anything -- getting the dreaded &amp;quot;you&amp;#39;re out of storage space&amp;quot; notifications. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a filmmaker shooting 4k+ for personal and commercial film projects. This data is my whole life. It exists (in theory) on little hard drives all around my office, but who knows when they may fail. So essentially this cloud storage is a repository of my whole career.&lt;/p&gt;\n\n&lt;p&gt;Details: I have access to a gbps ethernet cable and I&amp;#39;m working off of Mac OS. I&amp;#39;m thinking of ordering 20+TB drives and downloading everything as it is off of Google Drive, to then reupload to another cloud solution. However, I&amp;#39;m leaving on Tuesday for 3 weeks on a shoot in Europe. Even if I downloaded Parallels Access and left my computer open for 3 weeks and remotely checked on the status of these transfers, something will still probably go wrong with all the data I have to download and/or transfer.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also been experiencing &lt;a href=\"https://support.google.com/drive/thread/125012643/fix-google-drive-for-desktop-mac-big-sur-server-connections-interrupted-message-on-finder?hl=en\"&gt;this issue&lt;/a&gt; from Google Drive where I go to transfer lots of data from Google Drive to a destination, and within a few minutes it says &amp;quot;server connection interrupted.&amp;quot; So manually dragging and dropping and letting it cook is causing a ton of problems for me.&lt;/p&gt;\n\n&lt;p&gt;Woof.&lt;/p&gt;\n\n&lt;p&gt;My questions, in order:&lt;br/&gt;\n1) Any recommendations on cloud storage with unlimited data? I&amp;#39;m tempted by &lt;a href=\"https://Sync.com\"&gt;Sync.com&lt;/a&gt;, but hearing that they only offer storage. I honestly dgaf, as long as it&amp;#39;s accessible. Should I bite? Any other unlimited options that I should consider? I would like to only do this big migration once. &lt;/p&gt;\n\n&lt;p&gt;2) Would it be terribly outside the order of operations to transfer this entirely to another cloud solution, and then download to a NAS? I just don&amp;#39;t have the time to get the hardware purchased, installed, and set up before I leave for this shoot in a few days.&lt;/p&gt;\n\n&lt;p&gt;3) Any thoughts on when Google might start deleting my data? &lt;/p&gt;\n\n&lt;p&gt;4) What are the best cloud storage migration solutions? I used to use CloudHQ for this, but it looks like they don&amp;#39;t transfer to sync.com.&lt;/p&gt;\n\n&lt;p&gt;5) What questions have I not asked that I should be considering here?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;//&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your kindness and advice at this ridiculous and potentially painful juncture.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sbblz", "is_robot_indexable": true, "report_reasons": null, "author": "staroats", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sbblz/filmmaker_with_54_tb_of_data_on_google_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sbblz/filmmaker_with_54_tb_of_data_on_google_drive/", "subreddit_subscribers": 691463, "created_utc": 1688654756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A question for hoarders in general:\n\nIs there a file system similar to Mergerfs (i. e. mounted over existing directories) but aimed at redundancy?\n\nLet's say I have a fraction of my data that needs more than an occasional cold backup (two external HDs, mergerd too) but at the same time doesn't fit into a versioning service (git like).\n\nBasically today I use syncthing for, um, loss prevention, but I'd like something that works on a single, multi-disk PC", "author_fullname": "t2_sg76dvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mergerfs, but with redundancy. Anyone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sa2v1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688651995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A question for hoarders in general:&lt;/p&gt;\n\n&lt;p&gt;Is there a file system similar to Mergerfs (i. e. mounted over existing directories) but aimed at redundancy?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have a fraction of my data that needs more than an occasional cold backup (two external HDs, mergerd too) but at the same time doesn&amp;#39;t fit into a versioning service (git like).&lt;/p&gt;\n\n&lt;p&gt;Basically today I use syncthing for, um, loss prevention, but I&amp;#39;d like something that works on a single, multi-disk PC&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sa2v1", "is_robot_indexable": true, "report_reasons": null, "author": "AlternativeBasis", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sa2v1/mergerfs_but_with_redundancy_anyone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sa2v1/mergerfs_but_with_redundancy_anyone/", "subreddit_subscribers": 691463, "created_utc": 1688651995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, to put it short, my best friend was murdered earlier this year and I would really like to retrieve this one picture that was deleted off her Twitter page. The tweet is from December of 2022 and I have multiple *alive* Twitter links of random people replying to the deleted picture. I\u2019ve already tried multiple archive websites, the wayback machine, google\u2018s cache, etc., and nothing is working. Is there any sort of script or anything that could possibly retrieve the deleted picture from the Twitter links I have? I can even pay some money for someone to retrieve this one deleted picture of her if you want and if it\u2019s possible.", "author_fullname": "t2_pfgeprgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to retrieve a deleted Twitter photo of my dead best friend.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14st8q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688695271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, to put it short, my best friend was murdered earlier this year and I would really like to retrieve this one picture that was deleted off her Twitter page. The tweet is from December of 2022 and I have multiple &lt;em&gt;alive&lt;/em&gt; Twitter links of random people replying to the deleted picture. I\u2019ve already tried multiple archive websites, the wayback machine, google\u2018s cache, etc., and nothing is working. Is there any sort of script or anything that could possibly retrieve the deleted picture from the Twitter links I have? I can even pay some money for someone to retrieve this one deleted picture of her if you want and if it\u2019s possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14st8q7", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Layer_6017", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14st8q7/trying_to_retrieve_a_deleted_twitter_photo_of_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14st8q7/trying_to_retrieve_a_deleted_twitter_photo_of_my/", "subreddit_subscribers": 691463, "created_utc": 1688695271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I recently bought 2x Seagate Ironwolf drives. \n\nA few days ago, I threw them in an external drive bay and was AB testing some USB C cables I had. Unfortunately, one of the cables was faulty and caused the command timeouts to quickly spike in a span of less than 20 minutes to 553 and 527, per DriveDx. \n\nAs these drives are brand new, every other indicator is perfectly normal, and since switching back to a reliable cable, the command timeout values have stayed the same.  \n\nI\u2019ve seen mixed feedback on various subreddits about the importance of command timeouts so hoping to get some clarity here. \n\nI\u2019m just curious if anybody has any insight if significant command timeouts in a short period can cause damage to the drive(s), and I should just return them without a second thought? Or, is it no big deal and I should hold onto them?\n\nThanks! Appreciate your help.", "author_fullname": "t2_qoetr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you return drives over high command timeouts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 10, "top_awarded_type": null, "hide_score": true, "name": "t3_14ssg04", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XT-FWc32t8zZqKmuAnd286dnOxMlG7Zds2sHJrGdo1w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1688693186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I recently bought 2x Seagate Ironwolf drives. &lt;/p&gt;\n\n&lt;p&gt;A few days ago, I threw them in an external drive bay and was AB testing some USB C cables I had. Unfortunately, one of the cables was faulty and caused the command timeouts to quickly spike in a span of less than 20 minutes to 553 and 527, per DriveDx. &lt;/p&gt;\n\n&lt;p&gt;As these drives are brand new, every other indicator is perfectly normal, and since switching back to a reliable cable, the command timeout values have stayed the same.  &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen mixed feedback on various subreddits about the importance of command timeouts so hoping to get some clarity here. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m just curious if anybody has any insight if significant command timeouts in a short period can cause damage to the drive(s), and I should just return them without a second thought? Or, is it no big deal and I should hold onto them?&lt;/p&gt;\n\n&lt;p&gt;Thanks! Appreciate your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/snyywkgu3gab1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/snyywkgu3gab1.jpg?auto=webp&amp;v=enabled&amp;s=4d484357f9e0dbdbd8118e57638e6df521b2e95d", "width": 710, "height": 52}, "resolutions": [{"url": "https://preview.redd.it/snyywkgu3gab1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91482412844fcc07d2ed45b2aa2d085b029dbe8a", "width": 108, "height": 7}, {"url": "https://preview.redd.it/snyywkgu3gab1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56fadeba5f69789eb4970da5b9f0aa80ea0ec7c1", "width": 216, "height": 15}, {"url": "https://preview.redd.it/snyywkgu3gab1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=883b24c85e1e6eef98e893a907b47a9159388da9", "width": 320, "height": 23}, {"url": "https://preview.redd.it/snyywkgu3gab1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=138923750f3d586a29dcc8c5a4a0a73086919def", "width": 640, "height": 46}], "variants": {}, "id": "acOmfhZMOhur1CO9sfyP7GjFy_UpOl4QH1ktZnTsCt4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14ssg04", "is_robot_indexable": true, "report_reasons": null, "author": "GrammarSloot", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14ssg04/would_you_return_drives_over_high_command_timeouts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/snyywkgu3gab1.jpg", "subreddit_subscribers": 691463, "created_utc": 1688693186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to digitalize some mini dv, but paying a service would cost around 300 for all of my tapes. So I thought maybe a mini dv player and a wire to connect it but im not sure what camcorder or whatever I need.", "author_fullname": "t2_6zwekfvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know a good way to digitalize some mini dv tapes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14smos8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688679468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to digitalize some mini dv, but paying a service would cost around 300 for all of my tapes. So I thought maybe a mini dv player and a wire to connect it but im not sure what camcorder or whatever I need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14smos8", "is_robot_indexable": true, "report_reasons": null, "author": "michiel11069", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14smos8/anyone_know_a_good_way_to_digitalize_some_mini_dv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14smos8/anyone_know_a_good_way_to_digitalize_some_mini_dv/", "subreddit_subscribers": 691463, "created_utc": 1688679468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI tried to download this complete blog with HTTrack, but this only works for the first page of the blog.\n\nFrom the second page ongoing the links are referring to the original website.\n\nDoes anybody know how to download this complete blog?\n\n[https://yourtradingcoach.com/](https://yourtradingcoach.com/)\n\nThank you very much in advance!", "author_fullname": "t2_6bi1lh3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download this complete blog? HTTrack did not work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14smo82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688679429.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I tried to download this complete blog with HTTrack, but this only works for the first page of the blog.&lt;/p&gt;\n\n&lt;p&gt;From the second page ongoing the links are referring to the original website.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know how to download this complete blog?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://yourtradingcoach.com/\"&gt;https://yourtradingcoach.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you very much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KDYBJ2xXu8rh1auK4LBQhnhZE0j409OE8ZQT94P5gOM.jpg?auto=webp&amp;v=enabled&amp;s=3f63e02fa8cbfb2927a27e4d3cd915e22abc56e7", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/KDYBJ2xXu8rh1auK4LBQhnhZE0j409OE8ZQT94P5gOM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67444be17b194d745325d3cf594be375ed005678", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KDYBJ2xXu8rh1auK4LBQhnhZE0j409OE8ZQT94P5gOM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f03da6e2a7065253bf7df91d0c4b94fb102dba71", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KDYBJ2xXu8rh1auK4LBQhnhZE0j409OE8ZQT94P5gOM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39facf14fb5e65fdfe926cdb94a178f3ae9c9c17", "width": 320, "height": 168}], "variants": {}, "id": "KcqVdDfE9DYOQMnjkHIHyf6FA6Tw5PBcbz47O2112qI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14smo82", "is_robot_indexable": true, "report_reasons": null, "author": "kally3", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14smo82/how_to_download_this_complete_blog_httrack_did/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14smo82/how_to_download_this_complete_blog_httrack_did/", "subreddit_subscribers": 691463, "created_utc": 1688679429.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "just hooked up my first ds4246 and noticed that some of the led's are quite dim (unit works great though!).  got me wondering if anyone has replaced these led's before and how hard it is to do that?", "author_fullname": "t2_4i9q4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "netapp ds4246: replace front panel LED's?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14skdch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688674353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just hooked up my first ds4246 and noticed that some of the led&amp;#39;s are quite dim (unit works great though!).  got me wondering if anyone has replaced these led&amp;#39;s before and how hard it is to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1PB raw", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14skdch", "is_robot_indexable": true, "report_reasons": null, "author": "nefrina", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14skdch/netapp_ds4246_replace_front_panel_leds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14skdch/netapp_ds4246_replace_front_panel_leds/", "subreddit_subscribers": 691463, "created_utc": 1688674353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If a flash drive passes the test can I be sure that its 100% legitimate", "author_fullname": "t2_8oihaf0t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is h2wtestw enough to check legitimacy of flash drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14shwqj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688668856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If a flash drive passes the test can I be sure that its 100% legitimate&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14shwqj", "is_robot_indexable": true, "report_reasons": null, "author": "onionbiscuits", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14shwqj/is_h2wtestw_enough_to_check_legitimacy_of_flash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14shwqj/is_h2wtestw_enough_to_check_legitimacy_of_flash/", "subreddit_subscribers": 691463, "created_utc": 1688668856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to host a bunch of very high resolution images using the [DZI (deep zoom) format](https://en.wikipedia.org/wiki/Deep_Zoom). I am hoping to use object storage because that is generally cheaper. Unfortunately, when I tried doing this with Backblaze B2, I hit my request limit (2,500/day) very quickly. \n\nThat is because DZI format works by breaking an image up into smaller tiles, which are then dynamically loaded as the user zooms and pans around the image. Usually these are separate files, but to make it work with object hosting, some people more clever than myself have created a way to zip the files and then use byte ranges to extract the needed tiles.\n\nThus, my issue on Backblaze B2 wasn't that there was so much data to be transferred, but that there were a ton of requests for each byte range representing a tile (just panning and image can make dozens of requests in a few seconds).\n\nI am still very new to all this cloud stuff, so I am trying to figure out what the most cost effective and performant options are. Amazon S3 looks to be more generous with requests, but I hate to do business with Amazon (they kill workers).\n\nCan anyone recommend object hosting that is well priced for the use-case of needing many requests but only moderate storage needs?", "author_fullname": "t2_r25yo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting objects with few or no caps on the number of requests. Cheapest services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14sh69q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688667185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to host a bunch of very high resolution images using the &lt;a href=\"https://en.wikipedia.org/wiki/Deep_Zoom\"&gt;DZI (deep zoom) format&lt;/a&gt;. I am hoping to use object storage because that is generally cheaper. Unfortunately, when I tried doing this with Backblaze B2, I hit my request limit (2,500/day) very quickly. &lt;/p&gt;\n\n&lt;p&gt;That is because DZI format works by breaking an image up into smaller tiles, which are then dynamically loaded as the user zooms and pans around the image. Usually these are separate files, but to make it work with object hosting, some people more clever than myself have created a way to zip the files and then use byte ranges to extract the needed tiles.&lt;/p&gt;\n\n&lt;p&gt;Thus, my issue on Backblaze B2 wasn&amp;#39;t that there was so much data to be transferred, but that there were a ton of requests for each byte range representing a tile (just panning and image can make dozens of requests in a few seconds).&lt;/p&gt;\n\n&lt;p&gt;I am still very new to all this cloud stuff, so I am trying to figure out what the most cost effective and performant options are. Amazon S3 looks to be more generous with requests, but I hate to do business with Amazon (they kill workers).&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend object hosting that is well priced for the use-case of needing many requests but only moderate storage needs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_tyfLJSNff7UmmpJMgQSkmApl5_cd102Q8tZXktHAug.jpg?auto=webp&amp;v=enabled&amp;s=6fd4021dcae5c4015dea0302df7f58e39b16fd0a", "width": 120, "height": 93}, "resolutions": [{"url": "https://external-preview.redd.it/_tyfLJSNff7UmmpJMgQSkmApl5_cd102Q8tZXktHAug.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0306b06a8d3cc3c3257749d7cf7c84d90a1ceeb0", "width": 108, "height": 83}], "variants": {}, "id": "TQ9dK5CcQ5IczuvNX7J1xi_GzGPlE8nl0QFBM4HWzLg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14sh69q", "is_robot_indexable": true, "report_reasons": null, "author": "nKephalos", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14sh69q/hosting_objects_with_few_or_no_caps_on_the_number/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14sh69q/hosting_objects_with_few_or_no_caps_on_the_number/", "subreddit_subscribers": 691463, "created_utc": 1688667185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Let' s say, I have like 100 files that start with \"abcd\\_00\" at the start. Suppose the files are \"abcd\\_00001\" to \"abcd\\_00100\". And all these files are amongs thousands of files with different names, and those files might also have their similar file name patterns.  \nNow I need a software, where I would instruct it to take the first 7 words of the file name into account, and running it will make the program go through all the existing files in the folder and show me a list like this:  \n\n\n1)  \"abcd\\_00\" ---&gt; 100 Files  \n2) \"1234\\_ab\" --&gt; 5 Files  \n3) \"loremip\" --&gt; 30 Files  \n....  \n...  \n...  \n\n\nI have tried to find something like this but to no avail. Can anyone help me find something similar to this?", "author_fullname": "t2_1bupqeao", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any software that goes through all the files in a folder and lists all the file names in a ranking order, with the top name having the most amount of files with the similar name structure? Detailed example below:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s9lfu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688650893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39; s say, I have like 100 files that start with &amp;quot;abcd_00&amp;quot; at the start. Suppose the files are &amp;quot;abcd_00001&amp;quot; to &amp;quot;abcd_00100&amp;quot;. And all these files are amongs thousands of files with different names, and those files might also have their similar file name patterns.&lt;br/&gt;\nNow I need a software, where I would instruct it to take the first 7 words of the file name into account, and running it will make the program go through all the existing files in the folder and show me a list like this:  &lt;/p&gt;\n\n&lt;p&gt;1)  &amp;quot;abcd_00&amp;quot; ---&amp;gt; 100 Files&lt;br/&gt;\n2) &amp;quot;1234_ab&amp;quot; --&amp;gt; 5 Files&lt;br/&gt;\n3) &amp;quot;loremip&amp;quot; --&amp;gt; 30 Files&lt;br/&gt;\n....&lt;br/&gt;\n...&lt;br/&gt;\n...  &lt;/p&gt;\n\n&lt;p&gt;I have tried to find something like this but to no avail. Can anyone help me find something similar to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s9lfu", "is_robot_indexable": true, "report_reasons": null, "author": "BeastBoiii2000", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14s9lfu/is_there_any_software_that_goes_through_all_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s9lfu/is_there_any_software_that_goes_through_all_the/", "subreddit_subscribers": 691463, "created_utc": 1688650893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all. Recently found a few songs by some artists that I love that have exclusively been uploaded to Vimeo. I was curious to see what the highest quality audio that Vimeo provides, so that I can rip the audio in the best quality possible, without re-encoding and/or having any loss of quality. Like with YouTube and their OPUS streams which sometimes allow higher than 128kbps audio, is there a possibility to do this with Vimeo as well? What YT-DLP string would work the best for achieving all of this? Thanks :)", "author_fullname": "t2_14sosd3h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping / Archiving Audio From Vimeo Videos In Highest Quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s9cxh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688650364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. Recently found a few songs by some artists that I love that have exclusively been uploaded to Vimeo. I was curious to see what the highest quality audio that Vimeo provides, so that I can rip the audio in the best quality possible, without re-encoding and/or having any loss of quality. Like with YouTube and their OPUS streams which sometimes allow higher than 128kbps audio, is there a possibility to do this with Vimeo as well? What YT-DLP string would work the best for achieving all of this? Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "256GB sadly", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s9cxh", "is_robot_indexable": true, "report_reasons": null, "author": "FinleyGomez", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14s9cxh/ripping_archiving_audio_from_vimeo_videos_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s9cxh/ripping_archiving_audio_from_vimeo_videos_in/", "subreddit_subscribers": 691463, "created_utc": 1688650364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't know what the easiest option for the average person would be though. \n\nhttps://www.rareddit.com/ is an example of such a site. If an easy how-to guide could be created that would allow the average non-techy person to create a free website like that I think it would then be able to spread.", "author_fullname": "t2_d5eznvwb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We should spread a way for people to host their own Reddit data before deleting it. It can be done for free with static site generators and Netlify, Cloudflare pages, and Github pages.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s7cm7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688645501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know what the easiest option for the average person would be though. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.rareddit.com/\"&gt;https://www.rareddit.com/&lt;/a&gt; is an example of such a site. If an easy how-to guide could be created that would allow the average non-techy person to create a free website like that I think it would then be able to spread.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s7cm7", "is_robot_indexable": true, "report_reasons": null, "author": "iloveheyzeus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14s7cm7/we_should_spread_a_way_for_people_to_host_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s7cm7/we_should_spread_a_way_for_people_to_host_their/", "subreddit_subscribers": 691463, "created_utc": 1688645501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can someone who has a clue what they are doing help me...pretty please?!?!\n\nI will fully admit to being relatively clueless when it comes to code of any kind. After fighting with this for over a week, I need help!\n\nI am trying to automate my daily downloads through Task Scheduler using yt-dlp. I have it working when I did 1 task per YT channel. That meant a ton of windows popping up every hour all day long. So, I tried to switch to a batch file. For some reason (which I am hoping someone can tell me) this will not work. When I run the yt-dlp command directly in Terminal or PS, it runs with no issues. When I run it through TS, the PS window pops up then disappears immediately before I can even see it. The log history says the task ran. Usually when there is an error I can see it. Also, when the task actually runs I can see it.\n\nI downloaded RoboIntern, thinking maybe it was something weird with TS. The same thing happened.\n\nThis is the command (please don't judge, like I said I can barely get by with the basics! lol):\n\nyt-dlp --live-from-start --paths H:/ -o \"%(channel)s/%(upload\\_date)s %(title)s.%(ext)s\" --dateafter yesterday -a \"D:\\\\Files\\\\tasks\\\\batch1.txt\"\n\nThe batch file is just the list of YT channels.\n\nLike I said, everything works perfect when run manually through PS. That task with -a on replaced with a url works when run through Task Scheduler. Can someone tell me what I am missing? THANK YOU!!!", "author_fullname": "t2_fl7ie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "yt-dlp: Automating Download Question - Task Scheduler not working with Batch File", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s67y4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688642516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone who has a clue what they are doing help me...pretty please?!?!&lt;/p&gt;\n\n&lt;p&gt;I will fully admit to being relatively clueless when it comes to code of any kind. After fighting with this for over a week, I need help!&lt;/p&gt;\n\n&lt;p&gt;I am trying to automate my daily downloads through Task Scheduler using yt-dlp. I have it working when I did 1 task per YT channel. That meant a ton of windows popping up every hour all day long. So, I tried to switch to a batch file. For some reason (which I am hoping someone can tell me) this will not work. When I run the yt-dlp command directly in Terminal or PS, it runs with no issues. When I run it through TS, the PS window pops up then disappears immediately before I can even see it. The log history says the task ran. Usually when there is an error I can see it. Also, when the task actually runs I can see it.&lt;/p&gt;\n\n&lt;p&gt;I downloaded RoboIntern, thinking maybe it was something weird with TS. The same thing happened.&lt;/p&gt;\n\n&lt;p&gt;This is the command (please don&amp;#39;t judge, like I said I can barely get by with the basics! lol):&lt;/p&gt;\n\n&lt;p&gt;yt-dlp --live-from-start --paths H:/ -o &amp;quot;%(channel)s/%(upload_date)s %(title)s.%(ext)s&amp;quot; --dateafter yesterday -a &amp;quot;D:\\Files\\tasks\\batch1.txt&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The batch file is just the list of YT channels.&lt;/p&gt;\n\n&lt;p&gt;Like I said, everything works perfect when run manually through PS. That task with -a on replaced with a url works when run through Task Scheduler. Can someone tell me what I am missing? THANK YOU!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14s67y4", "is_robot_indexable": true, "report_reasons": null, "author": "MSK7", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14s67y4/ytdlp_automating_download_question_task_scheduler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s67y4/ytdlp_automating_download_question_task_scheduler/", "subreddit_subscribers": 691463, "created_utc": 1688642516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TLDR : Decently powerful ARM SBC running Pimox (an unofficial version ARM version of proxmox), emulating an x86 system to run TrueNAS. Thoughts?\n\nLonger version : I'm going to be installing Pimox on an Orange Pi 5+ (8 cores / 16GB RAM / NVMe) for ARM vs x86 tinkering. Since I'm going to be running x86 anyway and I hate OMV... I might as well use my preferred NAS OS TrueNAS. (Performance is a concern, but see the unrelated section below)\n\nI move between 2 locations somewhat frequently, so this machine will be 'my' offsite backup 50% of the time. The other 50% will be handled by a raspberry pi running OMV. Plus I have a Storj account and oracle cloud account for an extra couple hundred GB of offsite storage, but I don't consider those as mine.\n\nI actually do pay oracle cloud some money so I doubt they'll close my account randomly(?) But still, I refuse to plan around that as a certainty.\n\nBoth the emulated TrueNAS and OMV instances will have 1TB of dedicated storage for the things I consider most valuable. These machines will have read only access to the main server and solely access it through a VPN (or through LAN). The main server doesn't have any credentials for the backup servers.\n\nI haven't decided on the syncing method (cron vs task, rsync vs zfs send etc). But in any case both the offsite backup machines will make periodic snapshots with an undetermined retention policy in order to protect against ransomware.\n\nPotential jankiness of x86 emulated on ARM aside (although one of the devs said they don't see an issue with it provided the same caveats as regular virtualization), I think this is a pretty good set up. 1 of the machines will always be local for fast recoveries, 1 machine will be remote. That handles 3 2 1.\n\nLong snapshot retention policies with ZFS's checksumming hopefully lets me deal with some of the nastier scenarios. The main server has a pretty robust pool to reduce the chances of needing a backup in the first place. Plus throw in about a terabyte of cloud storage here and there for some questionable recovery paths...\n\nI think that checks all the boxes aside from capacity. I don't have the cash for a full backup, and the sentimental value per bit goes WAAAAY down after the first terabyte or so. It's probably fine\n\nUnrelated : x86 emulation does take a lot of resources but it's not as bad as you might think. Debian ran pretty much in realtime on my phone (lags behind by a second or so). TrueNAS takes like 15 minutes to boot but a lot of that seems to be caused by some processes it's expecting but times out, which delays things. Once you get into the OS the terminal works pretty OK.\n\nI haven't been able to run any actual performance benchmarks on it, but WINE + BOX64 let me run prime95 on my phone as well. Cinebench and handbrake doesn't work though.", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pimox + Emulated TrueNAS VM for backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14s3rtu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688635507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR : Decently powerful ARM SBC running Pimox (an unofficial version ARM version of proxmox), emulating an x86 system to run TrueNAS. Thoughts?&lt;/p&gt;\n\n&lt;p&gt;Longer version : I&amp;#39;m going to be installing Pimox on an Orange Pi 5+ (8 cores / 16GB RAM / NVMe) for ARM vs x86 tinkering. Since I&amp;#39;m going to be running x86 anyway and I hate OMV... I might as well use my preferred NAS OS TrueNAS. (Performance is a concern, but see the unrelated section below)&lt;/p&gt;\n\n&lt;p&gt;I move between 2 locations somewhat frequently, so this machine will be &amp;#39;my&amp;#39; offsite backup 50% of the time. The other 50% will be handled by a raspberry pi running OMV. Plus I have a Storj account and oracle cloud account for an extra couple hundred GB of offsite storage, but I don&amp;#39;t consider those as mine.&lt;/p&gt;\n\n&lt;p&gt;I actually do pay oracle cloud some money so I doubt they&amp;#39;ll close my account randomly(?) But still, I refuse to plan around that as a certainty.&lt;/p&gt;\n\n&lt;p&gt;Both the emulated TrueNAS and OMV instances will have 1TB of dedicated storage for the things I consider most valuable. These machines will have read only access to the main server and solely access it through a VPN (or through LAN). The main server doesn&amp;#39;t have any credentials for the backup servers.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t decided on the syncing method (cron vs task, rsync vs zfs send etc). But in any case both the offsite backup machines will make periodic snapshots with an undetermined retention policy in order to protect against ransomware.&lt;/p&gt;\n\n&lt;p&gt;Potential jankiness of x86 emulated on ARM aside (although one of the devs said they don&amp;#39;t see an issue with it provided the same caveats as regular virtualization), I think this is a pretty good set up. 1 of the machines will always be local for fast recoveries, 1 machine will be remote. That handles 3 2 1.&lt;/p&gt;\n\n&lt;p&gt;Long snapshot retention policies with ZFS&amp;#39;s checksumming hopefully lets me deal with some of the nastier scenarios. The main server has a pretty robust pool to reduce the chances of needing a backup in the first place. Plus throw in about a terabyte of cloud storage here and there for some questionable recovery paths...&lt;/p&gt;\n\n&lt;p&gt;I think that checks all the boxes aside from capacity. I don&amp;#39;t have the cash for a full backup, and the sentimental value per bit goes WAAAAY down after the first terabyte or so. It&amp;#39;s probably fine&lt;/p&gt;\n\n&lt;p&gt;Unrelated : x86 emulation does take a lot of resources but it&amp;#39;s not as bad as you might think. Debian ran pretty much in realtime on my phone (lags behind by a second or so). TrueNAS takes like 15 minutes to boot but a lot of that seems to be caused by some processes it&amp;#39;s expecting but times out, which delays things. Once you get into the OS the terminal works pretty OK.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to run any actual performance benchmarks on it, but WINE + BOX64 let me run prime95 on my phone as well. Cinebench and handbrake doesn&amp;#39;t work though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "14s3rtu", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14s3rtu/pimox_emulated_truenas_vm_for_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14s3rtu/pimox_emulated_truenas_vm_for_backups/", "subreddit_subscribers": 691463, "created_utc": 1688635507.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}