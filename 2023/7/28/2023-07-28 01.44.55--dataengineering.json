{"kind": "Listing", "data": {"after": "t3_15b9c6d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As someone who works in Snowflake day to day but has had little to no exposure to Databricks I'm curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?   \nYes, I know it's not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.   \nThis isn't meant to be a comparison of which you think is necessarily *BETTER*... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who has worked with both Snowflake and Databricks and what do you enjoy/dislike about each?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b85up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690479473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As someone who works in Snowflake day to day but has had little to no exposure to Databricks I&amp;#39;m curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?&lt;br/&gt;\nYes, I know it&amp;#39;s not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.&lt;br/&gt;\nThis isn&amp;#39;t meant to be a comparison of which you think is necessarily &lt;em&gt;BETTER&lt;/em&gt;... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b85up", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/", "subreddit_subscribers": 118577, "created_utc": 1690479473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nWe recently ran an experiment at [Dozer](https://github.com/getdozer/dozer) that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:\n\n[https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce](https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce)\n\nWould love to get your feedback.  \n\n\nThanks  \nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing 160m of records (with 4 JOINs and 1 aggregation) in real-time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15axba2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690450727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;We recently ran an experiment at &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce\"&gt;https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback.  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;br/&gt;\nMatteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?auto=webp&amp;s=205a7ce9ef4cd41ab708aeb4c8b2f411ade2b5c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2137c62de69a82f790eca2cf323b4e9fa49a42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e9e76b368d9bd2737828569495d8f080ea1ca66", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecc4927d891da4391fe409958ef35c9d5e83965e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b42c63769cc4774b26d17ac2f1dbbcc10fd21a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ba1086ce1186414717e41899ed3393ff00818f4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f22dfc70fa64869063b5eedee17d4f7b841f7dc", "width": 1080, "height": 540}], "variants": {}, "id": "bsBU-V26yp9Cn213aetzEQxsmD1y-nrva8jMGPRMp5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15axba2", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "subreddit_subscribers": 118577, "created_utc": 1690450727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We got a FastAPI server at work that that's used for calculating machine learning features. It receives a big json blob, calculates features and responds with another big json blob. We run these complex CPU operations in a process pool so that we don't block the main event loop. These operations are mostly Pandas operations that can be written in Polars nowadays. So what I'm thinking is that if Polars completely released the GIL we would be able to use a thread pool inside this service rather than a process pool, incurring in less overhead and resource consumption. ", "author_fullname": "t2_v0xlvj6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Python Polars completely release the GIL ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1lnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We got a FastAPI server at work that that&amp;#39;s used for calculating machine learning features. It receives a big json blob, calculates features and responds with another big json blob. We run these complex CPU operations in a process pool so that we don&amp;#39;t block the main event loop. These operations are mostly Pandas operations that can be written in Polars nowadays. So what I&amp;#39;m thinking is that if Polars completely released the GIL we would be able to use a thread pool inside this service rather than a process pool, incurring in less overhead and resource consumption. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b1lnf", "is_robot_indexable": true, "report_reasons": null, "author": "nightwolfomar", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1lnf/does_python_polars_completely_release_the_gil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1lnf/does_python_polars_completely_release_the_gil/", "subreddit_subscribers": 118577, "created_utc": 1690463652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, great data people.\n\nThe **direct question** I want to understand is: Who is using Delta Lake outside Databricks environments?\n\nThe **indirect question** is: what is the \"type\" of companies and use cases using Delta Lake and Iceberg?  \n\n\nUsing Delta Lake when your data platform sits on Databricks is a no-brainer. It works just wonderfully (I used it myself). Delta Lake OSS is also a very good piece of software for itself.\n\nHowever, how is the usage distribution outside Databricks envs? \nI've heard Iceberg is more used for on-prem companies, mainly big tech. \nOn \"non-databricks-cloud\", Iceberg has better support AFAIK. For example, for Snowflake and BigQuery.\nOn the other hand, Delta-rs integration with Arrow made Delta available to all tools supporting Arrow, which is a huge step towards being a standard outside the JVM, like for ML tools. While Icerbeg still lives in the JVM mostly.\n\nSo, how is the market using Delta Lake and Iceberg outside Databricks, in your opinion and/or experience?", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who is using Delta Lake outside Databricks environments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bap0n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690485477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, great data people.&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;direct question&lt;/strong&gt; I want to understand is: Who is using Delta Lake outside Databricks environments?&lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;indirect question&lt;/strong&gt; is: what is the &amp;quot;type&amp;quot; of companies and use cases using Delta Lake and Iceberg?  &lt;/p&gt;\n\n&lt;p&gt;Using Delta Lake when your data platform sits on Databricks is a no-brainer. It works just wonderfully (I used it myself). Delta Lake OSS is also a very good piece of software for itself.&lt;/p&gt;\n\n&lt;p&gt;However, how is the usage distribution outside Databricks envs? \nI&amp;#39;ve heard Iceberg is more used for on-prem companies, mainly big tech. \nOn &amp;quot;non-databricks-cloud&amp;quot;, Iceberg has better support AFAIK. For example, for Snowflake and BigQuery.\nOn the other hand, Delta-rs integration with Arrow made Delta available to all tools supporting Arrow, which is a huge step towards being a standard outside the JVM, like for ML tools. While Icerbeg still lives in the JVM mostly.&lt;/p&gt;\n\n&lt;p&gt;So, how is the market using Delta Lake and Iceberg outside Databricks, in your opinion and/or experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15bap0n", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15bap0n/who_is_using_delta_lake_outside_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15bap0n/who_is_using_delta_lake_outside_databricks/", "subreddit_subscribers": 118577, "created_utc": 1690485477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "3.5 years as a data analyst. Learning SSIS ETL to try to become my company's data engineer. So far, for data profiling I have just been using custom SQL queries to find basic information about the columns (length, count of blank cells, precision of numerics, etc), but it's very manual work. \n\nIs there a good piece of software that can answer these basic questions for me? Are there tutorials out there that would teach more efficient methods?", "author_fullname": "t2_4xvmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software and learning material to improve my data profiling -&gt; data cleansing work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1i8y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;3.5 years as a data analyst. Learning SSIS ETL to try to become my company&amp;#39;s data engineer. So far, for data profiling I have just been using custom SQL queries to find basic information about the columns (length, count of blank cells, precision of numerics, etc), but it&amp;#39;s very manual work. &lt;/p&gt;\n\n&lt;p&gt;Is there a good piece of software that can answer these basic questions for me? Are there tutorials out there that would teach more efficient methods?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b1i8y", "is_robot_indexable": true, "report_reasons": null, "author": "funkyman50", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1i8y/software_and_learning_material_to_improve_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1i8y/software_and_learning_material_to_improve_my_data/", "subreddit_subscribers": 118577, "created_utc": 1690463403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit community!\r  \n\r  \nI came across this insightful blog post on SG Analytics about \"Data Trust: How to Build Data Trust.\" I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.\r  \n\r  \n**URL:** [***Data Trust: How to Build Data Trust***](https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/)\r  \n\r  \nIn this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.\r  \n\r  \nI found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.\r  \n\r  \nLet's dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!", "author_fullname": "t2_t1lpdh2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust the Data: A Guide to Building Data Trust in Your Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b0869", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690459849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit community!&lt;/p&gt;\n\n&lt;p&gt;I came across this insightful blog post on SG Analytics about &amp;quot;Data Trust: How to Build Data Trust.&amp;quot; I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;URL:&lt;/strong&gt; &lt;a href=\"https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/\"&gt;&lt;strong&gt;&lt;em&gt;Data Trust: How to Build Data Trust&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.&lt;/p&gt;\n\n&lt;p&gt;I found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b0869", "is_robot_indexable": true, "report_reasons": null, "author": "David_starc150", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "subreddit_subscribers": 118577, "created_utc": 1690459849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Last time, all the time hear that rust is a great language. How good it is for DE and what as a DE I can do with Rust?", "author_fullname": "t2_b1ud1vk1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How good Rust is for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ba7ei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690484334.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last time, all the time hear that rust is a great language. How good it is for DE and what as a DE I can do with Rust?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ba7ei", "is_robot_indexable": true, "report_reasons": null, "author": "AdClean1116", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ba7ei/how_good_rust_is_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ba7ei/how_good_rust_is_for_de/", "subreddit_subscribers": 118577, "created_utc": 1690484334.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI work as the main data person for a fintech startup running a Buy Now Pay Later scheme. My tasks involve business intelligence and data engineering, where my final output is generally dashboards on Power BI for different teams. As we are planning to scale up, I am tasked with ensuring our data tasks follow good data engineering practices.\n\nWhile I'm experienced with Power BI, Python, SQL, and Python, I'm not a seasoned data engineer. Hence, I am seeking advice on how to optimize our data engineering further.\n\nOur current setup uses AWS RDS and DynamoDB as primary data sources. I create MySQL views for AWS RDS to transform tables, add auxiliary columns, and then connect to Power BI with a Mysql connector. For DynamoDB, I use AWS Glue to catalog data into S3, clean up necessary parts with Athena views, and connect to Power Bi with an Amazon Athena connector.\n\nThe tables in AWS RDS are transactions, users, and payment links with thousands of rows and the table of the DynamoDB is the biggest one with millions of rows.\n\nWe also use Google Sheets for additional data like exchange rates (converted manually once a month), and some user information provided by our sales team. I have considered automating the exchange rates by using a Python script with AWS Lambda, and perhaps storing them in RDS or somewhere better than Google Sheets. All these tables have just hundreds of rows.\n\nTransformations are mostly done in MySQL views and some in DAX.\n\nMy question is, should I consider incorporating data engineering tools and concepts like pyspark, scala, airflow, dbt, git, etc., that I have been learning but not yet using? Any advice or recommendations will be appreciated.\n\nThank you!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Optimize Data Transformations in a Buy Now Pay Later Fintech Startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b2gh2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690465916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I work as the main data person for a fintech startup running a Buy Now Pay Later scheme. My tasks involve business intelligence and data engineering, where my final output is generally dashboards on Power BI for different teams. As we are planning to scale up, I am tasked with ensuring our data tasks follow good data engineering practices.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m experienced with Power BI, Python, SQL, and Python, I&amp;#39;m not a seasoned data engineer. Hence, I am seeking advice on how to optimize our data engineering further.&lt;/p&gt;\n\n&lt;p&gt;Our current setup uses AWS RDS and DynamoDB as primary data sources. I create MySQL views for AWS RDS to transform tables, add auxiliary columns, and then connect to Power BI with a Mysql connector. For DynamoDB, I use AWS Glue to catalog data into S3, clean up necessary parts with Athena views, and connect to Power Bi with an Amazon Athena connector.&lt;/p&gt;\n\n&lt;p&gt;The tables in AWS RDS are transactions, users, and payment links with thousands of rows and the table of the DynamoDB is the biggest one with millions of rows.&lt;/p&gt;\n\n&lt;p&gt;We also use Google Sheets for additional data like exchange rates (converted manually once a month), and some user information provided by our sales team. I have considered automating the exchange rates by using a Python script with AWS Lambda, and perhaps storing them in RDS or somewhere better than Google Sheets. All these tables have just hundreds of rows.&lt;/p&gt;\n\n&lt;p&gt;Transformations are mostly done in MySQL views and some in DAX.&lt;/p&gt;\n\n&lt;p&gt;My question is, should I consider incorporating data engineering tools and concepts like pyspark, scala, airflow, dbt, git, etc., that I have been learning but not yet using? Any advice or recommendations will be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b2gh2", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b2gh2/how_to_optimize_data_transformations_in_a_buy_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b2gh2/how_to_optimize_data_transformations_in_a_buy_now/", "subreddit_subscribers": 118577, "created_utc": 1690465916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we obviously have classic batch pipelines and now evaluating whether we really need/want realtime pipelines. The problem we have with the realtime pipelines is there is no idempotency, if something fails, no retries etc. What we are thinking is to utilize something like micro-batch architecture where we would run jobs e.g. every minute.. Or every x seconds. Until the pipeline triggers, messages would buffer in some message queue (Kafka?). Then e.g. Airflow would spawn a job that would take all messages from T-1min and process them. \n\nWhat do you think of such approach? How common is it? How good/bad idea is it? Do you use it as well? Are you aware of some do's/dont\u2019s? Some references?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micro-batch architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1l0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we obviously have classic batch pipelines and now evaluating whether we really need/want realtime pipelines. The problem we have with the realtime pipelines is there is no idempotency, if something fails, no retries etc. What we are thinking is to utilize something like micro-batch architecture where we would run jobs e.g. every minute.. Or every x seconds. Until the pipeline triggers, messages would buffer in some message queue (Kafka?). Then e.g. Airflow would spawn a job that would take all messages from T-1min and process them. &lt;/p&gt;\n\n&lt;p&gt;What do you think of such approach? How common is it? How good/bad idea is it? Do you use it as well? Are you aware of some do&amp;#39;s/dont\u2019s? Some references?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b1l0m", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1l0m/microbatch_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1l0m/microbatch_architecture/", "subreddit_subscribers": 118577, "created_utc": 1690463603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**DISCLAIMER**\n\nI am no data engineer and I don't know python or any programming language. I can put together stuff into VScode but thats about the extent of my programming abilities. \n\n&amp;#x200B;\n\n**INTRO &amp; CONTEXT**\n\nI\u2019m working on a personal project that takes a PDF containing medical lab results, extracts the data and processes the data into a new centralized schema. From that schema, data is imported into an Excel or Sheets table, which is used as a data source for a no-code website builder. (like Noloco)\n\nI designed the centralized database schema both as a JSON and as an Excel.\n\nI\u2019ve managed to extract some data successfully using AWS Textract GUI console demo. The outputs that AWS gives are JSONs and CSVs.\n\nI now need to transform the data from the AWS output, to match the centralized schema so I can map the data. And here is where im stuck..\n\n**PROBLEM**\n\nMy table has a hierarchical layout with nested rows under one column. I need to transform the data, split the first column into 2 columns, do some parsing etc.\n\nBut in order to transform the data I need to programmatically identify what data needs to be manipulated and somehow mark it. Theoretically, my table has 2 types of entries / records. Type 1 entries are singleTests and type 2 entries are collectionTests.\n\n[The table output sample and entries types.](https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2)\n\nI need to think of a parsing logic or a logic to identify the data.\n\n* I first thought about **identification by keywords** and indeed there is an \u201cLC\u201d at the beginning of each record. That LC stands for Central Lab and its the location where the tests have been processed. The problem with this that LC appears irregularly and its not reliable.\n* The second method I thought was **identification by pattern**. For a singleTest there are always 2 rows, the first one having empty values (besides the first column). For a collectionTest, the first 2 rows are always empty (besides the first column). A collectionTest has a variable number of sub-tests.\n\nHere is a draft of the centralized schema database:\n\nhttps://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;format=png&amp;auto=webp&amp;s=c06740e81178ea509d56aefa1849db26e50796cd\n\nIn this first iteration, the original TestName values that were spanning multiple rows have been split into CollectionName and CollectionMethod according to the entry type.\n\n&amp;#x200B;\n\n**NEED**\n\nI need a parsing logic implemented in python (pandas) that can identify entries types and mark them accordingly. Im not sure if a script can do this. Suggest alternatives if you know.\n\n&amp;#x200B;\n\n**PROGRESS**\n\nI\u2019ve tried about 25 iterations of threads with GPT-4 and Code Interpreter. The problem here is I don\u2019t know exactly what to ask since I don\u2019t know data science specific terms and what technique or collection of techniques should be used. Even so, I\u2019ve managed to get very close to marking the entries correctly with Code Interpreter but it seems that, even though it understands the logic, It can\u2019t write a working script.\n\n&amp;#x200B;\n\n**RESOURCES**\n\n* A Notion with my progress on the data identification logic with my prompts [LINK](https://dennis-kampien.notion.site/Parsing-logic-c70e92b49a8b433398a85d49a0d06343?pvs=4)\n* Links to the sheets containing the tables [LINK](https://docs.google.com/spreadsheets/d/1U7bbTX7qPVFb9FeSGzxxWZyDVk1q48lx/edit?usp=sharing&amp;ouid=107105976591677253200&amp;rtpof=true&amp;sd=true)\n* A link with a partial successful GPT thread [LINK](https://chat.openai.com/share/fbac4366-5b57-4dc4-82d0-f1cf946fa58f)\n\n&amp;#x200B;\n\n If you have ideas where else I can get help, would be appreciated. Thank you.  ", "author_fullname": "t2_14ss6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on Python (pandas) script for data identification logic. Working to extract and transform data from PDF\u2019s into Excel. (Long post)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ft5c4hn88jeb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ab8840e242592775ac67637541564549cd8a8d5"}, {"y": 84, "x": 216, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e8c30e3c03c79b8fc2b9f54a6590914e8339292"}, {"y": 124, "x": 320, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=288d2bab0d4a2de8a2cd1708b231b51882b2ac25"}, {"y": 249, "x": 640, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=caff806277a3bcb4200b33eee3a52bd2f1ba5026"}, {"y": 374, "x": 960, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fca6fddfbcb2814fb6bdbce3f3c02d336f47169f"}, {"y": 421, "x": 1080, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33a4e65b84aaef814225f3ef6664b5b88b342b1e"}], "s": {"y": 624, "x": 1600, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2"}, "id": "ft5c4hn88jeb1"}, "fzj0thjd8jeb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 20, "x": 108, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2ad1cff2f1410be46cb494df6b9e050a5464a0d"}, {"y": 40, "x": 216, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=862a962acc049d806c34236588d3fdb48e8f4511"}, {"y": 59, "x": 320, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8de591af43390b23b44c89554fc03600bb1a06c3"}, {"y": 118, "x": 640, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84ae66308adeee36d66da091225fde18955c785b"}, {"y": 178, "x": 960, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cdbe062dc0bda64c6ba2917e89ff60b6005d39c"}, {"y": 200, "x": 1080, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17c41336da949d82933fa4d19cfaf729430ce3f5"}], "s": {"y": 265, "x": 1428, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;format=png&amp;auto=webp&amp;s=c06740e81178ea509d56aefa1849db26e50796cd"}, "id": "fzj0thjd8jeb1"}}, "name": "t3_15b68s8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b-fDRzF5urH9nJsyANLJage8vwRiW7cKtTDc42UNRzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690474956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am no data engineer and I don&amp;#39;t know python or any programming language. I can put together stuff into VScode but thats about the extent of my programming abilities. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;INTRO &amp;amp; CONTEXT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019m working on a personal project that takes a PDF containing medical lab results, extracts the data and processes the data into a new centralized schema. From that schema, data is imported into an Excel or Sheets table, which is used as a data source for a no-code website builder. (like Noloco)&lt;/p&gt;\n\n&lt;p&gt;I designed the centralized database schema both as a JSON and as an Excel.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve managed to extract some data successfully using AWS Textract GUI console demo. The outputs that AWS gives are JSONs and CSVs.&lt;/p&gt;\n\n&lt;p&gt;I now need to transform the data from the AWS output, to match the centralized schema so I can map the data. And here is where im stuck..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;PROBLEM&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My table has a hierarchical layout with nested rows under one column. I need to transform the data, split the first column into 2 columns, do some parsing etc.&lt;/p&gt;\n\n&lt;p&gt;But in order to transform the data I need to programmatically identify what data needs to be manipulated and somehow mark it. Theoretically, my table has 2 types of entries / records. Type 1 entries are singleTests and type 2 entries are collectionTests.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2\"&gt;The table output sample and entries types.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I need to think of a parsing logic or a logic to identify the data.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I first thought about &lt;strong&gt;identification by keywords&lt;/strong&gt; and indeed there is an \u201cLC\u201d at the beginning of each record. That LC stands for Central Lab and its the location where the tests have been processed. The problem with this that LC appears irregularly and its not reliable.&lt;/li&gt;\n&lt;li&gt;The second method I thought was &lt;strong&gt;identification by pattern&lt;/strong&gt;. For a singleTest there are always 2 rows, the first one having empty values (besides the first column). For a collectionTest, the first 2 rows are always empty (besides the first column). A collectionTest has a variable number of sub-tests.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here is a draft of the centralized schema database:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c06740e81178ea509d56aefa1849db26e50796cd\"&gt;https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c06740e81178ea509d56aefa1849db26e50796cd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this first iteration, the original TestName values that were spanning multiple rows have been split into CollectionName and CollectionMethod according to the entry type.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;NEED&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I need a parsing logic implemented in python (pandas) that can identify entries types and mark them accordingly. Im not sure if a script can do this. Suggest alternatives if you know.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;PROGRESS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried about 25 iterations of threads with GPT-4 and Code Interpreter. The problem here is I don\u2019t know exactly what to ask since I don\u2019t know data science specific terms and what technique or collection of techniques should be used. Even so, I\u2019ve managed to get very close to marking the entries correctly with Code Interpreter but it seems that, even though it understands the logic, It can\u2019t write a working script.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RESOURCES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Notion with my progress on the data identification logic with my prompts &lt;a href=\"https://dennis-kampien.notion.site/Parsing-logic-c70e92b49a8b433398a85d49a0d06343?pvs=4\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Links to the sheets containing the tables &lt;a href=\"https://docs.google.com/spreadsheets/d/1U7bbTX7qPVFb9FeSGzxxWZyDVk1q48lx/edit?usp=sharing&amp;amp;ouid=107105976591677253200&amp;amp;rtpof=true&amp;amp;sd=true\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;A link with a partial successful GPT thread &lt;a href=\"https://chat.openai.com/share/fbac4366-5b57-4dc4-82d0-f1cf946fa58f\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have ideas where else I can get help, would be appreciated. Thank you.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b68s8", "is_robot_indexable": true, "report_reasons": null, "author": "dkampien", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b68s8/help_on_python_pandas_script_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b68s8/help_on_python_pandas_script_for_data/", "subreddit_subscribers": 118577, "created_utc": 1690474956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe are doing a POC for DB right now and it's going pretty well.  One thing though is we don't have access to the photon engine for querying, only as the executor for Spark SQL.  It's been performant compared to what we have now, but I guess I get paranoid when someone tells me I can't see something they want me to buy.  \n\nMy concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?\n\nAppreciate any feedback.", "author_fullname": "t2_117fpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Photon engine question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b159i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690462447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We are doing a POC for DB right now and it&amp;#39;s going pretty well.  One thing though is we don&amp;#39;t have access to the photon engine for querying, only as the executor for Spark SQL.  It&amp;#39;s been performant compared to what we have now, but I guess I get paranoid when someone tells me I can&amp;#39;t see something they want me to buy.  &lt;/p&gt;\n\n&lt;p&gt;My concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b159i", "is_robot_indexable": true, "report_reasons": null, "author": "Gators1992", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "subreddit_subscribers": 118577, "created_utc": 1690462447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific", "author_fullname": "t2_2fcmcq98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are good tools and references to learn design and architecting data engineering solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ay13l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690453100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ay13l", "is_robot_indexable": true, "report_reasons": null, "author": "rockeyjam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "subreddit_subscribers": 118577, "created_utc": 1690453100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.\n\nCurrently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&gt; performing transformations -&gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the \"origin\" for specific IDs. \n\nI'm treating it like a standard application architecture, with the ETL process acting as the \"front end,\" and the database holding the persisted data serving as the \"back end.\" The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. \n\n Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task \"front end\" container needs to execute on the PostgreSQL database backend, given that the database doesn't exist until after the application is up and running. I can't just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can't seem to wrap my head around this one. \n\nHope this makes sense, I can clarify and makes edits if needed. Thanks! ", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Container Docker Application for ETL Task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15arvak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690432327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&amp;gt; performing transformations -&amp;gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the &amp;quot;origin&amp;quot; for specific IDs. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m treating it like a standard application architecture, with the ETL process acting as the &amp;quot;front end,&amp;quot; and the database holding the persisted data serving as the &amp;quot;back end.&amp;quot; The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. &lt;/p&gt;\n\n&lt;p&gt;Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task &amp;quot;front end&amp;quot; container needs to execute on the PostgreSQL database backend, given that the database doesn&amp;#39;t exist until after the application is up and running. I can&amp;#39;t just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can&amp;#39;t seem to wrap my head around this one. &lt;/p&gt;\n\n&lt;p&gt;Hope this makes sense, I can clarify and makes edits if needed. Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15arvak", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "subreddit_subscribers": 118577, "created_utc": 1690432327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_15ar04s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/15ar04s", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UTHqpTlFHrrrFREZFZTlXAqR4y13Ig3P1bFS37PHAVQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690429699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Q3g8NFhvddk", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?auto=webp&amp;s=58f6eb9c6da7e629e8d7d9552a11c526550b142f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=093b1485febcece41dbb6ab0d6c431d65c795dd9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c3d43d042ab2f713542f19a17bdfa229b03d4a0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88609eb050a6bf25a93e32b82ce82244d89e5855", "width": 320, "height": 240}], "variants": {}, "id": "8_alXOI9EvUESL8dnHyGGGOsh0IN9L4WwQtx6q-hGZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15ar04s", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ar04s/ep26_versioning_data_in_the_data_lakehouse_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Q3g8NFhvddk", "subreddit_subscribers": 118577, "created_utc": 1690429699.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After 2 years of experience in data science, having done multiple ETL pipelines, prediction models and some dashboards here and there, I've decided to mainly focus on data engineering. However, I'm currently applying for data engineering jobs in my country and not getting responses for my applications.\n\nI've decided to study and brush up my Airflow and Apache Spark skills as well as study to get the azure dp-203 cert. Could someone give me additional tips or maybe even insights into why I'm not hearing back from those applications?", "author_fullname": "t2_625bbvhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning from DS to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bfdkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690496405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After 2 years of experience in data science, having done multiple ETL pipelines, prediction models and some dashboards here and there, I&amp;#39;ve decided to mainly focus on data engineering. However, I&amp;#39;m currently applying for data engineering jobs in my country and not getting responses for my applications.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve decided to study and brush up my Airflow and Apache Spark skills as well as study to get the azure dp-203 cert. Could someone give me additional tips or maybe even insights into why I&amp;#39;m not hearing back from those applications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15bfdkd", "is_robot_indexable": true, "report_reasons": null, "author": "Bira-of-louders", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15bfdkd/transitioning_from_ds_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15bfdkd/transitioning_from_ds_to_de/", "subreddit_subscribers": 118577, "created_utc": 1690496405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are plenty of tools, some are good, some are nice to have and some are just a pain in the ass.\nWhat is your favourite ETL/ELT tool?", "author_fullname": "t2_9pyk5rj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your favourite ETL/ELT tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bec37", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690493889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are plenty of tools, some are good, some are nice to have and some are just a pain in the ass.\nWhat is your favourite ETL/ELT tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15bec37", "is_robot_indexable": true, "report_reasons": null, "author": "seayk", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15bec37/what_is_your_favourite_etlelt_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15bec37/what_is_your_favourite_etlelt_tool/", "subreddit_subscribers": 118577, "created_utc": 1690493889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Reddit \ud83d\udc4b\n\nData documentation is still too messy. Editing YAML files is tedious and oftentimes when teams buy data cataloging tools they are faced with sparse and out of date documentation.\n\nI'm excited to show an early preview of a my new vs code extension - Docs Composer \u2728 for dbt core.\n\n  \nRead &amp; edit your dbt model docs side-by-side as you develop and leverage our AI assistant to help write a first draft for you.  \n\n\nIt's free and takes 30 seconds to get started. Reach out if you'd like to try it out!\n\n&amp;#x200B;\n\nhttps://reddit.com/link/15b5uxb/video/hjboe9u17jeb1/player", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a vs code extension to help write dbt docs for you", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"hjboe9u17jeb1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/15b5uxb/asset/hjboe9u17jeb1/DASHPlaylist.mpd?a=1693100695%2CMDE2NDRmMzgzNWZiNGVjMjdlYTk5M2Y0NTVjYTUzNTBhMjlkNDc3YWEwYWE1YWI3ZmMwMTkxODg0ZDk4MDg5MA%3D%3D&amp;v=1&amp;f=sd", "x": 1328, "y": 1080, "hlsUrl": "https://v.redd.it/link/15b5uxb/asset/hjboe9u17jeb1/HLSPlaylist.m3u8?a=1693100695%2CY2E5NjRmN2ExNWE2ZjcwYzQ0ZjMxYWIwMjBiYWNlNGM5YjIzNTBiNWE5NjgyYzY4NjE0NjUwMjA0MGNlY2YxOA%3D%3D&amp;v=1&amp;f=sd", "id": "hjboe9u17jeb1", "isGif": false}}, "name": "t3_15b5uxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690474026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;Data documentation is still too messy. Editing YAML files is tedious and oftentimes when teams buy data cataloging tools they are faced with sparse and out of date documentation.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to show an early preview of a my new vs code extension - Docs Composer \u2728 for dbt core.&lt;/p&gt;\n\n&lt;p&gt;Read &amp;amp; edit your dbt model docs side-by-side as you develop and leverage our AI assistant to help write a first draft for you.  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s free and takes 30 seconds to get started. Reach out if you&amp;#39;d like to try it out!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/15b5uxb/video/hjboe9u17jeb1/player\"&gt;https://reddit.com/link/15b5uxb/video/hjboe9u17jeb1/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b5uxb", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b5uxb/building_a_vs_code_extension_to_help_write_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b5uxb/building_a_vs_code_extension_to_help_write_dbt/", "subreddit_subscribers": 118577, "created_utc": 1690474026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For reference, i manage my org's airflow instance. all on prem, running on a linux server via docker containers. pretty basic. i've been wanting to push us gently into dbt, at least for some jobs where it makes sense, and i feel like this is the impetus i've needed.\n\nso... any thoughts? any gotchas, especially with dbt core and self managed open source airflow? it looks relatively simple, install in a separate venv on your airflow instance. until i came across this, my plan was to just use dbt in a container and run via docker operator.", "author_fullname": "t2_v3k0dc9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "anyone try out astro's cosmos/dbt integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b2b7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690465537.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For reference, i manage my org&amp;#39;s airflow instance. all on prem, running on a linux server via docker containers. pretty basic. i&amp;#39;ve been wanting to push us gently into dbt, at least for some jobs where it makes sense, and i feel like this is the impetus i&amp;#39;ve needed.&lt;/p&gt;\n\n&lt;p&gt;so... any thoughts? any gotchas, especially with dbt core and self managed open source airflow? it looks relatively simple, install in a separate venv on your airflow instance. until i came across this, my plan was to just use dbt in a container and run via docker operator.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b2b7w", "is_robot_indexable": true, "report_reasons": null, "author": "zazzersmel", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b2b7w/anyone_try_out_astros_cosmosdbt_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b2b7w/anyone_try_out_astros_cosmosdbt_integration/", "subreddit_subscribers": 118577, "created_utc": 1690465537.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to work within the cloud space but it's tough to decide which route to take.\nThis is at one of the big 4, I am ok with either or but I am scared in cloud security consulting role I won't be as techinal as the data engineering role in cloud.\n\nIf any data engineer has worked in cloud,could you tell me about the wlb,salary,and remote opportunities?", "author_fullname": "t2_77qdsbiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Security Consultant or Cloud Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15awncy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690448460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to work within the cloud space but it&amp;#39;s tough to decide which route to take.\nThis is at one of the big 4, I am ok with either or but I am scared in cloud security consulting role I won&amp;#39;t be as techinal as the data engineering role in cloud.&lt;/p&gt;\n\n&lt;p&gt;If any data engineer has worked in cloud,could you tell me about the wlb,salary,and remote opportunities?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15awncy", "is_robot_indexable": true, "report_reasons": null, "author": "YuriHaThicc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15awncy/cloud_security_consultant_or_cloud_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15awncy/cloud_security_consultant_or_cloud_data_engineer/", "subreddit_subscribers": 118577, "created_utc": 1690448460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "dbt on Spark vs dbt on Trino\n\nHi, we're running a on-prem data platform solution with stacks of **Iceberg** (parquet), **Hive**, **Spark**, **MINIO**, etc. and planning to use **dbt** for our **SQL transformation** of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch\n\n* Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n   * I don't have much experience with Trino, so I'm not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I've seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it's true.\n* For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n   * Spark Thrift: Spark Thrift Server seems to have many issues mentioned here ([https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi\\_vs\\_thriftserver.html#kyuubi-vs-spark-thrift-server](https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server)), which make us think Apache Kyuubi would be a good choice. However, we couldn't find any document/post related to running dbt on Spark Apache Kyuubi\n   * dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn't have much update recently, so I'm afraid we will go into another dead end with this approach\n\n&amp;#x200B;", "author_fullname": "t2_4j9omvnso", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Spark vs dbt Trino", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15av4jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690443143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;dbt on Spark vs dbt on Trino&lt;/p&gt;\n\n&lt;p&gt;Hi, we&amp;#39;re running a on-prem data platform solution with stacks of &lt;strong&gt;Iceberg&lt;/strong&gt; (parquet), &lt;strong&gt;Hive&lt;/strong&gt;, &lt;strong&gt;Spark&lt;/strong&gt;, &lt;strong&gt;MINIO&lt;/strong&gt;, etc. and planning to use &lt;strong&gt;dbt&lt;/strong&gt; for our &lt;strong&gt;SQL transformation&lt;/strong&gt; of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n\n&lt;ul&gt;\n&lt;li&gt;I don&amp;#39;t have much experience with Trino, so I&amp;#39;m not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I&amp;#39;ve seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it&amp;#39;s true.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n\n&lt;ul&gt;\n&lt;li&gt;Spark Thrift: Spark Thrift Server seems to have many issues mentioned here (&lt;a href=\"https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server\"&gt;https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server&lt;/a&gt;), which make us think Apache Kyuubi would be a good choice. However, we couldn&amp;#39;t find any document/post related to running dbt on Spark Apache Kyuubi&lt;/li&gt;\n&lt;li&gt;dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn&amp;#39;t have much update recently, so I&amp;#39;m afraid we will go into another dead end with this approach&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15av4jc", "is_robot_indexable": true, "report_reasons": null, "author": "chuqbach", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "subreddit_subscribers": 118577, "created_utc": 1690443143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. i want to build a concurrent api that can be used for bulk hitting. I just want to know what are some industry standard practices that I can put in the process. I have previously used flask with asynchronous process but just curious about how the process get performed in the industry.", "author_fullname": "t2_270isr53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Concurrent API with bulk hit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15atkim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690437849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. i want to build a concurrent api that can be used for bulk hitting. I just want to know what are some industry standard practices that I can put in the process. I have previously used flask with asynchronous process but just curious about how the process get performed in the industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15atkim", "is_robot_indexable": true, "report_reasons": null, "author": "rishabhdev_rd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15atkim/concurrent_api_with_bulk_hit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15atkim/concurrent_api_with_bulk_hit/", "subreddit_subscribers": 118577, "created_utc": 1690437849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "would you recommend THE BEST SQL course that could help me get a job even if I have no prior SQL experience? At the end of project I would like to complete some real world projects as well..  I really want to be an SQL expert and work at domain level expertise/ projects in Retail, Telecom, and other domains.  \n\n ", "author_fullname": "t2_3sqs3uub", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking the Best SQL Course to Kickstart My Career with Zero Experience! With some CapStone Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bel6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690494495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;would you recommend THE BEST SQL course that could help me get a job even if I have no prior SQL experience? At the end of project I would like to complete some real world projects as well..  I really want to be an SQL expert and work at domain level expertise/ projects in Retail, Telecom, and other domains.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15bel6r", "is_robot_indexable": true, "report_reasons": null, "author": "priyasweety1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15bel6r/seeking_the_best_sql_course_to_kickstart_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15bel6r/seeking_the_best_sql_course_to_kickstart_my/", "subreddit_subscribers": 118577, "created_utc": 1690494495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "SQL Server shop here moving to Databricks with Unity Catalog. We've started to curate data in Databricks and we are getting column to column level lineage captured at transform. I'm expecting this to be game changimg for us. Anyone else getting many to many column level linage like this with other stacks?", "author_fullname": "t2_4cuwr0py", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Column to column lineage at transform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bd12s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690490853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SQL Server shop here moving to Databricks with Unity Catalog. We&amp;#39;ve started to curate data in Databricks and we are getting column to column level lineage captured at transform. I&amp;#39;m expecting this to be game changimg for us. Anyone else getting many to many column level linage like this with other stacks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15bd12s", "is_robot_indexable": true, "report_reasons": null, "author": "chcahx", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15bd12s/column_to_column_lineage_at_transform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15bd12s/column_to_column_lineage_at_transform/", "subreddit_subscribers": 118577, "created_utc": 1690490853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI am going to be designing a DB soon for a medium sized business and other than the foundational tenets of DB design, I was hoping for book suggestions/courses on curating good training data.  What are some pitfalls that I need to know about, how should I be tagging data, what data is important to tag, etc.  I have a good grasp on the business needs, but I am nervous about missing the low hanging fruit while in the early stages of design.\n\n&amp;#x200B;\n\nThanks for the suggestions!", "author_fullname": "t2_yrcdh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for learning how to do DB design with AI-centric tagging/annotating as a priority", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15banxs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690485405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I am going to be designing a DB soon for a medium sized business and other than the foundational tenets of DB design, I was hoping for book suggestions/courses on curating good training data.  What are some pitfalls that I need to know about, how should I be tagging data, what data is important to tag, etc.  I have a good grasp on the business needs, but I am nervous about missing the low hanging fruit while in the early stages of design.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for the suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15banxs", "is_robot_indexable": true, "report_reasons": null, "author": "TylerP3358", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15banxs/resources_for_learning_how_to_do_db_design_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15banxs/resources_for_learning_how_to_do_db_design_with/", "subreddit_subscribers": 118577, "created_utc": 1690485405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! I have 2 streams coming from event hub and I need to implement stream-stream join and perform an upsert operation into a delta table using spark structured streaming. The issue is that structured streaming doesn't allow joining operations in update mode while writing the stream to the table. Just wanted to know if someone has implemented stream-stream join with upsert and if so, how?\n\nOne possible solution that comes to mind is to persist the two streams in two temporary tables (which will be static) and then perform the join and upsert operation but I don't want to use this solution as this won't be a continous job and the job will have to be scheduled with some frequency (x minutes recurrence).\n\n&amp;#x200B;", "author_fullname": "t2_3qcs2dxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Stream-Stream joins with spark structured streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b9c6d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690482265.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I have 2 streams coming from event hub and I need to implement stream-stream join and perform an upsert operation into a delta table using spark structured streaming. The issue is that structured streaming doesn&amp;#39;t allow joining operations in update mode while writing the stream to the table. Just wanted to know if someone has implemented stream-stream join with upsert and if so, how?&lt;/p&gt;\n\n&lt;p&gt;One possible solution that comes to mind is to persist the two streams in two temporary tables (which will be static) and then perform the join and upsert operation but I don&amp;#39;t want to use this solution as this won&amp;#39;t be a continous job and the job will have to be scheduled with some frequency (x minutes recurrence).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b9c6d", "is_robot_indexable": true, "report_reasons": null, "author": "chilllman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b9c6d/need_help_with_streamstream_joins_with_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b9c6d/need_help_with_streamstream_joins_with_spark/", "subreddit_subscribers": 118577, "created_utc": 1690482265.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}