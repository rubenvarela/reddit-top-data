{"kind": "Listing", "data": {"after": null, "dist": 19, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_al1087x2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most thankless job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_156g8xd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 313, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 313, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RxNrSeQ4Y8ECAdbaWYqH_0gj_CGai2J4zp0bAcn5v-M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690020835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rtd453kmrhdb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rtd453kmrhdb1.jpg?auto=webp&amp;s=05644f1e3c1c91a9642dd9b415da0597925e8ad7", "width": 532, "height": 1024}, "resolutions": [{"url": "https://preview.redd.it/rtd453kmrhdb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ddcdc9c174fc905bcbc21fdbe3902a834bedb891", "width": 108, "height": 207}, {"url": "https://preview.redd.it/rtd453kmrhdb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=abda8f1ca1f408127196906510096e441d9034b4", "width": 216, "height": 415}, {"url": "https://preview.redd.it/rtd453kmrhdb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cae9941e78facb4c77083cb18b954fcd535f2f88", "width": 320, "height": 615}], "variants": {}, "id": "tI2ljbBrH32kfrUuBqu5KHiVXn1rM_4RN9NjAOY9fgI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "156g8xd", "is_robot_indexable": true, "report_reasons": null, "author": "OverratedDataScience", "discussion_type": null, "num_comments": 76, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156g8xd/most_thankless_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rtd453kmrhdb1.jpg", "subreddit_subscribers": 117412, "created_utc": 1690020835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In college a lot of time I spent taking handwritten notes and then doing practice exams or problem sets for economics, but I feel like I need a better way to study since I\u2019m self teaching when not working.\n\nMy questions is\n\n- is it even worth taking handwritten notes when coding or should I just read and try to do a problem right away \n- is there any time I should taken handwritten notes? Like maybe definitions for OOP, or data structures?\n\nI\u2019m just feeling pretty dumb because I\u2019m going through a Python book and I feel like it\u2019s taking me forever to get through using my old study habits.", "author_fullname": "t2_oasgyrv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To those who are self-taught coders/data engineers, how did you study?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156sgv3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690053001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In college a lot of time I spent taking handwritten notes and then doing practice exams or problem sets for economics, but I feel like I need a better way to study since I\u2019m self teaching when not working.&lt;/p&gt;\n\n&lt;p&gt;My questions is&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;is it even worth taking handwritten notes when coding or should I just read and try to do a problem right away &lt;/li&gt;\n&lt;li&gt;is there any time I should taken handwritten notes? Like maybe definitions for OOP, or data structures?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I\u2019m just feeling pretty dumb because I\u2019m going through a Python book and I feel like it\u2019s taking me forever to get through using my old study habits.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156sgv3", "is_robot_indexable": true, "report_reasons": null, "author": "Weary-Individual-309", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156sgv3/to_those_who_are_selftaught_codersdata_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156sgv3/to_those_who_are_selftaught_codersdata_engineers/", "subreddit_subscribers": 117412, "created_utc": 1690053001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Related to any stage of the process. Or anything at all?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some common data engineering mistakes you\u2019ve seen in your career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1570ujm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690074320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Related to any stage of the process. Or anything at all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1570ujm", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1570ujm/what_are_some_common_data_engineering_mistakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1570ujm/what_are_some_common_data_engineering_mistakes/", "subreddit_subscribers": 117412, "created_utc": 1690074320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm working a non-it job, having a CS Bachelor's degree from a tier 3 college. I am looking to switch to a data engineer role. Please suggest a complete course/s for data engineering where I can begin learning from the basics.\n\n&amp;#x200B;\n\nNote: I have a basic understanding of Python and access to Udemy through my company portal.\n\n&amp;#x200B;\n\nAny help is much\u00a0appreciated\ud83d\ude07\n\n&amp;#x200B;", "author_fullname": "t2_2q0mazr3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156aq9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690002264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m working a non-it job, having a CS Bachelor&amp;#39;s degree from a tier 3 college. I am looking to switch to a data engineer role. Please suggest a complete course/s for data engineering where I can begin learning from the basics.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note: I have a basic understanding of Python and access to Udemy through my company portal.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help is much\u00a0appreciated\ud83d\ude07&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156aq9z", "is_robot_indexable": true, "report_reasons": null, "author": "sankextend", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156aq9z/data_engineering_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156aq9z/data_engineering_course/", "subreddit_subscribers": 117412, "created_utc": 1690002264.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_79i948aj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern Data Engineering and the Lost Art of Data Modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_156ftu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9FJBQxk_7zI7NKNEJD0muj_BCHnaokscbLuVTANiejA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690019424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "aidancooper.co.uk", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.aidancooper.co.uk/the-lost-art-of-data-modelling/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?auto=webp&amp;s=3f098f60bffa36fb5e8089952cfef7ce126c4128", "width": 1900, "height": 1900}, "resolutions": [{"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc444a298fec6ff39ee4656b78ef75b359efda45", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9de54cf02ec535358035c13a43fa5ed746adf93b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf094aba7c57f04b5a46101dfd892bd66f962dd3", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e565a15407d4d6b000a2950c1e84a5aabd766c2", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=92fd374dc5446f31554eefeec5ea4a041324eb70", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/CW0i3sjKUtGmpLk7ERehxe4y_ZUWd1yjiTyLZcJy6iY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f012a049d3a8bbacaff86025101e38f1af2d6ce", "width": 1080, "height": 1080}], "variants": {}, "id": "z-RZR91Di9_1WOZ6XgIX_eczFECXVc4T6nd2nbn3-_4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "156ftu3", "is_robot_indexable": true, "report_reasons": null, "author": "aidantcooper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156ftu3/modern_data_engineering_and_the_lost_art_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.aidancooper.co.uk/the-lost-art-of-data-modelling/", "subreddit_subscribers": 117412, "created_utc": 1690019424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Essentially what the title says. I have an end to end pipeline that at a high level is \n* extract data\n* do some python transformations\n* export data  \n  \nI want to add a 4th step of  \n  \n* run the relevant dbt models\n\nThe thing is that I\u2019m running into problems with invoking it programmatically, as the pipeline file is in a different directory than the dbt project (so it doesn\u2019t detect the project yaml file) and I can\u2019t use absolute paths to specify which model to run, which I need to do because the pipeline is in a different directory\n\nThis is for a personal project, so I am trying to implement dbt just for the sake of learning it. The thing is though, that the pipeline runs automatically on a cron job, and I need the model to be run at the time of the pipeline\n\nIs this the wrong usage pattern for dbt? I suspect it is because it\u2019s not straightforward to do so far", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to be able to programmatically integrate dbt into an end-to-end pipeline in python. Is this the wrong usage of dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156v3fo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690059713.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690059483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Essentially what the title says. I have an end to end pipeline that at a high level is \n* extract data\n* do some python transformations\n* export data  &lt;/p&gt;\n\n&lt;p&gt;I want to add a 4th step of  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;run the relevant dbt models&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The thing is that I\u2019m running into problems with invoking it programmatically, as the pipeline file is in a different directory than the dbt project (so it doesn\u2019t detect the project yaml file) and I can\u2019t use absolute paths to specify which model to run, which I need to do because the pipeline is in a different directory&lt;/p&gt;\n\n&lt;p&gt;This is for a personal project, so I am trying to implement dbt just for the sake of learning it. The thing is though, that the pipeline runs automatically on a cron job, and I need the model to be run at the time of the pipeline&lt;/p&gt;\n\n&lt;p&gt;Is this the wrong usage pattern for dbt? I suspect it is because it\u2019s not straightforward to do so far&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "156v3fo", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156v3fo/i_want_to_be_able_to_programmatically_integrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156v3fo/i_want_to_be_able_to_programmatically_integrate/", "subreddit_subscribers": 117412, "created_utc": 1690059483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uypc8pwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars vs Pandas. Inside an AWS Lambda.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "name": "t3_156yqjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Fx8866f2QhZ8_c_TlFyhszEmVt4dS0VpZuRLTL-tYvw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690068587.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confessionsofadataguy.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confessionsofadataguy.com/polars-vs-pandas-inside-an-aws-lambda/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?auto=webp&amp;s=984d4904683a78cd3669ec8cc95bf151ce4901d3", "width": 1030, "height": 407}, "resolutions": [{"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f316fdc66227686d49273d627574d96ef7da138f", "width": 108, "height": 42}, {"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be1a70495f071a025cf357793e3847ec87acc3b6", "width": 216, "height": 85}, {"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25c5dc36477503778c8153c51b033eb65c2d57dd", "width": 320, "height": 126}, {"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e353cc8bc4595b3b882e83dc34ed73b07d400c1c", "width": 640, "height": 252}, {"url": "https://external-preview.redd.it/EsNsZvx3qiOBMBLEBuhY0nCtf8nmaJB_9vGBx1TeXTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f70a228a6f648f14e269f0bffe81e7561db75ac", "width": 960, "height": 379}], "variants": {}, "id": "-nV_x0fps6dbvy50xBKdZk_dZsYL6Rtq0vfR6doeFsU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "156yqjp", "is_robot_indexable": true, "report_reasons": null, "author": "DarkClear3881", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156yqjp/polars_vs_pandas_inside_an_aws_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confessionsofadataguy.com/polars-vs-pandas-inside-an-aws-lambda/", "subreddit_subscribers": 117412, "created_utc": 1690068587.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In one of our use cases, a \"compliance alert\" must be delivered depending on guidelines found in a collection of documents.  A transactional database houses the criteria data (triggers), and some event data is streamed.\n\na) Does anybody have a **similar use case**?\n\nb) Does anybody have use cases around data pipelines for LLMs and structured data sources?  If so how do you deal with the following :\n\n1. **Context mismatch** \\-  since enterprise databases have very specific schemas and terminology that the LLM is unaware of, how do you prevent misunderstanding?\n2. **Entity ignorance -**  entities are a relational db terminology but how do you get that into the LLM world?\n3. **Compositionality** \\- LLMs lean towards a holistic view, and our database queries require hierarchical, compositional views. How do you deal with this?\n\nNot to mention other performance-related issues ...", "author_fullname": "t2_48nrgvyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use cases for LLMs on structured data (sourced from databases and streaming data)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156ltst", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690074186.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690036730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In one of our use cases, a &amp;quot;compliance alert&amp;quot; must be delivered depending on guidelines found in a collection of documents.  A transactional database houses the criteria data (triggers), and some event data is streamed.&lt;/p&gt;\n\n&lt;p&gt;a) Does anybody have a &lt;strong&gt;similar use case&lt;/strong&gt;?&lt;/p&gt;\n\n&lt;p&gt;b) Does anybody have use cases around data pipelines for LLMs and structured data sources?  If so how do you deal with the following :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Context mismatch&lt;/strong&gt; -  since enterprise databases have very specific schemas and terminology that the LLM is unaware of, how do you prevent misunderstanding?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Entity ignorance -&lt;/strong&gt;  entities are a relational db terminology but how do you get that into the LLM world?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Compositionality&lt;/strong&gt; - LLMs lean towards a holistic view, and our database queries require hierarchical, compositional views. How do you deal with this?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Not to mention other performance-related issues ...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "156ltst", "is_robot_indexable": true, "report_reasons": null, "author": "anupsurendran", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156ltst/use_cases_for_llms_on_structured_data_sourced/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156ltst/use_cases_for_llms_on_structured_data_sourced/", "subreddit_subscribers": 117412, "created_utc": 1690036730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone i have a question with Glue Exclude pattern, really appreciate if anyone can help me\n\nI'm having an S3 bucket that contains many csv files in same folder and i want to run Crawlers on that folder to create some tables. The S3 bucket structure:\n\n* bucket\\_name/folder\\_name\n   * xxx-apple-xxx-1.csv\n   * xxx-apple-xxx-2.csv\n   * xxx-orange-xxx-1.csv\n   * xxx-strawberry-xxx-1.csv\n   * xxx-chicken-xxx-1.csv\n   * xxx-chicken-xxx-2.csv\n   * xxx-dog-xxx-1.csv\n   * xxx-cat-xxx-1.csv\n   * and many more....\n\nAnd i want to create Crawler to create 1 table for each category, such as 1 table for apple, 1 table for orange (1 crawler --&gt; 1 table)..... And the number of categories is over 100. I see that Glue Crawler has a property, name [Exclude](https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude) pattern (but i do not see the Include pattern property) that we use filter out files when running crawler.\n\nNow i want to create a Crawler for apple, so it should ignore all files except \\*apple\\* ones. I've tried with the Exclude but not achieve this yet. Do anyone know how to do this ?\n\nCrawler name: apple\\_crawler\n\nS3\\_path: bucket\\_name/folder\\_name\n\nExclude: i have tried things like: \\*\\[!{apple}\\]\\* or \\*!(apple)\\* or \\*\\[!apple\\]\\* but it did not work\n\n&amp;#x200B;\n\nThank you !", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Glue Crawler Exclude pattern", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156usm1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690060211.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690058735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone i have a question with Glue Exclude pattern, really appreciate if anyone can help me&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having an S3 bucket that contains many csv files in same folder and i want to run Crawlers on that folder to create some tables. The S3 bucket structure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;bucket_name/folder_name\n\n&lt;ul&gt;\n&lt;li&gt;xxx-apple-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;xxx-apple-xxx-2.csv&lt;/li&gt;\n&lt;li&gt;xxx-orange-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;xxx-strawberry-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;xxx-chicken-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;xxx-chicken-xxx-2.csv&lt;/li&gt;\n&lt;li&gt;xxx-dog-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;xxx-cat-xxx-1.csv&lt;/li&gt;\n&lt;li&gt;and many more....&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And i want to create Crawler to create 1 table for each category, such as 1 table for apple, 1 table for orange (1 crawler --&amp;gt; 1 table)..... And the number of categories is over 100. I see that Glue Crawler has a property, name &lt;a href=\"https://docs.aws.amazon.com/glue/latest/dg/define-crawler.html#crawler-data-stores-exclude\"&gt;Exclude&lt;/a&gt; pattern (but i do not see the Include pattern property) that we use filter out files when running crawler.&lt;/p&gt;\n\n&lt;p&gt;Now i want to create a Crawler for apple, so it should ignore all files except *apple* ones. I&amp;#39;ve tried with the Exclude but not achieve this yet. Do anyone know how to do this ?&lt;/p&gt;\n\n&lt;p&gt;Crawler name: apple_crawler&lt;/p&gt;\n\n&lt;p&gt;S3_path: bucket_name/folder_name&lt;/p&gt;\n\n&lt;p&gt;Exclude: i have tried things like: *[!{apple}]* or *!(apple)* or *[!apple]* but it did not work&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156usm1", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156usm1/aws_glue_crawler_exclude_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156usm1/aws_glue_crawler_exclude_pattern/", "subreddit_subscribers": 117412, "created_utc": 1690058735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We use AWS at my work for ETL to work with small datasets (~5000 rows). We have multiple pipelines with the following setup.\n\n1. External systems send files using SFTP to S3 raw files folder.\n2. Lambda gets triggered and moves files to staging folder in S3.\n3. Glue job gets triggered and transforms the data and moves transformed file to transformed folder in S3.\n4. Lambda gets triggered and uploads data to Salesforce.\n\nThe painful part about all of this is that all AWS resources are created manually. All the S3 folders, lambdas, roles, glue jobs, SFTP, etc. There really is no way easy to see if a lambda or glue job failed. I usually look at the logs after the failure to find out if they failed. My question to you guys is, how do I automate this pipeline? Also, how do I get alerted when a lambda or glue job fails?", "author_fullname": "t2_cbdg7n9sh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with automation and alerts in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156nl1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690040960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We use AWS at my work for ETL to work with small datasets (~5000 rows). We have multiple pipelines with the following setup.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;External systems send files using SFTP to S3 raw files folder.&lt;/li&gt;\n&lt;li&gt;Lambda gets triggered and moves files to staging folder in S3.&lt;/li&gt;\n&lt;li&gt;Glue job gets triggered and transforms the data and moves transformed file to transformed folder in S3.&lt;/li&gt;\n&lt;li&gt;Lambda gets triggered and uploads data to Salesforce.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The painful part about all of this is that all AWS resources are created manually. All the S3 folders, lambdas, roles, glue jobs, SFTP, etc. There really is no way easy to see if a lambda or glue job failed. I usually look at the logs after the failure to find out if they failed. My question to you guys is, how do I automate this pipeline? Also, how do I get alerted when a lambda or glue job fails?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156nl1s", "is_robot_indexable": true, "report_reasons": null, "author": "fun_coordinator", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156nl1s/need_help_with_automation_and_alerts_in_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156nl1s/need_help_with_automation_and_alerts_in_aws/", "subreddit_subscribers": 117412, "created_utc": 1690040960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Importing files into mysql using mysql command-line client. How do nk I modify the 'secure-file-priv' option to an empty string?.\n\nI'm unable to import files directly into mysql using the Data Import Wizard", "author_fullname": "t2_k8yx6w60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data importation issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156ogaf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690043067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Importing files into mysql using mysql command-line client. How do nk I modify the &amp;#39;secure-file-priv&amp;#39; option to an empty string?.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m unable to import files directly into mysql using the Data Import Wizard&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156ogaf", "is_robot_indexable": true, "report_reasons": null, "author": "GhostPrime3111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156ogaf/data_importation_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156ogaf/data_importation_issues/", "subreddit_subscribers": 117412, "created_utc": 1690043067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I have been asked to test a delta table inside databricks and i'm running in short of ideas to test case them.\n\nOther than basic sanity checks like nulls - dupe check - count match. Are there are any creative test cases that i can use over my tables in databricks? We are doing an sap to databricks data transfer which updated the records based on scd-1. I'm not able to think much of a test case to try on with.\n\nAny help/resources highly appreciated!", "author_fullname": "t2_nyd96q0y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Test cases to try on a delta table in databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156dy2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690012909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been asked to test a delta table inside databricks and i&amp;#39;m running in short of ideas to test case them.&lt;/p&gt;\n\n&lt;p&gt;Other than basic sanity checks like nulls - dupe check - count match. Are there are any creative test cases that i can use over my tables in databricks? We are doing an sap to databricks data transfer which updated the records based on scd-1. I&amp;#39;m not able to think much of a test case to try on with.&lt;/p&gt;\n\n&lt;p&gt;Any help/resources highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156dy2q", "is_robot_indexable": true, "report_reasons": null, "author": "theaitribe", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156dy2q/test_cases_to_try_on_a_delta_table_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156dy2q/test_cases_to_try_on_a_delta_table_in_databricks/", "subreddit_subscribers": 117412, "created_utc": 1690012909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working for a big energy company in the UK, and was thinking of switching my visa to the global talent visa after getting endorsed by [technation](https://technation.io/). One of their requirements state that you must have experience working in a product-led digital tech company and have recommendations from people working there.\n\nNow I am confused about what counts as a 'product-led digital tech company'. I work in the information systems department of my present company, tasked with making the whole data infrastructure for all their data science, analytics and machine learning workloads. My company is not a service based company for technology, but their main business driver is exploration/production of oil/gas and carbon capture.\n\nI work on the tech side of things for them and helping them reach their net zero targets. Is this experience not counted as working for a 'product company'?", "author_fullname": "t2_l35gwhuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Counts As a Product-led Digital Tech Company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156k1ma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690032123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working for a big energy company in the UK, and was thinking of switching my visa to the global talent visa after getting endorsed by &lt;a href=\"https://technation.io/\"&gt;technation&lt;/a&gt;. One of their requirements state that you must have experience working in a product-led digital tech company and have recommendations from people working there.&lt;/p&gt;\n\n&lt;p&gt;Now I am confused about what counts as a &amp;#39;product-led digital tech company&amp;#39;. I work in the information systems department of my present company, tasked with making the whole data infrastructure for all their data science, analytics and machine learning workloads. My company is not a service based company for technology, but their main business driver is exploration/production of oil/gas and carbon capture.&lt;/p&gt;\n\n&lt;p&gt;I work on the tech side of things for them and helping them reach their net zero targets. Is this experience not counted as working for a &amp;#39;product company&amp;#39;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?auto=webp&amp;s=5b96fd09d2a8f835c4f1edf102bd4dfe6ad93b50", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a323c5555b8478579a1c5bb600bddb24b8b29d2", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3adaad99f04a098c40ba80f3f28c54dc9b91cee5", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=77ebe6f12611d9617794c7a7e04cafc65b7cad23", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3d7e5562070bec36ebc74ff366b5acd638959fb", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=82ec7b5f6ba81a46fb81b8bb6cfbd62dbffa8802", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/lBZiXJu9sCR2EHutbShHRdhdF5dsD7W9WdzQXkci87Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89d98d7751f78b84cc1bf3e87039aea2b19b054e", "width": 1080, "height": 607}], "variants": {}, "id": "mzrGcfw8aMOS9oEIoN0Zq-1gLrgzdj_QqOjNOWejydQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156k1ma", "is_robot_indexable": true, "report_reasons": null, "author": "Senior_Anteater4688", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156k1ma/what_counts_as_a_productled_digital_tech_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156k1ma/what_counts_as_a_productled_digital_tech_company/", "subreddit_subscribers": 117412, "created_utc": 1690032123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have a traditional BI and data analyst background, I\u2019ve done some DE but not full time. Recently I completed a data engineering certification, working with team members to develop a working data pipeline from an external api to redshift using AWS glue, AWS Athena, and redshift.\n\nI understand basics and have good chops in technicals. If I want to continue my personal growth and keep developing my personal projects, at what point do I try to introduce SWE practices or tools into my skill set? \n\nI know of docker/K8s for containerizations or CI/CD using GitHub or AWS code commit,or even a little about IAC using Terraform, but it\u2019s a big difference between knowing when and where to implement these features in an actual pipeline.", "author_fullname": "t2_bg7xa0oxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning to implement SWE practices as a newbie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156ifdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690027664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a traditional BI and data analyst background, I\u2019ve done some DE but not full time. Recently I completed a data engineering certification, working with team members to develop a working data pipeline from an external api to redshift using AWS glue, AWS Athena, and redshift.&lt;/p&gt;\n\n&lt;p&gt;I understand basics and have good chops in technicals. If I want to continue my personal growth and keep developing my personal projects, at what point do I try to introduce SWE practices or tools into my skill set? &lt;/p&gt;\n\n&lt;p&gt;I know of docker/K8s for containerizations or CI/CD using GitHub or AWS code commit,or even a little about IAC using Terraform, but it\u2019s a big difference between knowing when and where to implement these features in an actual pipeline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "156ifdd", "is_robot_indexable": true, "report_reasons": null, "author": "Smellingbigbutts", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156ifdd/learning_to_implement_swe_practices_as_a_newbie/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156ifdd/learning_to_implement_swe_practices_as_a_newbie/", "subreddit_subscribers": 117412, "created_utc": 1690027664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My legacy code base is pretty much Spark-focused (mix of Pyspark and Scala Spark). We have a few Python UDFs but for performance reasons, we try to avoid udfs wherever possible. Lately, I'm pretty impressed by Polars and I'm wondering if anyone has started to mix in Polars (either for its expressive API or ability to handle larger than RAM data on a single node) with an existing Spark cluster. ", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone maintain a pipeline that has a good mix of Polars and Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156dhxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690011379.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My legacy code base is pretty much Spark-focused (mix of Pyspark and Scala Spark). We have a few Python UDFs but for performance reasons, we try to avoid udfs wherever possible. Lately, I&amp;#39;m pretty impressed by Polars and I&amp;#39;m wondering if anyone has started to mix in Polars (either for its expressive API or ability to handle larger than RAM data on a single node) with an existing Spark cluster. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "156dhxt", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156dhxt/does_anyone_maintain_a_pipeline_that_has_a_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156dhxt/does_anyone_maintain_a_pipeline_that_has_a_good/", "subreddit_subscribers": 117412, "created_utc": 1690011379.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking to obtain real-time data from Twitter to conduct sentiment analysis using Spark Streaming. However, I've encountered difficulties while attempting to retrieve this data through Tweepy. After some research, I believe the issue might be related to limitations with Tweepy's free access. Could you suggest alternative methods or approaches to acquire real-time data without relying on Tweepy or any other alternatives to it?", "author_fullname": "t2_feara4tb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-Time Twitter Data Acquisition for Sentiment Analysis: Seeking Alternatives to Tweepy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156zpo3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690071181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to obtain real-time data from Twitter to conduct sentiment analysis using Spark Streaming. However, I&amp;#39;ve encountered difficulties while attempting to retrieve this data through Tweepy. After some research, I believe the issue might be related to limitations with Tweepy&amp;#39;s free access. Could you suggest alternative methods or approaches to acquire real-time data without relying on Tweepy or any other alternatives to it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156zpo3", "is_robot_indexable": true, "report_reasons": null, "author": "Kratos_1412", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156zpo3/realtime_twitter_data_acquisition_for_sentiment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156zpo3/realtime_twitter_data_acquisition_for_sentiment/", "subreddit_subscribers": 117412, "created_utc": 1690071181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello everyone,\n\nI wanted to express my gratitude to this subreddit for helping me become a certified Data Engineer. It feels amazing to have achieved this milestone with your support.\n\nRecently, my company approached me to create a learning roadmap for aspiring Data Engineers. The goal is to provide newcomers with a clear and structured learning path to follow. Understanding that it can be overwhelming to determine which skills to develop and the right order to do so, I have carefully designed a learning roadmap that consists of recommended courses to take, arranged in a specific order.\n\nBelow, you'll find the learning roadmap I've created:\n\n[Here it is](https://imgur.com/oKh3S4r) \n\nFurthermore, I've compiled a stack of technologies that the company should prioritize and gain experience with for the Data Engineer and Data Architect department.\n\nI would greatly appreciate your feedback on this roadmap. If you feel that any modifications, additions, or removals would be beneficial, please let me know.\n\nThank you all for your continuous support and contributions to this community.\n\n&amp;#x200B;\n\n **Relevant Technologies**\n\n&amp;#x200B;\n\n|GCP|\n|:-|\n|Azure|\n|AWS|\n|Pentaho|\n|SQL|\n|Python|\n|Spark|\n|DBT|\n|Airflow|\n|Databricks|\n\n&amp;#x200B;", "author_fullname": "t2_ch1z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certified Data Engineer Offering Learning Roadmap and Technology Stack - Seeking Community Feedback", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156qo29", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1690048500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I wanted to express my gratitude to this subreddit for helping me become a certified Data Engineer. It feels amazing to have achieved this milestone with your support.&lt;/p&gt;\n\n&lt;p&gt;Recently, my company approached me to create a learning roadmap for aspiring Data Engineers. The goal is to provide newcomers with a clear and structured learning path to follow. Understanding that it can be overwhelming to determine which skills to develop and the right order to do so, I have carefully designed a learning roadmap that consists of recommended courses to take, arranged in a specific order.&lt;/p&gt;\n\n&lt;p&gt;Below, you&amp;#39;ll find the learning roadmap I&amp;#39;ve created:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/oKh3S4r\"&gt;Here it is&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Furthermore, I&amp;#39;ve compiled a stack of technologies that the company should prioritize and gain experience with for the Data Engineer and Data Architect department.&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate your feedback on this roadmap. If you feel that any modifications, additions, or removals would be beneficial, please let me know.&lt;/p&gt;\n\n&lt;p&gt;Thank you all for your continuous support and contributions to this community.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Relevant Technologies&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;GCP&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Azure&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AWS&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Pentaho&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SQL&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Python&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Spark&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;DBT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Airflow&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Databricks&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/f7EAShxhi-FZmUuSmrZ1pQZipKdy9_6JPqCQzcGv3ts.jpg?auto=webp&amp;s=942e0caad63600129c6b89f7a0e11469a2cbd322", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/f7EAShxhi-FZmUuSmrZ1pQZipKdy9_6JPqCQzcGv3ts.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=763357754b7b888280200b747a036a160234d559", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/f7EAShxhi-FZmUuSmrZ1pQZipKdy9_6JPqCQzcGv3ts.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e38e34364f6be36022f4b5647152dbe4ce66221", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/f7EAShxhi-FZmUuSmrZ1pQZipKdy9_6JPqCQzcGv3ts.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b58280e32494ae22e122bdc0515e265b4d456f32", "width": 320, "height": 168}], "variants": {}, "id": "h1mjMrtfPMNL9exEKl_TPr4mhvRXbIZP_RvHn8J2--E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156qo29", "is_robot_indexable": true, "report_reasons": null, "author": "lSniperwolfl", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156qo29/certified_data_engineer_offering_learning_roadmap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156qo29/certified_data_engineer_offering_learning_roadmap/", "subreddit_subscribers": 117412, "created_utc": 1690048500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Machine learning in data engineer", "author_fullname": "t2_ifyv45uo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My first article focused on the field of data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_156qfc5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HbfS7HjqVgvETniMb6gWL9jLxRnuF4fFowv07d26X-E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690047884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Machine learning in data engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@thiago2002sr/machine-learning-in-data-engineering-turning-data-into-precious-knowledge-201224922a3c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RKdUkwTFuP43O_0eXsYhJa2lHMIXYCX3wpg5_y9Mnfc.jpg?auto=webp&amp;s=23f2ef8ecde21ce5dbc0e521a45fc56a51f16b09", "width": 275, "height": 183}, "resolutions": [{"url": "https://external-preview.redd.it/RKdUkwTFuP43O_0eXsYhJa2lHMIXYCX3wpg5_y9Mnfc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c94c3fb99db60f720e6cd678659407e81e999bfd", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/RKdUkwTFuP43O_0eXsYhJa2lHMIXYCX3wpg5_y9Mnfc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9514fb5bae38ebb39530e61206bca40b4a73591c", "width": 216, "height": 143}], "variants": {}, "id": "YTmNATRz_IxIBno4oepJxOCZr1g122YzwyQ7ee4NlJg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "156qfc5", "is_robot_indexable": true, "report_reasons": null, "author": "adabbledragon85", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156qfc5/my_first_article_focused_on_the_field_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@thiago2002sr/machine-learning-in-data-engineering-turning-data-into-precious-knowledge-201224922a3c", "subreddit_subscribers": 117412, "created_utc": 1690047884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone.\n\n&amp;#x200B;\n\nI got an internship in data science but I was told I was going to do some data engineering as well.   \nI come from a math background, so my programming skills are not as strong as someone from computer science or something like that.\n\nHowever, I got some time before I start and asked what I could learn before starting and they gave me this list:  \n\n\n* ETL Processes\n* MQTT\n* Kafka\n* Airflow\n* Cube\n* Superset\n\n&amp;#x200B;\n\nIs there any youtube, course or book that covers all of this?  \n\n\nThanks a lot for the help.  \n", "author_fullname": "t2_d57qc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will do Data Engineering in a Data Scientist Internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_156gy2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690023157.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I got an internship in data science but I was told I was going to do some data engineering as well.&lt;br/&gt;\nI come from a math background, so my programming skills are not as strong as someone from computer science or something like that.&lt;/p&gt;\n\n&lt;p&gt;However, I got some time before I start and asked what I could learn before starting and they gave me this list:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ETL Processes&lt;/li&gt;\n&lt;li&gt;MQTT&lt;/li&gt;\n&lt;li&gt;Kafka&lt;/li&gt;\n&lt;li&gt;Airflow&lt;/li&gt;\n&lt;li&gt;Cube&lt;/li&gt;\n&lt;li&gt;Superset&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any youtube, course or book that covers all of this?  &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot for the help.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "156gy2r", "is_robot_indexable": true, "report_reasons": null, "author": "lavadao", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/156gy2r/will_do_data_engineering_in_a_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/156gy2r/will_do_data_engineering_in_a_data_scientist/", "subreddit_subscribers": 117412, "created_utc": 1690023157.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}