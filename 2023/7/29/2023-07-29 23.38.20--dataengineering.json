{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5qg3y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This explains A LOT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15d1z98", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": "transparent", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TtBWX7gmlaBDiDWxfgG18OJbBjB5srIFvlPxnvnH8OU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690662377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/m5tcvsl7ryeb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/m5tcvsl7ryeb1.png?auto=webp&amp;s=f7ae601aa0a27f91e9d4dbb019e079cc6dfc05fc", "width": 714, "height": 717}, "resolutions": [{"url": "https://preview.redd.it/m5tcvsl7ryeb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=594691c66aae8d504850af5afe34ac1a9364bc00", "width": 108, "height": 108}, {"url": "https://preview.redd.it/m5tcvsl7ryeb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=02c5d5e71550bdd5d8667f7c4c50d85e34ef2dcb", "width": 216, "height": 216}, {"url": "https://preview.redd.it/m5tcvsl7ryeb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=50587755f2fa2ceb837164df68d5e35c0eed09e6", "width": 320, "height": 321}, {"url": "https://preview.redd.it/m5tcvsl7ryeb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b7dbbfa264d356ec434804355e70ac084b1d195", "width": 640, "height": 642}], "variants": {}, "id": "TdyoVxy6dqhqoqrQr6SxmdK9qX_66oG6Kk-m11aWZzQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Se\u00f1or Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "15d1z98", "is_robot_indexable": true, "report_reasons": null, "author": "morpho4444", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15d1z98/this_explains_a_lot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/m5tcvsl7ryeb1.png", "subreddit_subscribers": 118974, "created_utc": 1690662377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!\n\n[https://www.youtube.com/watch?v=aiHSMYvoqYE](https://www.youtube.com/watch?v=aiHSMYvoqYE)", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cnwi1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690622900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=aiHSMYvoqYE\"&gt;https://www.youtube.com/watch?v=aiHSMYvoqYE&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rIeX28O2XfIv3Y4U1OTyTkfRQORKzqxxw43Jw5vYzh4.jpg?auto=webp&amp;s=96140c2eb3acba0ac6beadc2b0fde8a7c5df9dc6", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/rIeX28O2XfIv3Y4U1OTyTkfRQORKzqxxw43Jw5vYzh4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0cced515ca16fd65de90344f4505be38bae9be1", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/rIeX28O2XfIv3Y4U1OTyTkfRQORKzqxxw43Jw5vYzh4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2ceb1cb25a99dab9a5756f465fc984003531baa", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/rIeX28O2XfIv3Y4U1OTyTkfRQORKzqxxw43Jw5vYzh4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a691db8960c92445122d69e8eea5a7bf8b600430", "width": 320, "height": 240}], "variants": {}, "id": "_rVKQNaxcPqL1qZYn-JkINnf7oHCvdPuQq4k6I3ej4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15cnwi1", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cnwi1/i_recorded_a_crash_course_on_polars_library_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cnwi1/i_recorded_a_crash_course_on_polars_library_of/", "subreddit_subscribers": 118974, "created_utc": 1690622900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just started writing the backend service with Fiber (GoLang) in my job. Surely it is a backend development, but I wonder if it is also covered under the scope of data engineering?\n\nHow do backend developers and data engineers communicate generally?\n\nAnd are there any parts that both do?", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The intersection of DE and Backend Dev.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cglfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690597909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started writing the backend service with Fiber (GoLang) in my job. Surely it is a backend development, but I wonder if it is also covered under the scope of data engineering?&lt;/p&gt;\n\n&lt;p&gt;How do backend developers and data engineers communicate generally?&lt;/p&gt;\n\n&lt;p&gt;And are there any parts that both do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15cglfc", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cglfc/the_intersection_of_de_and_backend_dev/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cglfc/the_intersection_of_de_and_backend_dev/", "subreddit_subscribers": 118974, "created_utc": 1690597909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I placed into my junior Data Engineer role (current) through a rotational development program. \nI've never done a leetcode-style interview before and was wondering whether Data Engineering job interviews usually require this type of testing.", "author_fullname": "t2_vmcasqim", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How important is Leetcode for DE interviews?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15d0a1c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690657978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I placed into my junior Data Engineer role (current) through a rotational development program. \nI&amp;#39;ve never done a leetcode-style interview before and was wondering whether Data Engineering job interviews usually require this type of testing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15d0a1c", "is_robot_indexable": true, "report_reasons": null, "author": "aaloo_chaat", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15d0a1c/how_important_is_leetcode_for_de_interviews/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15d0a1c/how_important_is_leetcode_for_de_interviews/", "subreddit_subscribers": 118974, "created_utc": 1690657978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At a recent meetup I saw guys punting AbInitio for all ETL and WH processes. Is this the way to go? I know it's got some nuances to the language, but it's got some simple looking connectors and components. Is it worth it? Connections I deal with are the normal SQL, XLS, Kafka, etc. I'm proficient in Python and SQL. Will it be tough to learn? And is it worth the effort?", "author_fullname": "t2_11t26p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AbInitio a yes or no?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cod1t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690624550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At a recent meetup I saw guys punting AbInitio for all ETL and WH processes. Is this the way to go? I know it&amp;#39;s got some nuances to the language, but it&amp;#39;s got some simple looking connectors and components. Is it worth it? Connections I deal with are the normal SQL, XLS, Kafka, etc. I&amp;#39;m proficient in Python and SQL. Will it be tough to learn? And is it worth the effort?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15cod1t", "is_robot_indexable": true, "report_reasons": null, "author": "byeproduct", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cod1t/abinitio_a_yes_or_no/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cod1t/abinitio_a_yes_or_no/", "subreddit_subscribers": 118974, "created_utc": 1690624550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im a junior data engineer and due to budget cuts within my company it looks like I\u2019m getting laid off. I\u2019ve been with this company for 8 months. My only offer right now is for a cybersecurity job with decent pay for a pretty large company. Assuming that while working in this cybersecurity job, I maintain my knowledge of DE while also practicing leetcode, would it be possible to make the switch back to DE in a year or so? Any help would be appreciated!!", "author_fullname": "t2_9ghl0zs1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE to cybersecurity and then back to DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cix3x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690605298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a junior data engineer and due to budget cuts within my company it looks like I\u2019m getting laid off. I\u2019ve been with this company for 8 months. My only offer right now is for a cybersecurity job with decent pay for a pretty large company. Assuming that while working in this cybersecurity job, I maintain my knowledge of DE while also practicing leetcode, would it be possible to make the switch back to DE in a year or so? Any help would be appreciated!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15cix3x", "is_robot_indexable": true, "report_reasons": null, "author": "Perfect_Kangaroo6233", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cix3x/de_to_cybersecurity_and_then_back_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cix3x/de_to_cybersecurity_and_then_back_to_de/", "subreddit_subscribers": 118974, "created_utc": 1690605298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have just started my new job as a junior data engineer a month ago. In the beginning, I was re-designing a relational database, which includes more than +100. As I am at the very beginning of my career, it was nice to have experience with relational systems in detail, before deep diving into dimensional modeling, etc. for a data warehouse later on.\n\nUnfortunately, I'm the only one that works with data in the company (It's a small startup). Because of that, I'm having trouble deciding how the whole system should work. I need to mine data from different sources in batches for scheduled periods. I'm thinking to write this operation using Airflow, and putting the raw data in S3 buckets as parquet files. After merging, cleansing, and quality-checking with Pandas, I'm thinking to insert the data into the database using the API I am writing now. For analytics, I'm planning to take snapshots of the database weekly and insert them into a warehouse (probably Snowflake) using DBT for transformations.\n\nI have no idea if this is a good plan. What tools&amp;frameworks do you recommend me to look at? What sources may help me to cover what's under the hood in batch processing and data mining? As a data engineer titled employee, what other things do I need to check for, or what else is considered in the data engineering scope?", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to Roadmap My Data Engineering Processes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cgjd1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690597990.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690597743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just started my new job as a junior data engineer a month ago. In the beginning, I was re-designing a relational database, which includes more than +100. As I am at the very beginning of my career, it was nice to have experience with relational systems in detail, before deep diving into dimensional modeling, etc. for a data warehouse later on.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I&amp;#39;m the only one that works with data in the company (It&amp;#39;s a small startup). Because of that, I&amp;#39;m having trouble deciding how the whole system should work. I need to mine data from different sources in batches for scheduled periods. I&amp;#39;m thinking to write this operation using Airflow, and putting the raw data in S3 buckets as parquet files. After merging, cleansing, and quality-checking with Pandas, I&amp;#39;m thinking to insert the data into the database using the API I am writing now. For analytics, I&amp;#39;m planning to take snapshots of the database weekly and insert them into a warehouse (probably Snowflake) using DBT for transformations.&lt;/p&gt;\n\n&lt;p&gt;I have no idea if this is a good plan. What tools&amp;amp;frameworks do you recommend me to look at? What sources may help me to cover what&amp;#39;s under the hood in batch processing and data mining? As a data engineer titled employee, what other things do I need to check for, or what else is considered in the data engineering scope?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cgjd1", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cgjd1/struggling_to_roadmap_my_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cgjd1/struggling_to_roadmap_my_data_engineering/", "subreddit_subscribers": 118974, "created_utc": 1690597743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_mhyr29t1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Command Cheatsheet - join our LinkedIn dbt Developer Group for more content: https://www.linkedin.com/groups/12857345/", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 98, "top_awarded_type": null, "hide_score": false, "name": "t3_15cwagm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/T9ndC7xcEgnI5PNA1a6L2FoWv3vek6NzvdY8AQe0vjw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690647697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8vv2c73ijxeb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?auto=webp&amp;s=b382988a0fc2b6d0737b1ec1f09522999216bffd", "width": 2245, "height": 1587}, "resolutions": [{"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b464b07b2aeb028934cb336397268bbbcd3c805b", "width": 108, "height": 76}, {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e7c8d9abb76dfc05149b751d1806cfd5dfa66e6", "width": 216, "height": 152}, {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=998ce771e4b0b3f6418943d8a7b6b0a3d074e59e", "width": 320, "height": 226}, {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1c0c5d4be1284781fdc3b1b1f5a8fb9b5d661a7", "width": 640, "height": 452}, {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9dcc3a4c31b6fe9c0af43fe84b23859b5766456", "width": 960, "height": 678}, {"url": "https://preview.redd.it/8vv2c73ijxeb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28bf339f5fca623ad9cc8797e952d70be34fa27c", "width": 1080, "height": 763}], "variants": {}, "id": "cwaFeb5fLoKAAy7tLKh8x7IqVQx8x5I1In-ykIl19wU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15cwagm", "is_robot_indexable": true, "report_reasons": null, "author": "Datafluent", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cwagm/dbt_command_cheatsheet_join_our_linkedin_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8vv2c73ijxeb1.png", "subreddit_subscribers": 118974, "created_utc": 1690647697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently use Azure Data Factory to do transformations (data flows) but it's not fun. It's slow and expensive.\n\nGiven absolute freedom, what tools would you use?\n\nOur architecture looks like this: we gather data from many sources and push it to delta lake. Then we do some basic transformations and put data in delta lake folders. Then we do complex transformations using these folders as sources. We do joins etc, and we use some databases as sources to help us do transformations (like providing missing values). Finally, we sink to our databases where we store data that is displayed for users. I think we process like 10-100 GB of data everytime.\n\nBtw, I'm complete noob, backend engineer, never did any data engineering, just started recently.", "author_fullname": "t2_8kfdimyn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives for ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cz9ep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690655400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently use Azure Data Factory to do transformations (data flows) but it&amp;#39;s not fun. It&amp;#39;s slow and expensive.&lt;/p&gt;\n\n&lt;p&gt;Given absolute freedom, what tools would you use?&lt;/p&gt;\n\n&lt;p&gt;Our architecture looks like this: we gather data from many sources and push it to delta lake. Then we do some basic transformations and put data in delta lake folders. Then we do complex transformations using these folders as sources. We do joins etc, and we use some databases as sources to help us do transformations (like providing missing values). Finally, we sink to our databases where we store data that is displayed for users. I think we process like 10-100 GB of data everytime.&lt;/p&gt;\n\n&lt;p&gt;Btw, I&amp;#39;m complete noob, backend engineer, never did any data engineering, just started recently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cz9ep", "is_robot_indexable": true, "report_reasons": null, "author": "Apprehensive-Lead794", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cz9ep/alternatives_for_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cz9ep/alternatives_for_adf/", "subreddit_subscribers": 118974, "created_utc": 1690655400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I will consume an API using Python, specifically the YouTube API, to get data from channels. It won't be a massive amount of data, maybe around 20 channels. \n\nSo, I want to store this data in a database, probably SQL. After storing this data, I'll need to perform a query to filter items with a specific string.\n\n  \nThen, I need to send the filtered data to a WordPress page where some data visualizations are present. I still need to familiarize myself with the implementation, but basically, I need to send the filtered data from the database to the WordPress front-end. \n\nWhat do you recommend for deploying this application? Considering that it's not a huge amount of data (neither in terms of data acquisition nor the site's expected traffic). \n\nI thought about using Heroku, but I'm also wondering if the free tier of AWS or GCP could handle it. Any advice? ", "author_fullname": "t2_5wo1rzy0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Python App to Fetch, Store, and Display Data - Need Advice for Deployment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cyq11", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690654003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will consume an API using Python, specifically the YouTube API, to get data from channels. It won&amp;#39;t be a massive amount of data, maybe around 20 channels. &lt;/p&gt;\n\n&lt;p&gt;So, I want to store this data in a database, probably SQL. After storing this data, I&amp;#39;ll need to perform a query to filter items with a specific string.&lt;/p&gt;\n\n&lt;p&gt;Then, I need to send the filtered data to a WordPress page where some data visualizations are present. I still need to familiarize myself with the implementation, but basically, I need to send the filtered data from the database to the WordPress front-end. &lt;/p&gt;\n\n&lt;p&gt;What do you recommend for deploying this application? Considering that it&amp;#39;s not a huge amount of data (neither in terms of data acquisition nor the site&amp;#39;s expected traffic). &lt;/p&gt;\n\n&lt;p&gt;I thought about using Heroku, but I&amp;#39;m also wondering if the free tier of AWS or GCP could handle it. Any advice? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cyq11", "is_robot_indexable": true, "report_reasons": null, "author": "interferemadly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cyq11/building_a_python_app_to_fetch_store_and_display/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cyq11/building_a_python_app_to_fetch_store_and_display/", "subreddit_subscribers": 118974, "created_utc": 1690654003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone use an open source warehouse for their solution? Something like clickhouse, Doris or Starrocks? Curious what your architecture is like? Do you load the data and raw and transform in the warehouse or transform in your storage and load final tables into the warehouse? Do you deploy in a container on a k8s cluster to manage compute? \n\nWas thinking about going the open source route and deploying in a docker container on a k8s cluster but wanted to get opinions?", "author_fullname": "t2_ffzuzn1vk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with open source warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cr30l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690633590.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone use an open source warehouse for their solution? Something like clickhouse, Doris or Starrocks? Curious what your architecture is like? Do you load the data and raw and transform in the warehouse or transform in your storage and load final tables into the warehouse? Do you deploy in a container on a k8s cluster to manage compute? &lt;/p&gt;\n\n&lt;p&gt;Was thinking about going the open source route and deploying in a docker container on a k8s cluster but wanted to get opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15cr30l", "is_robot_indexable": true, "report_reasons": null, "author": "Public_Fart42069", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cr30l/experience_with_open_source_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cr30l/experience_with_open_source_warehouse/", "subreddit_subscribers": 118974, "created_utc": 1690633590.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently grinding the easy-medium difficulty sql problems, and notice I need 2-3 attempts to pass all test cases because of some minor errors.\n\nI am wondering if the actual sql interview will expect an one-take pass from me, or will I have to write down the solution on a white board without any test cases?\n\nSuggestions about how to become sql proficient just like doing 1+1?", "author_fullname": "t2_csk6gf7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does most of the SQL coding interview requires a one-take pass?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15d2nue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690664133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently grinding the easy-medium difficulty sql problems, and notice I need 2-3 attempts to pass all test cases because of some minor errors.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if the actual sql interview will expect an one-take pass from me, or will I have to write down the solution on a white board without any test cases?&lt;/p&gt;\n\n&lt;p&gt;Suggestions about how to become sql proficient just like doing 1+1?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15d2nue", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Astronomer-471", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15d2nue/does_most_of_the_sql_coding_interview_requires_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15d2nue/does_most_of_the_sql_coding_interview_requires_a/", "subreddit_subscribers": 118974, "created_utc": 1690664133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_qvzmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grab Reduces Traffic Cost for Kafka Consumers on AWS to Zero", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15cnrhf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CHzxOlIdFai73jdps92QFmTBe2u_Muatxm35LWGgVVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690622386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "infoq.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.infoq.com/news/2023/07/grab-apache-kafka-aws-cost/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?auto=webp&amp;s=0215757ae25713ea844ebf251d8017ef21a46dcf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8c4b47c1e2c2616fc7bbbd7ab15707f9527f3555", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97432d70baea78d7c5fd7caa036cd57c55ca5fbb", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=212c8330116c5159534c69e1ffc2f844ecb2b1a2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=39eed804b9cfe931843414614801fc3d0abcce68", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa570a8e3701065aee193930e08baf55979373b5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qxxJXdmMUJ3dsKeLP7nXD14D7XXZqDZU5I74XsvsEdU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34802f7b3f0c815b784533832b12539e38a522b6", "width": 1080, "height": 567}], "variants": {}, "id": "aTjRY1Q9I9oaBT4jYUFfCvmgvYk6dxx7itnnM5s6qXs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15cnrhf", "is_robot_indexable": true, "report_reasons": null, "author": "rgancarz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cnrhf/grab_reduces_traffic_cost_for_kafka_consumers_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.infoq.com/news/2023/07/grab-apache-kafka-aws-cost/", "subreddit_subscribers": 118974, "created_utc": 1690622386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a side project. It is to just fetch the data from the API. There are 2 columns which changes its status. Like say - instore availability and online availability of the product.\nMy aim to track these two column to see how much time the product was available instore and how much time the product was online available.\n\nFor now, I have designed a pipeline, that fetches the data from the api and store in GCP cloud storage. The data is appended every time.\nNext I\u2019m planning to apply Slowly Changing Dimension - Type 2, to track both history data and new data. Is it the correct way to do that,? I\u2019m using big query for this step.\n\nAlso to get new data, I would be polling the data every say 15 minutes using Airflow.\n\nFor the target, I would be polling big query and create a dashboard may be using streamlit - https://streamlit.io ( need your inputs here too, since I\u2019m new to creating dashboards )\n\nWhat are your thoughts on this?", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need your inputs on the mentioned scenario", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cjvjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690608772.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690608485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a side project. It is to just fetch the data from the API. There are 2 columns which changes its status. Like say - instore availability and online availability of the product.\nMy aim to track these two column to see how much time the product was available instore and how much time the product was online available.&lt;/p&gt;\n\n&lt;p&gt;For now, I have designed a pipeline, that fetches the data from the api and store in GCP cloud storage. The data is appended every time.\nNext I\u2019m planning to apply Slowly Changing Dimension - Type 2, to track both history data and new data. Is it the correct way to do that,? I\u2019m using big query for this step.&lt;/p&gt;\n\n&lt;p&gt;Also to get new data, I would be polling the data every say 15 minutes using Airflow.&lt;/p&gt;\n\n&lt;p&gt;For the target, I would be polling big query and create a dashboard may be using streamlit - &lt;a href=\"https://streamlit.io\"&gt;https://streamlit.io&lt;/a&gt; ( need your inputs here too, since I\u2019m new to creating dashboards )&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?auto=webp&amp;s=70e3c02be5b52bbc6d00188b82f6dfa84a731b65", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a92b0d1ef64b226674bd70f1bde51615fe66f9a3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4740ba3d52f606bdfee4a35310a66175f43f95f0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9f6c843305aeadb71a22ccaaf9554276bcdb406", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5250c7f9d45c0bd0083d0d3256b63afb177a60c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f187a0a348b10af41d73e3eac03d2e02fdd744fa", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NXBBgLYiyJjXURFzDGoQsVzAI9Fy597r1v9btldZXsw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8be103ecad30e16e3c1bf437e18e92c5238d6d2d", "width": 1080, "height": 567}], "variants": {}, "id": "2GkEr-f7WU61lyrRdUq4Vdk8_qLmlzKzmtn7dmNgzUA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cjvjs", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15cjvjs/need_your_inputs_on_the_mentioned_scenario/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cjvjs/need_your_inputs_on_the_mentioned_scenario/", "subreddit_subscribers": 118974, "created_utc": 1690608485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a current Senior CS student graduating in the spring. I have been researching a lot about data engineering and it seems super interesting.\n\nI have SQL experience and I have been using Python for a while.\n\nI am extremely interested in distributed systems and want to work with them How can I better prepare for an entry level data engineer role? Course/Book/Project?\n\nI have an SWE internship currently and I have some projects but those are swe related", "author_fullname": "t2_84qgyclze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Entry Level Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15d4fek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690668596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a current Senior CS student graduating in the spring. I have been researching a lot about data engineering and it seems super interesting.&lt;/p&gt;\n\n&lt;p&gt;I have SQL experience and I have been using Python for a while.&lt;/p&gt;\n\n&lt;p&gt;I am extremely interested in distributed systems and want to work with them How can I better prepare for an entry level data engineer role? Course/Book/Project?&lt;/p&gt;\n\n&lt;p&gt;I have an SWE internship currently and I have some projects but those are swe related&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15d4fek", "is_robot_indexable": true, "report_reasons": null, "author": "ilovemorbius69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15d4fek/entry_level_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15d4fek/entry_level_advice/", "subreddit_subscribers": 118974, "created_utc": 1690668596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am needing to maintain an enterprise data model for the company and don\u2019t want any table changes in prod unless it is approved by the architect. Since it is very easy to change DDLs with dbt, how would you do that? Make sure architect reviews all dbt code changes?  Plus I would like to start having ERDs through like sqldbm as our data documentation for users. Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt within enterprise data model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15d0lq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690658823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am needing to maintain an enterprise data model for the company and don\u2019t want any table changes in prod unless it is approved by the architect. Since it is very easy to change DDLs with dbt, how would you do that? Make sure architect reviews all dbt code changes?  Plus I would like to start having ERDs through like sqldbm as our data documentation for users. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15d0lq0", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15d0lq0/dbt_within_enterprise_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15d0lq0/dbt_within_enterprise_data_model/", "subreddit_subscribers": 118974, "created_utc": 1690658823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Greetings, \n\nI am creating this post to discuss slowly changing dimensions of type two and implementation plans, strategies in Snowflake. \n\nExample: \n\nImagine one has a table that is being delivered nightly via Snowpipe (full-load) into the same table daily, and in the table there is an added column ETL-DATE which is of data type timestamp_ntz, and for this table one must implement a slowly changing dimension (type 2) to unlock historical analysis over time.\n\nWhat are some strategies that are used for this set up? I have read a bit about streams and tasks for SCD, but placing a stream on this table only categorizes items as all update since this is a full load with new ETL-DATE everyday the table is fully loaded. \n\nAny ideas, thoughts, or different documentation guides that you could help me point me to in the correct direction?\n\nThanks!", "author_fullname": "t2_7mwoeyyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCD Type 2 (Snowflake)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cyw1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690654439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, &lt;/p&gt;\n\n&lt;p&gt;I am creating this post to discuss slowly changing dimensions of type two and implementation plans, strategies in Snowflake. &lt;/p&gt;\n\n&lt;p&gt;Example: &lt;/p&gt;\n\n&lt;p&gt;Imagine one has a table that is being delivered nightly via Snowpipe (full-load) into the same table daily, and in the table there is an added column ETL-DATE which is of data type timestamp_ntz, and for this table one must implement a slowly changing dimension (type 2) to unlock historical analysis over time.&lt;/p&gt;\n\n&lt;p&gt;What are some strategies that are used for this set up? I have read a bit about streams and tasks for SCD, but placing a stream on this table only categorizes items as all update since this is a full load with new ETL-DATE everyday the table is fully loaded. &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts, or different documentation guides that you could help me point me to in the correct direction?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cyw1f", "is_robot_indexable": true, "report_reasons": null, "author": "electronicentropy5", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cyw1f/scd_type_2_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cyw1f/scd_type_2_snowflake/", "subreddit_subscribers": 118974, "created_utc": 1690654439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The company im working in is an AWS shop now and most of our data sits in S3. My boss is seriously considering Microsoft Data Fabric after meeting with sales team. \n\nCan anyone give an opinion on using Shortcuts in Microsoft Data Fabric to connect to S3 in order to realise a data mesh architecture? Will it rack up massive egress costs from s3?", "author_fullname": "t2_mr7f2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft data fabric and working with AWS via shortcuts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15colib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690625352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The company im working in is an AWS shop now and most of our data sits in S3. My boss is seriously considering Microsoft Data Fabric after meeting with sales team. &lt;/p&gt;\n\n&lt;p&gt;Can anyone give an opinion on using Shortcuts in Microsoft Data Fabric to connect to S3 in order to realise a data mesh architecture? Will it rack up massive egress costs from s3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15colib", "is_robot_indexable": true, "report_reasons": null, "author": "detaurus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15colib/microsoft_data_fabric_and_working_with_aws_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15colib/microsoft_data_fabric_and_working_with_aws_via/", "subreddit_subscribers": 118974, "created_utc": 1690625352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear r/dataenginering,\n\nI'm seeking a solution that supports performing ETL on petabyte-sized data and subsequent ML deployment. \n\nHigh-level production stages are:\n\nPerform ETL -&gt; Feed features to a model -&gt; Calculate predictions -&gt; Save predictions to the database\n\nSo far, I have found 5 possible combinations to make this work:  \n1) pyspark for ETL + Ray for ML deployment. \n\n2) Spark on Ray (Ray DP) for ETL + Ray for ML. This is an integrated solution: [https://github.com/oap-project/raydp](https://github.com/oap-project/raydp).\n\n3) Dask on Ray for ETL + Ray for ML. \n\n4) Standalone Dask.\n\n5) Standalone pyspark.\n\nIn addition to production deployment, I plan to use the stack for data science experiments. Ideally, I'd want to perform data computations on a remote cluster via local Jupyter. A great example is [https://www.youtube.com/watch?v=nH\\_AQo8WdKw](https://www.youtube.com/watch?v=nH_AQo8WdKw).  \n\n\nI want to get the best-performing solution that fits the use cases. Any suggestion are highly appreciated! Thanks.  \n\n\n&amp;#x200B;", "author_fullname": "t2_en4s89gx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please help me choose a stack (Spark, Ray, Dask, Modin)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cgqni", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690598347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear &lt;a href=\"/r/dataenginering\"&gt;r/dataenginering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking a solution that supports performing ETL on petabyte-sized data and subsequent ML deployment. &lt;/p&gt;\n\n&lt;p&gt;High-level production stages are:&lt;/p&gt;\n\n&lt;p&gt;Perform ETL -&amp;gt; Feed features to a model -&amp;gt; Calculate predictions -&amp;gt; Save predictions to the database&lt;/p&gt;\n\n&lt;p&gt;So far, I have found 5 possible combinations to make this work:&lt;br/&gt;\n1) pyspark for ETL + Ray for ML deployment. &lt;/p&gt;\n\n&lt;p&gt;2) Spark on Ray (Ray DP) for ETL + Ray for ML. This is an integrated solution: &lt;a href=\"https://github.com/oap-project/raydp\"&gt;https://github.com/oap-project/raydp&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;3) Dask on Ray for ETL + Ray for ML. &lt;/p&gt;\n\n&lt;p&gt;4) Standalone Dask.&lt;/p&gt;\n\n&lt;p&gt;5) Standalone pyspark.&lt;/p&gt;\n\n&lt;p&gt;In addition to production deployment, I plan to use the stack for data science experiments. Ideally, I&amp;#39;d want to perform data computations on a remote cluster via local Jupyter. A great example is &lt;a href=\"https://www.youtube.com/watch?v=nH_AQo8WdKw\"&gt;https://www.youtube.com/watch?v=nH_AQo8WdKw&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;I want to get the best-performing solution that fits the use cases. Any suggestion are highly appreciated! Thanks.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?auto=webp&amp;s=153c7aaa0e3eb794afc025cc78339d5354f787c0", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b08382c1d1a87912293a4d86c38da136d24d5d93", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2671d57c1e6b74bbc0b590a4bfbe0b9a56568775", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7b36a315773a7d206164a50e5fe8947a683ffe3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0df101d0c2553e5f26bc89d32da610de1c1caf25", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49508f5345ac22cd9f5f00d2dff74a177eb7cc6f", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/yTbjYpUxUlF0AiC10_MnpoBIcUc597PtyJ65Lzrl9yI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da1a37a27635498cf9f31577ed0d73195cfeee7e", "width": 1080, "height": 540}], "variants": {}, "id": "v8_QSyChaxiwWfo6zTA_Vv6f536PrbAbpbQFhd-i0eY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cgqni", "is_robot_indexable": true, "report_reasons": null, "author": "nirewi1508", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cgqni/please_help_me_choose_a_stack_spark_ray_dask_modin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cgqni/please_help_me_choose_a_stack_spark_ray_dask_modin/", "subreddit_subscribers": 118974, "created_utc": 1690598347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've always wanted pipe syntax like R has in Python. Although you can chain pandas methods, there are a lot of cases where (to me) it feels clunky to write and read. There are a couple of existing packages, but I didn't feel like any of them really met my needs and preferences. \n\nIf anyone wants to give it a shot and provide feedback, I'd like to improve it. There are built in datasets and examples in the README.md. \n\nInstallation:\npip install PandaPlyr\n\nRepo:\nhttps://github.com/OlivierNDO/PandaPlyr", "author_fullname": "t2_ha2yf9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PandaPlyr - Pipe Syntax for Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cyzmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690654694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve always wanted pipe syntax like R has in Python. Although you can chain pandas methods, there are a lot of cases where (to me) it feels clunky to write and read. There are a couple of existing packages, but I didn&amp;#39;t feel like any of them really met my needs and preferences. &lt;/p&gt;\n\n&lt;p&gt;If anyone wants to give it a shot and provide feedback, I&amp;#39;d like to improve it. There are built in datasets and examples in the README.md. &lt;/p&gt;\n\n&lt;p&gt;Installation:\npip install PandaPlyr&lt;/p&gt;\n\n&lt;p&gt;Repo:\n&lt;a href=\"https://github.com/OlivierNDO/PandaPlyr\"&gt;https://github.com/OlivierNDO/PandaPlyr&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?auto=webp&amp;s=8546b7035066b53a89f1df263c536aeb3f1b1dbf", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=915006049e3e4d5464953d0c8874c6b156c29b3a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d8382d76f28d3edc2094965533818c049b504c1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c53ab1dc0db39fe9ea9ef19eb28f1d6ce6d4ecd3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7303b72c37111d45acbda20209c208fa1753e091", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9d548933b5989fdce5db73a06cdb7c4a2a97168", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9tpv1bN-Mmv-lOfGTgbTjKpfBO68IMmo2L638ity-wQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2b2741624c062f274093b51bdd8cefe2532ce516", "width": 1080, "height": 540}], "variants": {}, "id": "pvsmY3yQ7rSXzfsqeEaJIKih57SyVvjKAfSz4Ixezjk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15cyzmw", "is_robot_indexable": true, "report_reasons": null, "author": "HungryQuant", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cyzmw/pandaplyr_pipe_syntax_for_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cyzmw/pandaplyr_pipe_syntax_for_pandas/", "subreddit_subscribers": 118974, "created_utc": 1690654694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. How would you define data products?\n2. If you are in a company, and you need to share data products with other departments or people outside the company, what would you care about?\n3. What approach is good to you for sharing data products?\n4. Do you think there is value for sharing data  products?\n5. Applying the mindset of thinking backwards. What is the ideal world to you about data and what are missing?\n\nFeel free to answer any question I came up with. Thank you!", "author_fullname": "t2_1vdngj8p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about data products", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cxlfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690651079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;How would you define data products?&lt;/li&gt;\n&lt;li&gt;If you are in a company, and you need to share data products with other departments or people outside the company, what would you care about?&lt;/li&gt;\n&lt;li&gt;What approach is good to you for sharing data products?&lt;/li&gt;\n&lt;li&gt;Do you think there is value for sharing data  products?&lt;/li&gt;\n&lt;li&gt;Applying the mindset of thinking backwards. What is the ideal world to you about data and what are missing?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Feel free to answer any question I came up with. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15cxlfb", "is_robot_indexable": true, "report_reasons": null, "author": "cyyeh", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cxlfb/questions_about_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cxlfb/questions_about_data_products/", "subreddit_subscribers": 118974, "created_utc": 1690651079.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there any  Linkurious  like software but cheaper? cus  Linkurious  is 990$ per year so for learning purpose, it is kinda expensive... is there any software like  Linkurious  but at lower cost?", "author_fullname": "t2_9wizwc3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linkurious like software but cheaper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15cjv36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690608435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any  Linkurious  like software but cheaper? cus  Linkurious  is 990$ per year so for learning purpose, it is kinda expensive... is there any software like  Linkurious  but at lower cost?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15cjv36", "is_robot_indexable": true, "report_reasons": null, "author": "justwaiyan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15cjv36/linkurious_like_software_but_cheaper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15cjv36/linkurious_like_software_but_cheaper/", "subreddit_subscribers": 118974, "created_utc": 1690608435.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}