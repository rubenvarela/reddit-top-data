{"kind": "Listing", "data": {"after": "t3_15c55jd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "You all messed up. You've been relying on free cloud services and now the free ride is over. You're going to have to start paying real money to pursue this hobby like a lot of us already do. If you have a question regarding alternative storage options please do a search as it's being asked multiple times a day now.", "author_fullname": "t2_4h1y3k8i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can the mods somehow put a stop to all of the questions regarding cloud hosting alternatives it's getting old.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c0o2r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 166, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 166, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690558262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You all messed up. You&amp;#39;ve been relying on free cloud services and now the free ride is over. You&amp;#39;re going to have to start paying real money to pursue this hobby like a lot of us already do. If you have a question regarding alternative storage options please do a search as it&amp;#39;s being asked multiple times a day now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 300, "id": "award_725b427d-320b-4d02-8fb0-8bb7aa7b78aa", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=16&amp;height=16&amp;auto=webp&amp;s=b3bb991aac7c446063cc3b91d71d8547db0f7d6d", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=32&amp;height=32&amp;auto=webp&amp;s=881b998ff73380d3f02d27e7536aba842df055c1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=48&amp;height=48&amp;auto=webp&amp;s=325a8549233c6457eaf4eaef948230af4d062f0a", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a5261140af96699d24ded7497d3b10c831464ba", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=128&amp;height=128&amp;auto=webp&amp;s=6069896f540b6928b86a55082ae4d55f823ce094", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Sometimes you just got to doot.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Updoot", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=16&amp;height=16&amp;auto=webp&amp;s=b3bb991aac7c446063cc3b91d71d8547db0f7d6d", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=32&amp;height=32&amp;auto=webp&amp;s=881b998ff73380d3f02d27e7536aba842df055c1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=48&amp;height=48&amp;auto=webp&amp;s=325a8549233c6457eaf4eaef948230af4d062f0a", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=64&amp;height=64&amp;auto=webp&amp;s=9a5261140af96699d24ded7497d3b10c831464ba", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png?width=128&amp;height=128&amp;auto=webp&amp;s=6069896f540b6928b86a55082ae4d55f823ce094", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/7atjjqpy1mc41_Updoot.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "166 TB unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c0o2r", "is_robot_indexable": true, "report_reasons": null, "author": "acbadam42", "discussion_type": null, "num_comments": 194, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15c0o2r/can_the_mods_somehow_put_a_stop_to_all_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c0o2r/can_the_mods_somehow_put_a_stop_to_all_of_the/", "subreddit_subscribers": 695162, "created_utc": 1690558262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys.\nI'm wondering if any of you feel the same way. \nI have about 30 tb of data stored at home. \nMovies / Series and adult stuff.\n\nRecently the feeling of just deleting everything and stop hoarding gets bigger and bigger. \nI feel like it could be kind of a relief\n\nDou you ever feel the same? \nI'm almost sure I really would feel relieved for some time. The question to me is if the regrets will be bigger than the relief after some time xD", "author_fullname": "t2_dkkf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel an urge to delete everything!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15btoo2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 62, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690540274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.\nI&amp;#39;m wondering if any of you feel the same way. \nI have about 30 tb of data stored at home. \nMovies / Series and adult stuff.&lt;/p&gt;\n\n&lt;p&gt;Recently the feeling of just deleting everything and stop hoarding gets bigger and bigger. \nI feel like it could be kind of a relief&lt;/p&gt;\n\n&lt;p&gt;Dou you ever feel the same? \nI&amp;#39;m almost sure I really would feel relieved for some time. The question to me is if the regrets will be bigger than the relief after some time xD&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15btoo2", "is_robot_indexable": true, "report_reasons": null, "author": "nydal89", "discussion_type": null, "num_comments": 121, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15btoo2/i_feel_an_urge_to_delete_everything/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15btoo2/i_feel_an_urge_to_delete_everything/", "subreddit_subscribers": 695162, "created_utc": 1690540274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_85burqfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Definitely didn't just get this for $100", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 128, "top_awarded_type": null, "hide_score": false, "media_metadata": {"goqhlg4gbreb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 99, "x": 108, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=636fc02ab9cc339357a2d7249d61314bbff61295"}, {"y": 198, "x": 216, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2da811d006f197b5d94c57e18fc3f51a64e019b9"}, {"y": 293, "x": 320, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e116d1d470825f637167f98844bcc4b6aad80fc"}, {"y": 586, "x": 640, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=efc6729646079eff7c5d1afad4c1c78d8234f9f4"}, {"y": 880, "x": 960, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce620c3d89109fb162987640ab6fb8e4953e691c"}, {"y": 990, "x": 1080, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=065af77d510bbb1e4b9463b646791c2c7c158322"}], "s": {"y": 990, "x": 1080, "u": "https://preview.redd.it/goqhlg4gbreb1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e4c31885320a906dd2200f9a075405ec6968b0cb"}, "id": "goqhlg4gbreb1"}, "y8jvlx7gbreb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=727fc86de8f92325faac91df94e434f97a4c2e17"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c140023f05dc07cfd5ec3d667b2f75fbec5ef142"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9178b07eb98ce4866517d31c6769cb0c0a5e9068"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9095c3d81bfd738416c35478438764181e3f5e0"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=00b069d02e6ce20bb34855d773609a5bdb62ac62"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bd520ece6f3cf190cea7cea4e0cd6ae72b0fe54"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/y8jvlx7gbreb1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;s=f6c297453fc6dec655635bb15f9e32844e9de08e"}, "id": "y8jvlx7gbreb1"}, "nudhxrbgbreb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 192, "x": 108, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=118f91c824408449d94f52820b57244eb7853b3c"}, {"y": 384, "x": 216, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8962ab5004e2b8b1dbd328937f9f6cc7359a0bed"}, {"y": 568, "x": 320, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=079a18738789cb3124a52bbb8392c29f10f80552"}, {"y": 1137, "x": 640, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f7c6b8398ce9448be99533700359d3a0965fcb8"}, {"y": 1706, "x": 960, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=622151219b060d101d4891cce5b2537e6979fd82"}, {"y": 1920, "x": 1080, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=724b4485f1a5122de0308c9ceccaeaabf22f337f"}], "s": {"y": 4032, "x": 2268, "u": "https://preview.redd.it/nudhxrbgbreb1.jpg?width=2268&amp;format=pjpg&amp;auto=webp&amp;s=aeee1b55f9a4c196c10239d502bd0bbf578aaecb"}, "id": "nudhxrbgbreb1"}}, "name": "t3_15c6nsj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 64, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "goqhlg4gbreb1", "id": 307961823}, {"media_id": "y8jvlx7gbreb1", "id": 307961824}, {"media_id": "nudhxrbgbreb1", "id": 307961825}]}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dqNPqRHpHcxVJgFFIj3Z-cMzINH8zeZQpA8zgiKRhik.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690572312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/15c6nsj", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c6nsj", "is_robot_indexable": true, "report_reasons": null, "author": "kasualtiess", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c6nsj/definitely_didnt_just_get_this_for_100/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/15c6nsj", "subreddit_subscribers": 695162, "created_utc": 1690572312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9j2ab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Broader availability\" of 30TB Seagate HAMR by end of 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15c1jgl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iQOuao6nT0pE1A8I5I1KYAT_zW93g-YQHOLKWXWCnNU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690560256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "anandtech.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.anandtech.com/show/18984/seagate-ships-first-commercial-hamr-hard-drives", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HvqbxATHfBLwflMCeU_0hj2sz8I0opCa2h5WNYBFNu8.jpg?auto=webp&amp;s=0444a417a0381fa450883a009b8da53e5aa6290f", "width": 678, "height": 381}, "resolutions": [{"url": "https://external-preview.redd.it/HvqbxATHfBLwflMCeU_0hj2sz8I0opCa2h5WNYBFNu8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3274493bf6e0060c4092b703fba2fcb3832a52d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HvqbxATHfBLwflMCeU_0hj2sz8I0opCa2h5WNYBFNu8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=acb77e4323841382345132eb12baf7a7d0a27522", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HvqbxATHfBLwflMCeU_0hj2sz8I0opCa2h5WNYBFNu8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=305f524dc3d0312e04a74b1e4d4d1ed81c585546", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/HvqbxATHfBLwflMCeU_0hj2sz8I0opCa2h5WNYBFNu8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a05a234ac2ec5901d316764a72b19f100a48eace", "width": 640, "height": 359}], "variants": {}, "id": "ZDos2GydBuP_NqCaebSRqFIuf4eMDwl9EX2W499Tcew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c1jgl", "is_robot_indexable": true, "report_reasons": null, "author": "michaelmalak", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c1jgl/broader_availability_of_30tb_seagate_hamr_by_end/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.anandtech.com/show/18984/seagate-ships-first-commercial-hamr-hard-drives", "subreddit_subscribers": 695162, "created_utc": 1690560256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Supreme Court ruled last year that scraping is legal. \n\n[https://techcrunch.com/2022/04/18/web-scraping-legal-court/](https://techcrunch.com/2022/04/18/web-scraping-legal-court/)\n\nbut there is case law from 2013 where ninth circuit made it illegal\n\n&amp;#x200B;\n\nAny feedback into this will be appreciated?", "author_fullname": "t2_5tcdvj2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is scraping craigslist legal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bpi3f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690526018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Supreme Court ruled last year that scraping is legal. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://techcrunch.com/2022/04/18/web-scraping-legal-court/\"&gt;https://techcrunch.com/2022/04/18/web-scraping-legal-court/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;but there is case law from 2013 where ninth circuit made it illegal&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any feedback into this will be appreciated?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?auto=webp&amp;s=62a2c9c1a57a9f74a79f38a2d20a9a5b6d5a4958", "width": 2878, "height": 1768}, "resolutions": [{"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e5d4cea5288c1c3321f23d5f497c80ebf5d2bdaf", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ad0bb048df9a22655e055f3c8f473fd95b2469b", "width": 216, "height": 132}, {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0db3aeace52223cd0356d3f688cf8915f5385435", "width": 320, "height": 196}, {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e35f249771242001cb0d2ec652394b997fb45435", "width": 640, "height": 393}, {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eb207ff5f5068e7490763a1ca5af34b704799350", "width": 960, "height": 589}, {"url": "https://external-preview.redd.it/bAKE_zfPhLB3V2vh2VMSgt_4dtdlS_LYFZ9vCn_i6HI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50340260a6ca8d6510edf5e3b4896f9db6e4a335", "width": 1080, "height": 663}], "variants": {}, "id": "pBXY9Df7pR_wUj5yFf_oVjLWX-xTR6H67Y-J-Z4qpYE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15bpi3f", "is_robot_indexable": true, "report_reasons": null, "author": "self_help_", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15bpi3f/is_scraping_craigslist_legal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15bpi3f/is_scraping_craigslist_legal/", "subreddit_subscribers": 695162, "created_utc": 1690526018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've used a number of tools for file renaming and tagging over the years (KRename, Exfalso, MP3tag, bash scripts) and am wondering if there is anything new out that has more automation capability?\n\nFor example, it would be nice if I could just type into an LLM/ChatGPT-like tool \"find the metadata for X release on discogs and apply it to these files using the naming scheme 'artist - album - track number - track.flac'\"\n\nI have a personal naming convention for music files that involves creating vorbis tags for catalogue number, record label, release date etc. Even when semi-automated using existing tools, creating these tag fields and copy/pasting the relevant info from discogs can take a lot of time when sorting hundreds of albums.", "author_fullname": "t2_jlfav83u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modern tools for advanced file renaming and metadata tagging?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c1vrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690561347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690561047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used a number of tools for file renaming and tagging over the years (KRename, Exfalso, MP3tag, bash scripts) and am wondering if there is anything new out that has more automation capability?&lt;/p&gt;\n\n&lt;p&gt;For example, it would be nice if I could just type into an LLM/ChatGPT-like tool &amp;quot;find the metadata for X release on discogs and apply it to these files using the naming scheme &amp;#39;artist - album - track number - track.flac&amp;#39;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I have a personal naming convention for music files that involves creating vorbis tags for catalogue number, record label, release date etc. Even when semi-automated using existing tools, creating these tag fields and copy/pasting the relevant info from discogs can take a lot of time when sorting hundreds of albums.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c1vrj", "is_robot_indexable": true, "report_reasons": null, "author": "SpecificPanic503", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c1vrj/modern_tools_for_advanced_file_renaming_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c1vrj/modern_tools_for_advanced_file_renaming_and/", "subreddit_subscribers": 695162, "created_utc": 1690561047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am a regular user with limited knowledge on how they work, but I will list some of mine. Correct me if I am wrong please.\n\nHDD:\n- Leave 20% space free\n- Defrag if its fragmented above 10%\n- Don't defragment and chkdsk too often and without good reason\n- Keep heat below 50 degrees Celcius\n- Avoid dropping or shaking\n- Put computer to sleep or hibernate overnight instead of shut down unless you won't be using the computer for a while.\n\nSSD:\n- Keep 20% free space\n- Don't defrag or chkdsk, though Windows 10 is said to be adapted for SSDs, I avoid doing it to be safe.\n- Filling up with data and then deleting it often will shorten SSD service life because its cells have limited ability for rewrites, though I heard this is not a big concern like 10 years ago.\n- Keep heat to safe levels, not sure which temperature is safe for SSDs.", "author_fullname": "t2_pr4df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your tips to maximise service life of HDDs and SSDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15btipc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690539759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a regular user with limited knowledge on how they work, but I will list some of mine. Correct me if I am wrong please.&lt;/p&gt;\n\n&lt;p&gt;HDD:\n- Leave 20% space free\n- Defrag if its fragmented above 10%\n- Don&amp;#39;t defragment and chkdsk too often and without good reason\n- Keep heat below 50 degrees Celcius\n- Avoid dropping or shaking\n- Put computer to sleep or hibernate overnight instead of shut down unless you won&amp;#39;t be using the computer for a while.&lt;/p&gt;\n\n&lt;p&gt;SSD:\n- Keep 20% free space\n- Don&amp;#39;t defrag or chkdsk, though Windows 10 is said to be adapted for SSDs, I avoid doing it to be safe.\n- Filling up with data and then deleting it often will shorten SSD service life because its cells have limited ability for rewrites, though I heard this is not a big concern like 10 years ago.\n- Keep heat to safe levels, not sure which temperature is safe for SSDs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15btipc", "is_robot_indexable": true, "report_reasons": null, "author": "Dron22", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15btipc/what_are_your_tips_to_maximise_service_life_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15btipc/what_are_your_tips_to_maximise_service_life_of/", "subreddit_subscribers": 695162, "created_utc": 1690539759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "or not? i try to preserve them whenever i can. how about you?", "author_fullname": "t2_14uv2r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "do you consider original file/directory creation time an important part of your data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c8z90", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690577870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;or not? i try to preserve them whenever i can. how about you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c8z90", "is_robot_indexable": true, "report_reasons": null, "author": "paprok", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c8z90/do_you_consider_original_filedirectory_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c8z90/do_you_consider_original_filedirectory_creation/", "subreddit_subscribers": 695162, "created_utc": 1690577870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\n  I am looking to see if anyone can recommend a professional conversion service or is able to do it themselves?  \n\n\nI am looking to find someone who has the capability to put the tapes through TBC and deinterlace the files.  \n\n\nI would like a professional use TBC not just a DVD recorder passthrough.  \n\n\nPlease let me know if you have any suggestions.  ", "author_fullname": "t2_hilrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Professional VHS conversion [request]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c28fp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690561874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am looking to see if anyone can recommend a professional conversion service or is able to do it themselves?  &lt;/p&gt;\n\n&lt;p&gt;I am looking to find someone who has the capability to put the tapes through TBC and deinterlace the files.  &lt;/p&gt;\n\n&lt;p&gt;I would like a professional use TBC not just a DVD recorder passthrough.  &lt;/p&gt;\n\n&lt;p&gt;Please let me know if you have any suggestions.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c28fp", "is_robot_indexable": true, "report_reasons": null, "author": "Matthew_C1314", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c28fp/professional_vhs_conversion_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c28fp/professional_vhs_conversion_request/", "subreddit_subscribers": 695162, "created_utc": 1690561874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Don\u2019t know where to post. This is my best bet you hoarding guys. \n\n\nI have a work desktop with a specific app which has a database in the appdata under C:\n\nOn my home PC (connected via site to site VPN) I cannot point to this specific folder in the same app. \n\nI was thinking about syncing these two folder but cannot find anything. \n\nI can\u2019t even backup this folder on my synology, gdrive etc\u2026\n\nSurely someone has come across this.\n\n\nThanks!", "author_fullname": "t2_gybstjov", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync files between two PCs (hidden folder)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bxztl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690552124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Don\u2019t know where to post. This is my best bet you hoarding guys. &lt;/p&gt;\n\n&lt;p&gt;I have a work desktop with a specific app which has a database in the appdata under C:&lt;/p&gt;\n\n&lt;p&gt;On my home PC (connected via site to site VPN) I cannot point to this specific folder in the same app. &lt;/p&gt;\n\n&lt;p&gt;I was thinking about syncing these two folder but cannot find anything. &lt;/p&gt;\n\n&lt;p&gt;I can\u2019t even backup this folder on my synology, gdrive etc\u2026&lt;/p&gt;\n\n&lt;p&gt;Surely someone has come across this.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15bxztl", "is_robot_indexable": true, "report_reasons": null, "author": "Silver_Dentist_9563", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15bxztl/sync_files_between_two_pcs_hidden_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15bxztl/sync_files_between_two_pcs_hidden_folder/", "subreddit_subscribers": 695162, "created_utc": 1690552124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to capture video off a Hi8 tape from a Sony Handycam playback but keep encountering an issue with the video being [scrambled](https://imgur.com/zpbjT5Z). Sometimes the remnants of an image will come through but otherwise it's just a mess. I've used the same output/recording options from a few guides online but still can't seem to get a usable result. Any help would be greatly appreciated as I'm new to this.\n\n&amp;nbsp;\n\n* Camera: Sony DCR-TRV130 NTSC\n* S Video out to RCA adapter\n* Capture Card: Elgato Video capture\n\n&amp;nbsp;\n\nIn total I'm running the camera to the Elgato's RCA input and using that in OBS as a video capture source.\n\n[here are the recording settings I'm using](https://imgur.com/dr1AHFQ)", "author_fullname": "t2_5n08z67o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sony Hi8 capture scrambled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15cdsyr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690589958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to capture video off a Hi8 tape from a Sony Handycam playback but keep encountering an issue with the video being &lt;a href=\"https://imgur.com/zpbjT5Z\"&gt;scrambled&lt;/a&gt;. Sometimes the remnants of an image will come through but otherwise it&amp;#39;s just a mess. I&amp;#39;ve used the same output/recording options from a few guides online but still can&amp;#39;t seem to get a usable result. Any help would be greatly appreciated as I&amp;#39;m new to this.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Camera: Sony DCR-TRV130 NTSC&lt;/li&gt;\n&lt;li&gt;S Video out to RCA adapter&lt;/li&gt;\n&lt;li&gt;Capture Card: Elgato Video capture&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;In total I&amp;#39;m running the camera to the Elgato&amp;#39;s RCA input and using that in OBS as a video capture source.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/dr1AHFQ\"&gt;here are the recording settings I&amp;#39;m using&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ghALI1B7ci0GYapqybDyLGsHA68Sj1zzUSAT9C_q0w4.jpg?auto=webp&amp;s=be2c367a63bd20fc0b9efa979579838a59ebd605", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/ghALI1B7ci0GYapqybDyLGsHA68Sj1zzUSAT9C_q0w4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d258db391b7064b8c302f42f640e96464a2b22a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ghALI1B7ci0GYapqybDyLGsHA68Sj1zzUSAT9C_q0w4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e6c562e63b266bbfe71ec2d3553b255c0c220af", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ghALI1B7ci0GYapqybDyLGsHA68Sj1zzUSAT9C_q0w4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0d8cab25bb6bb402e3259af49e394ddfd42a8ca", "width": 320, "height": 168}], "variants": {}, "id": "MTr0ZRwNtF3TyDhe0IM93ujjRh-pLWMrOIHT2n6kJow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15cdsyr", "is_robot_indexable": true, "report_reasons": null, "author": "bakedvoltage", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15cdsyr/sony_hi8_capture_scrambled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15cdsyr/sony_hi8_capture_scrambled/", "subreddit_subscribers": 695162, "created_utc": 1690589958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This morning one of my 8TB drives threw a SMART Warning so it's time. I've been using a RAID-1 in my main desktop for 15 years, and a bunch of high-capacity thumb drives switched out every year or so for a subset of really important stuff e.g. what everyone needs to have with them immediately if their house goes up in flames. That particular stuff is also in the cloud, none of the rest is.\n\nI haven't been keeping up with storage technology much at all. This will get me started:\n\n- What are good 16+TB / 7200 RPM *quiet* drives? No clicking, low seek noise. I will keep those in RAID because I keep large project files on them. I just don't know what price range I'm really looking at for **new**, not shucked drives. Are WD Blacks still good or are the bigger ones noisy? Any reason to go for Gold? (I read the Ultrastars are noisy.)\n\n- I don't have a proper backup drive. Should I just mirror the RAID onto a single, third drive every week and call it a day?\n\n- Is there a way to have 3 internal drives (2 for RAID, 1 for backup) living in a NAS: I'd love to tell my setup \"Keep the backup drive *off* except when I schedule a backup, keep the RAID *always on* in a low-powered state even when my desktop goes to sleep\"? I know zero about NAS.\n\n- And, regarding the thumb drive backups. Is there a more climate-proof/wear-proof option than using thumb drives on a keychain inside a waterproof container for the stuff I need to keep with me all the time?\n\nThank you.", "author_fullname": "t2_88pam", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to upgrade my setup with a proper backup drive &amp; replace thumb drives. Do I want a NAS? Opinions/suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c269b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690561745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This morning one of my 8TB drives threw a SMART Warning so it&amp;#39;s time. I&amp;#39;ve been using a RAID-1 in my main desktop for 15 years, and a bunch of high-capacity thumb drives switched out every year or so for a subset of really important stuff e.g. what everyone needs to have with them immediately if their house goes up in flames. That particular stuff is also in the cloud, none of the rest is.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been keeping up with storage technology much at all. This will get me started:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What are good 16+TB / 7200 RPM &lt;em&gt;quiet&lt;/em&gt; drives? No clicking, low seek noise. I will keep those in RAID because I keep large project files on them. I just don&amp;#39;t know what price range I&amp;#39;m really looking at for &lt;strong&gt;new&lt;/strong&gt;, not shucked drives. Are WD Blacks still good or are the bigger ones noisy? Any reason to go for Gold? (I read the Ultrastars are noisy.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I don&amp;#39;t have a proper backup drive. Should I just mirror the RAID onto a single, third drive every week and call it a day?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is there a way to have 3 internal drives (2 for RAID, 1 for backup) living in a NAS: I&amp;#39;d love to tell my setup &amp;quot;Keep the backup drive &lt;em&gt;off&lt;/em&gt; except when I schedule a backup, keep the RAID &lt;em&gt;always on&lt;/em&gt; in a low-powered state even when my desktop goes to sleep&amp;quot;? I know zero about NAS.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;And, regarding the thumb drive backups. Is there a more climate-proof/wear-proof option than using thumb drives on a keychain inside a waterproof container for the stuff I need to keep with me all the time?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c269b", "is_robot_indexable": true, "report_reasons": null, "author": "Geethebluesky", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c269b/looking_to_upgrade_my_setup_with_a_proper_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c269b/looking_to_upgrade_my_setup_with_a_proper_backup/", "subreddit_subscribers": 695162, "created_utc": 1690561745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\n\nI have a 33TB XFS filesystem that was created just before reflink=1 became default and I potentially want to explore using duperemove on it. Is it possible to enable reflink without reformatting / moving data elsewhere and then back onto a new FS? (Difficult due to the size and lack of available temporary space)\n\nThe underlying structure is mdraid -&gt; lvm -&gt; xfs\n\nxfs\\_info output\n\n    meta-data=/dev/mapper/data       isize=512    agcount=33, agsize=268435328 blks\n             =                       sectsz=4096  attr=2, projid32bit=1\n             =                       crc=1        finobt=1, sparse=1, rmapbt=0\n             =                       reflink=0    bigtime=0 inobtcount=0 nrext64=0\n    data     =                       bsize=4096   blocks=8789096448, imaxpct=5\n             =                       sunit=128    swidth=256 blks\n    naming   =version 2              bsize=4096   ascii-ci=0, ftype=1\n    log      =internal log           bsize=4096   blocks=521728, version=2\n             =                       sectsz=4096  sunit=1 blks, lazy-count=1\n    realtime =none                   extsz=4096   blocks=0, rtextents=0\n    \n\n&amp;#x200B;", "author_fullname": "t2_j4ct9d1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "XFS Reflink after the fact?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c0b69", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690557444.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I have a 33TB XFS filesystem that was created just before reflink=1 became default and I potentially want to explore using duperemove on it. Is it possible to enable reflink without reformatting / moving data elsewhere and then back onto a new FS? (Difficult due to the size and lack of available temporary space)&lt;/p&gt;\n\n&lt;p&gt;The underlying structure is mdraid -&amp;gt; lvm -&amp;gt; xfs&lt;/p&gt;\n\n&lt;p&gt;xfs_info output&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;meta-data=/dev/mapper/data       isize=512    agcount=33, agsize=268435328 blks\n         =                       sectsz=4096  attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=0\n         =                       reflink=0    bigtime=0 inobtcount=0 nrext64=0\ndata     =                       bsize=4096   blocks=8789096448, imaxpct=5\n         =                       sunit=128    swidth=256 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=521728, version=2\n         =                       sectsz=4096  sunit=1 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c0b69", "is_robot_indexable": true, "report_reasons": null, "author": "tsumaru720", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c0b69/xfs_reflink_after_the_fact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c0b69/xfs_reflink_after_the_fact/", "subreddit_subscribers": 695162, "created_utc": 1690557444.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Migrating my Plex/Seeding setup from a dedicated server to a home build and probably made a mistake somewhere so any advice is appreciated.\n\nI plan on using Proxmox with a TrueNAS VM for ZFS storage pools and additional VMs for everything else. 4k Plex transcoding using Intel Quick Sync Video is a requirement which is why I'm using a 13900k instead of a server CPU. \n\nHere is my current parts list:\n\n*Case* - Fractal Design Define 7 XL\n\n*CPU* - Intel Core i9-13900K\n\n*Heatsink* - Noctua NH-D15\n\n*Motherboard* - SUPERMICRO MBD-X13SAE-F-O\n\n*RAM* - Kingston 32GB 288-Pin PC RAM DDR5 4800 X4\n\n*SSD Storage* - SAMSUNG 990 PRO w/ Heatsink 2TB X2\n\n*Data Storage* - WD Red Pro WD201KFGX 20TB X22\n\n*SAS Controller* - LSI LSI00244 (9201-16i) \n\n*Power Supply* - CORSAIR HX850 \n\nThe NVMe drives will be configured in RAID 1 and be used for the Proxmox install and VMs.\n\nThe storage drives will be split into two RAID-Z1 pools. I'm fine adding additional drives for RAID-Z2 if running that big a pool with only one disk of redundancy is risky. \n\nStill pretty new to all this type of setup, so again, any advice is appreciated.", "author_fullname": "t2_xdrvc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Server Build Check", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15brmwy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690533457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Migrating my Plex/Seeding setup from a dedicated server to a home build and probably made a mistake somewhere so any advice is appreciated.&lt;/p&gt;\n\n&lt;p&gt;I plan on using Proxmox with a TrueNAS VM for ZFS storage pools and additional VMs for everything else. 4k Plex transcoding using Intel Quick Sync Video is a requirement which is why I&amp;#39;m using a 13900k instead of a server CPU. &lt;/p&gt;\n\n&lt;p&gt;Here is my current parts list:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Case&lt;/em&gt; - Fractal Design Define 7 XL&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;CPU&lt;/em&gt; - Intel Core i9-13900K&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Heatsink&lt;/em&gt; - Noctua NH-D15&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Motherboard&lt;/em&gt; - SUPERMICRO MBD-X13SAE-F-O&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RAM&lt;/em&gt; - Kingston 32GB 288-Pin PC RAM DDR5 4800 X4&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;SSD Storage&lt;/em&gt; - SAMSUNG 990 PRO w/ Heatsink 2TB X2&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Data Storage&lt;/em&gt; - WD Red Pro WD201KFGX 20TB X22&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;SAS Controller&lt;/em&gt; - LSI LSI00244 (9201-16i) &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Power Supply&lt;/em&gt; - CORSAIR HX850 &lt;/p&gt;\n\n&lt;p&gt;The NVMe drives will be configured in RAID 1 and be used for the Proxmox install and VMs.&lt;/p&gt;\n\n&lt;p&gt;The storage drives will be split into two RAID-Z1 pools. I&amp;#39;m fine adding additional drives for RAID-Z2 if running that big a pool with only one disk of redundancy is risky. &lt;/p&gt;\n\n&lt;p&gt;Still pretty new to all this type of setup, so again, any advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15brmwy", "is_robot_indexable": true, "report_reasons": null, "author": "stuff12321", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15brmwy/server_build_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15brmwy/server_build_check/", "subreddit_subscribers": 695162, "created_utc": 1690533457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, I have a &lt;source&gt; hard disk and a &lt;dump&gt; hard disk. The &lt;source&gt; hdd gets the most **reorganization in file hierarchy** and **gets fed recent data** and ***more frequently*****.** The &lt;dump&gt; is just a **mirror image** that gets updated less frequently.   \n\n\nNow, I am tired of doing things manually (comparing, deleting, copying, moving). Each time I have to spend hours figuring out what to delete copy and move while **updating the &lt;dump&gt;**. Is there a way around it? I mean I plug both &lt;source&gt; &amp; &lt;dump&gt; to a system.....they talk to each other.....in few hours I get \"updated\" &lt;dump&gt; with **new data and same hierarchy** as &lt;source&gt;.   \n(PS: Extra data on &lt;dump&gt; that is no longer in &lt;source&gt; needs to be deleted too)  \n\n\nIs there a smart way to do it?", "author_fullname": "t2_ap1x99qg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirroring 2 Hard disks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bmvz6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690517356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, I have a &amp;lt;source&amp;gt; hard disk and a &amp;lt;dump&amp;gt; hard disk. The &amp;lt;source&amp;gt; hdd gets the most &lt;strong&gt;reorganization in file hierarchy&lt;/strong&gt; and &lt;strong&gt;gets fed recent data&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;more frequently&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt; The &amp;lt;dump&amp;gt; is just a &lt;strong&gt;mirror image&lt;/strong&gt; that gets updated less frequently.   &lt;/p&gt;\n\n&lt;p&gt;Now, I am tired of doing things manually (comparing, deleting, copying, moving). Each time I have to spend hours figuring out what to delete copy and move while &lt;strong&gt;updating the &amp;lt;dump&amp;gt;&lt;/strong&gt;. Is there a way around it? I mean I plug both &amp;lt;source&amp;gt; &amp;amp; &amp;lt;dump&amp;gt; to a system.....they talk to each other.....in few hours I get &amp;quot;updated&amp;quot; &amp;lt;dump&amp;gt; with &lt;strong&gt;new data and same hierarchy&lt;/strong&gt; as &amp;lt;source&amp;gt;.&lt;br/&gt;\n(PS: Extra data on &amp;lt;dump&amp;gt; that is no longer in &amp;lt;source&amp;gt; needs to be deleted too)  &lt;/p&gt;\n\n&lt;p&gt;Is there a smart way to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15bmvz6", "is_robot_indexable": true, "report_reasons": null, "author": "An0nym0u547", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15bmvz6/mirroring_2_hard_disks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15bmvz6/mirroring_2_hard_disks/", "subreddit_subscribers": 695162, "created_utc": 1690517356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I've working on archiving quite a few older mechanical keyboard forums and wikis to try and preserve the vast information of the hobby they hold. Many that have already shut down or nearing shutdown. However, there is a particularly large forum that is using Simple Machine Forums (SMF). I have tried a few options but none were successful at archiving and building the site structure with all of the nested pages. \n\nHTTrack providing the most success with other sites, I would like to use this for the SMF site as well but it seems to only mirror the first page of each forum section. \n\nAny tips on how to successfully archive an SMF site? I am also not an admin of this site, so I don't have an option of making a live snapshot/backup or access the database.", "author_fullname": "t2_5dbjvvur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issues archiving active SMF forum with HTTrack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15cdqgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690589760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve working on archiving quite a few older mechanical keyboard forums and wikis to try and preserve the vast information of the hobby they hold. Many that have already shut down or nearing shutdown. However, there is a particularly large forum that is using Simple Machine Forums (SMF). I have tried a few options but none were successful at archiving and building the site structure with all of the nested pages. &lt;/p&gt;\n\n&lt;p&gt;HTTrack providing the most success with other sites, I would like to use this for the SMF site as well but it seems to only mirror the first page of each forum section. &lt;/p&gt;\n\n&lt;p&gt;Any tips on how to successfully archive an SMF site? I am also not an admin of this site, so I don&amp;#39;t have an option of making a live snapshot/backup or access the database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15cdqgm", "is_robot_indexable": true, "report_reasons": null, "author": "a_saker", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15cdqgm/issues_archiving_active_smf_forum_with_httrack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15cdqgm/issues_archiving_active_smf_forum_with_httrack/", "subreddit_subscribers": 695162, "created_utc": 1690589760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A little more context on what I want to do here. I want to take a software vendors documentation site, crawl it, convert it to a pdf document with all the documentation contained, and then feed it to a large language model to be able to summarize it, ask questions of it, etc. I figure the users in this sub would know how to do this best.", "author_fullname": "t2_fxhk9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crawl a website and turn text into PDF documents?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c9xnb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690580155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A little more context on what I want to do here. I want to take a software vendors documentation site, crawl it, convert it to a pdf document with all the documentation contained, and then feed it to a large language model to be able to summarize it, ask questions of it, etc. I figure the users in this sub would know how to do this best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c9xnb", "is_robot_indexable": true, "report_reasons": null, "author": "septic_sergeant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c9xnb/crawl_a_website_and_turn_text_into_pdf_documents/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c9xnb/crawl_a_website_and_turn_text_into_pdf_documents/", "subreddit_subscribers": 695162, "created_utc": 1690580155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m wondering if it still works, some of the scripts don\u2019t work anymore like the adding of service accounts to a google group. However I\u2019m trying to copy over data I have from one drive to another and I keep getting it can\u2019t connect to local host:5597. If there\u2019s a better alternative for copying TBs I\u2019d be happy to take a look at it, but I\u2019d like to be able to use the server side copy function still.", "author_fullname": "t2_ia5s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does AutoRClone still work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c82i6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690575655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m wondering if it still works, some of the scripts don\u2019t work anymore like the adding of service accounts to a google group. However I\u2019m trying to copy over data I have from one drive to another and I keep getting it can\u2019t connect to local host:5597. If there\u2019s a better alternative for copying TBs I\u2019d be happy to take a look at it, but I\u2019d like to be able to use the server side copy function still.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c82i6", "is_robot_indexable": true, "report_reasons": null, "author": "cewong2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c82i6/does_autorclone_still_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c82i6/does_autorclone_still_work/", "subreddit_subscribers": 695162, "created_utc": 1690575655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI just got a  Qnap TS-EC1680U from ebay (16 drive nas) to use as backup for my other NAS. My idea is to use single drives to backup my other NAS, and, that way, in order for data to be lost i would need both the RAID on the first nas to break, and, my backup drives to fail.  \n\n\nHowever, i noticed you can't add drives to a JBOD array, and, so, i will end up having a bunch of single drive pools (as i add more disks) down the line. The procedure for backup becomes complicated now as i have to specify source/destination and now need to balance between each pool.  \n\n\nIn an ideal situation i would love it if i could combine all drives, ie. say 5 X 10TB + 1 5TB = 55TB, to backup my old NAS, and, should 1 drive fail, i could just replace it, and let the backup procedure rebuild the missing drive next time it runs, but, this doesn't seem to be the way JBOD works, any other ideas? I was trying not to use RAID because i do have other drives of different sizes, so, combining them all would be ideal and  think JBOD is the only way.  \n\n\nThanks,\n\n  \n", "author_fullname": "t2_g5ds4a4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old QNAP for backups - best use of space?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c5kfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690569766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I just got a  Qnap TS-EC1680U from ebay (16 drive nas) to use as backup for my other NAS. My idea is to use single drives to backup my other NAS, and, that way, in order for data to be lost i would need both the RAID on the first nas to break, and, my backup drives to fail.  &lt;/p&gt;\n\n&lt;p&gt;However, i noticed you can&amp;#39;t add drives to a JBOD array, and, so, i will end up having a bunch of single drive pools (as i add more disks) down the line. The procedure for backup becomes complicated now as i have to specify source/destination and now need to balance between each pool.  &lt;/p&gt;\n\n&lt;p&gt;In an ideal situation i would love it if i could combine all drives, ie. say 5 X 10TB + 1 5TB = 55TB, to backup my old NAS, and, should 1 drive fail, i could just replace it, and let the backup procedure rebuild the missing drive next time it runs, but, this doesn&amp;#39;t seem to be the way JBOD works, any other ideas? I was trying not to use RAID because i do have other drives of different sizes, so, combining them all would be ideal and  think JBOD is the only way.  &lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c5kfk", "is_robot_indexable": true, "report_reasons": null, "author": "petrosk182k", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c5kfk/old_qnap_for_backups_best_use_of_space/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c5kfk/old_qnap_for_backups_best_use_of_space/", "subreddit_subscribers": 695162, "created_utc": 1690569766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,\n\n&amp;#x200B;\n\nunfortunately my FLAC folder looks like this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/e76lrqnb9qeb1.png?width=452&amp;format=png&amp;auto=webp&amp;s=06dd0867f0ee111b0afa4c4c7af1cd5b51a41644\n\nI am looking for a way that all folders of a certain artist are moved to a separate folder (which at best is created automatically). Is mp3tag somehow capable of that?\n\n&amp;#x200B;\n\nThanks in advance &amp; please don't mind my taste in music", "author_fullname": "t2_5kz79jsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create artist folder automatically", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"e76lrqnb9qeb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/e76lrqnb9qeb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3725234532570fc549cd831f9716d09e8127cc2a"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/e76lrqnb9qeb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b63d052eb717490f7cecf3c594fb79c061b6e198"}, {"y": 179, "x": 320, "u": "https://preview.redd.it/e76lrqnb9qeb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bac14ea8a2bdaae209d0cff494962c1dceb2ea63"}], "s": {"y": 254, "x": 452, "u": "https://preview.redd.it/e76lrqnb9qeb1.png?width=452&amp;format=png&amp;auto=webp&amp;s=06dd0867f0ee111b0afa4c4c7af1cd5b51a41644"}, "id": "e76lrqnb9qeb1"}}, "name": "t3_15c19hw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/DVTakjY71OlhMzcgswvlG1UOBRBMaegTLyeG0Jv7OD8.jpg", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690559632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;unfortunately my FLAC folder looks like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e76lrqnb9qeb1.png?width=452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06dd0867f0ee111b0afa4c4c7af1cd5b51a41644\"&gt;https://preview.redd.it/e76lrqnb9qeb1.png?width=452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06dd0867f0ee111b0afa4c4c7af1cd5b51a41644&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am looking for a way that all folders of a certain artist are moved to a separate folder (which at best is created automatically). Is mp3tag somehow capable of that?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &amp;amp; please don&amp;#39;t mind my taste in music&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c19hw", "is_robot_indexable": true, "report_reasons": null, "author": "tom_marten_mark", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15c19hw/create_artist_folder_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c19hw/create_artist_folder_automatically/", "subreddit_subscribers": 695162, "created_utc": 1690559632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I tried using httrack, which works for normal websites, but this [one](https://anatomylearning.com/webgl2023v3/browser.php) isn't orthodox. As soon as I try downloading I receive an error saying the downloaded folders are all empty\n\nThe rules of the sub says very technical question only, I apologise if it isn't, I tried looking for answers online without success.\n\nThanks", "author_fullname": "t2_8ftxakvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download unity anatomy website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15butlj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690543764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using httrack, which works for normal websites, but this &lt;a href=\"https://anatomylearning.com/webgl2023v3/browser.php\"&gt;one&lt;/a&gt; isn&amp;#39;t orthodox. As soon as I try downloading I receive an error saying the downloaded folders are all empty&lt;/p&gt;\n\n&lt;p&gt;The rules of the sub says very technical question only, I apologise if it isn&amp;#39;t, I tried looking for answers online without success.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15butlj", "is_robot_indexable": true, "report_reasons": null, "author": "No-Explorer-5637", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15butlj/how_to_download_unity_anatomy_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15butlj/how_to_download_unity_anatomy_website/", "subreddit_subscribers": 695162, "created_utc": 1690543764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. I'm on Windows 11 and I have hundreds of sub folders with a lot of data in them which have the SCENE naming scheme. The current folder naming scheme is no longer fit for purpose, and it's the reason for the want in renaming of the folders.\n\nThen there's another issue too; the character length of a large bulk of the files. Would renaming sub folders to a new naming scheme with more characters affect the data in those sub folders?\n\nThanks!", "author_fullname": "t2_geh0alfkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to mass rename existing sub folders? And not files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15btv29", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690540862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I&amp;#39;m on Windows 11 and I have hundreds of sub folders with a lot of data in them which have the SCENE naming scheme. The current folder naming scheme is no longer fit for purpose, and it&amp;#39;s the reason for the want in renaming of the folders.&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s another issue too; the character length of a large bulk of the files. Would renaming sub folders to a new naming scheme with more characters affect the data in those sub folders?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15btv29", "is_robot_indexable": true, "report_reasons": null, "author": "Infamous_Side6576", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15btv29/is_it_possible_to_mass_rename_existing_sub/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15btv29/is_it_possible_to_mass_rename_existing_sub/", "subreddit_subscribers": 695162, "created_utc": 1690540862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,  I've got a 4TB WD my passport drive (encrypted with Vera crypt) which was nearly full. I'm trying to copy the contents to another 5 TB drive. The job is half done at the moment but the encrypted drive had slowed down to a crawl , running at 10kbps. I've re-mounted it and it's still slow and unresponsive.  I am using teracopy for the copying, not doing all at once but trying to be selective/prioritizing the stuff.  How should i go about finishing the job? Is there better softwares out there to copy the drive. I do have another 5TB drive(bought two in total) that can be used if needed. There have been a lot of issues with the 4TB drive in the past ,like it can be unresponsive at times, delayed mounting time in Veracrypt etc.  I would really appreciate some help here,  really scared that i might lose the rest of the contents.  :(", "author_fullname": "t2_fgx6twl7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with copying from an encrypted and possibly dying drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15bow8b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690523988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,  I&amp;#39;ve got a 4TB WD my passport drive (encrypted with Vera crypt) which was nearly full. I&amp;#39;m trying to copy the contents to another 5 TB drive. The job is half done at the moment but the encrypted drive had slowed down to a crawl , running at 10kbps. I&amp;#39;ve re-mounted it and it&amp;#39;s still slow and unresponsive.  I am using teracopy for the copying, not doing all at once but trying to be selective/prioritizing the stuff.  How should i go about finishing the job? Is there better softwares out there to copy the drive. I do have another 5TB drive(bought two in total) that can be used if needed. There have been a lot of issues with the 4TB drive in the past ,like it can be unresponsive at times, delayed mounting time in Veracrypt etc.  I would really appreciate some help here,  really scared that i might lose the rest of the contents.  :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15bow8b", "is_robot_indexable": true, "report_reasons": null, "author": "Maleficent_Potato649", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15bow8b/need_help_with_copying_from_an_encrypted_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15bow8b/need_help_with_copying_from_an_encrypted_and/", "subreddit_subscribers": 695162, "created_utc": 1690523988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As of today, I have a WD EX2 ULTRA NAS and it has one 1TB drive and I want to upgrade it to two drives in RAID 1, each drive is 4TB. And currently there is data on the 1TB drive that I need to transfer to the new drives, so I thought of doing it with a SATA to USB adapter and re-creating the shares on the new drives. Is it possible or should it be done another way?\n\nThanks in advance", "author_fullname": "t2_6o4g2brl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15by8xq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690552749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As of today, I have a WD EX2 ULTRA NAS and it has one 1TB drive and I want to upgrade it to two drives in RAID 1, each drive is 4TB. And currently there is data on the 1TB drive that I need to transfer to the new drives, so I thought of doing it with a SATA to USB adapter and re-creating the shares on the new drives. Is it possible or should it be done another way?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15by8xq", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary_Pin339", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15by8xq/is_it_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15by8xq/is_it_possible/", "subreddit_subscribers": 695162, "created_utc": 1690552749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi friends.  Recently I bought 2 18TB easystores  but when trying to turn the PC on all I get is a click from the PSU (Fuse maybe?).  If I try to turn the computer on after it has been sitting for a while without the HDD's connected it powers on just fine but as soon as I try and turn it on with the HDD's connected  I can no longer turn it on even with them disconnected without waiting for a while.  I have the first 3 pins taped over on the drives.  I have also tried a HDD power connector that has the third pin removed but still the same issues.\n\nSpecs:\n\ni3 13100\n\nAsus Prime Z790M-D4\n\nSeasonic Prime 750 platinum\n\n18TB EDGZ\n\n&amp;#x200B;\n\nAny help would be mucho appreciado ", "author_fullname": "t2_u2a5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PC not turning on after connecting shucked drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15c55jd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690568783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends.  Recently I bought 2 18TB easystores  but when trying to turn the PC on all I get is a click from the PSU (Fuse maybe?).  If I try to turn the computer on after it has been sitting for a while without the HDD&amp;#39;s connected it powers on just fine but as soon as I try and turn it on with the HDD&amp;#39;s connected  I can no longer turn it on even with them disconnected without waiting for a while.  I have the first 3 pins taped over on the drives.  I have also tried a HDD power connector that has the third pin removed but still the same issues.&lt;/p&gt;\n\n&lt;p&gt;Specs:&lt;/p&gt;\n\n&lt;p&gt;i3 13100&lt;/p&gt;\n\n&lt;p&gt;Asus Prime Z790M-D4&lt;/p&gt;\n\n&lt;p&gt;Seasonic Prime 750 platinum&lt;/p&gt;\n\n&lt;p&gt;18TB EDGZ&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be mucho appreciado &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15c55jd", "is_robot_indexable": true, "report_reasons": null, "author": "Yakuya_Ahirya", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15c55jd/pc_not_turning_on_after_connecting_shucked_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15c55jd/pc_not_turning_on_after_connecting_shucked_drives/", "subreddit_subscribers": 695162, "created_utc": 1690568783.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}