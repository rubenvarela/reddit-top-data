{"kind": "Listing", "data": {"after": "t3_159nqet", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been in my first DS job for almost 2 years now. It's really a Analyst job: dashboarding, basic queries, etc.\n\nI have learned a good bit here: Python with Pandas and some basic linear regression, more \"complex\" SQL queries like multiple joins, CTEs, subqueries, and how to translate data into business needs.\n\nThe people here are all so nice and supportive, but there's nothing to do. Week after week it's the same requests, with an ad-hoc request maybe once every quarter. I have asked my team lead for more work or requests, but he says their is nothing at this time. I am pretty sure that the higher-ups know their is nothing to do, and part of me wonders if I'll be on the chopping block soon.\n\nI have simply worked on kaggle projects until my eyes have bled, but for the past year I have just put a bottle of water on my spacebar to appear that I am online, then go walk my dog, go to the gym, play video games. Nobody even notices that I am gone, nor would they care as long as I get my work done.\n\nOn one hand, I am happy to have so much free time. On the other hand, I feel way far behind the average DS/DA when it comes to technical and interpersonal skills, so I'm wondering if I am shooting myself in the foot by having so much free time. I wouldn't be able to answer anything outside foundational skills in sql, python, or statistics.\n\nThoughts?", "author_fullname": "t2_g7hqomd62", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science job is BORING but I have so much free time. May I have some advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1599njr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 175, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 175, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690294280.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been in my first DS job for almost 2 years now. It&amp;#39;s really a Analyst job: dashboarding, basic queries, etc.&lt;/p&gt;\n\n&lt;p&gt;I have learned a good bit here: Python with Pandas and some basic linear regression, more &amp;quot;complex&amp;quot; SQL queries like multiple joins, CTEs, subqueries, and how to translate data into business needs.&lt;/p&gt;\n\n&lt;p&gt;The people here are all so nice and supportive, but there&amp;#39;s nothing to do. Week after week it&amp;#39;s the same requests, with an ad-hoc request maybe once every quarter. I have asked my team lead for more work or requests, but he says their is nothing at this time. I am pretty sure that the higher-ups know their is nothing to do, and part of me wonders if I&amp;#39;ll be on the chopping block soon.&lt;/p&gt;\n\n&lt;p&gt;I have simply worked on kaggle projects until my eyes have bled, but for the past year I have just put a bottle of water on my spacebar to appear that I am online, then go walk my dog, go to the gym, play video games. Nobody even notices that I am gone, nor would they care as long as I get my work done.&lt;/p&gt;\n\n&lt;p&gt;On one hand, I am happy to have so much free time. On the other hand, I feel way far behind the average DS/DA when it comes to technical and interpersonal skills, so I&amp;#39;m wondering if I am shooting myself in the foot by having so much free time. I wouldn&amp;#39;t be able to answer anything outside foundational skills in sql, python, or statistics.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1599njr", "is_robot_indexable": true, "report_reasons": null, "author": "Paladin_player69", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1599njr/data_science_job_is_boring_but_i_have_so_much/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1599njr/data_science_job_is_boring_but_i_have_so_much/", "subreddit_subscribers": 961903, "created_utc": 1690294280.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Why do recruiters make promises to update applicants within two weeks for the next round interview, ask for coding challenge completion time, and encourage questions via email, only to completely ghost applicants afterwards, leaving them in frustrating limbo for over a month without any response, even after sending follow-up emails?", "author_fullname": "t2_3lz0pnks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you\u2019re a recruiter, take a time to inform, after an interview and a coding challenge, if they have been rejected.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158zc3g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 125, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 125, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690264652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why do recruiters make promises to update applicants within two weeks for the next round interview, ask for coding challenge completion time, and encourage questions via email, only to completely ghost applicants afterwards, leaving them in frustrating limbo for over a month without any response, even after sending follow-up emails?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158zc3g", "is_robot_indexable": true, "report_reasons": null, "author": "m30aru", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158zc3g/if_youre_a_recruiter_take_a_time_to_inform_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158zc3g/if_youre_a_recruiter_take_a_time_to_inform_after/", "subreddit_subscribers": 961903, "created_utc": 1690264652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "For anyone interested, here are some of the trends that we have observed in the world of data when it comes to career as a data analyst/data scientist. (We are a German based data &amp; software recruitment agency).\n\nThe role of a data analyst was in some ways more well defined previously. Where there are fewer possibilities, there are fewer expectations. Now, by the proliferation of tools and expansion of expectation, it blends increasingly with that of a data scientist. \n\nIt\u2019s now essential to have expertise in one or more programming languages, as well as database querying methodologies, both of which we\u2019ll explore in more detail.\u00a0\n\nProficiency in R and Python, as well as knowing how to utilise DBT (data building tool). More on this [here](https://www.pl-talents.com/expertise-to-look-out-for-when-hiring-a-data-analyst/). \n\nData analysts work with databases, so they absolutely need familiarity with writing SQL queries. PostgreSQL, Oracle SQL, Microsoft SQL Server, etc.\n\nAdditionally, data analysts can benefit from expertise with SQL-related technologies and frameworks:  SQLAlchemy,  Apache Spark SQL, HiveQL.\n\nData Visualisation and Storytelling tools. \n\nWorking with databases is one thing, but data analysts need more than SQL. Data analysts now handle more complex and diverse datasets, including structured, semi-structured, and unstructured data. \n\n\\+  Proficiency in statistical analysis is crucial for data analysts.  \n\n\\+ Domain Expertise and business acumen. \n\n[Full thing](https://www.pl-talents.com/expertise-to-look-out-for-when-hiring-a-data-analyst/).\n\nAsk away if you have any follow up questions  [\ud83d\udc4b](https://emojipedia.org/waving-hand)", "author_fullname": "t2_akx5qwlrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This is what companies look for when hiring data analysts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1594i5v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 79, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 79, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690280770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For anyone interested, here are some of the trends that we have observed in the world of data when it comes to career as a data analyst/data scientist. (We are a German based data &amp;amp; software recruitment agency).&lt;/p&gt;\n\n&lt;p&gt;The role of a data analyst was in some ways more well defined previously. Where there are fewer possibilities, there are fewer expectations. Now, by the proliferation of tools and expansion of expectation, it blends increasingly with that of a data scientist. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s now essential to have expertise in one or more programming languages, as well as database querying methodologies, both of which we\u2019ll explore in more detail.\u00a0&lt;/p&gt;\n\n&lt;p&gt;Proficiency in R and Python, as well as knowing how to utilise DBT (data building tool). More on this &lt;a href=\"https://www.pl-talents.com/expertise-to-look-out-for-when-hiring-a-data-analyst/\"&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Data analysts work with databases, so they absolutely need familiarity with writing SQL queries. PostgreSQL, Oracle SQL, Microsoft SQL Server, etc.&lt;/p&gt;\n\n&lt;p&gt;Additionally, data analysts can benefit from expertise with SQL-related technologies and frameworks:  SQLAlchemy,  Apache Spark SQL, HiveQL.&lt;/p&gt;\n\n&lt;p&gt;Data Visualisation and Storytelling tools. &lt;/p&gt;\n\n&lt;p&gt;Working with databases is one thing, but data analysts need more than SQL. Data analysts now handle more complex and diverse datasets, including structured, semi-structured, and unstructured data. &lt;/p&gt;\n\n&lt;p&gt;+  Proficiency in statistical analysis is crucial for data analysts.  &lt;/p&gt;\n\n&lt;p&gt;+ Domain Expertise and business acumen. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pl-talents.com/expertise-to-look-out-for-when-hiring-a-data-analyst/\"&gt;Full thing&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Ask away if you have any follow up questions  &lt;a href=\"https://emojipedia.org/waving-hand\"&gt;\ud83d\udc4b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?auto=webp&amp;s=17ef1355473eb3bff99fdc2e50d13b5e1b35682d", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=07d4416a830239f30519a721142305f3c2f9ae4c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4d935fc2970c38865dd542321927be73c375dee", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6fbf0451ff7aab3b2d8ac075b4130e02c86e8d0", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6fa61b952cf911a3cc14539e48ee2cba96b687b7", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5dcafebcc1000c73ba7cf7ac64ef2a64e75de699", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/71DaLmfnF47EGNwkzCxkRPpPN5HSVA05N4KvREXka4w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e737fe5f50aeadb25c217332f788858a1c56555", "width": 1080, "height": 565}], "variants": {}, "id": "P1a44f1_D9lPBT5uk7jANAdB6Mc2Zv-XNbs1v-SV8Ww"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1594i5v", "is_robot_indexable": true, "report_reasons": null, "author": "PLTalent", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1594i5v/this_is_what_companies_look_for_when_hiring_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1594i5v/this_is_what_companies_look_for_when_hiring_data/", "subreddit_subscribers": 961903, "created_utc": 1690280770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\nWas affected by a recent wave of layoffs in my prior company, and Even though I've applied to over 40 different jobs within that company so I can stay with them, I've only gotten two interviews and their reception towards me has been pretty lukewarm. It's clear they are looking for people who are veterans with over 10 years of experience, I only have 5 years of analytics experience as a data analyst. I've applied for about 500 jobs so far, networked with tons of recruiters, searched on LinkedIn for hiring and analyst keywords, very warm and receptive to recruiters. So far I have no job offers or prospects or anything. In about 1 month, I will be completely out of severance and forced to leave my house. \n\n\nSo per the title, looking for any clever suggestions that might help me avoid being completely homeless due to being laid off.", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unable to find my next job. What can I do to avoid being homeless?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159lw8c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690321072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was affected by a recent wave of layoffs in my prior company, and Even though I&amp;#39;ve applied to over 40 different jobs within that company so I can stay with them, I&amp;#39;ve only gotten two interviews and their reception towards me has been pretty lukewarm. It&amp;#39;s clear they are looking for people who are veterans with over 10 years of experience, I only have 5 years of analytics experience as a data analyst. I&amp;#39;ve applied for about 500 jobs so far, networked with tons of recruiters, searched on LinkedIn for hiring and analyst keywords, very warm and receptive to recruiters. So far I have no job offers or prospects or anything. In about 1 month, I will be completely out of severance and forced to leave my house. &lt;/p&gt;\n\n&lt;p&gt;So per the title, looking for any clever suggestions that might help me avoid being completely homeless due to being laid off.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159lw8c", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159lw8c/unable_to_find_my_next_job_what_can_i_do_to_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159lw8c/unable_to_find_my_next_job_what_can_i_do_to_avoid/", "subreddit_subscribers": 961903, "created_utc": 1690321072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Getting a bit philosophical on why is called Data Science. I might been asking a dumb question. From my experience as Data Scientist I have felt more as an Engineer rather than a Scientist. In the context of business you are required to build an app that uses ML to be profitable to a company. I guess that the 'Science' in DS comes from extracting knowledge from data?\n\n&amp;#x200B;", "author_fullname": "t2_oxk1j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why the 'Science' in DS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159hi4a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690311546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Getting a bit philosophical on why is called Data Science. I might been asking a dumb question. From my experience as Data Scientist I have felt more as an Engineer rather than a Scientist. In the context of business you are required to build an app that uses ML to be profitable to a company. I guess that the &amp;#39;Science&amp;#39; in DS comes from extracting knowledge from data?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159hi4a", "is_robot_indexable": true, "report_reasons": null, "author": "PinstripePride97", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159hi4a/why_the_science_in_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159hi4a/why_the_science_in_ds/", "subreddit_subscribers": 961903, "created_utc": 1690311546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vnan4h8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do we realize we're living in a dystopia?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_159dcn2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9Vs9NeyuP803oHxlr3pbqaRSFiACtbjzCaBRw-gn4fU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690302503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3545up3315eb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3545up3315eb1.png?auto=webp&amp;s=f42654ebda1c37463c16050ac0e6684c91007b7e", "width": 1329, "height": 687}, "resolutions": [{"url": "https://preview.redd.it/3545up3315eb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=189308fc37b27ce74815eedfb52994dc88eea8ad", "width": 108, "height": 55}, {"url": "https://preview.redd.it/3545up3315eb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c4e4dad763f4a3f61231b77b08c24fe0a189422", "width": 216, "height": 111}, {"url": "https://preview.redd.it/3545up3315eb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a99540475d6cc3f007b95c65387f69ed9d2d14a2", "width": 320, "height": 165}, {"url": "https://preview.redd.it/3545up3315eb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0523dfbfe8cefa2b438634757fff28a9aef230a2", "width": 640, "height": 330}, {"url": "https://preview.redd.it/3545up3315eb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e49b7a0b047a846f252a558c5325e1b2a3bcff95", "width": 960, "height": 496}, {"url": "https://preview.redd.it/3545up3315eb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eafd97de4770353991b959d3984d410df8143b72", "width": 1080, "height": 558}], "variants": {}, "id": "1eMilGuxzn498DImW4mnvCzE_WeKF276Rf2xkRNVcBE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "159dcn2", "is_robot_indexable": true, "report_reasons": null, "author": "Analyst2163", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159dcn2/at_what_point_do_we_realize_were_living_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3545up3315eb1.png", "subreddit_subscribers": 961903, "created_utc": 1690302503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently working on some data analysis tasks using Pandas in Python, and I came across four different lines of code used to filter a DataFrame based on lower and upper limits. While I understand the basic intent of each line, I'm curious about the specific differences and implications they may have on the resulting DataFrame.\n\n&amp;#x200B;\n\nHere are the four lines of code:\n\n&amp;#x200B;\n\n1. \\`df\\_cleaned = df\\[\\~((df &lt; lower\\_limit) | (df &gt; upper\\_limit)).any(axis=1)\\]\\`\n\n2. \\`df\\_cleaned2 = df\\[((df &gt;= lower\\_limit) &amp; (df &lt;= upper\\_limit)).any(axis=1)\\]\\`\n\n3. \\`df\\_cleaned3 = df\\[\\~((df &lt; lower\\_limit) | (df &gt; upper\\_limit))\\]\\`\n\n4. \\`df\\_cleaned4 = df\\[((df &gt;= lower\\_limit) &amp; (df &lt;= upper\\_limit))\\]\\` \n\nI've noticed that the shape of the resulting DataFrame, \\`df\\_cleaned\\`, is different from the others, and I'm trying to understand why.\n\n&amp;#x200B;\n\nCould someone please explain the specific differences between these four lines of code? How does the use of \\`any(axis=1)\\` affect the filtering process, and why does it lead to different shapes in the resulting DataFrames?\n\n&amp;#x200B;\n\nAny insights or examples would be greatly appreciated. Thank you in advance for your help!\n\nhttps://preview.redd.it/ikqbyl1hv1eb1.png?width=978&amp;format=png&amp;auto=webp&amp;s=d9f8700eabc75d8c2dfd3a5e296e649a6ccda192\n\nhttps://preview.redd.it/ahr3wp1hv1eb1.png?width=556&amp;format=png&amp;auto=webp&amp;s=a69480ecf7a94b3570f034e256cd7f2987eecd3a", "author_fullname": "t2_7kwofuq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused to Understand the Difference of Pandas DataFrame Filtering Operation based on Lower Bound and Upper Bound", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ikqbyl1hv1eb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=128a6bbe796f259b2e1233e73a7656356fda6bde"}, {"y": 118, "x": 216, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ee4e4e9e97f4b28cc92221aa53a97ff8346ddcb"}, {"y": 175, "x": 320, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a2e46a4adec236a74e7a160aedec5c959989e72"}, {"y": 350, "x": 640, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=173e19541ad25c5eda1874f82910c6657e1aa2db"}, {"y": 525, "x": 960, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea689b8ca54b8a622e654b996404801a2d60475b"}], "s": {"y": 535, "x": 978, "u": "https://preview.redd.it/ikqbyl1hv1eb1.png?width=978&amp;format=png&amp;auto=webp&amp;s=d9f8700eabc75d8c2dfd3a5e296e649a6ccda192"}, "id": "ikqbyl1hv1eb1"}, "ahr3wp1hv1eb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/ahr3wp1hv1eb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5cbb3f8f9c6e089053d62f179362b367a0a1415"}, {"y": 72, "x": 216, "u": "https://preview.redd.it/ahr3wp1hv1eb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f129f6af2a0850f9163536544d6293ff8a8e60"}, {"y": 107, "x": 320, "u": "https://preview.redd.it/ahr3wp1hv1eb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3147c7c90bd5bbff1efa814c06d56e87b1ab55a5"}], "s": {"y": 187, "x": 556, "u": "https://preview.redd.it/ahr3wp1hv1eb1.png?width=556&amp;format=png&amp;auto=webp&amp;s=a69480ecf7a94b3570f034e256cd7f2987eecd3a"}, "id": "ahr3wp1hv1eb1"}}, "name": "t3_158z85n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2aS3sLiIE55rvvIeBDBhp32A-pG2JA8AEpSfUhKGpgo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690264278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on some data analysis tasks using Pandas in Python, and I came across four different lines of code used to filter a DataFrame based on lower and upper limits. While I understand the basic intent of each line, I&amp;#39;m curious about the specific differences and implications they may have on the resulting DataFrame.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here are the four lines of code:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;`df_cleaned = df[~((df &amp;lt; lower_limit) | (df &amp;gt; upper_limit)).any(axis=1)]`&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;`df_cleaned2 = df[((df &amp;gt;= lower_limit) &amp;amp; (df &amp;lt;= upper_limit)).any(axis=1)]`&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;`df_cleaned3 = df[~((df &amp;lt; lower_limit) | (df &amp;gt; upper_limit))]`&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;`df_cleaned4 = df[((df &amp;gt;= lower_limit) &amp;amp; (df &amp;lt;= upper_limit))]` &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed that the shape of the resulting DataFrame, `df_cleaned`, is different from the others, and I&amp;#39;m trying to understand why.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Could someone please explain the specific differences between these four lines of code? How does the use of `any(axis=1)` affect the filtering process, and why does it lead to different shapes in the resulting DataFrames?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any insights or examples would be greatly appreciated. Thank you in advance for your help!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ikqbyl1hv1eb1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f8700eabc75d8c2dfd3a5e296e649a6ccda192\"&gt;https://preview.redd.it/ikqbyl1hv1eb1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f8700eabc75d8c2dfd3a5e296e649a6ccda192&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ahr3wp1hv1eb1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a69480ecf7a94b3570f034e256cd7f2987eecd3a\"&gt;https://preview.redd.it/ahr3wp1hv1eb1.png?width=556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a69480ecf7a94b3570f034e256cd7f2987eecd3a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158z85n", "is_robot_indexable": true, "report_reasons": null, "author": "Cheap_Problem_9657", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158z85n/confused_to_understand_the_difference_of_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158z85n/confused_to_understand_the_difference_of_pandas/", "subreddit_subscribers": 961903, "created_utc": 1690264278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There is a fourth option...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_158wy1n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_dibn5", "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AZ3FbsWiGOvj-dzm7Hb5Lui7gxRqq03GM0AF3kG9NPw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "memes", "selftext": "", "author_fullname": "t2_e9fnewhd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does DS stand for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/memes", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_158okz6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": "#edeff1", "subreddit_type": "public", "ups": 39920, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "a1659962-26f2-11ec-8067-26487ac3621d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 39920, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AZ3FbsWiGOvj-dzm7Hb5Lui7gxRqq03GM0AF3kG9NPw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [{"e": "text", "t": "OC Meme Maker"}], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "mod_note": null, "created": 1690236079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "richtext", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zi2uz2vmjzdb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?auto=webp&amp;s=03d24329e50c71fcd8cbc501b8950c6e607c96cb", "width": 1200, "height": 770}, "resolutions": [{"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a53c3031ee4f42a09a94f93d144001dd220ed410", "width": 108, "height": 69}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=683043bf1ea5b83058fbdd8481be2da08eeec395", "width": 216, "height": 138}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60c460d16c67198b8c50ce73193b3db3bb17e3f7", "width": 320, "height": 205}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fa912656f9ea5960ae88050b4c3f137f6c6f65b", "width": 640, "height": 410}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=05a3aadb00c042faac70f0185409da73a206d114", "width": 960, "height": 616}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cc4d4771b70582457a483025144186c89149f3b", "width": 1080, "height": 693}], "variants": {}, "id": "C0bzTE2oePeFOTDh_sZzhrFD-8OzPqfclEp_URQ8pl4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OC Meme Maker", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qjpg", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158okz6", "is_robot_indexable": true, "report_reasons": null, "author": "Meseion", "discussion_type": null, "num_comments": 3843, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/memes/comments/158okz6/what_does_ds_stand_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zi2uz2vmjzdb1.jpg", "subreddit_subscribers": 26667289, "created_utc": 1690236079.0, "num_crossposts": 7, "media": null, "is_video": false}], "created": 1690257367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zi2uz2vmjzdb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?auto=webp&amp;s=03d24329e50c71fcd8cbc501b8950c6e607c96cb", "width": 1200, "height": 770}, "resolutions": [{"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a53c3031ee4f42a09a94f93d144001dd220ed410", "width": 108, "height": 69}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=683043bf1ea5b83058fbdd8481be2da08eeec395", "width": 216, "height": 138}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60c460d16c67198b8c50ce73193b3db3bb17e3f7", "width": 320, "height": 205}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fa912656f9ea5960ae88050b4c3f137f6c6f65b", "width": 640, "height": 410}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=05a3aadb00c042faac70f0185409da73a206d114", "width": 960, "height": 616}, {"url": "https://preview.redd.it/zi2uz2vmjzdb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cc4d4771b70582457a483025144186c89149f3b", "width": 1080, "height": 693}], "variants": {}, "id": "C0bzTE2oePeFOTDh_sZzhrFD-8OzPqfclEp_URQ8pl4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158wy1n", "is_robot_indexable": true, "report_reasons": null, "author": "Imperial_Squid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_158okz6", "author_flair_text_color": null, "permalink": "/r/datascience/comments/158wy1n/there_is_a_fourth_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zi2uz2vmjzdb1.jpg", "subreddit_subscribers": 961903, "created_utc": 1690257367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a US bachelor's in Mathematics and I'm trying to get into data science. I know I can take some programs to learn sql % etc. But is there any jobs you guys had that helped you learn sql in the real world or jobs that weren't data science related but looked really good on your resume. I'm looking a a good beginner job that'll help me Jumpstart my career other than online programs and the endless applying.\n\nBTW if you are in the US living in any major cities (LA, NY, Chicago), Apply to COOP (co-op) they are a free program partnered with Google to train you with certain data/finance/marketing jobs you've been trying to apply to.", "author_fullname": "t2_6lm2nw90", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sql Jobs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159h83d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690310950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a US bachelor&amp;#39;s in Mathematics and I&amp;#39;m trying to get into data science. I know I can take some programs to learn sql % etc. But is there any jobs you guys had that helped you learn sql in the real world or jobs that weren&amp;#39;t data science related but looked really good on your resume. I&amp;#39;m looking a a good beginner job that&amp;#39;ll help me Jumpstart my career other than online programs and the endless applying.&lt;/p&gt;\n\n&lt;p&gt;BTW if you are in the US living in any major cities (LA, NY, Chicago), Apply to COOP (co-op) they are a free program partnered with Google to train you with certain data/finance/marketing jobs you&amp;#39;ve been trying to apply to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159h83d", "is_robot_indexable": true, "report_reasons": null, "author": "Angel-YinYang", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159h83d/sql_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159h83d/sql_jobs/", "subreddit_subscribers": 961903, "created_utc": 1690310950.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nLet's say I am working with a classifier that is already in use in a production system. The number of classes our model is trying to predict is around 1,000 classes. Some of these classes may overlap (and often do).\n\nDue to the overlapping of the classifications, our team decides we would like to start reducing the number of classes in an attempt to raise prediction accuracy.\n\nAre there some tried and true methods to go about this? I am thinking of doing some nearest neighbor analysis and using some nearest neighbor algorithm could tell us which classes are likely to overlap but it might give us a heuristic on *how much* they overlap.\n\nTo phrase the question another way: **Given 1000 classes, how can we identify the classes with the most similar training data?** (hypothesis being that these classes are potentially redundant or unnecessary.)\n\nThanks :D", "author_fullname": "t2_uf5mx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Methods for identifying redundant classes in a classifier with ~1000 classes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159dmuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690303133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I am working with a classifier that is already in use in a production system. The number of classes our model is trying to predict is around 1,000 classes. Some of these classes may overlap (and often do).&lt;/p&gt;\n\n&lt;p&gt;Due to the overlapping of the classifications, our team decides we would like to start reducing the number of classes in an attempt to raise prediction accuracy.&lt;/p&gt;\n\n&lt;p&gt;Are there some tried and true methods to go about this? I am thinking of doing some nearest neighbor analysis and using some nearest neighbor algorithm could tell us which classes are likely to overlap but it might give us a heuristic on &lt;em&gt;how much&lt;/em&gt; they overlap.&lt;/p&gt;\n\n&lt;p&gt;To phrase the question another way: &lt;strong&gt;Given 1000 classes, how can we identify the classes with the most similar training data?&lt;/strong&gt; (hypothesis being that these classes are potentially redundant or unnecessary.)&lt;/p&gt;\n\n&lt;p&gt;Thanks :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159dmuh", "is_robot_indexable": true, "report_reasons": null, "author": "HippieInDisguise2_0", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159dmuh/methods_for_identifying_redundant_classes_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159dmuh/methods_for_identifying_redundant_classes_in_a/", "subreddit_subscribers": 961903, "created_utc": 1690303133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Greetings, everyone!\n\nI could really use some technical guidance. I'm well-versed in handling static datasets with pandas and other tools, but I'm eager to level up my skills and work with real-time data. \n\nCan anyone shed some light on the essential steps to get started with real-time data profiling? I'm particularly curious about the tools, techniques, and best practices to make sense of data as it streams in continuously. \n\nAny recommended resources, tutorials, or personal experiences you can share would be immensely appreciated! Thank you \ud83d\ude4f", "author_fullname": "t2_cuf4pcx7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help! How to Kickstart Real-Time Data Profiling? \ud83d\ude80\ud83d\udcca", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1598unr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690292395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, everyone!&lt;/p&gt;\n\n&lt;p&gt;I could really use some technical guidance. I&amp;#39;m well-versed in handling static datasets with pandas and other tools, but I&amp;#39;m eager to level up my skills and work with real-time data. &lt;/p&gt;\n\n&lt;p&gt;Can anyone shed some light on the essential steps to get started with real-time data profiling? I&amp;#39;m particularly curious about the tools, techniques, and best practices to make sense of data as it streams in continuously. &lt;/p&gt;\n\n&lt;p&gt;Any recommended resources, tutorials, or personal experiences you can share would be immensely appreciated! Thank you \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1598unr", "is_robot_indexable": true, "report_reasons": null, "author": "CryptographerDry7458", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1598unr/help_how_to_kickstart_realtime_data_profiling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1598unr/help_how_to_kickstart_realtime_data_profiling/", "subreddit_subscribers": 961903, "created_utc": 1690292395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/158xyht)", "author_fullname": "t2_5f2eg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you pronounce sklearn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158xyht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690260348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/158xyht\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "158xyht", "is_robot_indexable": true, "report_reasons": null, "author": "minimaxir", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1690519549026, "options": [{"text": "scikit-learn", "id": "24046250"}, {"text": "s-k-learn", "id": "24046251"}, {"text": "sclern", "id": "24046252"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 499, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158xyht/how_do_you_pronounce_sklearn/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/158xyht/how_do_you_pronounce_sklearn/", "subreddit_subscribers": 961903, "created_utc": 1690260348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So we are colaborating with the data analytics department at our company and the business came up with the following hypothesis: \"Customers who are engaged through the website have a better experience and hence a better turn-over against the ones that don't engage through the website\" with the intention to prove that \"pure digital\" customers are more efficient at creating revenue against the \"classically engaged\" customers.\n\nI am helping the analytics guys gather data for this but I just don't get how can they actually compare. Now, I majored in marketing and definitely don't have the statistical background they have to perform this analysis, but I've been asked to come up with data of customers that have ONLY engaged through website and data from customers that engage in every other channel BUT website. Can this even be compared? Shouldn't there be an overlap in this populations for the AB Test to work? \n\nI just want a more experienced and statistically versed opinion on this mainly because there is a huge manpower requirement from my team to deliver all these different pieces of data and I want to expedite as much as possible, but the manager of the analytics team is really unreachable.\n\nThanks in advance guys", "author_fullname": "t2_ezz3omiq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Real life struggle) A/B Testing for measuring the effects of different touchpoints on customers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_159sns9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690337665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we are colaborating with the data analytics department at our company and the business came up with the following hypothesis: &amp;quot;Customers who are engaged through the website have a better experience and hence a better turn-over against the ones that don&amp;#39;t engage through the website&amp;quot; with the intention to prove that &amp;quot;pure digital&amp;quot; customers are more efficient at creating revenue against the &amp;quot;classically engaged&amp;quot; customers.&lt;/p&gt;\n\n&lt;p&gt;I am helping the analytics guys gather data for this but I just don&amp;#39;t get how can they actually compare. Now, I majored in marketing and definitely don&amp;#39;t have the statistical background they have to perform this analysis, but I&amp;#39;ve been asked to come up with data of customers that have ONLY engaged through website and data from customers that engage in every other channel BUT website. Can this even be compared? Shouldn&amp;#39;t there be an overlap in this populations for the AB Test to work? &lt;/p&gt;\n\n&lt;p&gt;I just want a more experienced and statistically versed opinion on this mainly because there is a huge manpower requirement from my team to deliver all these different pieces of data and I want to expedite as much as possible, but the manager of the analytics team is really unreachable.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance guys&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159sns9", "is_robot_indexable": true, "report_reasons": null, "author": "CluelessBudget", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159sns9/real_life_struggle_ab_testing_for_measuring_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159sns9/real_life_struggle_ab_testing_for_measuring_the/", "subreddit_subscribers": 961903, "created_utc": 1690337665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_rmueqnwg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Database Of Every AI Tool Ever Made", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_159r998", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/E8ag91TPMl8t6mcbfhobCHVWfdW0jmEZ05vcTcdQcW8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690333817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "gpte.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://gpte.ai", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ErQZMe0fLN5c9-r9tK2RssQOaiaxq5PiAJfSOaTP-kM.jpg?auto=webp&amp;s=05b6f9f8e1b82ce916a17e252a28f0ae6a2bcad5", "width": 833, "height": 565}, "resolutions": [{"url": "https://external-preview.redd.it/ErQZMe0fLN5c9-r9tK2RssQOaiaxq5PiAJfSOaTP-kM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca3f3181427ca127156c75d4cea053c40a96bcd1", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/ErQZMe0fLN5c9-r9tK2RssQOaiaxq5PiAJfSOaTP-kM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=71598e568dc868476d3096451bf240bdb0ec32c4", "width": 216, "height": 146}, {"url": "https://external-preview.redd.it/ErQZMe0fLN5c9-r9tK2RssQOaiaxq5PiAJfSOaTP-kM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24b34ed6e88ed76935de8729827fb4763bde3a81", "width": 320, "height": 217}, {"url": "https://external-preview.redd.it/ErQZMe0fLN5c9-r9tK2RssQOaiaxq5PiAJfSOaTP-kM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7faec0161f3d00fe1f77252b43c93e63a0780d77", "width": 640, "height": 434}], "variants": {}, "id": "I-JC_zUwxE9jB5ZKSS9kp74Nu-jf33W0Safboo0oLzE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "159r998", "is_robot_indexable": true, "report_reasons": null, "author": "Slow_Interest_1273", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159r998/free_database_of_every_ai_tool_ever_made/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://gpte.ai", "subreddit_subscribers": 961903, "created_utc": 1690333817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi /r/datascience! I've been an applied ML engineer for over a decade and have built a number of one-off dataset visualization tools over the years.\n\nRecently I've been working on a more generalized dataset collaboration tool that allows data scientists / ML researchers to quickly share datasets and model predictions with people they're collaborating with, without having to teach them how to get a Python notebook up and running. The main features we've prioritized so far are:\n\n* Easily be able to upload slices of data directly from a Python notebook and then view them via a shareable browser link.\n* Customize the way the data is displayed (e.g. changing text size/color, choosing to display examples in rows vs. a grid, and being able to custom render unique/complex data types).\n* Easily search through and favorite, tag, or add notes to interesting examples in the dataset.\n* Be able to interactively view examples where ground truth data differs from model predictions.\n* Be able to share specific examples or subsets of data with other people you're working with without them having to replicate your Python notebook environment.\n\nWe have a quick demo [video](https://youtu.be/utkSCU2ktck) that walks through our current flow.\n\nWould love to know from the /r/datascience community:\n\n1. What features do you think would be must-haves in a generalized dataset browser?\n2. Would anyone be interested in helping beta test an early version?\n\nThanks!", "author_fullname": "t2_qwbxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What features would you want in a generalized dataset visualization tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159nbp3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690324206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;/r/datascience&lt;/a&gt;! I&amp;#39;ve been an applied ML engineer for over a decade and have built a number of one-off dataset visualization tools over the years.&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve been working on a more generalized dataset collaboration tool that allows data scientists / ML researchers to quickly share datasets and model predictions with people they&amp;#39;re collaborating with, without having to teach them how to get a Python notebook up and running. The main features we&amp;#39;ve prioritized so far are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Easily be able to upload slices of data directly from a Python notebook and then view them via a shareable browser link.&lt;/li&gt;\n&lt;li&gt;Customize the way the data is displayed (e.g. changing text size/color, choosing to display examples in rows vs. a grid, and being able to custom render unique/complex data types).&lt;/li&gt;\n&lt;li&gt;Easily search through and favorite, tag, or add notes to interesting examples in the dataset.&lt;/li&gt;\n&lt;li&gt;Be able to interactively view examples where ground truth data differs from model predictions.&lt;/li&gt;\n&lt;li&gt;Be able to share specific examples or subsets of data with other people you&amp;#39;re working with without them having to replicate your Python notebook environment.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We have a quick demo &lt;a href=\"https://youtu.be/utkSCU2ktck\"&gt;video&lt;/a&gt; that walks through our current flow.&lt;/p&gt;\n\n&lt;p&gt;Would love to know from the &lt;a href=\"/r/datascience\"&gt;/r/datascience&lt;/a&gt; community:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What features do you think would be must-haves in a generalized dataset browser?&lt;/li&gt;\n&lt;li&gt;Would anyone be interested in helping beta test an early version?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/abIW3txhJp_YBYSNB39sY-Ln4CUakwaA3hcZlXnvl8k.jpg?auto=webp&amp;s=74e903ca852482cffe5a0674b885dec59b420889", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/abIW3txhJp_YBYSNB39sY-Ln4CUakwaA3hcZlXnvl8k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5877187c5af5d4a5e9cd03cc54ca24205869a7d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/abIW3txhJp_YBYSNB39sY-Ln4CUakwaA3hcZlXnvl8k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b72d17060aea72336a155cd1d40b60144119636", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/abIW3txhJp_YBYSNB39sY-Ln4CUakwaA3hcZlXnvl8k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=432a6a540283fe87cc9412f94d21addf5d0d81b9", "width": 320, "height": 240}], "variants": {}, "id": "yOPCvEBrFZd3ps6cM0CjKRkKJgcotgzteKE5ly_pBYo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159nbp3", "is_robot_indexable": true, "report_reasons": null, "author": "arkmastermind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159nbp3/what_features_would_you_want_in_a_generalized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159nbp3/what_features_would_you_want_in_a_generalized/", "subreddit_subscribers": 961903, "created_utc": 1690324206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apologies if it sounded exaggerated, I was assuming that was the case. This is because I had a project with LSTM in 2019 and had another one this year (practically no LSTM model in between these 2 years).\n\nFor over 4 years, I have not found a library or method to explain LSTM forecasting/regression tasks. I am looking for something similar to ELI5, LIME, or SHAP (although SHAP provides a tutorial for LSTM classification).\n\nMy question is, from a technical/mathematical perspective, how hard is it to create one? Or even more fundamental question, is it possible to develop one LSTM regression explainer?\n\nI understand how LSTM works (the math behind it) but I have no math background regarding model explainer. Please kindly share your thoughts about this. \n\nThank you!", "author_fullname": "t2_dnekp18a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does explaining a trained LSTM for regression a hard thing to do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159ifp3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690313557.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if it sounded exaggerated, I was assuming that was the case. This is because I had a project with LSTM in 2019 and had another one this year (practically no LSTM model in between these 2 years).&lt;/p&gt;\n\n&lt;p&gt;For over 4 years, I have not found a library or method to explain LSTM forecasting/regression tasks. I am looking for something similar to ELI5, LIME, or SHAP (although SHAP provides a tutorial for LSTM classification).&lt;/p&gt;\n\n&lt;p&gt;My question is, from a technical/mathematical perspective, how hard is it to create one? Or even more fundamental question, is it possible to develop one LSTM regression explainer?&lt;/p&gt;\n\n&lt;p&gt;I understand how LSTM works (the math behind it) but I have no math background regarding model explainer. Please kindly share your thoughts about this. &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159ifp3", "is_robot_indexable": true, "report_reasons": null, "author": "Upstairs-Deer8805", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159ifp3/why_does_explaining_a_trained_lstm_for_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159ifp3/why_does_explaining_a_trained_lstm_for_regression/", "subreddit_subscribers": 961903, "created_utc": 1690313557.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it possible to get an admission in University of Washington without the pre-requisite demonstration of Computer Science skills?\n\nI have done my Bachelors in Electronics and Electrical Engineering from IIT Guwahati but fail to showcase my relevant Computer Science skills. Currently I am working as a Senior Data Analyst in American Express and am looking for doing my Masters in Data Science from University of Washington. Did anyone else in the similar situation crack it? If yes then how?", "author_fullname": "t2_4g655xdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS in DS at UW without Computer Science coursework demonstration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159i34t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690312790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to get an admission in University of Washington without the pre-requisite demonstration of Computer Science skills?&lt;/p&gt;\n\n&lt;p&gt;I have done my Bachelors in Electronics and Electrical Engineering from IIT Guwahati but fail to showcase my relevant Computer Science skills. Currently I am working as a Senior Data Analyst in American Express and am looking for doing my Masters in Data Science from University of Washington. Did anyone else in the similar situation crack it? If yes then how?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159i34t", "is_robot_indexable": true, "report_reasons": null, "author": "Ungrateful_asshole", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159i34t/ms_in_ds_at_uw_without_computer_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159i34t/ms_in_ds_at_uw_without_computer_science/", "subreddit_subscribers": 961903, "created_utc": 1690312790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_jxndm14v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPT-4 function calling can label hospital price data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1597kvs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pZuBPAFFnYAMhU0B8gIXlelJ2jLes00-lJgppGO-SGM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690289281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dolthub.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.dolthub.com/blog/2023-07-24-gpt-4-insurance-labeling/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?auto=webp&amp;s=a443c992782880d52fee8dfe2711519f7f5df6d2", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1d8c83603bfa76e9a45096206fe830967c5b336", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fb1b2a5b15793608da5b103fef3f5293577b3f7", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a4a273c8436925d7a950a6102e6b270a658902d", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=50fe8e1bd7909f7ba7139f966bda8811cbb67f69", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c555769f930aa1eda9d4b79b59b4f7c7f5d81f2b", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/xu9OXuNv5tFVs_i1nc2sWVnVdPCY4c9db0eIWYm6wcQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f2c2dc6c8341e3c8ddf43638e4895283cfa1338", "width": 1080, "height": 564}], "variants": {}, "id": "yDfj-9w3aM9n2aoVQKLDJov5larsVO8nlDOwF3o4kqs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "1597kvs", "is_robot_indexable": true, "report_reasons": null, "author": "alecs-dolt", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1597kvs/gpt4_function_calling_can_label_hospital_price/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dolthub.com/blog/2023-07-24-gpt-4-insurance-labeling/", "subreddit_subscribers": 961903, "created_utc": 1690289281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  \nI'm a data scientist, and part of my job is to prepare a bi-weekly update for execs on the key metrics, the change in the past two weeks, and the key reasoning behind the metrics movement. I felt it annoying that every time I have to manually slice dice with different combinations of dimensions slices to find out the ones that need to pay attention.  \n\nWe do have the dashboard for these metrics and some breakdowns (e.g.: location, age group, device system) but they still don't cover my case because I have to manually check a number of different ways of data breakdown. For instance, there could be only a moderate change in the topline time spent, but some significant change in some combinations. For instance, time spent by young adults in the US on ios increased significantly while the elder group of people in the EU on Android decreased, so overall the metric doesn't change much. But I need to find out all these cuts. \n\n I wonder is there a tool that could help me list out all the possible combinations and their impact so I could have a holistic view at a glance? Thanks!", "author_fullname": "t2_vrym8avu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an effective way of summarizing the root causes of a metric movement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158zz7h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690266707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist, and part of my job is to prepare a bi-weekly update for execs on the key metrics, the change in the past two weeks, and the key reasoning behind the metrics movement. I felt it annoying that every time I have to manually slice dice with different combinations of dimensions slices to find out the ones that need to pay attention.  &lt;/p&gt;\n\n&lt;p&gt;We do have the dashboard for these metrics and some breakdowns (e.g.: location, age group, device system) but they still don&amp;#39;t cover my case because I have to manually check a number of different ways of data breakdown. For instance, there could be only a moderate change in the topline time spent, but some significant change in some combinations. For instance, time spent by young adults in the US on ios increased significantly while the elder group of people in the EU on Android decreased, so overall the metric doesn&amp;#39;t change much. But I need to find out all these cuts. &lt;/p&gt;\n\n&lt;p&gt;I wonder is there a tool that could help me list out all the possible combinations and their impact so I could have a holistic view at a glance? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158zz7h", "is_robot_indexable": true, "report_reasons": null, "author": "Swimming_Adeptness64", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158zz7h/is_there_an_effective_way_of_summarizing_the_root/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158zz7h/is_there_an_effective_way_of_summarizing_the_root/", "subreddit_subscribers": 961903, "created_utc": 1690266707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI know it must be bewildering to see the title. But here is why I am doing this rewrite:\n\n1. Personally, I detest OOP interface for machine learning tasks. Mixins are nonsense. All pipeline functions should be \"Functions\", as we are dealing with mappings between mathematical spaces (from space of dataframe to space of dataframe).\n2. With the advent of dataframe package like Polars, there is absolutely no need to pass tasks to NumPy (for most of the daily tasks). This way, we pass dataframe in, we get dataframe out. Way better UX.\n3. I want to use Polars (almost) entirely for my machine learning pipelines for performance reasons.\n\nI believe I have made enough progress to showcase my project. Roast me all you want. Any feedback will be welcomed.\n\nBenchmarks vs. Scikit-learn: [https://github.com/abstractqqq/dsds/blob/main/examples/dsds\\_comparisons.ipynb](https://github.com/abstractqqq/dsds/blob/main/examples/dsds_comparisons.ipynb)\n\nFunctional and Pickle-able Pipelines:\n\n[https://github.com/abstractqqq/dsds/blob/main/examples/pipeline.ipynb](https://github.com/abstractqqq/dsds/blob/main/examples/pipeline.ipynb)", "author_fullname": "t2_fqnvrovgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rewriting Non-Model Functionality in Scikit-Learn", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158yjd3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690262115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I know it must be bewildering to see the title. But here is why I am doing this rewrite:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Personally, I detest OOP interface for machine learning tasks. Mixins are nonsense. All pipeline functions should be &amp;quot;Functions&amp;quot;, as we are dealing with mappings between mathematical spaces (from space of dataframe to space of dataframe).&lt;/li&gt;\n&lt;li&gt;With the advent of dataframe package like Polars, there is absolutely no need to pass tasks to NumPy (for most of the daily tasks). This way, we pass dataframe in, we get dataframe out. Way better UX.&lt;/li&gt;\n&lt;li&gt;I want to use Polars (almost) entirely for my machine learning pipelines for performance reasons.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I believe I have made enough progress to showcase my project. Roast me all you want. Any feedback will be welcomed.&lt;/p&gt;\n\n&lt;p&gt;Benchmarks vs. Scikit-learn: &lt;a href=\"https://github.com/abstractqqq/dsds/blob/main/examples/dsds_comparisons.ipynb\"&gt;https://github.com/abstractqqq/dsds/blob/main/examples/dsds_comparisons.ipynb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Functional and Pickle-able Pipelines:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/abstractqqq/dsds/blob/main/examples/pipeline.ipynb\"&gt;https://github.com/abstractqqq/dsds/blob/main/examples/pipeline.ipynb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?auto=webp&amp;s=88c6b75773d16d870fef04e67085fb3e2feb9d88", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8349f1908689380c9a9a79b7aeb74e9bf78705db", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=09f40d1e9dbcdc05ce0f581828f9d183fd4c9189", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9b5f9ef63a9f4bad5e9cbac013289f3a5594d17", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c0926cc702cacb5bc1d4f90add9ff799778b5ee8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cf1fab029c8637fa2cebfdd5b6b51b68c5c96636", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SbE_OqJTcIc1NpYXGgnqiNXxrlucj8QaP8r1gqDF6jk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89442f69ac17df4fd5e2738676e3cd9552d9261d", "width": 1080, "height": 540}], "variants": {}, "id": "o-xDnUvkXatG6g-E6ift-whe9CbbpnfjLXV0AcYi0X8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "158yjd3", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Extension7906", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/158yjd3/rewriting_nonmodel_functionality_in_scikitlearn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/158yjd3/rewriting_nonmodel_functionality_in_scikitlearn/", "subreddit_subscribers": 961903, "created_utc": 1690262115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm here , cause I don't like x..\n\nIt's so so ugly to instead of the Twitter. \n\nWell .\n\nDo you like to use mongodb or mysql for your database ?", "author_fullname": "t2_g8rx64xfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Don't need x", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_159t4v4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690338997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m here , cause I don&amp;#39;t like x..&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s so so ugly to instead of the Twitter. &lt;/p&gt;\n\n&lt;p&gt;Well .&lt;/p&gt;\n\n&lt;p&gt;Do you like to use mongodb or mysql for your database ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159t4v4", "is_robot_indexable": true, "report_reasons": null, "author": "geomusecs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159t4v4/dont_need_x/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159t4v4/dont_need_x/", "subreddit_subscribers": 961903, "created_utc": 1690338997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys, so I wanted to get your opinion on a particular problem.\n\nLet\u2019s say I have a dataset from a manufacturing environment which has very low volume (50-150 products per year). Each data entry in this set is a product failure, and the independent variables are the products serial number, failure mode, root cause/final fix of that particular failure as well as information on any part which has been replaced.\n\nThe product is very complex, it has several different systems and parts which interact with each other, so the number of possible failure modes, root causes and parts replaced far exceed the number of products manufactured each year. The same failure mode can have multiple root causes and the same root cause can have multiple failure modes.\n\nIntuitively I question how much insight we can have from this dataset. Obviously we are always looking for more volume than the number of variables. Since multiple failures can occur on the same serial number, our data volume is one order of magnitude higher, but still seems to me pretty low. The categorical variables each have multiple hundreds if not thousands of categories.\n\nAnd all that is excluding issues with data quality\u2026\n\nGiven my objectives of reducing failure incidences (improving product quality), my questions are:\n- Is there a way to quantify how much insight we can obtain (like translate data entropy into something management will understand)?\n- Is this even the right way to look at this data or should we have a different dataset altogether?\n\nI can provide more details as required.", "author_fullname": "t2_h06fa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low data volume", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159qavy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690331289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, so I wanted to get your opinion on a particular problem.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s say I have a dataset from a manufacturing environment which has very low volume (50-150 products per year). Each data entry in this set is a product failure, and the independent variables are the products serial number, failure mode, root cause/final fix of that particular failure as well as information on any part which has been replaced.&lt;/p&gt;\n\n&lt;p&gt;The product is very complex, it has several different systems and parts which interact with each other, so the number of possible failure modes, root causes and parts replaced far exceed the number of products manufactured each year. The same failure mode can have multiple root causes and the same root cause can have multiple failure modes.&lt;/p&gt;\n\n&lt;p&gt;Intuitively I question how much insight we can have from this dataset. Obviously we are always looking for more volume than the number of variables. Since multiple failures can occur on the same serial number, our data volume is one order of magnitude higher, but still seems to me pretty low. The categorical variables each have multiple hundreds if not thousands of categories.&lt;/p&gt;\n\n&lt;p&gt;And all that is excluding issues with data quality\u2026&lt;/p&gt;\n\n&lt;p&gt;Given my objectives of reducing failure incidences (improving product quality), my questions are:\n- Is there a way to quantify how much insight we can obtain (like translate data entropy into something management will understand)?\n- Is this even the right way to look at this data or should we have a different dataset altogether?&lt;/p&gt;\n\n&lt;p&gt;I can provide more details as required.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159qavy", "is_robot_indexable": true, "report_reasons": null, "author": "Panthums", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159qavy/low_data_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159qavy/low_data_volume/", "subreddit_subscribers": 961903, "created_utc": 1690331289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi I recently graduated from UMD, I\u2019m starting to apply for jobs. What has help you land your first DS job? I\u2019m feeling stressed since I don\u2019t have previous experience related to DS? Only a few basic projects I made.", "author_fullname": "t2_8rz6q73u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recent college grad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159oh7b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690326832.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I recently graduated from UMD, I\u2019m starting to apply for jobs. What has help you land your first DS job? I\u2019m feeling stressed since I don\u2019t have previous experience related to DS? Only a few basic projects I made.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159oh7b", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated_Wish_762", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159oh7b/recent_college_grad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159oh7b/recent_college_grad/", "subreddit_subscribers": 961903, "created_utc": 1690326832.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Title basically.\nI was just curious how common it is to write an algorithm from scratch (or built upon common known algorithms)? And those who do it maybe can say what is the process of designing,testing and implementing it and why did they need to write it instead of using an existing one?\n\nMuch appreciated", "author_fullname": "t2_3hqmko1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many of you actually write new algorithms from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159nqn8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690325143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title basically.\nI was just curious how common it is to write an algorithm from scratch (or built upon common known algorithms)? And those who do it maybe can say what is the process of designing,testing and implementing it and why did they need to write it instead of using an existing one?&lt;/p&gt;\n\n&lt;p&gt;Much appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159nqn8", "is_robot_indexable": true, "report_reasons": null, "author": "nuriel8833", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159nqn8/how_many_of_you_actually_write_new_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159nqn8/how_many_of_you_actually_write_new_algorithms/", "subreddit_subscribers": 961903, "created_utc": 1690325143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Anyone here ever have a data scientist position where they used Cuda C? Was that your main programming language at work and what was the work like? I\u2019m currently a data scientist who solely works in Python and is learning Cuda C on the side for fun.", "author_fullname": "t2_8w64m39a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science With Cuda C Knowledge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159nqet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690325127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here ever have a data scientist position where they used Cuda C? Was that your main programming language at work and what was the work like? I\u2019m currently a data scientist who solely works in Python and is learning Cuda C on the side for fun.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "159nqet", "is_robot_indexable": true, "report_reasons": null, "author": "Secure-Report-207", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/159nqet/data_science_with_cuda_c_knowledge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/159nqet/data_science_with_cuda_c_knowledge/", "subreddit_subscribers": 961903, "created_utc": 1690325127.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}