{"kind": "Listing", "data": {"after": "t3_1592b20", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with a client that has about 5500 excel files stored on a shared drive, and I need to merge them into a single csv file. \n\nThe files have common format, so I wrote a simple python script to loop through the drive, load each file into a dataframe, standardize column headers, and then union to an output dataframe.\n\nSome initial testing shows that it takes an average of 40 seconds to process each file, which means it would take about 60 hours to do everything.\n\nIs there a faster way to do this?", "author_fullname": "t2_99d5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best strategy to merge 5500 excel files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158sqwa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 108, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690246162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with a client that has about 5500 excel files stored on a shared drive, and I need to merge them into a single csv file. &lt;/p&gt;\n\n&lt;p&gt;The files have common format, so I wrote a simple python script to loop through the drive, load each file into a dataframe, standardize column headers, and then union to an output dataframe.&lt;/p&gt;\n\n&lt;p&gt;Some initial testing shows that it takes an average of 40 seconds to process each file, which means it would take about 60 hours to do everything.&lt;/p&gt;\n\n&lt;p&gt;Is there a faster way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "158sqwa", "is_robot_indexable": true, "report_reasons": null, "author": "Ein_Bear", "discussion_type": null, "num_comments": 161, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/158sqwa/whats_the_best_strategy_to_merge_5500_excel_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/158sqwa/whats_the_best_strategy_to_merge_5500_excel_files/", "subreddit_subscribers": 117940, "created_utc": 1690246162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow engineers,\n\nI\u2019ve been recently creating documentation for a new process that we\u2019ve built and was wondering how other engineers approach writing documentation? what tools do you use? how'd you keep documentation up to date? Etc\n\nI think the main challenge we are running into is the keeping docs up to date part. \n\nWe\u2019re using confluence to write docs and we\u2019re also using Comala Document Management to help provide a review process to docs as well as auto expiring pages after a year so we can re-review but it still requires people to be checking for expired docs etc.", "author_fullname": "t2_1drdwjiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you and your team write good documentation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1594xvv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690305337.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690282093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow engineers,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been recently creating documentation for a new process that we\u2019ve built and was wondering how other engineers approach writing documentation? what tools do you use? how&amp;#39;d you keep documentation up to date? Etc&lt;/p&gt;\n\n&lt;p&gt;I think the main challenge we are running into is the keeping docs up to date part. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re using confluence to write docs and we\u2019re also using Comala Document Management to help provide a review process to docs as well as auto expiring pages after a year so we can re-review but it still requires people to be checking for expired docs etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1594xvv", "is_robot_indexable": true, "report_reasons": null, "author": "yorkshireSpud12", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1594xvv/how_do_you_and_your_team_write_good_documentation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1594xvv/how_do_you_and_your_team_write_good_documentation/", "subreddit_subscribers": 117940, "created_utc": 1690282093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In a recent Data Engineering Podcast episode I heard that there are data engineers and architects working on distributed data pipelines processing large amounts of data and a bunch of tools including distributed messaging systems or pub-sub tools like kafka, kinesis and data processing like spark, flink among others to deliver analytics and predictive algorithms.\n\nThere were a couple of anecdotes that made me curious.\n\nFirst one was that in a well known networking and tooling company they planned for 9 months to setup their analytics stack and ended up taking 3 years to build something that could do the job but not scale.\n\nSecond one was that these systems were so complex that the data orgs were often working 18 hours a day 7 days a week to operate and maintain the data stack.\n\nI can relate to the first claim, and partially to the second one. I want to learn from the community.\n\nThis community is 117k strong, therefore a pretty solid sample size. I want to learn from your experience about the veracity of the anecdotes. Here are a couple of prompts/questions for you to share.\n\n* What is the gap between planned work and the actual time taken to build the data pipelines that you have worked on?  \n9 months plan and 36 months delivery is a 300% additional time required over the planned effort, I can't see that in startups, growth stage companies, even scale ups. What is your experience?\n* On an average what would you average hours per week to support the data pipelines operations?  \n18 hours a day 7 days a week amounts to 126 hour weeks, I can attest to 70 maybe even 80 hour weeks with on-call issues. is 126 hours per week, for real?", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are data engineers taking 4X the amount of time to build, or working 18 hours a week maintaining data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159cf8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690300484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a recent Data Engineering Podcast episode I heard that there are data engineers and architects working on distributed data pipelines processing large amounts of data and a bunch of tools including distributed messaging systems or pub-sub tools like kafka, kinesis and data processing like spark, flink among others to deliver analytics and predictive algorithms.&lt;/p&gt;\n\n&lt;p&gt;There were a couple of anecdotes that made me curious.&lt;/p&gt;\n\n&lt;p&gt;First one was that in a well known networking and tooling company they planned for 9 months to setup their analytics stack and ended up taking 3 years to build something that could do the job but not scale.&lt;/p&gt;\n\n&lt;p&gt;Second one was that these systems were so complex that the data orgs were often working 18 hours a day 7 days a week to operate and maintain the data stack.&lt;/p&gt;\n\n&lt;p&gt;I can relate to the first claim, and partially to the second one. I want to learn from the community.&lt;/p&gt;\n\n&lt;p&gt;This community is 117k strong, therefore a pretty solid sample size. I want to learn from your experience about the veracity of the anecdotes. Here are a couple of prompts/questions for you to share.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What is the gap between planned work and the actual time taken to build the data pipelines that you have worked on?&lt;br/&gt;\n9 months plan and 36 months delivery is a 300% additional time required over the planned effort, I can&amp;#39;t see that in startups, growth stage companies, even scale ups. What is your experience?&lt;/li&gt;\n&lt;li&gt;On an average what would you average hours per week to support the data pipelines operations?&lt;br/&gt;\n18 hours a day 7 days a week amounts to 126 hour weeks, I can attest to 70 maybe even 80 hour weeks with on-call issues. is 126 hours per week, for real?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159cf8h", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159cf8h/are_data_engineers_taking_4x_the_amount_of_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159cf8h/are_data_engineers_taking_4x_the_amount_of_time/", "subreddit_subscribers": 117940, "created_utc": 1690300484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm considering a job in insurance. It has all the tools and skills I want to learn, master, but I wonder what working in insurance will mean for my day-to-day. I come from a public sector background, so I'm used to feeling \"good\" about my work. Insurance isn't \"bad\" but it often has that reputation. ", "author_fullname": "t2_98rrwspa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you all care about the type of data you're working with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159g777", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690308697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering a job in insurance. It has all the tools and skills I want to learn, master, but I wonder what working in insurance will mean for my day-to-day. I come from a public sector background, so I&amp;#39;m used to feeling &amp;quot;good&amp;quot; about my work. Insurance isn&amp;#39;t &amp;quot;bad&amp;quot; but it often has that reputation. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159g777", "is_robot_indexable": true, "report_reasons": null, "author": "what_duck", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/159g777/do_you_all_care_about_the_type_of_data_youre/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159g777/do_you_all_care_about_the_type_of_data_youre/", "subreddit_subscribers": 117940, "created_utc": 1690308697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello DEs of Reddit, \n\n&amp;#x200B;\n\nI recently passed my first cloud certification (AWS CCP) and I spent a lot of time figuring out which certification is the most suitable one to pass next. I decided to design a roadmap that helps in choosing the right AWS certification.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fw1s85z774eb1.png?width=1501&amp;format=png&amp;auto=webp&amp;s=72b9bc2927ca77ea67d149416eea18afc3171138\n\nYou can read more about it in my blog post: [https://anniscodes.com/articles/cloud-certificates-data-engineering/](https://anniscodes.com/articles/cloud-certificates-data-engineering/)  \n\n\nI would love to get your input on that!", "author_fullname": "t2_nq65wse7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Roadmap to AWS Certifications for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fw1s85z774eb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 171, "x": 108, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0363daa310b99ef17de238e5ac32db92395b8ee9"}, {"y": 342, "x": 216, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b852b0c97af967c8567a9e729fb4c5b048e98dd2"}, {"y": 507, "x": 320, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4474667f42279b92c971823825bf1ea6bf59ee5c"}, {"y": 1015, "x": 640, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3ac4297de3d5c9de270a60bf92ee95a9b3bc39d"}, {"y": 1523, "x": 960, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcfd4a084c5153f324e8c9c78612088fdf67c1a3"}, {"y": 1713, "x": 1080, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21653005adea6942679bf46fad5a36b484901c9e"}], "s": {"y": 2382, "x": 1501, "u": "https://preview.redd.it/fw1s85z774eb1.png?width=1501&amp;format=png&amp;auto=webp&amp;s=72b9bc2927ca77ea67d149416eea18afc3171138"}, "id": "fw1s85z774eb1"}}, "name": "t3_1598wvm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TfhMZo3jAhR3k_LLI7XhuyyZ0LRZv5FNsD2JgMcIsts.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690292536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello DEs of Reddit, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I recently passed my first cloud certification (AWS CCP) and I spent a lot of time figuring out which certification is the most suitable one to pass next. I decided to design a roadmap that helps in choosing the right AWS certification.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fw1s85z774eb1.png?width=1501&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72b9bc2927ca77ea67d149416eea18afc3171138\"&gt;https://preview.redd.it/fw1s85z774eb1.png?width=1501&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72b9bc2927ca77ea67d149416eea18afc3171138&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can read more about it in my blog post: &lt;a href=\"https://anniscodes.com/articles/cloud-certificates-data-engineering/\"&gt;https://anniscodes.com/articles/cloud-certificates-data-engineering/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I would love to get your input on that!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1598wvm", "is_robot_indexable": true, "report_reasons": null, "author": "lancelot_of_camelot", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1598wvm/a_roadmap_to_aws_certifications_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1598wvm/a_roadmap_to_aws_certifications_for_data/", "subreddit_subscribers": 117940, "created_utc": 1690292536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Python to Elixir Machine Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_159709y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3sdftU-uPYJYLZMM0xHlzr0o8ApxS5mVF0LJXOfRV84.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690287837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thestackcanary.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?auto=webp&amp;s=5958a25708fab102c60b437777fc5e0561d137a5", "width": 2000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19c34925b4f31c565d842d4a796e2551587fad59", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e4c560acb45fd00a0b382a96d93b8826036064b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=06431ff24c29532ca33a97f584d7bf1d3e2cf58f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd81dfc5832c846d195c16e1738b88d80024492b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d88c2acf6fd4063f12f39bacc2874384fb543bb5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/exniv0gE847JVYjcpgZGJD8bP2iNsUuygG0RirN4-QY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83acb073afcc2561f7054900e766fb8b55956876", "width": 1080, "height": 540}], "variants": {}, "id": "3AetimPUxhSKfD40rc3Jhvm93KPZj-Psa6tf_Mmpgbo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159709y", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159709y/from_python_to_elixir_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thestackcanary.com/from-python-pytorch-to-elixir-nx/", "subreddit_subscribers": 117940, "created_utc": 1690287837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What ETL tools are people using to put data in to snowflake form sources such as SQL &amp; SAP", "author_fullname": "t2_viay6ncm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159bzjg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690299541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What ETL tools are people using to put data in to snowflake form sources such as SQL &amp;amp; SAP&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159bzjg", "is_robot_indexable": true, "report_reasons": null, "author": "dontevenaddme", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159bzjg/etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159bzjg/etl_tools/", "subreddit_subscribers": 117940, "created_utc": 1690299541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, let me preface this by saying my earliest career experience involved working in SQL Server. I worked in SQL server for the first 8-ish years of my data engineering journey (in addition to using other applications that connected to SQL server - SSIS, SSAS, Tableau, PowerBI, etc.).   \nHowever, the last 3.5 years have all been snowflake development except for one short client experience where I was basically working in ADF, azure and sql server. I really appreciate what snowflake has to offer but I'm curious to know from those of you who have worked with Azure synapse and/or the azure spectrum of products, whether you enjoy it or if (assuming you've had some experience with snowflake) you prefer working with a RDBMS tool like snowflake?   \nYes, I know this isn't exactly an apples to oranges comparison but I'm thinking of dipping my foot back into the azure space and I'd like to hear opinions from anyone who's experienced both?", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working in Snowflake vs Azure synapse (or the Azure suite in general)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159cd98", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690300358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, let me preface this by saying my earliest career experience involved working in SQL Server. I worked in SQL server for the first 8-ish years of my data engineering journey (in addition to using other applications that connected to SQL server - SSIS, SSAS, Tableau, PowerBI, etc.).&lt;br/&gt;\nHowever, the last 3.5 years have all been snowflake development except for one short client experience where I was basically working in ADF, azure and sql server. I really appreciate what snowflake has to offer but I&amp;#39;m curious to know from those of you who have worked with Azure synapse and/or the azure spectrum of products, whether you enjoy it or if (assuming you&amp;#39;ve had some experience with snowflake) you prefer working with a RDBMS tool like snowflake?&lt;br/&gt;\nYes, I know this isn&amp;#39;t exactly an apples to oranges comparison but I&amp;#39;m thinking of dipping my foot back into the azure space and I&amp;#39;d like to hear opinions from anyone who&amp;#39;s experienced both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159cd98", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159cd98/working_in_snowflake_vs_azure_synapse_or_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159cd98/working_in_snowflake_vs_azure_synapse_or_the/", "subreddit_subscribers": 117940, "created_utc": 1690300358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_d9bw4ufp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The e-graph data structure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1593052", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1690276170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cole-k.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.cole-k.com/2023/07/24/e-graphs-primer/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1593052", "is_robot_indexable": true, "report_reasons": null, "author": "codorace", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1593052/the_egraph_data_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.cole-k.com/2023/07/24/e-graphs-primer/", "subreddit_subscribers": 117940, "created_utc": 1690276170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I struggle to figure out how to build a pipeline from Postgres to BigQuery since in Postgres every customer is a separate db. Do I really need to build a separate pipeline for every db or can I somehow squeeze the db tables (since all the customer dbs have the same schema) all into same tables that I can export using e.g airbyte? Help!", "author_fullname": "t2_ml31smsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-tenant analytics pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158vd7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690253016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I struggle to figure out how to build a pipeline from Postgres to BigQuery since in Postgres every customer is a separate db. Do I really need to build a separate pipeline for every db or can I somehow squeeze the db tables (since all the customer dbs have the same schema) all into same tables that I can export using e.g airbyte? Help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "158vd7v", "is_robot_indexable": true, "report_reasons": null, "author": "skuutti", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/158vd7v/multitenant_analytics_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/158vd7v/multitenant_analytics_pipeline/", "subreddit_subscribers": 117940, "created_utc": 1690253016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# The Big Data Problem\n\nImagine you start in a new company as someone in charge of their real-time streaming infrastructure.\n\nYou are tasked with the problem of **computing the percentile distribution** of the company\u2019s terabytes\u2019 stream of data consisting of billions of records in Kafka, each representing a latency metric data point.\n\nCalculating a percentile for a large dataset is very expensive, to do so - you need to:\n\n1. store all the values\n2. sort them\n3. return the value whose rank matches the percentile (e.g 99th item)\n\nSuch big data aggregations are very tricky to solve. There is no way you can do this with terabytes.\n\nThe solution?\n\n# Streaming Stochastic Sublinear Algorithms\n\nAlso called **Sketch algorithms**, these are algorithms that trade off a bit of accuracy for massive efficiency gains. They are:\n\n* **probabilistic** (not 100% correct) - they usually have a strict, known ***error bound***.\n* **one-pass** \\- they go over each item in the stream only once.\n* have **sub-linear space growth** \\- input data grows, but the algorithm\u2019s memory requirement does NOT grow linearly with it.\n* **parallelizable** &amp; **composable** \\- you can split the data into two sets, compute sketches on them and then merge the results while guaranteeing the same accuracy. Parallelization can really scale this to infinity.\n* **data insensitive** \\- Big Data is extremely messy and disorganized. These algorithms handle things like NaN, infinites, nulls and are insensitive to the distribution/order of the data.\n\n# DataDog\u2019s Example\n\nIn the example above, you were actually placed in DataDog.\n\nThey invented their own sketch algorithm named [*DDSketch*](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3ZsZGIub3JnL3B2bGRiL3ZvbDEyL3AyMTk1LW1hc3Nvbi5wZGY_dXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.hC6ECvmb-GrfMbqoEglbq_vewfjS6SkKMqh6S54BG8s).\n\nIt offers a 2% relative error bound, which means that if the true p99 is 60s \u2192 the sketch would return **58.8-61.2s**.\n\nThe algorithm is pretty simple:\n\n1. Create buckets covering ranges of the desired error rate (+-2% in this case)\n2. Each bucket keeps a counter of the amount of data points within that range.\n3. When processing an item (latency metric data point), increment the counter of the appropriate bucket\n4. To count the desired percentile, you sum up the bucket\u2019s values until you get to the desired percentile. Whatever bucket that percentile is in - that\u2019s your value.\n\nWith this, you only need:\n\n**- 275** buckets  \n**-** **\\~2KB**  \nto cover the range from 1 millisecond to 1 minute.\n\nAnother key point? This can be endlessly parallelized.\n\nAs we [*learned from S3*](https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R3aXR0ZXIuY29tL0JkS296bG92c2tpL3N0YXR1cy8xNjYzODc5MjU3NDk4NzIyMzA2P3M9MjAmdXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.V9XUMqa4mMnJvdtbkT4CM9HUqBEpiVW0eOyfvPYCnCk), parallelization is key to unlocking great performance at tremendous scale.\n\nNotice - merging the sketch results together is as simple as merging two dictionaries/hashmaps of size 275!\n\n# Other Uses\n\nSketch algorithms are used heavily in the industry for other things like:\n\n* uniqueness - distinct elements\n* frequency of items (heavy hitters)\n* set union/intersection/difference\n* AI large vector/matrix decomposition\n* graph analysis - connectivity, weighted matching\n\nFor more examples &amp; visuals, see [the original place this was posted](https://2minutestreaming.beehiiv.com/p/streaming-sketch-algorithms).", "author_fullname": "t2_d2c8s0pb5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Sketch Algorithms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159grgy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690309915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;The Big Data Problem&lt;/h1&gt;\n\n&lt;p&gt;Imagine you start in a new company as someone in charge of their real-time streaming infrastructure.&lt;/p&gt;\n\n&lt;p&gt;You are tasked with the problem of &lt;strong&gt;computing the percentile distribution&lt;/strong&gt; of the company\u2019s terabytes\u2019 stream of data consisting of billions of records in Kafka, each representing a latency metric data point.&lt;/p&gt;\n\n&lt;p&gt;Calculating a percentile for a large dataset is very expensive, to do so - you need to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;store all the values&lt;/li&gt;\n&lt;li&gt;sort them&lt;/li&gt;\n&lt;li&gt;return the value whose rank matches the percentile (e.g 99th item)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Such big data aggregations are very tricky to solve. There is no way you can do this with terabytes.&lt;/p&gt;\n\n&lt;p&gt;The solution?&lt;/p&gt;\n\n&lt;h1&gt;Streaming Stochastic Sublinear Algorithms&lt;/h1&gt;\n\n&lt;p&gt;Also called &lt;strong&gt;Sketch algorithms&lt;/strong&gt;, these are algorithms that trade off a bit of accuracy for massive efficiency gains. They are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;probabilistic&lt;/strong&gt; (not 100% correct) - they usually have a strict, known &lt;strong&gt;&lt;em&gt;error bound&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;one-pass&lt;/strong&gt; - they go over each item in the stream only once.&lt;/li&gt;\n&lt;li&gt;have &lt;strong&gt;sub-linear space growth&lt;/strong&gt; - input data grows, but the algorithm\u2019s memory requirement does NOT grow linearly with it.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;parallelizable&lt;/strong&gt; &amp;amp; &lt;strong&gt;composable&lt;/strong&gt; - you can split the data into two sets, compute sketches on them and then merge the results while guaranteeing the same accuracy. Parallelization can really scale this to infinity.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;data insensitive&lt;/strong&gt; - Big Data is extremely messy and disorganized. These algorithms handle things like NaN, infinites, nulls and are insensitive to the distribution/order of the data.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;DataDog\u2019s Example&lt;/h1&gt;\n\n&lt;p&gt;In the example above, you were actually placed in DataDog.&lt;/p&gt;\n\n&lt;p&gt;They invented their own sketch algorithm named &lt;a href=\"https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3ZsZGIub3JnL3B2bGRiL3ZvbDEyL3AyMTk1LW1hc3Nvbi5wZGY_dXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.hC6ECvmb-GrfMbqoEglbq_vewfjS6SkKMqh6S54BG8s\"&gt;&lt;em&gt;DDSketch&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It offers a 2% relative error bound, which means that if the true p99 is 60s \u2192 the sketch would return &lt;strong&gt;58.8-61.2s&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;The algorithm is pretty simple:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create buckets covering ranges of the desired error rate (+-2% in this case)&lt;/li&gt;\n&lt;li&gt;Each bucket keeps a counter of the amount of data points within that range.&lt;/li&gt;\n&lt;li&gt;When processing an item (latency metric data point), increment the counter of the appropriate bucket&lt;/li&gt;\n&lt;li&gt;To count the desired percentile, you sum up the bucket\u2019s values until you get to the desired percentile. Whatever bucket that percentile is in - that\u2019s your value.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;With this, you only need:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;- 275&lt;/strong&gt; buckets&lt;br/&gt;\n&lt;strong&gt;-&lt;/strong&gt; &lt;strong&gt;~2KB&lt;/strong&gt;&lt;br/&gt;\nto cover the range from 1 millisecond to 1 minute.&lt;/p&gt;\n\n&lt;p&gt;Another key point? This can be endlessly parallelized.&lt;/p&gt;\n\n&lt;p&gt;As we &lt;a href=\"https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3R3aXR0ZXIuY29tL0JkS296bG92c2tpL3N0YXR1cy8xNjYzODc5MjU3NDk4NzIyMzA2P3M9MjAmdXRtX3NvdXJjZT0ybWludXRlc3RyZWFtaW5nLmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXN0b2NoYXN0aWMtc3VibGluZWFyLXN0cmVhbWluZy1hbGdvcml0aG1zIiwicG9zdF9pZCI6IjlkNjYyN2FhLWVkMDQtNDExMC04N2JhLWQyZmE0NDZjOTUwZSIsInB1YmxpY2F0aW9uX2lkIjoiMjYyYmJjNTktZDkyNS00MTcxLWFmZDYtYjE0NGEyYzM2MzZlIiwidmlzaXRfdG9rZW4iOiI3N2Q3MzkyMi1lY2QwLTRmN2ItYTNhYS05NDYwOTBlZDMzNDgiLCJpYXQiOjE2OTAzMDk4MDYuNzcsImlzcyI6Im9yY2hpZCJ9.V9XUMqa4mMnJvdtbkT4CM9HUqBEpiVW0eOyfvPYCnCk\"&gt;&lt;em&gt;learned from S3&lt;/em&gt;&lt;/a&gt;, parallelization is key to unlocking great performance at tremendous scale.&lt;/p&gt;\n\n&lt;p&gt;Notice - merging the sketch results together is as simple as merging two dictionaries/hashmaps of size 275!&lt;/p&gt;\n\n&lt;h1&gt;Other Uses&lt;/h1&gt;\n\n&lt;p&gt;Sketch algorithms are used heavily in the industry for other things like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;uniqueness - distinct elements&lt;/li&gt;\n&lt;li&gt;frequency of items (heavy hitters)&lt;/li&gt;\n&lt;li&gt;set union/intersection/difference&lt;/li&gt;\n&lt;li&gt;AI large vector/matrix decomposition&lt;/li&gt;\n&lt;li&gt;graph analysis - connectivity, weighted matching&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For more examples &amp;amp; visuals, see &lt;a href=\"https://2minutestreaming.beehiiv.com/p/streaming-sketch-algorithms\"&gt;the original place this was posted&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "159grgy", "is_robot_indexable": true, "report_reasons": null, "author": "2minutestreaming", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159grgy/introduction_to_sketch_algorithms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159grgy/introduction_to_sketch_algorithms/", "subreddit_subscribers": 117940, "created_utc": 1690309915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a senior DE at a midsize company. \nI see the following path -\nDE &gt; Senior &gt; Lead &gt; Principal &gt; Manager\n\nWhat would come next?\n\nI always wonder this because I can\u2019t remember the last time I heard a VP or DE. I know VP of Analytics exists but it\u2019s often the Analysts or Data Scientists who take up those roles. \n\nHence the question.", "author_fullname": "t2_bfd45sy5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What position after \u201cManager of DE\u201d?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159glvw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690309577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a senior DE at a midsize company. \nI see the following path -\nDE &amp;gt; Senior &amp;gt; Lead &amp;gt; Principal &amp;gt; Manager&lt;/p&gt;\n\n&lt;p&gt;What would come next?&lt;/p&gt;\n\n&lt;p&gt;I always wonder this because I can\u2019t remember the last time I heard a VP or DE. I know VP of Analytics exists but it\u2019s often the Analysts or Data Scientists who take up those roles. &lt;/p&gt;\n\n&lt;p&gt;Hence the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159glvw", "is_robot_indexable": true, "report_reasons": null, "author": "nrskmn", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159glvw/what_position_after_manager_of_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159glvw/what_position_after_manager_of_de/", "subreddit_subscribers": 117940, "created_utc": 1690309577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do we answer question about describing work experience in an interview if someone has more than 8+ years of experience in multiple organization. Sometimes I think I am going too long and sometimes I feel Its too short. Whats the best way to describe it . How long we should spend in describing it?2 mins 5 mins or more?Is there any template for this ?", "author_fullname": "t2_dn5rzvxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Describing previous work experiences in an Interview.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159g9rt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690308861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do we answer question about describing work experience in an interview if someone has more than 8+ years of experience in multiple organization. Sometimes I think I am going too long and sometimes I feel Its too short. Whats the best way to describe it . How long we should spend in describing it?2 mins 5 mins or more?Is there any template for this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "159g9rt", "is_robot_indexable": true, "report_reasons": null, "author": "Fickle-Picture-7674", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159g9rt/describing_previous_work_experiences_in_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159g9rt/describing_previous_work_experiences_in_an/", "subreddit_subscribers": 117940, "created_utc": 1690308861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all.\n\nI'm curious to hear your opinions about DMS tasks parallelization and replication instance size when running DMS for a database migration.\n\nI understand many factors to consider here. I will try to list them:\n\n* Database Size\n* Data distribution on tables (some tables can be really large while other are small, so best to load large tables in parallel per partition or by range)\n* Time window to perform Full Load (downtime)\n* Use of Full Load + CDC (needed if database is too huge and migration time does not fit in the downtime window)\n\n&amp;#x200B;\n\nSo if I choose a replication instance like\n\ndms.r4.4xlarge16 vCPU122 GIB\n\nDoes it make sense to have 16 threads in  MaxFullLoadSubTasks task settings? Like one thread per vCPU? Would it be better to split that in multiple DMS Migration Tasks? Maybe 2 tasks with 8 MaxFullLoadSubTasks?\n\nHow does one estimate how long it is going to take to migrate a 4TB database using this configuration?\n\nJust wondering how you guys make this kind of design decisions and your train of thought on it.\n\n&amp;#x200B;", "author_fullname": "t2_d60i64x6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DMS Task setting and replication instance size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159depf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690302627.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear your opinions about DMS tasks parallelization and replication instance size when running DMS for a database migration.&lt;/p&gt;\n\n&lt;p&gt;I understand many factors to consider here. I will try to list them:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Database Size&lt;/li&gt;\n&lt;li&gt;Data distribution on tables (some tables can be really large while other are small, so best to load large tables in parallel per partition or by range)&lt;/li&gt;\n&lt;li&gt;Time window to perform Full Load (downtime)&lt;/li&gt;\n&lt;li&gt;Use of Full Load + CDC (needed if database is too huge and migration time does not fit in the downtime window)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So if I choose a replication instance like&lt;/p&gt;\n\n&lt;p&gt;dms.r4.4xlarge16 vCPU122 GIB&lt;/p&gt;\n\n&lt;p&gt;Does it make sense to have 16 threads in  MaxFullLoadSubTasks task settings? Like one thread per vCPU? Would it be better to split that in multiple DMS Migration Tasks? Maybe 2 tasks with 8 MaxFullLoadSubTasks?&lt;/p&gt;\n\n&lt;p&gt;How does one estimate how long it is going to take to migrate a 4TB database using this configuration?&lt;/p&gt;\n\n&lt;p&gt;Just wondering how you guys make this kind of design decisions and your train of thought on it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159depf", "is_robot_indexable": true, "report_reasons": null, "author": "PidKiller09", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159depf/dms_task_setting_and_replication_instance_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159depf/dms_task_setting_and_replication_instance_size/", "subreddit_subscribers": 117940, "created_utc": 1690302627.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are your views on delete insert performance against merge? In snowflake since its hybrid column store, more often than not it is advised to use a merge which goes to say yiu dont really know which micropartition the deletes would be on and it can get expensive. While i agree to that, but isnt that true for a merge too? I would like to get clarity on this and a rough idea of the considerations where one method may outrace the other.", "author_fullname": "t2_eobyj34v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Views on merge vs delete insert", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158xxyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690260308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your views on delete insert performance against merge? In snowflake since its hybrid column store, more often than not it is advised to use a merge which goes to say yiu dont really know which micropartition the deletes would be on and it can get expensive. While i agree to that, but isnt that true for a merge too? I would like to get clarity on this and a rough idea of the considerations where one method may outrace the other.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "158xxyj", "is_robot_indexable": true, "report_reasons": null, "author": "Stoic_Akshay", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/158xxyj/views_on_merge_vs_delete_insert/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/158xxyj/views_on_merge_vs_delete_insert/", "subreddit_subscribers": 117940, "created_utc": 1690260308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nOur data infrastructure pulls in data from a variety of sources, with a heavy emphasis on well-known APIs such as Shopify, Google, Salesforce, and so forth. I'm looking to enhance this infrastructure with an open-source tool, one that allows for the creation of forms to facilitate data entry. At the moment, we're utilizing Google Sheets, but it's falling short in several areas, especially as users often fail to adhere to the prescribed format. While I understand Google Script can assist with validation, it doesn't adequately cope with our data volumes, rendering it impractical for our needs.\n\nI'm curious to hear how other Data Engineers are addressing this challenge.\n\nHere's a list of my requirements for the tool:\n\n\\- Open-source: It should be freely available, and I should be able to host it on our server.\n\n\\- Form creation: Admin users should have the ability to construct forms.\n\n\\- Role-based access control: Ideally, it should support this feature.\n\n\\- Data validation: The tool needs to be able to validate user input.\n\n\\- Grid view: It would be beneficial if the tool could provide a grid view (akin to Excel or Google Sheets), permitting users to enter multiple rows of data at once.\n\n&amp;#x200B;\n\nLooking forward to your insights and suggestions.", "author_fullname": "t2_g7dufpq97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-Source Data Entry and Validation Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15981gq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690290410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Our data infrastructure pulls in data from a variety of sources, with a heavy emphasis on well-known APIs such as Shopify, Google, Salesforce, and so forth. I&amp;#39;m looking to enhance this infrastructure with an open-source tool, one that allows for the creation of forms to facilitate data entry. At the moment, we&amp;#39;re utilizing Google Sheets, but it&amp;#39;s falling short in several areas, especially as users often fail to adhere to the prescribed format. While I understand Google Script can assist with validation, it doesn&amp;#39;t adequately cope with our data volumes, rendering it impractical for our needs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious to hear how other Data Engineers are addressing this challenge.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a list of my requirements for the tool:&lt;/p&gt;\n\n&lt;p&gt;- Open-source: It should be freely available, and I should be able to host it on our server.&lt;/p&gt;\n\n&lt;p&gt;- Form creation: Admin users should have the ability to construct forms.&lt;/p&gt;\n\n&lt;p&gt;- Role-based access control: Ideally, it should support this feature.&lt;/p&gt;\n\n&lt;p&gt;- Data validation: The tool needs to be able to validate user input.&lt;/p&gt;\n\n&lt;p&gt;- Grid view: It would be beneficial if the tool could provide a grid view (akin to Excel or Google Sheets), permitting users to enter multiple rows of data at once.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15981gq", "is_robot_indexable": true, "report_reasons": null, "author": "ZKR2000", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15981gq/opensource_data_entry_and_validation_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15981gq/opensource_data_entry_and_validation_tool/", "subreddit_subscribers": 117940, "created_utc": 1690290410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "While the job market was getting to its lowest point in the USA, Latin American companies keep hiring and hiring offering great job opportunities. My daily received 4 to 5 job offers from March to early July, in my mailbox and LinkedIn. But now the job offers have stopped, and only a few good ones are out there. I guess the freeze finally arrive here ( Colombia ) and the only thing we have left to do is to wait.", "author_fullname": "t2_7sxdmzpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "freeze market has arrived in Latin America", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_158uahr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690250177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While the job market was getting to its lowest point in the USA, Latin American companies keep hiring and hiring offering great job opportunities. My daily received 4 to 5 job offers from March to early July, in my mailbox and LinkedIn. But now the job offers have stopped, and only a few good ones are out there. I guess the freeze finally arrive here ( Colombia ) and the only thing we have left to do is to wait.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "158uahr", "is_robot_indexable": true, "report_reasons": null, "author": "felipeHernandez19", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/158uahr/freeze_market_has_arrived_in_latin_america/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/158uahr/freeze_market_has_arrived_in_latin_america/", "subreddit_subscribers": 117940, "created_utc": 1690250177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few weeks ago I wrote up my first blog!\n\nHaving started my career on a Data &amp; Analytics graduate scheme, I have been able to appreciate how internships, graduate programmes, re/up-skilling programmes \u2014 or any kind of trainee opportunities \u2014 are critical avenues to\u00a0investing in people who can evolve your team and bring in new ideas and ways of thinking, ultimately making your team more resilient to the ever-changing data landscape.\n\nTherefore, the purpose of this blog is to outline a\u00a0generalised, structured approach\u00a0to how data teams across\u00a0any\u00a0industry can deliver impactful training opportunities, by wrapping the projects you align to trainees into a\u00a0templated project proposal.\n\nBy the end of the blog, you will understand that by investing the time and effort upfront to have this in place for your team, not only will you reap the benefits outlined, but you are far more likely to have someone (who you are already acquainted with) wanting to be a part of your team down the road.\n\nI particularly think this would be an interesting read for those leading Data Engineering teams (that have an extensive list of backlog items). \n\nI would greatly appreciate if you could have a read and let me know your thoughts and feedback!", "author_fullname": "t2_mf4javym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making Early Investments Into Your Data Teams Through Valuable Trainee Experiences", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_159m336", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uddHpKt5r1D6tLZFbjRVSnuBxpXo4jY9kNuasVUbmGU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690321467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few weeks ago I wrote up my first blog!&lt;/p&gt;\n\n&lt;p&gt;Having started my career on a Data &amp;amp; Analytics graduate scheme, I have been able to appreciate how internships, graduate programmes, re/up-skilling programmes \u2014 or any kind of trainee opportunities \u2014 are critical avenues to\u00a0investing in people who can evolve your team and bring in new ideas and ways of thinking, ultimately making your team more resilient to the ever-changing data landscape.&lt;/p&gt;\n\n&lt;p&gt;Therefore, the purpose of this blog is to outline a\u00a0generalised, structured approach\u00a0to how data teams across\u00a0any\u00a0industry can deliver impactful training opportunities, by wrapping the projects you align to trainees into a\u00a0templated project proposal.&lt;/p&gt;\n\n&lt;p&gt;By the end of the blog, you will understand that by investing the time and effort upfront to have this in place for your team, not only will you reap the benefits outlined, but you are far more likely to have someone (who you are already acquainted with) wanting to be a part of your team down the road.&lt;/p&gt;\n\n&lt;p&gt;I particularly think this would be an interesting read for those leading Data Engineering teams (that have an extensive list of backlog items). &lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate if you could have a read and let me know your thoughts and feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@ronanwalters/making-early-investments-into-your-data-teams-through-valuable-internship-experiences-657edbee8908", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?auto=webp&amp;s=41195ca295fe3c10523a6919eccdbb7b88cdce69", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd56fe640c71dbccaa0e52d366884059a58dcc9e", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12b570c0fd2c3e24564d3ceab4749129392f446a", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99b18ad5dbe44481c7e818e7124f972ff79279f1", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a69f9acdfad14c42beb1380e33d84b29b9e2d03c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=de2c1fe144dd6f5574d916cb4f8cdc5e4e784539", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/5dhRR-zh_LNSSEyruFsj9dp0ObkPtDjx7ClURJlUHN0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81c494052b33df6e891993b49bdda18a6bcd00ec", "width": 1080, "height": 720}], "variants": {}, "id": "8fRD0EvrcOaPcjaAHek9qlBYyrYRX9iLwx9fCB66Pfc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "159m336", "is_robot_indexable": true, "report_reasons": null, "author": "Western-Opening-9212", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159m336/making_early_investments_into_your_data_teams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@ronanwalters/making-early-investments-into-your-data-teams-through-valuable-internship-experiences-657edbee8908", "subreddit_subscribers": 117940, "created_utc": 1690321467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI was wondering how are people in the industry scheduling azure data factory pipelines. I know that triggers exist but how to establish a dependency between two pipelines, is there a way to do that in ADF. \n\nCurrently, I just have one master pipeline that links all the other pipelines but at some point of time it gets out of hand.\n\nAlso, I am trying to schedule fact and dimension. Any help would be appreciated.", "author_fullname": "t2_uqu7iar6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory Scheduling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159jj7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690315896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I was wondering how are people in the industry scheduling azure data factory pipelines. I know that triggers exist but how to establish a dependency between two pipelines, is there a way to do that in ADF. &lt;/p&gt;\n\n&lt;p&gt;Currently, I just have one master pipeline that links all the other pipelines but at some point of time it gets out of hand.&lt;/p&gt;\n\n&lt;p&gt;Also, I am trying to schedule fact and dimension. Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "159jj7v", "is_robot_indexable": true, "report_reasons": null, "author": "InvestigatorMuted622", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159jj7v/azure_data_factory_scheduling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159jj7v/azure_data_factory_scheduling/", "subreddit_subscribers": 117940, "created_utc": 1690315896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on a financial consultancy and I discovered today about Fabric. I would like to know if it is possible to use this software to make a better link with clients and take their data directly in real time.  \n\n\nSo, for example, lets suppose I have a restaurant as a client, and that I want a system where he writes his daily revenues, foot traffic, costs and so on, so that these data go directly to my database without being necessary for me to receive his database, organize it with power query and make the entire process... His data would go direclty to an organized database that I can use fast to make analysis and to uptade Power BI Dashboards.  \n\n\nDo Factory and Fabric have this capacity? It would be very useful for me.\n\n&amp;#x200B;\n\nThank you.", "author_fullname": "t2_4mituu0s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About Data Factory on Microsoft Fabric", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159j3tv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690314989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a financial consultancy and I discovered today about Fabric. I would like to know if it is possible to use this software to make a better link with clients and take their data directly in real time.  &lt;/p&gt;\n\n&lt;p&gt;So, for example, lets suppose I have a restaurant as a client, and that I want a system where he writes his daily revenues, foot traffic, costs and so on, so that these data go directly to my database without being necessary for me to receive his database, organize it with power query and make the entire process... His data would go direclty to an organized database that I can use fast to make analysis and to uptade Power BI Dashboards.  &lt;/p&gt;\n\n&lt;p&gt;Do Factory and Fabric have this capacity? It would be very useful for me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159j3tv", "is_robot_indexable": true, "report_reasons": null, "author": "Relsen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159j3tv/about_data_factory_on_microsoft_fabric/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159j3tv/about_data_factory_on_microsoft_fabric/", "subreddit_subscribers": 117940, "created_utc": 1690314989.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**TL;DR incorporate SQL functionality within Jupyter, access to modern data processing DBs (like DuckDB), polars and data exploration through plotting easier with JupySQL.**\n\nhey r/dataengineering! I'd like to share some news about JupySQL, an open source project my team has been working on!\n\nJupySQL allows you to run SQL and perform exploratory data analysis in Jupyter via magics %sqland %%sqlmagics, and visualize your data with %sqlplot.\n\nThis project is open source, and a successor to [iPython-SQL](https://github.com/catherinedevlin/ipython-sql#legacy-project). It is compatible with all major databases (e.g., PostgreSQL, MySQL, SQL Server), data warehouses (e.g., Snowflake, BigQuery, Redshift), and embedded engines (SQLite, and DuckDB).\n\nThis week the team released version 0.8.0 with improved data profiling and DuckDB performance when converting resulting tables to other formats (pandas and polars).\n\nLearn more: [https://jupysql.ploomber.io/en/latest/quick-start.html](https://jupysql.ploomber.io/en/latest/quick-start.html)\n\nif you have any questions, feel free to ask! And show your support with a star on [GitHub](https://github.com/ploomber/jupysql)!", "author_fullname": "t2_fymggxzxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From iPython-sql to JupySQL: new updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159fmf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690307453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR incorporate SQL functionality within Jupyter, access to modern data processing DBs (like DuckDB), polars and data exploration through plotting easier with JupySQL.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I&amp;#39;d like to share some news about JupySQL, an open source project my team has been working on!&lt;/p&gt;\n\n&lt;p&gt;JupySQL allows you to run SQL and perform exploratory data analysis in Jupyter via magics %sqland %%sqlmagics, and visualize your data with %sqlplot.&lt;/p&gt;\n\n&lt;p&gt;This project is open source, and a successor to &lt;a href=\"https://github.com/catherinedevlin/ipython-sql#legacy-project\"&gt;iPython-SQL&lt;/a&gt;. It is compatible with all major databases (e.g., PostgreSQL, MySQL, SQL Server), data warehouses (e.g., Snowflake, BigQuery, Redshift), and embedded engines (SQLite, and DuckDB).&lt;/p&gt;\n\n&lt;p&gt;This week the team released version 0.8.0 with improved data profiling and DuckDB performance when converting resulting tables to other formats (pandas and polars).&lt;/p&gt;\n\n&lt;p&gt;Learn more: &lt;a href=\"https://jupysql.ploomber.io/en/latest/quick-start.html\"&gt;https://jupysql.ploomber.io/en/latest/quick-start.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;if you have any questions, feel free to ask! And show your support with a star on &lt;a href=\"https://github.com/ploomber/jupysql\"&gt;GitHub&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?auto=webp&amp;s=5eaa766829f09147ae3b2bc614f13187e68a7ed2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5a28510613b8de7ed1f6ec1538d38e82e08a43b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=590514cfa4354a7729e11d76f6f5c7030ccb4495", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e9fbbb2d65b14a1965551d9c1be14b5bbda33aca", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df82b64967e5b1f92f666deb91ea92f58224b274", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ea4bf05c21fc35ed35fa555aa184407b6aa6b7b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/90Cha81WaAMF6NYzqUXI9Ido2h_m2FIgr6oS4pL7b8Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ec1c6c82462e53121c27c95aa55a2bb87a46908", "width": 1080, "height": 540}], "variants": {}, "id": "PnfZqzALk55bbYPxf42ETuUy79_7mLJARaQ2dFLRMAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "159fmf5", "is_robot_indexable": true, "report_reasons": null, "author": "ploomber-dev", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159fmf5/from_ipythonsql_to_jupysql_new_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159fmf5/from_ipythonsql_to_jupysql_new_updates/", "subreddit_subscribers": 117940, "created_utc": 1690307453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello anyone working on Informatica PIM tool. Need help !!!\nI want to gain It's P360 certification.\nPlease recommend available resources or courses. \nWhats the right approach?\nThanks in advance.", "author_fullname": "t2_67azy9z8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PIM System", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159dzb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690303884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello anyone working on Informatica PIM tool. Need help !!!\nI want to gain It&amp;#39;s P360 certification.\nPlease recommend available resources or courses. \nWhats the right approach?\nThanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "159dzb0", "is_robot_indexable": true, "report_reasons": null, "author": "smilycat_ag", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159dzb0/pim_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159dzb0/pim_system/", "subreddit_subscribers": 117940, "created_utc": 1690303884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys!iam having an interview soon and i wonder what some good questions i should ask to learn about his role.\nSome things i though to ask\n1) Except DBT what other tools you use?\n2) what type of SQL you use\n As you can tell this are really simple and generic and iam looking for something more,to show iam really interested.", "author_fullname": "t2_713dnb8su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's some good questions i should ask to my Data Engineer interviewer as a Data analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_159d038", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690301735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys!iam having an interview soon and i wonder what some good questions i should ask to learn about his role.\nSome things i though to ask\n1) Except DBT what other tools you use?\n2) what type of SQL you use\n As you can tell this are really simple and generic and iam looking for something more,to show iam really interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "159d038", "is_robot_indexable": true, "report_reasons": null, "author": "idkman947", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/159d038/whats_some_good_questions_i_should_ask_to_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/159d038/whats_some_good_questions_i_should_ask_to_my_data/", "subreddit_subscribers": 117940, "created_utc": 1690301735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Data Engineers,\n\nMy company has used a client tracking system a few years ago (2010-2016) but choose to switch to a better system after that. They left all the data in the previous system and only accessed it on request via a read only portal.\n\nNow that we have some new policies, we are required to get the data that is still stored in the old application to ourselves, preferably in cloud storage.\n\nAs the supplier of this old system is a small party and they are not very tech savvy, they can only deliver the files in a OneDrive folder where all the data would be, folders ... What a nightmare. For reference, the data is about 200 GB large with loads of different files roughly counted it should be about 700.000 files (not sure yet).\n\nNow I am looking into options to get things from a large OneDrive folder to a storage solution while maintaining folder structure. Initially I thought Azure Blob with hierarchical namespaces would be good (whole company is Azure based, so anything Azure/MS related has a huge pre).  \n\n\nWhile looking at the options to move data I've tried things such as Power Automate/Logics Apps/Data factory but cannot find a good tool to do so. Power Automate/Logics apps have a throttled API which only allows 100 calls/60 seconds which makes it very inefficient. ADF doesn't have connectors for OneDrive which makes a Linked Service impossible to setup.\n\nDoes anyone have an idea how we can make this work? I'm sure there are possibilities we haven't considered yet! ", "author_fullname": "t2_183f7a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importing large file system from OneDrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1598kkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690291704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;My company has used a client tracking system a few years ago (2010-2016) but choose to switch to a better system after that. They left all the data in the previous system and only accessed it on request via a read only portal.&lt;/p&gt;\n\n&lt;p&gt;Now that we have some new policies, we are required to get the data that is still stored in the old application to ourselves, preferably in cloud storage.&lt;/p&gt;\n\n&lt;p&gt;As the supplier of this old system is a small party and they are not very tech savvy, they can only deliver the files in a OneDrive folder where all the data would be, folders ... What a nightmare. For reference, the data is about 200 GB large with loads of different files roughly counted it should be about 700.000 files (not sure yet).&lt;/p&gt;\n\n&lt;p&gt;Now I am looking into options to get things from a large OneDrive folder to a storage solution while maintaining folder structure. Initially I thought Azure Blob with hierarchical namespaces would be good (whole company is Azure based, so anything Azure/MS related has a huge pre).  &lt;/p&gt;\n\n&lt;p&gt;While looking at the options to move data I&amp;#39;ve tried things such as Power Automate/Logics Apps/Data factory but cannot find a good tool to do so. Power Automate/Logics apps have a throttled API which only allows 100 calls/60 seconds which makes it very inefficient. ADF doesn&amp;#39;t have connectors for OneDrive which makes a Linked Service impossible to setup.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea how we can make this work? I&amp;#39;m sure there are possibilities we haven&amp;#39;t considered yet! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1598kkd", "is_robot_indexable": true, "report_reasons": null, "author": "faalschildpad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1598kkd/importing_large_file_system_from_onedrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1598kkd/importing_large_file_system_from_onedrive/", "subreddit_subscribers": 117940, "created_utc": 1690291704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8rp73p0k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_1592b20", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/LSCeKHkb-xk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/LSCeKHkb-xk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration\"&gt;&lt;/iframe&gt;", "author_name": "Data Zen Community", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/LSCeKHkb-xk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataZenCommunity"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/LSCeKHkb-xk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1592b20", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/g4efKNubD8b6zQhSnkie17FR19Q2IdwS8mKmWcm9BjA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690274017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=LSCeKHkb-xk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/27Z1fa42TbZFi8x_vuJi7Iyl7aMMyHeR1-JUKh9Xics.jpg?auto=webp&amp;s=56f1ca84f456af8c8ef6edd2bd371534c93d6051", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/27Z1fa42TbZFi8x_vuJi7Iyl7aMMyHeR1-JUKh9Xics.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dee36b8df81c7c31a335e5a5028127ceb03f9bd6", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/27Z1fa42TbZFi8x_vuJi7Iyl7aMMyHeR1-JUKh9Xics.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a02ac2f6525afd5e40ac334d7395d604da95b391", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/27Z1fa42TbZFi8x_vuJi7Iyl7aMMyHeR1-JUKh9Xics.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c20b3597ee8a6f44d648e63c1ecc0c1cf24cad6", "width": 320, "height": 240}], "variants": {}, "id": "u5F8hh2ZKIS2RgLSkvQ3tv7fMtSe8sMtSNvTZYfRVYo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1592b20", "is_robot_indexable": true, "report_reasons": null, "author": "alexdby", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1592b20/push_dbt_beyond_boundaries_exploring_a_fresh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=LSCeKHkb-xk", "subreddit_subscribers": 117940, "created_utc": 1690274017.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/LSCeKHkb-xk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Push dbt beyond boundaries: Exploring a Fresh Approach to dbt Integration\"&gt;&lt;/iframe&gt;", "author_name": "Data Zen Community", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/LSCeKHkb-xk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataZenCommunity"}}, "is_video": false}}], "before": null}}