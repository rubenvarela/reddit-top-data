{"kind": "Listing", "data": {"after": "t3_155o8d6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seeking advice, please delete if against the rules.\n\nI want to share a bit of context first, please bare with me.\n\nI've always been frustrated with various bookmark managers and have tried many over the years. I have three main issues:\n\n* Content going offline: I want my bookmark manager to keep a copy of the content since link rot is common. Whether it's a webpage, YouTube video, or tweet thread, I believe I should have the right to keep a copy of what I can access online, regardless of the terms of service.\n\n* Difficulty in retrieval and organization: Adding tags or using specific structures, I ain't got time for that. We went to the moon, computers should be able to automatically extract information like keywords, author name, likes count, etc. And I want to be able to do full text search similar to how searching in Gmail works.\n\n* Discoverability: When I come across something interesting to read, I often can't stop to read it immediately. I want to bookmark it and receive a weekly reminder (like a mailing list) with a mix of unread content and previously checked bookmarks. This way, I can revisit old hidden gems and remove outdated content.\n\nTo address these issues, I set a goal to build a solution that fulfills my requirements. I have a basic, ugly but working prototype that I'm using successfully for my needs. Now, I'm wondering if this problem is unique to me. If I were to make it public, would you use such a tool? If yes, what features would you like to see implemented?\n(If I decide to go forward with this, being able to download a copy of your bookmarks is a non negotiable feature for me)", "author_fullname": "t2_3zmriqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for feedback: data hoarding as a service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155hls7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689929220.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689927295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking advice, please delete if against the rules.&lt;/p&gt;\n\n&lt;p&gt;I want to share a bit of context first, please bare with me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always been frustrated with various bookmark managers and have tried many over the years. I have three main issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Content going offline: I want my bookmark manager to keep a copy of the content since link rot is common. Whether it&amp;#39;s a webpage, YouTube video, or tweet thread, I believe I should have the right to keep a copy of what I can access online, regardless of the terms of service.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Difficulty in retrieval and organization: Adding tags or using specific structures, I ain&amp;#39;t got time for that. We went to the moon, computers should be able to automatically extract information like keywords, author name, likes count, etc. And I want to be able to do full text search similar to how searching in Gmail works.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Discoverability: When I come across something interesting to read, I often can&amp;#39;t stop to read it immediately. I want to bookmark it and receive a weekly reminder (like a mailing list) with a mix of unread content and previously checked bookmarks. This way, I can revisit old hidden gems and remove outdated content.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To address these issues, I set a goal to build a solution that fulfills my requirements. I have a basic, ugly but working prototype that I&amp;#39;m using successfully for my needs. Now, I&amp;#39;m wondering if this problem is unique to me. If I were to make it public, would you use such a tool? If yes, what features would you like to see implemented?\n(If I decide to go forward with this, being able to download a copy of your bookmarks is a non negotiable feature for me)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155hls7", "is_robot_indexable": true, "report_reasons": null, "author": "goodkernel", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155hls7/looking_for_feedback_data_hoarding_as_a_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155hls7/looking_for_feedback_data_hoarding_as_a_service/", "subreddit_subscribers": 693549, "created_utc": 1689927295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dropbox just yesterday limited it's \"unlimited\" subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.\n\nIt is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.\n\nThey had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.\n\nDropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.\n\nI'm not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.\n\nAnyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.", "author_fullname": "t2_milg6ems", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox Business Advanced not unlimited anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155so84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689956480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dropbox just yesterday limited it&amp;#39;s &amp;quot;unlimited&amp;quot; subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.&lt;/p&gt;\n\n&lt;p&gt;It is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.&lt;/p&gt;\n\n&lt;p&gt;They had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.&lt;/p&gt;\n\n&lt;p&gt;Dropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.&lt;/p&gt;\n\n&lt;p&gt;Anyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155so84", "is_robot_indexable": true, "report_reasons": null, "author": "joshuanicolaspark", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "subreddit_subscribers": 693549, "created_utc": 1689956480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently cloned my hard drive on laptop to a new one using Macrium Reflect, was around 500 GB of data. How reliable can I expect it to be? Like if I now replace the hard disk and use the new one, can I expect it to function exactly like it did before, or are errors common?\n\nI ask because I heard elsewhere that Macrium Reflect cloning works most of the time, but without specifics what can go wrong.", "author_fullname": "t2_pr4df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about using Macrium Reflect to clone hard drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155koim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689937142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently cloned my hard drive on laptop to a new one using Macrium Reflect, was around 500 GB of data. How reliable can I expect it to be? Like if I now replace the hard disk and use the new one, can I expect it to function exactly like it did before, or are errors common?&lt;/p&gt;\n\n&lt;p&gt;I ask because I heard elsewhere that Macrium Reflect cloning works most of the time, but without specifics what can go wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155koim", "is_robot_indexable": true, "report_reasons": null, "author": "Dron22", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155koim/question_about_using_macrium_reflect_to_clone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155koim/question_about_using_macrium_reflect_to_clone/", "subreddit_subscribers": 693549, "created_utc": 1689937142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey!\n\nBasically I have a home lab and have been using some external and internal drives\n\nThe problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.\n\nI've been investigating and learned about drives, bays and enclosures but I cannot understand them correctly\n\nWhich one is good for my use case? I want something that I can connect my drives to, like centralized\n\nI don't think I want synology or anything like that since that has a server which I don't need (have a mini pc)\n\nIf this is the wrong sub, please point me towards a more suitable one\n\nThanks!", "author_fullname": "t2_3f7jceck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about drives and how to scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_155xznl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689968560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Basically I have a home lab and have been using some external and internal drives&lt;/p&gt;\n\n&lt;p&gt;The problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been investigating and learned about drives, bays and enclosures but I cannot understand them correctly&lt;/p&gt;\n\n&lt;p&gt;Which one is good for my use case? I want something that I can connect my drives to, like centralized&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think I want synology or anything like that since that has a server which I don&amp;#39;t need (have a mini pc)&lt;/p&gt;\n\n&lt;p&gt;If this is the wrong sub, please point me towards a more suitable one&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155xznl", "is_robot_indexable": true, "report_reasons": null, "author": "Rafa130397", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "subreddit_subscribers": 693549, "created_utc": 1689968560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "via live usb, did:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img\n\nTime having passed, looking for recommends to:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img\n\n... compare w7[a-b].img &amp; detect which if any files were deleted, added, changed; preferably as discrete operations.", "author_fullname": "t2_dfujb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compare Subsequent Full Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155vc6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689962536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;via live usb, did:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img&lt;/p&gt;\n\n&lt;p&gt;Time having passed, looking for recommends to:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img&lt;/p&gt;\n\n&lt;p&gt;... compare w7[a-b].img &amp;amp; detect which if any files were deleted, added, changed; preferably as discrete operations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155vc6z", "is_robot_indexable": true, "report_reasons": null, "author": "zombi-roboto", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "subreddit_subscribers": 693549, "created_utc": 1689962536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After about 6 months of run time, one of my [WD HC530 14 TB HDDs](https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd) is/was reporting `Current_Pending_Sector` errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).\n\nThis drive is still under serverpartdeal's warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal's RMA process:\n\n&gt; Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.\n\nThank you!\n\nHere is the latest SMART report for the drive in question:\n\n\tsmartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\n\tCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n\t=== START OF INFORMATION SECTION ===\n\tModel Family:     Western Digital Ultrastar DC HC530\n\tDevice Model:     WDC  WUH721414ALE6L4\n\tSerial Number:    9JHDH26T\n\tLU WWN Device Id: 5 000cca 258d3c488\n\tFirmware Version: LDGNW2L0\n\tUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\n\tSector Sizes:     512 bytes logical, 4096 bytes physical\n\tRotation Rate:    7200 rpm\n\tForm Factor:      3.5 inches\n\tDevice is:        In smartctl database 7.3/5440\n\tATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\n\tSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\n\tLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\n\tSMART support is: Available - device has SMART capability.\n\tSMART support is: Enabled\n\tAAM feature is:   Unavailable\n\tAPM feature is:   Disabled\n\tRd look-ahead is: Enabled\n\tWrite cache is:   Enabled\n\tDSN feature is:   Unavailable\n\tATA Security is:  Disabled, frozen [SEC2]\n\tWt Cache Reorder: Enabled\n\n\t=== START OF READ SMART DATA SECTION ===\n\tSMART overall-health self-assessment test result: PASSED\n\n\tGeneral SMART Values:\n\tOffline data collection status:  (0x84)\tOffline data collection activity\n\t\t\t\t\t\twas suspended by an interrupting command from host.\n\t\t\t\t\t\tAuto Offline Data Collection: Enabled.\n\tSelf-test execution status:      ( 241)\tSelf-test routine in progress...\n\t\t\t\t\t\t10% of test remaining.\n\tTotal time to complete Offline \n\tdata collection: \t\t(  101) seconds.\n\tOffline data collection\n\tcapabilities: \t\t\t (0x5b) SMART execute Offline immediate.\n\t\t\t\t\t\tAuto Offline data collection on/off support.\n\t\t\t\t\t\tSuspend Offline collection upon new\n\t\t\t\t\t\tcommand.\n\t\t\t\t\t\tOffline surface scan supported.\n\t\t\t\t\t\tSelf-test supported.\n\t\t\t\t\t\tNo Conveyance Self-test supported.\n\t\t\t\t\t\tSelective Self-test supported.\n\tSMART capabilities:            (0x0003)\tSaves SMART data before entering\n\t\t\t\t\t\tpower-saving mode.\n\t\t\t\t\t\tSupports SMART auto save timer.\n\tError logging capability:        (0x01)\tError logging supported.\n\t\t\t\t\t\tGeneral Purpose Logging supported.\n\tShort self-test routine \n\trecommended polling time: \t (   2) minutes.\n\tExtended self-test routine\n\trecommended polling time: \t (1434) minutes.\n\tSCT capabilities: \t       (0x003d)\tSCT Status supported.\n\t\t\t\t\t\tSCT Error Recovery Control supported.\n\t\t\t\t\t\tSCT Feature Control supported.\n\t\t\t\t\t\tSCT Data Table supported.\n\n\tSMART Attributes Data Structure revision number: 16\n\tVendor Specific SMART Attributes with Thresholds:\n\tID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n\t  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n\t  2 Throughput_Performance  P-S---   137   137   054    -    92\n\t  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n\t  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n\t  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n\t  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n\t  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n\t  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n\t 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n\t 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n\t 22 Helium_Level            PO---K   100   100   025    -    100\n\t192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n\t193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n\t194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n\t196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n\t197 Current_Pending_Sector  -O---K   100   100   000    -    0\n\t198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n\t199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n\t\t\t\t\t\t\t\t||||||_ K auto-keep\n\t\t\t\t\t\t\t\t|||||__ C event count\n\t\t\t\t\t\t\t\t||||___ R error rate\n\t\t\t\t\t\t\t\t|||____ S speed/performance\n\t\t\t\t\t\t\t\t||_____ O updated online\n\t\t\t\t\t\t\t\t|______ P prefailure warning\n\n\tGeneral Purpose Log Directory Version 1\n\tSMART           Log Directory Version 1 [multi-sector log support]\n\tAddress    Access  R/W   Size  Description\n\t0x00       GPL,SL  R/O      1  Log Directory\n\t0x01           SL  R/O      1  Summary SMART error log\n\t0x02           SL  R/O      1  Comprehensive SMART error log\n\t0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n\t0x04       GPL     R/O    256  Device Statistics log\n\t0x04       SL      R/O    255  Device Statistics log\n\t0x06           SL  R/O      1  SMART self-test log\n\t0x07       GPL     R/O      1  Extended self-test log\n\t0x08       GPL     R/O      2  Power Conditions log\n\t0x09           SL  R/W      1  Selective self-test log\n\t0x0c       GPL     R/O   5501  Pending Defects log\n\t0x10       GPL     R/O      1  NCQ Command Error log\n\t0x11       GPL     R/O      1  SATA Phy Event Counters log\n\t0x12       GPL     R/O      1  SATA NCQ Non-Data log\n\t0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n\t0x15       GPL     R/W      1  Rebuild Assist log\n\t0x21       GPL     R/O      1  Write stream error log\n\t0x22       GPL     R/O      1  Read stream error log\n\t0x24       GPL     R/O    256  Current Device Internal Status Data log\n\t0x25       GPL     R/O    256  Saved Device Internal Status Data log\n\t0x2f       GPL     -        1  Set Sector Configuration\n\t0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n\t0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n\t0xe0       GPL,SL  R/W      1  SCT Command/Status\n\t0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\n\tSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\n\tDevice Error Count: 6 (device log contains only the most recent 4 errors)\n\t\tCR     = Command Register\n\t\tFEATR  = Features Register\n\t\tCOUNT  = Count (was: Sector Count) Register\n\t\tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n\t\tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n\t\tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n\t\tLL     = LBA Low (was: Sector Number) Register     ]\n\t\tDV     = Device (was: Device/Head) Register\n\t\tDC     = Device Control Register\n\t\tER     = Error register\n\t\tST     = Status register\n\tPowered_Up_Time is measured from power on, and printed as\n\tDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n\tSS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n\n\tError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n\t  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\n\tError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n\t  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n\t  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\n\tError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n\t  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\n\tError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n\t  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n\t  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\n\tSMART Extended Self-test Log Version: 1 (1 sectors)\n\tNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n\t# 1  Short offline       Completed without error       00%      4393         -\n\t# 2  Short offline       Completed: read failure       90%      4364         27015749936\n\t# 3  Short offline       Completed: read failure       10%      4344         27015749936\n\t# 4  Short offline       Completed: read failure       90%      4342         27015749936\n\t# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 7  Short offline       Completed without error       00%         0         -\n\n\tSMART Selective self-test log data structure revision number 1\n\t SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n\t\t1        0        0  Not_testing\n\t\t2        0        0  Not_testing\n\t\t3        0        0  Not_testing\n\t\t4        0        0  Not_testing\n\t\t5        0        0  Not_testing\n\tSelective self-test flags (0x0):\n\t  After scanning selected spans, do NOT read-scan remainder of disk.\n\tIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\n\tSCT Status Version:                  3\n\tSCT Version (vendor specific):       256 (0x0100)\n\tDevice State:                        DST executing in background (3)\n\tCurrent Temperature:                    36 Celsius\n\tPower Cycle Min/Max Temperature:     33/39 Celsius\n\tLifetime    Min/Max Temperature:     14/49 Celsius\n\tUnder/Over Temperature Limit Count:   0/0\n\tSMART Status:                        0xc24f (PASSED)\n\tMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\n\tSCT Temperature History Version:     2\n\tTemperature Sampling Period:         1 minute\n\tTemperature Logging Interval:        1 minute\n\tMin/Max recommended Temperature:      0/60 Celsius\n\tMin/Max Temperature Limit:           -40/70 Celsius\n\tTemperature History Size (Index):    128 (74)\n\n\tIndex    Estimated Time   Temperature Celsius\n\t  75    2023-07-21 11:16    36  *****************\n\t ...    ..(126 skipped).    ..  *****************\n\t  74    2023-07-21 13:23    36  *****************\n\n\tSCT Error Recovery Control:\n\t\t\t   Read: Disabled\n\t\t\t  Write: Disabled\n\n\tDevice Statistics (GP Log 0x04)\n\tPage  Offset Size        Value Flags Description\n\t0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n\t0x01  0x008  4              43  ---  Lifetime Power-On Resets\n\t0x01  0x010  4            4490  ---  Power-on Hours\n\t0x01  0x018  6     19732537033  ---  Logical Sectors Written\n\t0x01  0x020  6        67404062  ---  Number of Write Commands\n\t0x01  0x028  6    486533164956  ---  Logical Sectors Read\n\t0x01  0x030  6      2489352044  ---  Number of Read Commands\n\t0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n\t0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n\t0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n\t0x03  0x010  4            4470  ---  Head Flying Hours\n\t0x03  0x018  4             195  ---  Head Load Events\n\t0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n\t0x03  0x028  4              75  ---  Read Recovery Attempts\n\t0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n\t0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n\t0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n\t0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n\t0x04  0x018  4               0  ---  Physical Element Status Changed\n\t0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n\t0x05  0x008  1              36  ---  Current Temperature\n\t0x05  0x010  1              36  N--  Average Short Term Temperature\n\t0x05  0x018  1              35  N--  Average Long Term Temperature\n\t0x05  0x020  1              49  ---  Highest Temperature\n\t0x05  0x028  1              14  ---  Lowest Temperature\n\t0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n\t0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n\t0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n\t0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n\t0x05  0x050  4               0  ---  Time in Over-Temperature\n\t0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n\t0x05  0x060  4               0  ---  Time in Under-Temperature\n\t0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n\t0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n\t0x06  0x008  4             205  ---  Number of Hardware Resets\n\t0x06  0x010  4              44  ---  Number of ASR Events\n\t0x06  0x018  4               0  ---  Number of Interface CRC Errors\n\t0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n\t\t\t\t\t\t\t\t\t|||_ C monitored condition met\n\t\t\t\t\t\t\t\t\t||__ D supports DSN\n\t\t\t\t\t\t\t\t\t|___ N normalized value\n\n\tPending Defects log (GP Log 0x0c)\n\tNo Defects Logged\n\n\tSATA Phy Event Counters (GP Log 0x11)\n\tID      Size     Value  Description\n\t0x0001  2            0  Command failed due to ICRC error\n\t0x0002  2            1  R_ERR response for data FIS\n\t0x0003  2            1  R_ERR response for device-to-host data FIS\n\t0x0004  2            0  R_ERR response for host-to-device data FIS\n\t0x0005  2            0  R_ERR response for non-data FIS\n\t0x0006  2            0  R_ERR response for device-to-host non-data FIS\n\t0x0007  2            0  R_ERR response for host-to-device non-data FIS\n\t0x0008  2            0  Device-to-host non-data FIS retries\n\t0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n\t0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n\t0x000b  2            0  CRC errors within host-to-device FIS\n\t0x000d  2            0  Non-CRC errors within host-to-device FIS", "author_fullname": "t2_4t3a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Failing Drive? ServerPartDeals Return Process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155ugtf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689960767.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After about 6 months of run time, one of my &lt;a href=\"https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd\"&gt;WD HC530 14 TB HDDs&lt;/a&gt; is/was reporting &lt;code&gt;Current_Pending_Sector&lt;/code&gt; errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).&lt;/p&gt;\n\n&lt;p&gt;This drive is still under serverpartdeal&amp;#39;s warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal&amp;#39;s RMA process:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;Here is the latest SMART report for the drive in question:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Western Digital Ultrastar DC HC530\nDevice Model:     WDC  WUH721414ALE6L4\nSerial Number:    9JHDH26T\nLU WWN Device Id: 5 000cca 258d3c488\nFirmware Version: LDGNW2L0\nUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        In smartctl database 7.3/5440\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Unavailable\nAPM feature is:   Disabled\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, frozen [SEC2]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x84) Offline data collection activity\n                    was suspended by an interrupting command from host.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      ( 241) Self-test routine in progress...\n                    10% of test remaining.\nTotal time to complete Offline \ndata collection:        (  101) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    (1434) minutes.\nSCT capabilities:          (0x003d) SCT Status supported.\n                    SCT Error Recovery Control supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n  2 Throughput_Performance  P-S---   137   137   054    -    92\n  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n 22 Helium_Level            PO---K   100   100   025    -    100\n192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n197 Current_Pending_Sector  -O---K   100   100   000    -    0\n198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      1  Comprehensive SMART error log\n0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n0x04       GPL     R/O    256  Device Statistics log\n0x04       SL      R/O    255  Device Statistics log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      1  Extended self-test log\n0x08       GPL     R/O      2  Power Conditions log\n0x09           SL  R/W      1  Selective self-test log\n0x0c       GPL     R/O   5501  Pending Defects log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x12       GPL     R/O      1  SATA NCQ Non-Data log\n0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n0x15       GPL     R/W      1  Rebuild Assist log\n0x21       GPL     R/O      1  Write stream error log\n0x22       GPL     R/O      1  Read stream error log\n0x24       GPL     R/O    256  Current Device Internal Status Data log\n0x25       GPL     R/O    256  Saved Device Internal Status Data log\n0x2f       GPL     -        1  Set Sector Configuration\n0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\nDevice Error Count: 6 (device log contains only the most recent 4 errors)\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\nError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\nError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\nError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%      4393         -\n# 2  Short offline       Completed: read failure       90%      4364         27015749936\n# 3  Short offline       Completed: read failure       10%      4344         27015749936\n# 4  Short offline       Completed: read failure       90%      4342         27015749936\n# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n# 7  Short offline       Completed without error       00%         0         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  3\nSCT Version (vendor specific):       256 (0x0100)\nDevice State:                        DST executing in background (3)\nCurrent Temperature:                    36 Celsius\nPower Cycle Min/Max Temperature:     33/39 Celsius\nLifetime    Min/Max Temperature:     14/49 Celsius\nUnder/Over Temperature Limit Count:   0/0\nSMART Status:                        0xc24f (PASSED)\nMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         1 minute\nTemperature Logging Interval:        1 minute\nMin/Max recommended Temperature:      0/60 Celsius\nMin/Max Temperature Limit:           -40/70 Celsius\nTemperature History Size (Index):    128 (74)\n\nIndex    Estimated Time   Temperature Celsius\n  75    2023-07-21 11:16    36  *****************\n ...    ..(126 skipped).    ..  *****************\n  74    2023-07-21 13:23    36  *****************\n\nSCT Error Recovery Control:\n           Read: Disabled\n          Write: Disabled\n\nDevice Statistics (GP Log 0x04)\nPage  Offset Size        Value Flags Description\n0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n0x01  0x008  4              43  ---  Lifetime Power-On Resets\n0x01  0x010  4            4490  ---  Power-on Hours\n0x01  0x018  6     19732537033  ---  Logical Sectors Written\n0x01  0x020  6        67404062  ---  Number of Write Commands\n0x01  0x028  6    486533164956  ---  Logical Sectors Read\n0x01  0x030  6      2489352044  ---  Number of Read Commands\n0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n0x03  0x010  4            4470  ---  Head Flying Hours\n0x03  0x018  4             195  ---  Head Load Events\n0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n0x03  0x028  4              75  ---  Read Recovery Attempts\n0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n0x04  0x018  4               0  ---  Physical Element Status Changed\n0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n0x05  0x008  1              36  ---  Current Temperature\n0x05  0x010  1              36  N--  Average Short Term Temperature\n0x05  0x018  1              35  N--  Average Long Term Temperature\n0x05  0x020  1              49  ---  Highest Temperature\n0x05  0x028  1              14  ---  Lowest Temperature\n0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n0x05  0x050  4               0  ---  Time in Over-Temperature\n0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n0x05  0x060  4               0  ---  Time in Under-Temperature\n0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n0x06  0x008  4             205  ---  Number of Hardware Resets\n0x06  0x010  4              44  ---  Number of ASR Events\n0x06  0x018  4               0  ---  Number of Interface CRC Errors\n0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n                                |||_ C monitored condition met\n                                ||__ D supports DSN\n                                |___ N normalized value\n\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  2            0  Command failed due to ICRC error\n0x0002  2            1  R_ERR response for data FIS\n0x0003  2            1  R_ERR response for device-to-host data FIS\n0x0004  2            0  R_ERR response for host-to-device data FIS\n0x0005  2            0  R_ERR response for non-data FIS\n0x0006  2            0  R_ERR response for device-to-host non-data FIS\n0x0007  2            0  R_ERR response for host-to-device non-data FIS\n0x0008  2            0  Device-to-host non-data FIS retries\n0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n0x000b  2            0  CRC errors within host-to-device FIS\n0x000d  2            0  Non-CRC errors within host-to-device FIS\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?auto=webp&amp;s=fb62a62917a4cd681adfb2ed102d12b3bffec423", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e680cc615c3529565359d358068446dc7a749710", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4cfc8914fb31e963b8c976aa90a4c5dc0bee375", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=391fbea0666df1013ebe77e5378d9f0a44d734d8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec56cfb30b2d32af9c4e3eed80d13ab5050b0c85", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09986a2c305d87e6ce0985806f324daf8a8eef2c", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=294710c01d4d85ae44879409901f4555b571eb26", "width": 1080, "height": 1080}], "variants": {}, "id": "BM5_QdAyNCLTMrgIxGNUsuyqSJL4D1S83f-YL0tXUE0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155ugtf", "is_robot_indexable": true, "report_reasons": null, "author": "halexh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "subreddit_subscribers": 693549, "created_utc": 1689960560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I'm outgrowing again.\n\nHas anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. \n\nSeems a lot easier going forward than a Nas.", "author_fullname": "t2_2sr8ya35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ceph?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155uajf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I&amp;#39;m outgrowing again.&lt;/p&gt;\n\n&lt;p&gt;Has anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. &lt;/p&gt;\n\n&lt;p&gt;Seems a lot easier going forward than a Nas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155uajf", "is_robot_indexable": true, "report_reasons": null, "author": "Frewtti", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155uajf/ceph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155uajf/ceph/", "subreddit_subscribers": 693549, "created_utc": 1689960160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of any good 2D/3D file visualizers for Windows?\n\nI\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. \n\nWinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.\n\nAnyone know of anything similar?", "author_fullname": "t2_zxdz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual file visualizer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155quin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689952476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of any good 2D/3D file visualizers for Windows?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. &lt;/p&gt;\n\n&lt;p&gt;WinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.&lt;/p&gt;\n\n&lt;p&gt;Anyone know of anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155quin", "is_robot_indexable": true, "report_reasons": null, "author": "CCMadman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "subreddit_subscribers": 693549, "created_utc": 1689952476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a simple Linux i5 pc build with 6 hdds. I\u2019d like to upgrade to something more made for a server rack, with ECC ram, able to run high quality 4k and the ability to install many more hdds. Any guides or advice would be massively appreciated, I know a good amount about pc building and parts but almost nothing about the server rack components or what I\u2019d need, thank you!", "author_fullname": "t2_7djm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plex server suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155q8rx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689951152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a simple Linux i5 pc build with 6 hdds. I\u2019d like to upgrade to something more made for a server rack, with ECC ram, able to run high quality 4k and the ability to install many more hdds. Any guides or advice would be massively appreciated, I know a good amount about pc building and parts but almost nothing about the server rack components or what I\u2019d need, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155q8rx", "is_robot_indexable": true, "report_reasons": null, "author": "Darkstranger111", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155q8rx/plex_server_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155q8rx/plex_server_suggestions/", "subreddit_subscribers": 693549, "created_utc": 1689951152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure this question or a slight variation on it has been asked a bunch of times... I don't see any recent posts on it tho so here goes.\n\nI'm making a documentary that currently represents about 25TB of data, most of it is the 6k red raw files for interviews.\n\nMy current backup system:\n\n2 x OWC 28TB Gemini drives. 1 of them is the active media drive and the other is the backup drive that is auto backed up every hour using Arq.\n\n1 x OWC 30TB drive that is kept in another location. This one is brought to the edit and backed up to via Arq about twice a month.\n\nThat's it!! So far... We have spent a lot of money on this show and I want to be sure that we don't lose the footage.\n\nWe have 3 more intvs and tons of new archival to add in the coming months so will prob add 5-6 more TBs of data, making the entire show about 30TBs total.\n\nMy ISP is very meh on site and so I don't think cloud is an option for 25TBs.\n\nI've been looking at synology and qnap but I am not 100% certain of what they are capable of and what might be best for me.\n\nShould I just buy another OWC drive, back up to it and send it to a 3rd location?\n\nI'm an editor, not a storage/NAS person, but I've been tasked with this responsibility solely... Can r/datahoarders help me not fail?!\n\nThx!", "author_fullname": "t2_fxk5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Documentary Data Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155mmy0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689942502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure this question or a slight variation on it has been asked a bunch of times... I don&amp;#39;t see any recent posts on it tho so here goes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making a documentary that currently represents about 25TB of data, most of it is the 6k red raw files for interviews.&lt;/p&gt;\n\n&lt;p&gt;My current backup system:&lt;/p&gt;\n\n&lt;p&gt;2 x OWC 28TB Gemini drives. 1 of them is the active media drive and the other is the backup drive that is auto backed up every hour using Arq.&lt;/p&gt;\n\n&lt;p&gt;1 x OWC 30TB drive that is kept in another location. This one is brought to the edit and backed up to via Arq about twice a month.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s it!! So far... We have spent a lot of money on this show and I want to be sure that we don&amp;#39;t lose the footage.&lt;/p&gt;\n\n&lt;p&gt;We have 3 more intvs and tons of new archival to add in the coming months so will prob add 5-6 more TBs of data, making the entire show about 30TBs total.&lt;/p&gt;\n\n&lt;p&gt;My ISP is very meh on site and so I don&amp;#39;t think cloud is an option for 25TBs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at synology and qnap but I am not 100% certain of what they are capable of and what might be best for me.&lt;/p&gt;\n\n&lt;p&gt;Should I just buy another OWC drive, back up to it and send it to a 3rd location?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an editor, not a storage/NAS person, but I&amp;#39;ve been tasked with this responsibility solely... Can &lt;a href=\"/r/datahoarders\"&gt;r/datahoarders&lt;/a&gt; help me not fail?!&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155mmy0", "is_robot_indexable": true, "report_reasons": null, "author": "esboardnewb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155mmy0/help_with_documentary_data_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155mmy0/help_with_documentary_data_management/", "subreddit_subscribers": 693549, "created_utc": 1689942502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, not sure if this is the correct place to post this but I recently purchased a new Seagate Exos hard drive on Newegg with a 5 yr warranty in May 2023. When I checked the serial on the Seagate website, it says that it expires in 2027 (4 years from purchase date). No worries, I contacted Seagate through their support chat system and they told me that they would update the warranty within 24-48 hrs. \n\nI checked the warranty website 48 hrs later and it wasn\u2019t updated. No worries, I understand that sometimes support tickets get lost. I chat again and again they tell me wait 24-48 hrs. I wait 48 hrs and it still doesn\u2019t update. I am now on my 4th time chatting with their support team and got the same thing. I\u2019ll wait and see if it goes through this time but posting here to ask for advice on how to better navigate this. \n\nI know sometimes there is resistance from Seagate updating warranties to start when a drive is purchased because of OEM drives but whenever I chat with their support team they never bring this up. They always say \u2018yes we will update the warranty to reflect your purchase date\u2019. \n\nObviously I am frustrated. Any advice?", "author_fullname": "t2_edxvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problems Updating Seagate Warranty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155dnfu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689914722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, not sure if this is the correct place to post this but I recently purchased a new Seagate Exos hard drive on Newegg with a 5 yr warranty in May 2023. When I checked the serial on the Seagate website, it says that it expires in 2027 (4 years from purchase date). No worries, I contacted Seagate through their support chat system and they told me that they would update the warranty within 24-48 hrs. &lt;/p&gt;\n\n&lt;p&gt;I checked the warranty website 48 hrs later and it wasn\u2019t updated. No worries, I understand that sometimes support tickets get lost. I chat again and again they tell me wait 24-48 hrs. I wait 48 hrs and it still doesn\u2019t update. I am now on my 4th time chatting with their support team and got the same thing. I\u2019ll wait and see if it goes through this time but posting here to ask for advice on how to better navigate this. &lt;/p&gt;\n\n&lt;p&gt;I know sometimes there is resistance from Seagate updating warranties to start when a drive is purchased because of OEM drives but whenever I chat with their support team they never bring this up. They always say \u2018yes we will update the warranty to reflect your purchase date\u2019. &lt;/p&gt;\n\n&lt;p&gt;Obviously I am frustrated. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155dnfu", "is_robot_indexable": true, "report_reasons": null, "author": "linjsph", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155dnfu/problems_updating_seagate_warranty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155dnfu/problems_updating_seagate_warranty/", "subreddit_subscribers": 693549, "created_utc": 1689914722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Google says 3-5 years but that seems very low. Assuming you have a hard drive strictly to backup family photos and only use it a few times a year and it\u2019s safely kept in a drawer shouldn\u2019t it last much longer?", "author_fullname": "t2_v6avn3hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long will external hard drive last?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155a5bt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689904424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google says 3-5 years but that seems very low. Assuming you have a hard drive strictly to backup family photos and only use it a few times a year and it\u2019s safely kept in a drawer shouldn\u2019t it last much longer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155a5bt", "is_robot_indexable": true, "report_reasons": null, "author": "mike4674", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155a5bt/how_long_will_external_hard_drive_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155a5bt/how_long_will_external_hard_drive_last/", "subreddit_subscribers": 693549, "created_utc": 1689904424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I see a lot of comments for backup software saying \"just use rsync!\" but from what I can tell, rsync only syncs files from one place to another, and doesn't act as a backup in situations like user deleting or modifying file by accident, original file corruption, ransomware, etc.  Does anyone have a software they really like that has some sort of version history / restore point feature, so that if a file has a problem and it gets synced before you notice it, you can get a previous version?  \n\nI'm using windows on my main server and OMV on my diy NAS, but I can run linux in a VM if necessary.  Bonus point for a free software solution, but I'm open to paying a one-time fee for something great.  Not interested in anything subscription / cloud based.\n\nThanks for any help!", "author_fullname": "t2_11hqze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Favorite backup software with version history / restore points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1552rg0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689886001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of comments for backup software saying &amp;quot;just use rsync!&amp;quot; but from what I can tell, rsync only syncs files from one place to another, and doesn&amp;#39;t act as a backup in situations like user deleting or modifying file by accident, original file corruption, ransomware, etc.  Does anyone have a software they really like that has some sort of version history / restore point feature, so that if a file has a problem and it gets synced before you notice it, you can get a previous version?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using windows on my main server and OMV on my diy NAS, but I can run linux in a VM if necessary.  Bonus point for a free software solution, but I&amp;#39;m open to paying a one-time fee for something great.  Not interested in anything subscription / cloud based.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1552rg0", "is_robot_indexable": true, "report_reasons": null, "author": "Illeazar", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1552rg0/favorite_backup_software_with_version_history/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1552rg0/favorite_backup_software_with_version_history/", "subreddit_subscribers": 693549, "created_utc": 1689886001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I watched a review of the D4-300 and the guy showed that there is a fifth sata connector inside the enclosure. I guess Terramaster uses the same board for the D4-300 and D5-300.\n\nDoes anyone know if that fifth sata slot is usable or is it disabled in the firmware?", "author_fullname": "t2_3b8v43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Terramaster D4-300 fifth drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1552g68", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689885329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I watched a review of the D4-300 and the guy showed that there is a fifth sata connector inside the enclosure. I guess Terramaster uses the same board for the D4-300 and D5-300.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know if that fifth sata slot is usable or is it disabled in the firmware?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1552g68", "is_robot_indexable": true, "report_reasons": null, "author": "Lokkjeh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1552g68/terramaster_d4300_fifth_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1552g68/terramaster_d4300_fifth_drive/", "subreddit_subscribers": 693549, "created_utc": 1689885329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve 5x4Tb and 5x10 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.\n\nThe biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) \n\nI\u2019d like also to backup some of my kids data on it.\n\nWhat would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.\n\nThe OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.\n \nOne solution proposed was to make a ZFS layout like this: 2x4TB+2x10TB in pool, mirrored to another 2x4TB+10TB set. That would mean I will keep the remaining 4TB and 10TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.\n\nI am open to ideas!", "author_fullname": "t2_5jccxw9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help laying out hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_155wd6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689964927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve 5x4Tb and 5x10 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.&lt;/p&gt;\n\n&lt;p&gt;The biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) &lt;/p&gt;\n\n&lt;p&gt;I\u2019d like also to backup some of my kids data on it.&lt;/p&gt;\n\n&lt;p&gt;What would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.&lt;/p&gt;\n\n&lt;p&gt;The OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.&lt;/p&gt;\n\n&lt;p&gt;One solution proposed was to make a ZFS layout like this: 2x4TB+2x10TB in pool, mirrored to another 2x4TB+10TB set. That would mean I will keep the remaining 4TB and 10TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.&lt;/p&gt;\n\n&lt;p&gt;I am open to ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155wd6z", "is_robot_indexable": true, "report_reasons": null, "author": "no-dupe", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "subreddit_subscribers": 693549, "created_utc": 1689964927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a folder with thousands of files with names that names can sometimes look alike or have different extensions, like:\n\n\\- filename.txt  \n\\- filename.pdf  \n\\- file-name.txt\n\nThe three files above are the same file **but the checksum and file size are all different**.\n\nIs there a way to check and display all files with very similar names and I will check manually and delete the ones I don't want?\n\nI would love something like Czkawka where it checks for similar images, displays them and then, I can delete the similar ones or check them one by one...\n\nThank you!", "author_fullname": "t2_vc7clehs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to check very similar file names on Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155in2y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689931050.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689930726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a folder with thousands of files with names that names can sometimes look alike or have different extensions, like:&lt;/p&gt;\n\n&lt;p&gt;- filename.txt&lt;br/&gt;\n- filename.pdf&lt;br/&gt;\n- file-name.txt&lt;/p&gt;\n\n&lt;p&gt;The three files above are the same file &lt;strong&gt;but the checksum and file size are all different&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to check and display all files with very similar names and I will check manually and delete the ones I don&amp;#39;t want?&lt;/p&gt;\n\n&lt;p&gt;I would love something like Czkawka where it checks for similar images, displays them and then, I can delete the similar ones or check them one by one...&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155in2y", "is_robot_indexable": true, "report_reasons": null, "author": "Feeling_Usual1541", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155in2y/how_to_check_very_similar_file_names_on_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155in2y/how_to_check_very_similar_file_names_on_windows/", "subreddit_subscribers": 693549, "created_utc": 1689930726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im looking for a cheap / free solution to auto Sync between a remote Windows Machine, an attached External HDD, to TrueNAS and Cloud.\n\nThe Cloud isn't set yet (the current one is only syncing with proprietary Software).   \n End goal here is on demand Sync with a click of a Button.\n\nThe Windows machine isn't managed by me, so I only want read-only from my TrueNAS server.\n\nEnd user isn\u2019t Tech Savvy and the machine isn't always on, so the Backup, should be executable by a 1 click solution.   \nAt least between the Cloud and the External HDD.   \nMirroring to my TrueNAS can be done manually by me.\n\nWhat Software / Cloud Provider do you recommend?\n\nSnapshots would be great, but are optional. (As I sync manually, the chance of ransomware encryption is basically non existent)  \nThe Files are mostly Pictures / Videos but very badly Sorted, meaning very rudimentary Folder Structure.", "author_fullname": "t2_37a4qz81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync between Windows, External HDD, TrueNas and Cloud (500GB - 1TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155431x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689889014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking for a cheap / free solution to auto Sync between a remote Windows Machine, an attached External HDD, to TrueNAS and Cloud.&lt;/p&gt;\n\n&lt;p&gt;The Cloud isn&amp;#39;t set yet (the current one is only syncing with proprietary Software).&lt;br/&gt;\n End goal here is on demand Sync with a click of a Button.&lt;/p&gt;\n\n&lt;p&gt;The Windows machine isn&amp;#39;t managed by me, so I only want read-only from my TrueNAS server.&lt;/p&gt;\n\n&lt;p&gt;End user isn\u2019t Tech Savvy and the machine isn&amp;#39;t always on, so the Backup, should be executable by a 1 click solution.&lt;br/&gt;\nAt least between the Cloud and the External HDD.&lt;br/&gt;\nMirroring to my TrueNAS can be done manually by me.&lt;/p&gt;\n\n&lt;p&gt;What Software / Cloud Provider do you recommend?&lt;/p&gt;\n\n&lt;p&gt;Snapshots would be great, but are optional. (As I sync manually, the chance of ransomware encryption is basically non existent)&lt;br/&gt;\nThe Files are mostly Pictures / Videos but very badly Sorted, meaning very rudimentary Folder Structure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155431x", "is_robot_indexable": true, "report_reasons": null, "author": "offron1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155431x/sync_between_windows_external_hdd_truenas_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155431x/sync_between_windows_external_hdd_truenas_and/", "subreddit_subscribers": 693549, "created_utc": 1689889014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basically I've finally had the dreaded email from google and have 65TB of \"media\" that I need to download within the next 60 days. I'm (obviously) not a fan of spending loads of money in one go, so I was wondering if I could just copy everything over onto newly purchased HDDs and then later down the line have them in a NAS without first having to format them or anything?", "author_fullname": "t2_46i0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When setting up a NAS/Plex Server - can I first get the HDDs and fill them before getting Synology/DIY NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155s5e6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689955344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I&amp;#39;ve finally had the dreaded email from google and have 65TB of &amp;quot;media&amp;quot; that I need to download within the next 60 days. I&amp;#39;m (obviously) not a fan of spending loads of money in one go, so I was wondering if I could just copy everything over onto newly purchased HDDs and then later down the line have them in a NAS without first having to format them or anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155s5e6", "is_robot_indexable": true, "report_reasons": null, "author": "VadimH", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155s5e6/when_setting_up_a_nasplex_server_can_i_first_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155s5e6/when_setting_up_a_nasplex_server_can_i_first_get/", "subreddit_subscribers": 693549, "created_utc": 1689955344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So basically I have 4x3TB drives and 1x12TB drive, all setup in FreeNAS, no mirroring, just one pool (I know I know, I'm trying to fix it), and the pool is nearly full, which is around 20TB.\n\nI just ordered 2 12TB drives, and am planning on switching to UnRaid, as I need more flexibility. My issue though is that I believe I cannot just swap the drives into unraid and it will be happy, and I do not want to lose this data. My plan currently is to just throw the new 12TB drives into my PC, transfer the data from my NAS into those two drives, then setup the existing drives in UnRaid (wiping them), transfer the data from the 12TB drives on my PC into unraid once it's setup, then finally wipe the 12TB drives on my pc, and load them into unraid, with one of them as parity.\n\nI am pretty certain this will work, but I am looking at around a week of continuous data transfer based on my current speeds from a HDD -&gt; NAS(HDD) and vice versa. There are also probably other concerns doing it this way I am unaware of.\n\nAny suggestions on a better way to go about this? Thanks!!!", "author_fullname": "t2_10qx1z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I Move 20TB of Data AND Change OS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155pzww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689950601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So basically I have 4x3TB drives and 1x12TB drive, all setup in FreeNAS, no mirroring, just one pool (I know I know, I&amp;#39;m trying to fix it), and the pool is nearly full, which is around 20TB.&lt;/p&gt;\n\n&lt;p&gt;I just ordered 2 12TB drives, and am planning on switching to UnRaid, as I need more flexibility. My issue though is that I believe I cannot just swap the drives into unraid and it will be happy, and I do not want to lose this data. My plan currently is to just throw the new 12TB drives into my PC, transfer the data from my NAS into those two drives, then setup the existing drives in UnRaid (wiping them), transfer the data from the 12TB drives on my PC into unraid once it&amp;#39;s setup, then finally wipe the 12TB drives on my pc, and load them into unraid, with one of them as parity.&lt;/p&gt;\n\n&lt;p&gt;I am pretty certain this will work, but I am looking at around a week of continuous data transfer based on my current speeds from a HDD -&amp;gt; NAS(HDD) and vice versa. There are also probably other concerns doing it this way I am unaware of.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on a better way to go about this? Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155pzww", "is_robot_indexable": true, "report_reasons": null, "author": "artistbuddy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155pzww/how_do_i_move_20tb_of_data_and_change_os/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155pzww/how_do_i_move_20tb_of_data_and_change_os/", "subreddit_subscribers": 693549, "created_utc": 1689950601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm trying to back up picturea/videos from my trip, about 60-120gb, 2k files from my phone.\n\nMy process\n- from my camera sd card, move all to phone, and overnight backing up some portion to Google drive. \n\nProblem is I often get it randomly stopped, or files are not all uploaded, for example 78 out of 100, and then I need tod delete all and do it all over again as if I i have duplicates? \n\nThanks", "author_fullname": "t2_10tx5blw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why backing up to Google drive is such a pain, what am I doing wrong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155pzgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689950574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to back up picturea/videos from my trip, about 60-120gb, 2k files from my phone.&lt;/p&gt;\n\n&lt;p&gt;My process\n- from my camera sd card, move all to phone, and overnight backing up some portion to Google drive. &lt;/p&gt;\n\n&lt;p&gt;Problem is I often get it randomly stopped, or files are not all uploaded, for example 78 out of 100, and then I need tod delete all and do it all over again as if I i have duplicates? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155pzgm", "is_robot_indexable": true, "report_reasons": null, "author": "-i3arty-", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155pzgm/why_backing_up_to_google_drive_is_such_a_pain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155pzgm/why_backing_up_to_google_drive_is_such_a_pain/", "subreddit_subscribers": 693549, "created_utc": 1689950574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This may not be the perfect forum for this, but I'm giving it a shot. I need help recovering a key file with key information in it that's no longer available to me by any other means. \n\nI've got a company laptop I used once to download a .zip file, which was a personal archive from Facebook (contains .html files and .jpg files, mostly). Once I was done with this archive I deleted it. \n\n&amp;#x200B;\n\nNow I need it back. I cannot run recovery software from the laptop, since I cannot run .exe files without admin credentials. I figured no problem, I'll take the SSD out of the laptop, put it in my PC, and recover the files from there! \n\n&amp;#x200B;\n\nAmazingly, it's finding tons of deleted files, with the exception of anything located in /user/MyUserName/ default windows folders like Documents, Downloads, Pictures, Videos. \n\n&amp;#x200B;\n\nI don't understand. Why is it finding all files on this drive with the exception of the default user folders like Downloads? The files I need recovered are in Downloads. Any ideas? I've given myself permission to these folders on my PC in Windows, have run EaseUS and Recuva in Administrator mode, but they still don't find these default User folders which have my files. \n\n&amp;#x200B;\n\nAny suggestions? ", "author_fullname": "t2_ristggzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble Recovering Data from Work Laptop With Admin Rights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155rm4z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689954183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This may not be the perfect forum for this, but I&amp;#39;m giving it a shot. I need help recovering a key file with key information in it that&amp;#39;s no longer available to me by any other means. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a company laptop I used once to download a .zip file, which was a personal archive from Facebook (contains .html files and .jpg files, mostly). Once I was done with this archive I deleted it. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now I need it back. I cannot run recovery software from the laptop, since I cannot run .exe files without admin credentials. I figured no problem, I&amp;#39;ll take the SSD out of the laptop, put it in my PC, and recover the files from there! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Amazingly, it&amp;#39;s finding tons of deleted files, with the exception of anything located in /user/MyUserName/ default windows folders like Documents, Downloads, Pictures, Videos. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t understand. Why is it finding all files on this drive with the exception of the default user folders like Downloads? The files I need recovered are in Downloads. Any ideas? I&amp;#39;ve given myself permission to these folders on my PC in Windows, have run EaseUS and Recuva in Administrator mode, but they still don&amp;#39;t find these default User folders which have my files. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155rm4z", "is_robot_indexable": true, "report_reasons": null, "author": "TheLibertyOffensive", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155rm4z/trouble_recovering_data_from_work_laptop_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155rm4z/trouble_recovering_data_from_work_laptop_with/", "subreddit_subscribers": 693549, "created_utc": 1689954183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This PSID is required to reset SED to use in NAS, but I am certain it's not print on the white label. Also tried all the number on the label, nothing works. Thanks!", "author_fullname": "t2_mjwm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For WD shucked HDD, where I can find SED PSID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155iaj4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689929614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This PSID is required to reset SED to use in NAS, but I am certain it&amp;#39;s not print on the white label. Also tried all the number on the label, nothing works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155iaj4", "is_robot_indexable": true, "report_reasons": null, "author": "st0n39", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155iaj4/for_wd_shucked_hdd_where_i_can_find_sed_psid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155iaj4/for_wd_shucked_hdd_where_i_can_find_sed_psid/", "subreddit_subscribers": 693549, "created_utc": 1689929614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've had great luck with Exos drives. But I was wondering how the WD in title compares to the X18 Exos platform in terms of reliability and longevity", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos X18 18TB (ST18000NM000J) or WD Ultrastar DC HC550 18TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15594fl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689901519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had great luck with Exos drives. But I was wondering how the WD in title compares to the X18 Exos platform in terms of reliability and longevity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15594fl", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15594fl/seagate_exos_x18_18tb_st18000nm000j_or_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15594fl/seagate_exos_x18_18tb_st18000nm000j_or_wd/", "subreddit_subscribers": 693549, "created_utc": 1689901519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am using PhantomBuster to scrape 5-9k IG profile URLs per day. There is no problem there so far.\n\nHowever, I then want to scrape the info of those profiles (IG id, name, email, etc.) but PhantomBuster does not recommend scrapping more than 10 profiles per day this way. **This is too slow for me.**\n\nAre there any tools that allow me to scrape the info of a list of IG URLs I provide? Which can do so at a higher pace than PhantomBuster?", "author_fullname": "t2_pad7opa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Scrape More Than 10 Instagram Profiles per Day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155oqr1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689947720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using PhantomBuster to scrape 5-9k IG profile URLs per day. There is no problem there so far.&lt;/p&gt;\n\n&lt;p&gt;However, I then want to scrape the info of those profiles (IG id, name, email, etc.) but PhantomBuster does not recommend scrapping more than 10 profiles per day this way. &lt;strong&gt;This is too slow for me.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Are there any tools that allow me to scrape the info of a list of IG URLs I provide? Which can do so at a higher pace than PhantomBuster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155oqr1", "is_robot_indexable": true, "report_reasons": null, "author": "yunnospllrait", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155oqr1/how_to_scrape_more_than_10_instagram_profiles_per/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155oqr1/how_to_scrape_more_than_10_instagram_profiles_per/", "subreddit_subscribers": 693549, "created_utc": 1689947720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nso i know there are ton of tools that do sync but the thing is that i want to know is there a software that can compare 2 folders (folder1, folder2) and rename folder2 files from folder1 if there size is same. Not Copy. i tried FreeFileSync and Syncovery. no luck. i changed over like 20,000 files from folder1 which have about 100 sub folders.\n\n ", "author_fullname": "t2_dsr3fft40", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any software suggestion for syncing 2 folders.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155o8d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689946494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i know there are ton of tools that do sync but the thing is that i want to know is there a software that can compare 2 folders (folder1, folder2) and rename folder2 files from folder1 if there size is same. Not Copy. i tried FreeFileSync and Syncovery. no luck. i changed over like 20,000 files from folder1 which have about 100 sub folders.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155o8d6", "is_robot_indexable": true, "report_reasons": null, "author": "RealThug0005", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155o8d6/any_software_suggestion_for_syncing_2_folders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155o8d6/any_software_suggestion_for_syncing_2_folders/", "subreddit_subscribers": 693549, "created_utc": 1689946494.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}