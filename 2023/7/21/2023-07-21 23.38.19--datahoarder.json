{"kind": "Listing", "data": {"after": "t3_15594fl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Seeking advice, please delete if against the rules.\n\nI want to share a bit of context first, please bare with me.\n\nI've always been frustrated with various bookmark managers and have tried many over the years. I have three main issues:\n\n* Content going offline: I want my bookmark manager to keep a copy of the content since link rot is common. Whether it's a webpage, YouTube video, or tweet thread, I believe I should have the right to keep a copy of what I can access online, regardless of the terms of service.\n\n* Difficulty in retrieval and organization: Adding tags or using specific structures, I ain't got time for that. We went to the moon, computers should be able to automatically extract information like keywords, author name, likes count, etc. And I want to be able to do full text search similar to how searching in Gmail works.\n\n* Discoverability: When I come across something interesting to read, I often can't stop to read it immediately. I want to bookmark it and receive a weekly reminder (like a mailing list) with a mix of unread content and previously checked bookmarks. This way, I can revisit old hidden gems and remove outdated content.\n\nTo address these issues, I set a goal to build a solution that fulfills my requirements. I have a basic, ugly but working prototype that I'm using successfully for my needs. Now, I'm wondering if this problem is unique to me. If I were to make it public, would you use such a tool? If yes, what features would you like to see implemented?\n(If I decide to go forward with this, being able to download a copy of your bookmarks is a non negotiable feature for me)", "author_fullname": "t2_3zmriqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for feedback: data hoarding as a service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155hls7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689929220.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689927295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking advice, please delete if against the rules.&lt;/p&gt;\n\n&lt;p&gt;I want to share a bit of context first, please bare with me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always been frustrated with various bookmark managers and have tried many over the years. I have three main issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Content going offline: I want my bookmark manager to keep a copy of the content since link rot is common. Whether it&amp;#39;s a webpage, YouTube video, or tweet thread, I believe I should have the right to keep a copy of what I can access online, regardless of the terms of service.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Difficulty in retrieval and organization: Adding tags or using specific structures, I ain&amp;#39;t got time for that. We went to the moon, computers should be able to automatically extract information like keywords, author name, likes count, etc. And I want to be able to do full text search similar to how searching in Gmail works.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Discoverability: When I come across something interesting to read, I often can&amp;#39;t stop to read it immediately. I want to bookmark it and receive a weekly reminder (like a mailing list) with a mix of unread content and previously checked bookmarks. This way, I can revisit old hidden gems and remove outdated content.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To address these issues, I set a goal to build a solution that fulfills my requirements. I have a basic, ugly but working prototype that I&amp;#39;m using successfully for my needs. Now, I&amp;#39;m wondering if this problem is unique to me. If I were to make it public, would you use such a tool? If yes, what features would you like to see implemented?\n(If I decide to go forward with this, being able to download a copy of your bookmarks is a non negotiable feature for me)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155hls7", "is_robot_indexable": true, "report_reasons": null, "author": "goodkernel", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155hls7/looking_for_feedback_data_hoarding_as_a_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155hls7/looking_for_feedback_data_hoarding_as_a_service/", "subreddit_subscribers": 693573, "created_utc": 1689927295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dropbox just yesterday limited it's \"unlimited\" subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.\n\nIt is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.\n\nThey had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.\n\nDropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.\n\nI'm not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.\n\nAnyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.", "author_fullname": "t2_milg6ems", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox Business Advanced not unlimited anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155so84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689956480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dropbox just yesterday limited it&amp;#39;s &amp;quot;unlimited&amp;quot; subscriptions to 10TB/week. This seems to be a reaction to people migrating too much data from Google Drive, due to the limit Google is rolling out at the moment as well.&lt;/p&gt;\n\n&lt;p&gt;It is extremely upsetting for a lot of new customers right now, as we asked specifically for the exact amount of data we were planning to migrate and got a green light from Dropbox support and from sales beforehand. We have invested a lot of money on the migration, that is now completely lost.&lt;/p&gt;\n\n&lt;p&gt;They had to see this coming - they knew exactly, why we and many others are moving from Google to Dropbox and they obviously held back the information, that they will not be able to handle that much data.&lt;/p&gt;\n\n&lt;p&gt;Dropbox support and sales are telling us, that this new policy came completely out of the blue for them as well and that this is upsetting for them as well, as it was them who promised all of us a smooth migration. The German sales team had to find out about this limit through their customers and only got the official briefing afterwards.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not even talking about petabytes here. We are a film production and simply need some storage to keep our business running. We had been communicating the amount we needed very openly.&lt;/p&gt;\n\n&lt;p&gt;Anyone here, that is not affected by this? Can anyone give legal advice on this matter? Also thank you for sharing this to try to reach someone at Dropbox for help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155so84", "is_robot_indexable": true, "report_reasons": null, "author": "joshuanicolaspark", "discussion_type": null, "num_comments": 99, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155so84/dropbox_business_advanced_not_unlimited_anymore/", "subreddit_subscribers": 693573, "created_utc": 1689956480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently cloned my hard drive on laptop to a new one using Macrium Reflect, was around 500 GB of data. How reliable can I expect it to be? Like if I now replace the hard disk and use the new one, can I expect it to function exactly like it did before, or are errors common?\n\nI ask because I heard elsewhere that Macrium Reflect cloning works most of the time, but without specifics what can go wrong.", "author_fullname": "t2_pr4df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about using Macrium Reflect to clone hard drive.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155koim", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689937142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently cloned my hard drive on laptop to a new one using Macrium Reflect, was around 500 GB of data. How reliable can I expect it to be? Like if I now replace the hard disk and use the new one, can I expect it to function exactly like it did before, or are errors common?&lt;/p&gt;\n\n&lt;p&gt;I ask because I heard elsewhere that Macrium Reflect cloning works most of the time, but without specifics what can go wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155koim", "is_robot_indexable": true, "report_reasons": null, "author": "Dron22", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155koim/question_about_using_macrium_reflect_to_clone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155koim/question_about_using_macrium_reflect_to_clone/", "subreddit_subscribers": 693573, "created_utc": 1689937142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have hundreds of hours of mini-DV tapes that I need to develop for a documentary I'm making. I did a ton of research and got the necessary firewire cables / adapters, as well as a Canon XL-2 camera and the Lifelix software. Unfortunately, the Lifelix software stops my tapes constantly while importing, thinking that any lag or glitch is the end of the tape. This is unsustainable with the amount of tapes I'm developing, especially because it has a bug when using the Canon camera where you have to click play then start the import in order to read the audio. \n\nQuicktime refuses to save the footage when uploaded and doesn't actually support the native DV format, so that's not an option. iMovie hasn't recorded the audio at all, and Premiere doesn't even detect my camera. I've been working on this for weeks and had initial success with the Lifelix software but it seems to have gotten buggier with use. I'm considering trying with a different camera but was wondering if you guys have any experience or similar issues. \n\n&amp;#x200B;\n\nAppreciate it in advance, thank you ", "author_fullname": "t2_chvo1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to develop mini-DV tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155z60v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689971223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have hundreds of hours of mini-DV tapes that I need to develop for a documentary I&amp;#39;m making. I did a ton of research and got the necessary firewire cables / adapters, as well as a Canon XL-2 camera and the Lifelix software. Unfortunately, the Lifelix software stops my tapes constantly while importing, thinking that any lag or glitch is the end of the tape. This is unsustainable with the amount of tapes I&amp;#39;m developing, especially because it has a bug when using the Canon camera where you have to click play then start the import in order to read the audio. &lt;/p&gt;\n\n&lt;p&gt;Quicktime refuses to save the footage when uploaded and doesn&amp;#39;t actually support the native DV format, so that&amp;#39;s not an option. iMovie hasn&amp;#39;t recorded the audio at all, and Premiere doesn&amp;#39;t even detect my camera. I&amp;#39;ve been working on this for weeks and had initial success with the Lifelix software but it seems to have gotten buggier with use. I&amp;#39;m considering trying with a different camera but was wondering if you guys have any experience or similar issues. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Appreciate it in advance, thank you &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155z60v", "is_robot_indexable": true, "report_reasons": null, "author": "Frankieba", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155z60v/struggling_to_develop_minidv_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155z60v/struggling_to_develop_minidv_tapes/", "subreddit_subscribers": 693573, "created_utc": 1689971223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey!\n\nBasically I have a home lab and have been using some external and internal drives\n\nThe problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.\n\nI've been investigating and learned about drives, bays and enclosures but I cannot understand them correctly\n\nWhich one is good for my use case? I want something that I can connect my drives to, like centralized\n\nI don't think I want synology or anything like that since that has a server which I don't need (have a mini pc)\n\nIf this is the wrong sub, please point me towards a more suitable one\n\nThanks!", "author_fullname": "t2_3f7jceck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about drives and how to scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155xznl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689968560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;Basically I have a home lab and have been using some external and internal drives&lt;/p&gt;\n\n&lt;p&gt;The problem is that each 3.5 drive requires a 12v power connector. So I have been connecting a lot of those.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been investigating and learned about drives, bays and enclosures but I cannot understand them correctly&lt;/p&gt;\n\n&lt;p&gt;Which one is good for my use case? I want something that I can connect my drives to, like centralized&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think I want synology or anything like that since that has a server which I don&amp;#39;t need (have a mini pc)&lt;/p&gt;\n\n&lt;p&gt;If this is the wrong sub, please point me towards a more suitable one&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155xznl", "is_robot_indexable": true, "report_reasons": null, "author": "Rafa130397", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155xznl/question_about_drives_and_how_to_scale/", "subreddit_subscribers": 693573, "created_utc": 1689968560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, not sure if this is the correct place to post this but I recently purchased a new Seagate Exos hard drive on Newegg with a 5 yr warranty in May 2023. When I checked the serial on the Seagate website, it says that it expires in 2027 (4 years from purchase date). No worries, I contacted Seagate through their support chat system and they told me that they would update the warranty within 24-48 hrs. \n\nI checked the warranty website 48 hrs later and it wasn\u2019t updated. No worries, I understand that sometimes support tickets get lost. I chat again and again they tell me wait 24-48 hrs. I wait 48 hrs and it still doesn\u2019t update. I am now on my 4th time chatting with their support team and got the same thing. I\u2019ll wait and see if it goes through this time but posting here to ask for advice on how to better navigate this. \n\nI know sometimes there is resistance from Seagate updating warranties to start when a drive is purchased because of OEM drives but whenever I chat with their support team they never bring this up. They always say \u2018yes we will update the warranty to reflect your purchase date\u2019. \n\nObviously I am frustrated. Any advice?", "author_fullname": "t2_edxvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problems Updating Seagate Warranty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155dnfu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689914722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, not sure if this is the correct place to post this but I recently purchased a new Seagate Exos hard drive on Newegg with a 5 yr warranty in May 2023. When I checked the serial on the Seagate website, it says that it expires in 2027 (4 years from purchase date). No worries, I contacted Seagate through their support chat system and they told me that they would update the warranty within 24-48 hrs. &lt;/p&gt;\n\n&lt;p&gt;I checked the warranty website 48 hrs later and it wasn\u2019t updated. No worries, I understand that sometimes support tickets get lost. I chat again and again they tell me wait 24-48 hrs. I wait 48 hrs and it still doesn\u2019t update. I am now on my 4th time chatting with their support team and got the same thing. I\u2019ll wait and see if it goes through this time but posting here to ask for advice on how to better navigate this. &lt;/p&gt;\n\n&lt;p&gt;I know sometimes there is resistance from Seagate updating warranties to start when a drive is purchased because of OEM drives but whenever I chat with their support team they never bring this up. They always say \u2018yes we will update the warranty to reflect your purchase date\u2019. &lt;/p&gt;\n\n&lt;p&gt;Obviously I am frustrated. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155dnfu", "is_robot_indexable": true, "report_reasons": null, "author": "linjsph", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155dnfu/problems_updating_seagate_warranty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155dnfu/problems_updating_seagate_warranty/", "subreddit_subscribers": 693573, "created_utc": 1689914722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to backup my backups. I can't seem to drag and drop my time machine backups to another external drive or NAS.", "author_fullname": "t2_e0g8qmrsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Machine back ups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_156298q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689978319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to backup my backups. I can&amp;#39;t seem to drag and drop my time machine backups to another external drive or NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "156298q", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Elk3586", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/156298q/time_machine_back_ups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/156298q/time_machine_back_ups/", "subreddit_subscribers": 693573, "created_utc": 1689978319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After about 6 months of run time, one of my [WD HC530 14 TB HDDs](https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd) is/was reporting `Current_Pending_Sector` errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).\n\nThis drive is still under serverpartdeal's warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal's RMA process:\n\n&gt; Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.\n\nThank you!\n\nHere is the latest SMART report for the drive in question:\n\n\tsmartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\n\tCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n\t=== START OF INFORMATION SECTION ===\n\tModel Family:     Western Digital Ultrastar DC HC530\n\tDevice Model:     WDC  WUH721414ALE6L4\n\tSerial Number:    9JHDH26T\n\tLU WWN Device Id: 5 000cca 258d3c488\n\tFirmware Version: LDGNW2L0\n\tUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\n\tSector Sizes:     512 bytes logical, 4096 bytes physical\n\tRotation Rate:    7200 rpm\n\tForm Factor:      3.5 inches\n\tDevice is:        In smartctl database 7.3/5440\n\tATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\n\tSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\n\tLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\n\tSMART support is: Available - device has SMART capability.\n\tSMART support is: Enabled\n\tAAM feature is:   Unavailable\n\tAPM feature is:   Disabled\n\tRd look-ahead is: Enabled\n\tWrite cache is:   Enabled\n\tDSN feature is:   Unavailable\n\tATA Security is:  Disabled, frozen [SEC2]\n\tWt Cache Reorder: Enabled\n\n\t=== START OF READ SMART DATA SECTION ===\n\tSMART overall-health self-assessment test result: PASSED\n\n\tGeneral SMART Values:\n\tOffline data collection status:  (0x84)\tOffline data collection activity\n\t\t\t\t\t\twas suspended by an interrupting command from host.\n\t\t\t\t\t\tAuto Offline Data Collection: Enabled.\n\tSelf-test execution status:      ( 241)\tSelf-test routine in progress...\n\t\t\t\t\t\t10% of test remaining.\n\tTotal time to complete Offline \n\tdata collection: \t\t(  101) seconds.\n\tOffline data collection\n\tcapabilities: \t\t\t (0x5b) SMART execute Offline immediate.\n\t\t\t\t\t\tAuto Offline data collection on/off support.\n\t\t\t\t\t\tSuspend Offline collection upon new\n\t\t\t\t\t\tcommand.\n\t\t\t\t\t\tOffline surface scan supported.\n\t\t\t\t\t\tSelf-test supported.\n\t\t\t\t\t\tNo Conveyance Self-test supported.\n\t\t\t\t\t\tSelective Self-test supported.\n\tSMART capabilities:            (0x0003)\tSaves SMART data before entering\n\t\t\t\t\t\tpower-saving mode.\n\t\t\t\t\t\tSupports SMART auto save timer.\n\tError logging capability:        (0x01)\tError logging supported.\n\t\t\t\t\t\tGeneral Purpose Logging supported.\n\tShort self-test routine \n\trecommended polling time: \t (   2) minutes.\n\tExtended self-test routine\n\trecommended polling time: \t (1434) minutes.\n\tSCT capabilities: \t       (0x003d)\tSCT Status supported.\n\t\t\t\t\t\tSCT Error Recovery Control supported.\n\t\t\t\t\t\tSCT Feature Control supported.\n\t\t\t\t\t\tSCT Data Table supported.\n\n\tSMART Attributes Data Structure revision number: 16\n\tVendor Specific SMART Attributes with Thresholds:\n\tID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n\t  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n\t  2 Throughput_Performance  P-S---   137   137   054    -    92\n\t  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n\t  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n\t  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n\t  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n\t  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n\t  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n\t 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n\t 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n\t 22 Helium_Level            PO---K   100   100   025    -    100\n\t192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n\t193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n\t194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n\t196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n\t197 Current_Pending_Sector  -O---K   100   100   000    -    0\n\t198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n\t199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n\t\t\t\t\t\t\t\t||||||_ K auto-keep\n\t\t\t\t\t\t\t\t|||||__ C event count\n\t\t\t\t\t\t\t\t||||___ R error rate\n\t\t\t\t\t\t\t\t|||____ S speed/performance\n\t\t\t\t\t\t\t\t||_____ O updated online\n\t\t\t\t\t\t\t\t|______ P prefailure warning\n\n\tGeneral Purpose Log Directory Version 1\n\tSMART           Log Directory Version 1 [multi-sector log support]\n\tAddress    Access  R/W   Size  Description\n\t0x00       GPL,SL  R/O      1  Log Directory\n\t0x01           SL  R/O      1  Summary SMART error log\n\t0x02           SL  R/O      1  Comprehensive SMART error log\n\t0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n\t0x04       GPL     R/O    256  Device Statistics log\n\t0x04       SL      R/O    255  Device Statistics log\n\t0x06           SL  R/O      1  SMART self-test log\n\t0x07       GPL     R/O      1  Extended self-test log\n\t0x08       GPL     R/O      2  Power Conditions log\n\t0x09           SL  R/W      1  Selective self-test log\n\t0x0c       GPL     R/O   5501  Pending Defects log\n\t0x10       GPL     R/O      1  NCQ Command Error log\n\t0x11       GPL     R/O      1  SATA Phy Event Counters log\n\t0x12       GPL     R/O      1  SATA NCQ Non-Data log\n\t0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n\t0x15       GPL     R/W      1  Rebuild Assist log\n\t0x21       GPL     R/O      1  Write stream error log\n\t0x22       GPL     R/O      1  Read stream error log\n\t0x24       GPL     R/O    256  Current Device Internal Status Data log\n\t0x25       GPL     R/O    256  Saved Device Internal Status Data log\n\t0x2f       GPL     -        1  Set Sector Configuration\n\t0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n\t0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n\t0xe0       GPL,SL  R/W      1  SCT Command/Status\n\t0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\n\tSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\n\tDevice Error Count: 6 (device log contains only the most recent 4 errors)\n\t\tCR     = Command Register\n\t\tFEATR  = Features Register\n\t\tCOUNT  = Count (was: Sector Count) Register\n\t\tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n\t\tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n\t\tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n\t\tLL     = LBA Low (was: Sector Number) Register     ]\n\t\tDV     = Device (was: Device/Head) Register\n\t\tDC     = Device Control Register\n\t\tER     = Error register\n\t\tST     = Status register\n\tPowered_Up_Time is measured from power on, and printed as\n\tDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n\tSS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n\n\tError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n\t  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\t  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\n\tError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n\t  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n\t  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n\t  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n\t  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\n\tError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n\t  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\t  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\n\tError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n\t  When the command that caused the error occurred, the device was active or idle.\n\n\t  After command completion occurred, registers were:\n\t  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n\t  -- -- -- == -- == == == -- -- -- -- --\n\t  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n\t  Commands leading to the command that caused the error were:\n\t  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n\t  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n\t  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n\t  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n\t  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n\t  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\n\tSMART Extended Self-test Log Version: 1 (1 sectors)\n\tNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n\t# 1  Short offline       Completed without error       00%      4393         -\n\t# 2  Short offline       Completed: read failure       90%      4364         27015749936\n\t# 3  Short offline       Completed: read failure       10%      4344         27015749936\n\t# 4  Short offline       Completed: read failure       90%      4342         27015749936\n\t# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n\t# 7  Short offline       Completed without error       00%         0         -\n\n\tSMART Selective self-test log data structure revision number 1\n\t SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n\t\t1        0        0  Not_testing\n\t\t2        0        0  Not_testing\n\t\t3        0        0  Not_testing\n\t\t4        0        0  Not_testing\n\t\t5        0        0  Not_testing\n\tSelective self-test flags (0x0):\n\t  After scanning selected spans, do NOT read-scan remainder of disk.\n\tIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\n\tSCT Status Version:                  3\n\tSCT Version (vendor specific):       256 (0x0100)\n\tDevice State:                        DST executing in background (3)\n\tCurrent Temperature:                    36 Celsius\n\tPower Cycle Min/Max Temperature:     33/39 Celsius\n\tLifetime    Min/Max Temperature:     14/49 Celsius\n\tUnder/Over Temperature Limit Count:   0/0\n\tSMART Status:                        0xc24f (PASSED)\n\tMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\n\tSCT Temperature History Version:     2\n\tTemperature Sampling Period:         1 minute\n\tTemperature Logging Interval:        1 minute\n\tMin/Max recommended Temperature:      0/60 Celsius\n\tMin/Max Temperature Limit:           -40/70 Celsius\n\tTemperature History Size (Index):    128 (74)\n\n\tIndex    Estimated Time   Temperature Celsius\n\t  75    2023-07-21 11:16    36  *****************\n\t ...    ..(126 skipped).    ..  *****************\n\t  74    2023-07-21 13:23    36  *****************\n\n\tSCT Error Recovery Control:\n\t\t\t   Read: Disabled\n\t\t\t  Write: Disabled\n\n\tDevice Statistics (GP Log 0x04)\n\tPage  Offset Size        Value Flags Description\n\t0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n\t0x01  0x008  4              43  ---  Lifetime Power-On Resets\n\t0x01  0x010  4            4490  ---  Power-on Hours\n\t0x01  0x018  6     19732537033  ---  Logical Sectors Written\n\t0x01  0x020  6        67404062  ---  Number of Write Commands\n\t0x01  0x028  6    486533164956  ---  Logical Sectors Read\n\t0x01  0x030  6      2489352044  ---  Number of Read Commands\n\t0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n\t0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n\t0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n\t0x03  0x010  4            4470  ---  Head Flying Hours\n\t0x03  0x018  4             195  ---  Head Load Events\n\t0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n\t0x03  0x028  4              75  ---  Read Recovery Attempts\n\t0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n\t0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n\t0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n\t0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n\t0x04  0x018  4               0  ---  Physical Element Status Changed\n\t0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n\t0x05  0x008  1              36  ---  Current Temperature\n\t0x05  0x010  1              36  N--  Average Short Term Temperature\n\t0x05  0x018  1              35  N--  Average Long Term Temperature\n\t0x05  0x020  1              49  ---  Highest Temperature\n\t0x05  0x028  1              14  ---  Lowest Temperature\n\t0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n\t0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n\t0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n\t0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n\t0x05  0x050  4               0  ---  Time in Over-Temperature\n\t0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n\t0x05  0x060  4               0  ---  Time in Under-Temperature\n\t0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n\t0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n\t0x06  0x008  4             205  ---  Number of Hardware Resets\n\t0x06  0x010  4              44  ---  Number of ASR Events\n\t0x06  0x018  4               0  ---  Number of Interface CRC Errors\n\t0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n\t\t\t\t\t\t\t\t\t|||_ C monitored condition met\n\t\t\t\t\t\t\t\t\t||__ D supports DSN\n\t\t\t\t\t\t\t\t\t|___ N normalized value\n\n\tPending Defects log (GP Log 0x0c)\n\tNo Defects Logged\n\n\tSATA Phy Event Counters (GP Log 0x11)\n\tID      Size     Value  Description\n\t0x0001  2            0  Command failed due to ICRC error\n\t0x0002  2            1  R_ERR response for data FIS\n\t0x0003  2            1  R_ERR response for device-to-host data FIS\n\t0x0004  2            0  R_ERR response for host-to-device data FIS\n\t0x0005  2            0  R_ERR response for non-data FIS\n\t0x0006  2            0  R_ERR response for device-to-host non-data FIS\n\t0x0007  2            0  R_ERR response for host-to-device non-data FIS\n\t0x0008  2            0  Device-to-host non-data FIS retries\n\t0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n\t0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n\t0x000b  2            0  CRC errors within host-to-device FIS\n\t0x000d  2            0  Non-CRC errors within host-to-device FIS", "author_fullname": "t2_4t3a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Failing Drive? ServerPartDeals Return Process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155ugtf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689960767.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After about 6 months of run time, one of my &lt;a href=\"https://serverpartdeals.com/products/western-digital-ultrastar-dc-hc530-wuh721414ale6l4-0f31284-14tb-7-2k-rpm-sata-6gb-s-512e-512mb-3-5-se-manufacturer-recertified-hdd\"&gt;WD HC530 14 TB HDDs&lt;/a&gt; is/was reporting &lt;code&gt;Current_Pending_Sector&lt;/code&gt; errors. Since then, I have run a short SMART test successfully, and have been running an extended SMART test for the past ~5 days (unsure if this is a normal timeframe or not).&lt;/p&gt;\n\n&lt;p&gt;This drive is still under serverpartdeal&amp;#39;s warranty. I have already opened an RMA for it. However, I am afraid to  return it because I am not certain that it is indeed failing. Wondering if anyone here has any input or advice on how to move forward? If I can run something else besides an extended SMART test to be certain that its failing (that hopefully doesnt take a ~week), that would be great. I have no issue with replacing the drive at the moment. I just dont want to return it, only to find out serverpartdeals found no issue with it. I want to be certain it is indeed on its way out before returning it. The reason for that is because of the following from server part deal&amp;#39;s RMA process:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Please note that all returned drives undergo in-house testing and inspection. Based on the results, we will send the replacement to the address on file.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;Here is the latest SMART report for the drive in question:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;smartctl 7.3 2022-02-28 r5338 [x86_64-linux-6.1.36-Unraid] (local build)\nCopyright (C) 2002-22, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Family:     Western Digital Ultrastar DC HC530\nDevice Model:     WDC  WUH721414ALE6L4\nSerial Number:    9JHDH26T\nLU WWN Device Id: 5 000cca 258d3c488\nFirmware Version: LDGNW2L0\nUser Capacity:    14,000,519,643,136 bytes [14.0 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        In smartctl database 7.3/5440\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Fri Jul 21 13:23:51 2023 EDT\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\nAAM feature is:   Unavailable\nAPM feature is:   Disabled\nRd look-ahead is: Enabled\nWrite cache is:   Enabled\nDSN feature is:   Unavailable\nATA Security is:  Disabled, frozen [SEC2]\nWt Cache Reorder: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x84) Offline data collection activity\n                    was suspended by an interrupting command from host.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      ( 241) Self-test routine in progress...\n                    10% of test remaining.\nTotal time to complete Offline \ndata collection:        (  101) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    (1434) minutes.\nSCT capabilities:          (0x003d) SCT Status supported.\n                    SCT Error Recovery Control supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE\n  1 Raw_Read_Error_Rate     PO-R--   100   100   001    -    0\n  2 Throughput_Performance  P-S---   137   137   054    -    92\n  3 Spin_Up_Time            POS---   084   084   001    -    332 (Average 330)\n  4 Start_Stop_Count        -O--C-   100   100   000    -    46\n  5 Reallocated_Sector_Ct   PO--CK   100   100   001    -    0\n  7 Seek_Error_Rate         PO-R--   100   100   001    -    0\n  8 Seek_Time_Performance   P-S---   128   128   020    -    18\n  9 Power_On_Hours          -O--C-   100   100   000    -    4490\n 10 Spin_Retry_Count        PO--C-   100   100   001    -    0\n 12 Power_Cycle_Count       -O--CK   100   100   000    -    43\n 22 Helium_Level            PO---K   100   100   025    -    100\n192 Power-Off_Retract_Count -O--CK   100   100   000    -    195\n193 Load_Cycle_Count        -O--C-   100   100   000    -    195\n194 Temperature_Celsius     -O----   058   058   000    -    36 (Min/Max 14/49)\n196 Reallocated_Event_Count -O--CK   100   100   000    -    0\n197 Current_Pending_Sector  -O---K   100   100   000    -    0\n198 Offline_Uncorrectable   ---R--   100   100   000    -    0\n199 UDMA_CRC_Error_Count    -O-R--   100   100   000    -    0\n                            ||||||_ K auto-keep\n                            |||||__ C event count\n                            ||||___ R error rate\n                            |||____ S speed/performance\n                            ||_____ O updated online\n                            |______ P prefailure warning\n\nGeneral Purpose Log Directory Version 1\nSMART           Log Directory Version 1 [multi-sector log support]\nAddress    Access  R/W   Size  Description\n0x00       GPL,SL  R/O      1  Log Directory\n0x01           SL  R/O      1  Summary SMART error log\n0x02           SL  R/O      1  Comprehensive SMART error log\n0x03       GPL     R/O      1  Ext. Comprehensive SMART error log\n0x04       GPL     R/O    256  Device Statistics log\n0x04       SL      R/O    255  Device Statistics log\n0x06           SL  R/O      1  SMART self-test log\n0x07       GPL     R/O      1  Extended self-test log\n0x08       GPL     R/O      2  Power Conditions log\n0x09           SL  R/W      1  Selective self-test log\n0x0c       GPL     R/O   5501  Pending Defects log\n0x10       GPL     R/O      1  NCQ Command Error log\n0x11       GPL     R/O      1  SATA Phy Event Counters log\n0x12       GPL     R/O      1  SATA NCQ Non-Data log\n0x13       GPL     R/O      1  SATA NCQ Send and Receive log\n0x15       GPL     R/W      1  Rebuild Assist log\n0x21       GPL     R/O      1  Write stream error log\n0x22       GPL     R/O      1  Read stream error log\n0x24       GPL     R/O    256  Current Device Internal Status Data log\n0x25       GPL     R/O    256  Saved Device Internal Status Data log\n0x2f       GPL     -        1  Set Sector Configuration\n0x30       GPL,SL  R/O      9  IDENTIFY DEVICE data log\n0x80-0x9f  GPL,SL  R/W     16  Host vendor specific log\n0xe0       GPL,SL  R/W      1  SCT Command/Status\n0xe1       GPL,SL  R/W      1  SCT Data Transfer\n\nSMART Extended Comprehensive Error Log Version: 1 (1 sectors)\nDevice Error Count: 6 (device log contains only the most recent 4 errors)\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 6 [1] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 00 00 06 4a 43 9f 30 40 08  4d+00:40:26.202  READ FPDMA QUEUED\n  61 00 08 00 48 00 00 81 29 50 70 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 40 00 00 81 1f 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 38 00 00 81 1e ac 38 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n  61 00 20 00 30 00 00 81 17 06 d8 40 08  4d+00:40:23.289  WRITE FPDMA QUEUED\n\nError 5 [0] occurred at disk power-on lifetime: 4391 hours (182 days + 23 hours)\n  When the command that caused the error occurred, the device was doing SMART Offline or Self-test.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 b8 00 06 4a 43 9f 30 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 05 40 00 f8 00 06 4a 43 b4 b0 40 08  4d+00:40:17.853  READ FPDMA QUEUED\n  60 01 60 00 e0 00 06 4a 43 b3 50 40 08  4d+00:40:17.852  READ FPDMA QUEUED\n  60 01 00 00 d8 00 06 4a 43 b2 50 40 08  4d+00:40:14.958  READ FPDMA QUEUED\n  60 03 60 00 d0 00 06 4a 43 ae f0 40 08  4d+00:40:14.948  READ FPDMA QUEUED\n\nError 4 [3] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 a8 00 06 4a 43 a0 f0 40 08  2d+21:40:35.149  READ FPDMA QUEUED\n  60 02 40 00 a0 00 06 4a 43 a6 30 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 05 40 00 78 00 06 4a 43 a8 70 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 f8 00 70 00 06 4a 43 ad b0 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n  60 03 98 00 68 00 06 4a 43 b1 a8 40 08  2d+21:40:32.252  READ FPDMA QUEUED\n\nError 3 [2] occurred at disk power-on lifetime: 4364 hours (181 days + 20 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  40 -- 43 00 00 00 00 00 00 00 00 00 00  Error: UNC at LBA = 0x00000000 = 0\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  60 05 40 00 60 00 06 4a 43 a0 f0 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 04 b0 00 88 00 06 4a 43 b5 40 40 08  2d+21:40:16.224  READ FPDMA QUEUED\n  60 03 98 00 80 00 06 4a 43 b1 a8 40 08  2d+21:40:13.337  READ FPDMA QUEUED\n  60 03 f8 00 78 00 06 4a 43 ad b0 40 08  2d+21:40:13.336  READ FPDMA QUEUED\n  60 05 40 00 70 00 06 4a 43 a8 70 40 08  2d+21:40:13.331  READ FPDMA QUEUED\n\nSMART Extended Self-test Log Version: 1 (1 sectors)\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%      4393         -\n# 2  Short offline       Completed: read failure       90%      4364         27015749936\n# 3  Short offline       Completed: read failure       10%      4344         27015749936\n# 4  Short offline       Completed: read failure       90%      4342         27015749936\n# 5  Extended offline    Completed: read failure       90%      4342         27015749936\n# 6  Extended offline    Completed: read failure       90%      4342         27015749936\n# 7  Short offline       Completed without error       00%         0         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n\nSCT Status Version:                  3\nSCT Version (vendor specific):       256 (0x0100)\nDevice State:                        DST executing in background (3)\nCurrent Temperature:                    36 Celsius\nPower Cycle Min/Max Temperature:     33/39 Celsius\nLifetime    Min/Max Temperature:     14/49 Celsius\nUnder/Over Temperature Limit Count:   0/0\nSMART Status:                        0xc24f (PASSED)\nMinimum supported ERC Time Limit:    65 (6.5 seconds)\n\nSCT Temperature History Version:     2\nTemperature Sampling Period:         1 minute\nTemperature Logging Interval:        1 minute\nMin/Max recommended Temperature:      0/60 Celsius\nMin/Max Temperature Limit:           -40/70 Celsius\nTemperature History Size (Index):    128 (74)\n\nIndex    Estimated Time   Temperature Celsius\n  75    2023-07-21 11:16    36  *****************\n ...    ..(126 skipped).    ..  *****************\n  74    2023-07-21 13:23    36  *****************\n\nSCT Error Recovery Control:\n           Read: Disabled\n          Write: Disabled\n\nDevice Statistics (GP Log 0x04)\nPage  Offset Size        Value Flags Description\n0x01  =====  =               =  ===  == General Statistics (rev 1) ==\n0x01  0x008  4              43  ---  Lifetime Power-On Resets\n0x01  0x010  4            4490  ---  Power-on Hours\n0x01  0x018  6     19732537033  ---  Logical Sectors Written\n0x01  0x020  6        67404062  ---  Number of Write Commands\n0x01  0x028  6    486533164956  ---  Logical Sectors Read\n0x01  0x030  6      2489352044  ---  Number of Read Commands\n0x01  0x038  6     16167059850  ---  Date and Time TimeStamp\n0x03  =====  =               =  ===  == Rotating Media Statistics (rev 1) ==\n0x03  0x008  4            4470  ---  Spindle Motor Power-on Hours\n0x03  0x010  4            4470  ---  Head Flying Hours\n0x03  0x018  4             195  ---  Head Load Events\n0x03  0x020  4               0  ---  Number of Reallocated Logical Sectors\n0x03  0x028  4              75  ---  Read Recovery Attempts\n0x03  0x030  4               1  ---  Number of Mechanical Start Failures\n0x04  =====  =               =  ===  == General Errors Statistics (rev 1) ==\n0x04  0x008  4               6  ---  Number of Reported Uncorrectable Errors\n0x04  0x010  4               0  ---  Resets Between Cmd Acceptance and Completion\n0x04  0x018  4               0  ---  Physical Element Status Changed\n0x05  =====  =               =  ===  == Temperature Statistics (rev 1) ==\n0x05  0x008  1              36  ---  Current Temperature\n0x05  0x010  1              36  N--  Average Short Term Temperature\n0x05  0x018  1              35  N--  Average Long Term Temperature\n0x05  0x020  1              49  ---  Highest Temperature\n0x05  0x028  1              14  ---  Lowest Temperature\n0x05  0x030  1              47  N--  Highest Average Short Term Temperature\n0x05  0x038  1              21  N--  Lowest Average Short Term Temperature\n0x05  0x040  1              46  N--  Highest Average Long Term Temperature\n0x05  0x048  1              23  N--  Lowest Average Long Term Temperature\n0x05  0x050  4               0  ---  Time in Over-Temperature\n0x05  0x058  1              60  ---  Specified Maximum Operating Temperature\n0x05  0x060  4               0  ---  Time in Under-Temperature\n0x05  0x068  1               0  ---  Specified Minimum Operating Temperature\n0x06  =====  =               =  ===  == Transport Statistics (rev 1) ==\n0x06  0x008  4             205  ---  Number of Hardware Resets\n0x06  0x010  4              44  ---  Number of ASR Events\n0x06  0x018  4               0  ---  Number of Interface CRC Errors\n0xff  =====  =               =  ===  == Vendor Specific Statistics (rev 1) ==\n                                |||_ C monitored condition met\n                                ||__ D supports DSN\n                                |___ N normalized value\n\nPending Defects log (GP Log 0x0c)\nNo Defects Logged\n\nSATA Phy Event Counters (GP Log 0x11)\nID      Size     Value  Description\n0x0001  2            0  Command failed due to ICRC error\n0x0002  2            1  R_ERR response for data FIS\n0x0003  2            1  R_ERR response for device-to-host data FIS\n0x0004  2            0  R_ERR response for host-to-device data FIS\n0x0005  2            0  R_ERR response for non-data FIS\n0x0006  2            0  R_ERR response for device-to-host non-data FIS\n0x0007  2            0  R_ERR response for host-to-device non-data FIS\n0x0008  2            0  Device-to-host non-data FIS retries\n0x0009  2           16  Transition from drive PhyRdy to drive PhyNRdy\n0x000a  2           13  Device-to-host register FISes sent due to a COMRESET\n0x000b  2            0  CRC errors within host-to-device FIS\n0x000d  2            0  Non-CRC errors within host-to-device FIS\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?auto=webp&amp;s=fb62a62917a4cd681adfb2ed102d12b3bffec423", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e680cc615c3529565359d358068446dc7a749710", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4cfc8914fb31e963b8c976aa90a4c5dc0bee375", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=391fbea0666df1013ebe77e5378d9f0a44d734d8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ec56cfb30b2d32af9c4e3eed80d13ab5050b0c85", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09986a2c305d87e6ce0985806f324daf8a8eef2c", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/bENi_Sb3WrMecqCAy80-VCDEHitIB94Pfi8YVjo16v4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=294710c01d4d85ae44879409901f4555b571eb26", "width": 1080, "height": 1080}], "variants": {}, "id": "BM5_QdAyNCLTMrgIxGNUsuyqSJL4D1S83f-YL0tXUE0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155ugtf", "is_robot_indexable": true, "report_reasons": null, "author": "halexh", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155ugtf/failing_drive_serverpartdeals_return_process/", "subreddit_subscribers": 693573, "created_utc": 1689960560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I'm outgrowing again.\n\nHas anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. \n\nSeems a lot easier going forward than a Nas.", "author_fullname": "t2_2sr8ya35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ceph?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155uajf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689960160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a Linux Nas, then it filled up, so I added a drive, then an external for backup, and now I&amp;#39;m outgrowing again.&lt;/p&gt;\n\n&lt;p&gt;Has anyone looked at setting up ceph? It looks like a solution that would be easy to scale up, just add drives and nodes, and also easy to retire old hw. \nI was thinking a dual system, nvme for active data and HDD for archives. &lt;/p&gt;\n\n&lt;p&gt;Seems a lot easier going forward than a Nas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155uajf", "is_robot_indexable": true, "report_reasons": null, "author": "Frewtti", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155uajf/ceph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155uajf/ceph/", "subreddit_subscribers": 693573, "created_utc": 1689960160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just went to reconnect my home cloud duo on my Mac, got to the website that mounts the drive to my computer, and said no longer supported. This blows. Their web-based access still works, and so does the iPhone OS app.", "author_fullname": "t2_e0g8qmrsg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital stopped app support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15635n3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689980573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just went to reconnect my home cloud duo on my Mac, got to the website that mounts the drive to my computer, and said no longer supported. This blows. Their web-based access still works, and so does the iPhone OS app.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15635n3", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Elk3586", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15635n3/western_digital_stopped_app_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15635n3/western_digital_stopped_app_support/", "subreddit_subscribers": 693573, "created_utc": 1689980573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3wef6thkx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal Solid State Drive, Up to 7400MB/s, for Gamers and Creators, for $209.99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 38, "top_awarded_type": null, "hide_score": true, "name": "t3_1562zkx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Today&amp;#39;s &lt;a href=\"https://twitter.com/hashtag/GamingSSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#GamingSSD&lt;/a&gt; Deal &lt;a href=\"https://t.co/rlaLCo73nt\"&gt;https://t.co/rlaLCo73nt&lt;/a&gt;&lt;br&gt;&lt;br&gt;Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal Solid State  Drive, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators  (LNM790X004T-RNNNU), for $209.99&lt;a href=\"https://twitter.com/hashtag/Ad?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Ad&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/PS5SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#PS5SSD&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Deals?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Deals&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Gaming?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Gaming&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#SSD&lt;/a&gt; &lt;a href=\"https://t.co/AiqPtm25QY\"&gt;pic.twitter.com/AiqPtm25QY&lt;/a&gt;&lt;/p&gt;&amp;mdash; Help Me Find Deals (@HelpMeFindDeals) &lt;a href=\"https://twitter.com/HelpMeFindDeals/status/1682524528474484737?ref_src=twsrc%5Etfw\"&gt;July 21, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/HelpMeFindDeals/status/1682524528474484737", "author_name": "Help Me Find Deals", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Today&amp;#39;s &lt;a href=\"https://twitter.com/hashtag/GamingSSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#GamingSSD&lt;/a&gt; Deal &lt;a href=\"https://t.co/rlaLCo73nt\"&gt;https://t.co/rlaLCo73nt&lt;/a&gt;&lt;br&gt;&lt;br&gt;Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal Solid State  Drive, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators  (LNM790X004T-RNNNU), for $209.99&lt;a href=\"https://twitter.com/hashtag/Ad?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Ad&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/PS5SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#PS5SSD&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Deals?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Deals&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Gaming?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Gaming&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#SSD&lt;/a&gt; &lt;a href=\"https://t.co/AiqPtm25QY\"&gt;pic.twitter.com/AiqPtm25QY&lt;/a&gt;&lt;/p&gt;&amp;mdash; Help Me Find Deals (@HelpMeFindDeals) &lt;a href=\"https://twitter.com/HelpMeFindDeals/status/1682524528474484737?ref_src=twsrc%5Etfw\"&gt;July 21, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/HelpMeFindDeals", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Today&amp;#39;s &lt;a href=\"https://twitter.com/hashtag/GamingSSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#GamingSSD&lt;/a&gt; Deal &lt;a href=\"https://t.co/rlaLCo73nt\"&gt;https://t.co/rlaLCo73nt&lt;/a&gt;&lt;br&gt;&lt;br&gt;Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal Solid State  Drive, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators  (LNM790X004T-RNNNU), for $209.99&lt;a href=\"https://twitter.com/hashtag/Ad?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Ad&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/PS5SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#PS5SSD&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Deals?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Deals&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Gaming?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Gaming&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#SSD&lt;/a&gt; &lt;a href=\"https://t.co/AiqPtm25QY\"&gt;pic.twitter.com/AiqPtm25QY&lt;/a&gt;&lt;/p&gt;&amp;mdash; Help Me Find Deals (@HelpMeFindDeals) &lt;a href=\"https://twitter.com/HelpMeFindDeals/status/1682524528474484737?ref_src=twsrc%5Etfw\"&gt;July 21, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/1562zkx", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/cmjYdFUhPIIR81N3mxQe5vB4lwrVdMD_Y50NINaF5MM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689980168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/HelpMeFindDeals/status/1682524528474484737", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X66hHwmEfzE8-mYGQCcv11ctqIpdQQXLxfzfaxLODLQ.jpg?auto=webp&amp;s=4a1a8dc20bcc8baa37175b7a5ecfb67166e43b3c", "width": 140, "height": 38}, "resolutions": [{"url": "https://external-preview.redd.it/X66hHwmEfzE8-mYGQCcv11ctqIpdQQXLxfzfaxLODLQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8d3c358d0ccd7d090769734bd5eae52ec347bdd", "width": 108, "height": 29}], "variants": {}, "id": "fhr2C8AelDePt3Ut_oHHlFP4sv4HvOfy9NTe-FRVmVs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1562zkx", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious_Command15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1562zkx/lexar_nm790_ssd_4tb_pcie_gen4_nvme_m2_2280/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/HelpMeFindDeals/status/1682524528474484737", "subreddit_subscribers": 693573, "created_utc": 1689980168.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/HelpMeFindDeals/status/1682524528474484737", "author_name": "Help Me Find Deals", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Today&amp;#39;s &lt;a href=\"https://twitter.com/hashtag/GamingSSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#GamingSSD&lt;/a&gt; Deal &lt;a href=\"https://t.co/rlaLCo73nt\"&gt;https://t.co/rlaLCo73nt&lt;/a&gt;&lt;br&gt;&lt;br&gt;Lexar NM790 SSD 4TB PCIe Gen4 NVMe M.2 2280 Internal Solid State  Drive, Up to 7400MB/s, Compatible with PS5, for Gamers and Creators  (LNM790X004T-RNNNU), for $209.99&lt;a href=\"https://twitter.com/hashtag/Ad?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Ad&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/PS5SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#PS5SSD&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Deals?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Deals&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/Gaming?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#Gaming&lt;/a&gt; &lt;a href=\"https://twitter.com/hashtag/SSD?src=hash&amp;amp;ref_src=twsrc%5Etfw\"&gt;#SSD&lt;/a&gt; &lt;a href=\"https://t.co/AiqPtm25QY\"&gt;pic.twitter.com/AiqPtm25QY&lt;/a&gt;&lt;/p&gt;&amp;mdash; Help Me Find Deals (@HelpMeFindDeals) &lt;a href=\"https://twitter.com/HelpMeFindDeals/status/1682524528474484737?ref_src=twsrc%5Etfw\"&gt;July 21, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/HelpMeFindDeals", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Trying to connect MY EX2 Ultra to https://www.multcloud.com\n\nHi I just got a EX2 ultra and wanted to connected to https://www.multcloud.com. The three options that I think could work is connecting it via WebDAV, NAS or FTP. Only problem is I have very little knowledge in anything really server based. I'm not sure what server URL I'm supposed to use or what 3 would be the best option. If some one can help me out it would really mean a lot. have been trying to connect it every other day for about a week or two and I can\u2019t figure it out.", "author_fullname": "t2_8y4rqu6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to connect MY EX2 Ultra to https://www.multcloud.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1562cv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689978569.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to connect MY EX2 Ultra to &lt;a href=\"https://www.multcloud.com\"&gt;https://www.multcloud.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi I just got a EX2 ultra and wanted to connected to &lt;a href=\"https://www.multcloud.com\"&gt;https://www.multcloud.com&lt;/a&gt;. The three options that I think could work is connecting it via WebDAV, NAS or FTP. Only problem is I have very little knowledge in anything really server based. I&amp;#39;m not sure what server URL I&amp;#39;m supposed to use or what 3 would be the best option. If some one can help me out it would really mean a lot. have been trying to connect it every other day for about a week or two and I can\u2019t figure it out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1562cv8", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent_Hawk_8718", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1562cv8/trying_to_connect_my_ex2_ultra_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1562cv8/trying_to_connect_my_ex2_ultra_to/", "subreddit_subscribers": 693573, "created_utc": 1689978569.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve 5x4Tb and 5x10 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.\n\nThe biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) \n\nI\u2019d like also to backup some of my kids data on it.\n\nWhat would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.\n\nThe OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.\n \nOne solution proposed was to make a ZFS layout like this: 2x4TB+2x10TB in pool, mirrored to another 2x4TB+10TB set. That would mean I will keep the remaining 4TB and 10TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.\n\nI am open to ideas!", "author_fullname": "t2_5jccxw9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help laying out hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155wd6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689964927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve 5x4Tb and 5x10 Tb disks available and a machine where I can plug all of those. The total SATA ports available are 12, with 2 nvme slots.&lt;/p&gt;\n\n&lt;p&gt;The biggest chunk of my data are photos, videos and office documents - It\u2019s 20 years of stuff - many files still scattered in a bunch of USB disks and burned DVDs. :) &lt;/p&gt;\n\n&lt;p&gt;I\u2019d like also to backup some of my kids data on it.&lt;/p&gt;\n\n&lt;p&gt;What would be a good layout solution to have some redundancy of the disks? I would like to survive to 2 disk failures, if possible.&lt;/p&gt;\n\n&lt;p&gt;The OS can be any flavor of Linux, This machine is not to be a real NAS - it will also be used eventually as a desktop. OS will run on a separate ssd or nvme. I\u2019d avoid Unraid/TrueNAS/Openmediavault installations.&lt;/p&gt;\n\n&lt;p&gt;One solution proposed was to make a ZFS layout like this: 2x4TB+2x10TB in pool, mirrored to another 2x4TB+10TB set. That would mean I will keep the remaining 4TB and 10TB as spares. Does it seem a reasonable solution? Any suggestions? I do not need to use ZFS, it was the first thing to come to mind.&lt;/p&gt;\n\n&lt;p&gt;I am open to ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155wd6z", "is_robot_indexable": true, "report_reasons": null, "author": "no-dupe", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155wd6z/help_laying_out_hard_drives/", "subreddit_subscribers": 693573, "created_utc": 1689964927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "via live usb, did:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img\n\nTime having passed, looking for recommends to:\n\n$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img\n\n... compare w7[a-b].img &amp; detect which if any files were deleted, added, changed; preferably as discrete operations.", "author_fullname": "t2_dfujb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Compare Subsequent Full Images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155vc6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689962536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;via live usb, did:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7a.img&lt;/p&gt;\n\n&lt;p&gt;Time having passed, looking for recommends to:&lt;/p&gt;\n\n&lt;p&gt;$ sudo dd if=/dev/sda of=/run/media/usb3ext/w7b.img&lt;/p&gt;\n\n&lt;p&gt;... compare w7[a-b].img &amp;amp; detect which if any files were deleted, added, changed; preferably as discrete operations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155vc6z", "is_robot_indexable": true, "report_reasons": null, "author": "zombi-roboto", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155vc6z/compare_subsequent_full_images/", "subreddit_subscribers": 693573, "created_utc": 1689962536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of any good 2D/3D file visualizers for Windows?\n\nI\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. \n\nWinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.\n\nAnyone know of anything similar?", "author_fullname": "t2_zxdz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual file visualizer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155quin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689952476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of any good 2D/3D file visualizers for Windows?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m revamping my file system, and I\u2019d love something that visualizes file organization in 2D or 3D space. Preferably something customizable. &lt;/p&gt;\n\n&lt;p&gt;WinDirStat does it at a completely functional level, but I was hoping for something a little slicker that would allow me to arrange and organize my collections of files in a \u201croom\u201d of some sort, rather than a directory tree. Something of a virtual bookshelf or virtual library, but for my entire filing system. The organization would simply visualize the existing dir structure, not add its own.&lt;/p&gt;\n\n&lt;p&gt;Anyone know of anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155quin", "is_robot_indexable": true, "report_reasons": null, "author": "CCMadman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155quin/virtual_file_visualizer/", "subreddit_subscribers": 693573, "created_utc": 1689952476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a simple Linux i5 pc build with 6 hdds. I\u2019d like to upgrade to something more made for a server rack, with ECC ram, able to run high quality 4k and the ability to install many more hdds. Any guides or advice would be massively appreciated, I know a good amount about pc building and parts but almost nothing about the server rack components or what I\u2019d need, thank you!", "author_fullname": "t2_7djm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plex server suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155q8rx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689951152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a simple Linux i5 pc build with 6 hdds. I\u2019d like to upgrade to something more made for a server rack, with ECC ram, able to run high quality 4k and the ability to install many more hdds. Any guides or advice would be massively appreciated, I know a good amount about pc building and parts but almost nothing about the server rack components or what I\u2019d need, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155q8rx", "is_robot_indexable": true, "report_reasons": null, "author": "Darkstranger111", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155q8rx/plex_server_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155q8rx/plex_server_suggestions/", "subreddit_subscribers": 693573, "created_utc": 1689951152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure this question or a slight variation on it has been asked a bunch of times... I don't see any recent posts on it tho so here goes.\n\nI'm making a documentary that currently represents about 25TB of data, most of it is the 6k red raw files for interviews.\n\nMy current backup system:\n\n2 x OWC 28TB Gemini drives. 1 of them is the active media drive and the other is the backup drive that is auto backed up every hour using Arq.\n\n1 x OWC 30TB drive that is kept in another location. This one is brought to the edit and backed up to via Arq about twice a month.\n\nThat's it!! So far... We have spent a lot of money on this show and I want to be sure that we don't lose the footage.\n\nWe have 3 more intvs and tons of new archival to add in the coming months so will prob add 5-6 more TBs of data, making the entire show about 30TBs total.\n\nMy ISP is very meh on site and so I don't think cloud is an option for 25TBs.\n\nI've been looking at synology and qnap but I am not 100% certain of what they are capable of and what might be best for me.\n\nShould I just buy another OWC drive, back up to it and send it to a 3rd location?\n\nI'm an editor, not a storage/NAS person, but I've been tasked with this responsibility solely... Can r/datahoarders help me not fail?!\n\nThx!", "author_fullname": "t2_fxk5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Documentary Data Management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155mmy0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689942502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure this question or a slight variation on it has been asked a bunch of times... I don&amp;#39;t see any recent posts on it tho so here goes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making a documentary that currently represents about 25TB of data, most of it is the 6k red raw files for interviews.&lt;/p&gt;\n\n&lt;p&gt;My current backup system:&lt;/p&gt;\n\n&lt;p&gt;2 x OWC 28TB Gemini drives. 1 of them is the active media drive and the other is the backup drive that is auto backed up every hour using Arq.&lt;/p&gt;\n\n&lt;p&gt;1 x OWC 30TB drive that is kept in another location. This one is brought to the edit and backed up to via Arq about twice a month.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s it!! So far... We have spent a lot of money on this show and I want to be sure that we don&amp;#39;t lose the footage.&lt;/p&gt;\n\n&lt;p&gt;We have 3 more intvs and tons of new archival to add in the coming months so will prob add 5-6 more TBs of data, making the entire show about 30TBs total.&lt;/p&gt;\n\n&lt;p&gt;My ISP is very meh on site and so I don&amp;#39;t think cloud is an option for 25TBs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at synology and qnap but I am not 100% certain of what they are capable of and what might be best for me.&lt;/p&gt;\n\n&lt;p&gt;Should I just buy another OWC drive, back up to it and send it to a 3rd location?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an editor, not a storage/NAS person, but I&amp;#39;ve been tasked with this responsibility solely... Can &lt;a href=\"/r/datahoarders\"&gt;r/datahoarders&lt;/a&gt; help me not fail?!&lt;/p&gt;\n\n&lt;p&gt;Thx!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155mmy0", "is_robot_indexable": true, "report_reasons": null, "author": "esboardnewb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155mmy0/help_with_documentary_data_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155mmy0/help_with_documentary_data_management/", "subreddit_subscribers": 693573, "created_utc": 1689942502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Google says 3-5 years but that seems very low. Assuming you have a hard drive strictly to backup family photos and only use it a few times a year and it\u2019s safely kept in a drawer shouldn\u2019t it last much longer?", "author_fullname": "t2_v6avn3hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long will external hard drive last?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155a5bt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689904424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Google says 3-5 years but that seems very low. Assuming you have a hard drive strictly to backup family photos and only use it a few times a year and it\u2019s safely kept in a drawer shouldn\u2019t it last much longer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155a5bt", "is_robot_indexable": true, "report_reasons": null, "author": "mike4674", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155a5bt/how_long_will_external_hard_drive_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155a5bt/how_long_will_external_hard_drive_last/", "subreddit_subscribers": 693573, "created_utc": 1689904424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to upload images in my GDrive only 750mb Size but has a lot of images 100K+ and its taking hell a lot of time when uploading via desktop app. but copy winrar was a lot faster now I'm trying to unrar on the drive and that too very slow. ([see attachment](https://ibb.co/rxfx1Lp))  \n\n\nWhat's the best online drive for upload images. I have already dried box and onedrive.  \n\n\nPS: I'm trying to upload my images for database where i can get the link of the image and assign it to the book. any online drive which also cleans up the duplicate files that would be +1. Thanks", "author_fullname": "t2_6aqrg9xj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best online drive for uploading Images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155yrst", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689970337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to upload images in my GDrive only 750mb Size but has a lot of images 100K+ and its taking hell a lot of time when uploading via desktop app. but copy winrar was a lot faster now I&amp;#39;m trying to unrar on the drive and that too very slow. (&lt;a href=\"https://ibb.co/rxfx1Lp\"&gt;see attachment&lt;/a&gt;)  &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best online drive for upload images. I have already dried box and onedrive.  &lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m trying to upload my images for database where i can get the link of the image and assign it to the book. any online drive which also cleans up the duplicate files that would be +1. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7Z2a_CXOAAUudgvD5D-ve4wrsfyKYkjv_h7FO5gI2oc.jpg?auto=webp&amp;s=a42cef09ad1cd7e8b7103a95d9d28c6fd8452a4a", "width": 425, "height": 364}, "resolutions": [{"url": "https://external-preview.redd.it/7Z2a_CXOAAUudgvD5D-ve4wrsfyKYkjv_h7FO5gI2oc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=520434f25d162931d60cd081425bd47730a1c77d", "width": 108, "height": 92}, {"url": "https://external-preview.redd.it/7Z2a_CXOAAUudgvD5D-ve4wrsfyKYkjv_h7FO5gI2oc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1224a2146ecf4cf96f71047e9b9ccddc51bbeda2", "width": 216, "height": 184}, {"url": "https://external-preview.redd.it/7Z2a_CXOAAUudgvD5D-ve4wrsfyKYkjv_h7FO5gI2oc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16adc8b2ed1f62d97f1a97fdcd50d85d7b768fcd", "width": 320, "height": 274}], "variants": {}, "id": "ZBZUUc-AzuFPwrnLv8xpyX-G9-trErDwngpmxtCnl3A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155yrst", "is_robot_indexable": true, "report_reasons": null, "author": "iamubaid", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155yrst/best_online_drive_for_uploading_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155yrst/best_online_drive_for_uploading_images/", "subreddit_subscribers": 693573, "created_utc": 1689970337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a folder with thousands of files with names that names can sometimes look alike or have different extensions, like:\n\n\\- filename.txt  \n\\- filename.pdf  \n\\- file-name.txt\n\nThe three files above are the same file **but the checksum and file size are all different**.\n\nIs there a way to check and display all files with very similar names and I will check manually and delete the ones I don't want?\n\nI would love something like Czkawka where it checks for similar images, displays them and then, I can delete the similar ones or check them one by one...\n\nThank you!", "author_fullname": "t2_vc7clehs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to check very similar file names on Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155in2y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689931050.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689930726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a folder with thousands of files with names that names can sometimes look alike or have different extensions, like:&lt;/p&gt;\n\n&lt;p&gt;- filename.txt&lt;br/&gt;\n- filename.pdf&lt;br/&gt;\n- file-name.txt&lt;/p&gt;\n\n&lt;p&gt;The three files above are the same file &lt;strong&gt;but the checksum and file size are all different&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to check and display all files with very similar names and I will check manually and delete the ones I don&amp;#39;t want?&lt;/p&gt;\n\n&lt;p&gt;I would love something like Czkawka where it checks for similar images, displays them and then, I can delete the similar ones or check them one by one...&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155in2y", "is_robot_indexable": true, "report_reasons": null, "author": "Feeling_Usual1541", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155in2y/how_to_check_very_similar_file_names_on_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155in2y/how_to_check_very_similar_file_names_on_windows/", "subreddit_subscribers": 693573, "created_utc": 1689930726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So basically I have 4x3TB drives and 1x12TB drive, all setup in FreeNAS, no mirroring, just one pool (I know I know, I'm trying to fix it), and the pool is nearly full, which is around 20TB.\n\nI just ordered 2 12TB drives, and am planning on switching to UnRaid, as I need more flexibility. My issue though is that I believe I cannot just swap the drives into unraid and it will be happy, and I do not want to lose this data. My plan currently is to just throw the new 12TB drives into my PC, transfer the data from my NAS into those two drives, then setup the existing drives in UnRaid (wiping them), transfer the data from the 12TB drives on my PC into unraid once it's setup, then finally wipe the 12TB drives on my pc, and load them into unraid, with one of them as parity.\n\nI am pretty certain this will work, but I am looking at around a week of continuous data transfer based on my current speeds from a HDD -&gt; NAS(HDD) and vice versa. There are also probably other concerns doing it this way I am unaware of.\n\nAny suggestions on a better way to go about this? Thanks!!!", "author_fullname": "t2_10qx1z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I Move 20TB of Data AND Change OS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155pzww", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689950601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So basically I have 4x3TB drives and 1x12TB drive, all setup in FreeNAS, no mirroring, just one pool (I know I know, I&amp;#39;m trying to fix it), and the pool is nearly full, which is around 20TB.&lt;/p&gt;\n\n&lt;p&gt;I just ordered 2 12TB drives, and am planning on switching to UnRaid, as I need more flexibility. My issue though is that I believe I cannot just swap the drives into unraid and it will be happy, and I do not want to lose this data. My plan currently is to just throw the new 12TB drives into my PC, transfer the data from my NAS into those two drives, then setup the existing drives in UnRaid (wiping them), transfer the data from the 12TB drives on my PC into unraid once it&amp;#39;s setup, then finally wipe the 12TB drives on my pc, and load them into unraid, with one of them as parity.&lt;/p&gt;\n\n&lt;p&gt;I am pretty certain this will work, but I am looking at around a week of continuous data transfer based on my current speeds from a HDD -&amp;gt; NAS(HDD) and vice versa. There are also probably other concerns doing it this way I am unaware of.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on a better way to go about this? Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155pzww", "is_robot_indexable": true, "report_reasons": null, "author": "artistbuddy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155pzww/how_do_i_move_20tb_of_data_and_change_os/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155pzww/how_do_i_move_20tb_of_data_and_change_os/", "subreddit_subscribers": 693573, "created_utc": 1689950601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basically I've finally had the dreaded email from google and have 65TB of \"media\" that I need to download within the next 60 days. I'm (obviously) not a fan of spending loads of money in one go, so I was wondering if I could just copy everything over onto newly purchased HDDs and then later down the line have them in a NAS without first having to format them or anything?", "author_fullname": "t2_46i0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When setting up a NAS/Plex Server - can I first get the HDDs and fill them before getting Synology/DIY NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155s5e6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689955344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I&amp;#39;ve finally had the dreaded email from google and have 65TB of &amp;quot;media&amp;quot; that I need to download within the next 60 days. I&amp;#39;m (obviously) not a fan of spending loads of money in one go, so I was wondering if I could just copy everything over onto newly purchased HDDs and then later down the line have them in a NAS without first having to format them or anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155s5e6", "is_robot_indexable": true, "report_reasons": null, "author": "VadimH", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155s5e6/when_setting_up_a_nasplex_server_can_i_first_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155s5e6/when_setting_up_a_nasplex_server_can_i_first_get/", "subreddit_subscribers": 693573, "created_utc": 1689955344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm trying to back up picturea/videos from my trip, about 60-120gb, 2k files from my phone.\n\nMy process\n- from my camera sd card, move all to phone, and overnight backing up some portion to Google drive. \n\nProblem is I often get it randomly stopped, or files are not all uploaded, for example 78 out of 100, and then I need tod delete all and do it all over again as if I i have duplicates? \n\nThanks", "author_fullname": "t2_10tx5blw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why backing up to Google drive is such a pain, what am I doing wrong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155pzgm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689950574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to back up picturea/videos from my trip, about 60-120gb, 2k files from my phone.&lt;/p&gt;\n\n&lt;p&gt;My process\n- from my camera sd card, move all to phone, and overnight backing up some portion to Google drive. &lt;/p&gt;\n\n&lt;p&gt;Problem is I often get it randomly stopped, or files are not all uploaded, for example 78 out of 100, and then I need tod delete all and do it all over again as if I i have duplicates? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155pzgm", "is_robot_indexable": true, "report_reasons": null, "author": "-i3arty-", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155pzgm/why_backing_up_to_google_drive_is_such_a_pain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155pzgm/why_backing_up_to_google_drive_is_such_a_pain/", "subreddit_subscribers": 693573, "created_utc": 1689950574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This PSID is required to reset SED to use in NAS, but I am certain it's not print on the white label. Also tried all the number on the label, nothing works. Thanks!", "author_fullname": "t2_mjwm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For WD shucked HDD, where I can find SED PSID", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_155iaj4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689929614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This PSID is required to reset SED to use in NAS, but I am certain it&amp;#39;s not print on the white label. Also tried all the number on the label, nothing works. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "155iaj4", "is_robot_indexable": true, "report_reasons": null, "author": "st0n39", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/155iaj4/for_wd_shucked_hdd_where_i_can_find_sed_psid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/155iaj4/for_wd_shucked_hdd_where_i_can_find_sed_psid/", "subreddit_subscribers": 693573, "created_utc": 1689929614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've had great luck with Exos drives. But I was wondering how the WD in title compares to the X18 Exos platform in terms of reliability and longevity", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos X18 18TB (ST18000NM000J) or WD Ultrastar DC HC550 18TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15594fl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689901519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had great luck with Exos drives. But I was wondering how the WD in title compares to the X18 Exos platform in terms of reliability and longevity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15594fl", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15594fl/seagate_exos_x18_18tb_st18000nm000j_or_wd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15594fl/seagate_exos_x18_18tb_st18000nm000j_or_wd/", "subreddit_subscribers": 693573, "created_utc": 1689901519.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}