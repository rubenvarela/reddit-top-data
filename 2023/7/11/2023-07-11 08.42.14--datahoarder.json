{"kind": "Listing", "data": {"after": "t3_14vq909", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any good websites or shops would help me out a lot.", "author_fullname": "t2_bxvc9v2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Im looking for a 16tb hard disk drive for my sata hub and was wondering where the best place in Australia for hdd are as I\u2019ve been let down from Amazon and didn\u2019t know how to replace the drive overseas.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wako7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689032984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any good websites or shops would help me out a lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wako7", "is_robot_indexable": true, "report_reasons": null, "author": "JohnnyOaklegs", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wako7/im_looking_for_a_16tb_hard_disk_drive_for_my_sata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wako7/im_looking_for_a_16tb_hard_disk_drive_for_my_sata/", "subreddit_subscribers": 692075, "created_utc": 1689032984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is the collection - adding this link to WinHTTrack doesn't work:\n\nhttps://digital.lib.buffalo.edu/collection/LIB-SC001/\n\nI also tried individually adding these subpages, but it still wouldn't download anything:\n\nhttps://digital.lib.buffalo.edu/items/show/1007\n\nI'd probably have to individually add each image link (but that defeats the purpose of automated crawling):\n\nhttps://digital.lib.buffalo.edu/files/original/0129bc17ea1d1c7c0be489d4edb888a3.jpg\n\nI'd appreciate either an easy Windows or web-based tool that could crawl this, or if someone could crawl the files themselves and upload them here.\n\ncorrection: these aren't all Dore's engravings (e.g. Inferno from the Divine Comedy is missing, though it's available [here](https://archive.org/details/GustaveDoreIllustrationsForTheDivineComedyByDanteAlighieriHQ) in HD)", "author_fullname": "t2_7haig6fl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to rip this collection of all Gustave Dor\u00e9's engravings (best quality I've found so far, albeit watermarked), but WinHTTrack isn't working", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vplm1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689023471.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1688983143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is the collection - adding this link to WinHTTrack doesn&amp;#39;t work:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://digital.lib.buffalo.edu/collection/LIB-SC001/\"&gt;https://digital.lib.buffalo.edu/collection/LIB-SC001/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also tried individually adding these subpages, but it still wouldn&amp;#39;t download anything:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://digital.lib.buffalo.edu/items/show/1007\"&gt;https://digital.lib.buffalo.edu/items/show/1007&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d probably have to individually add each image link (but that defeats the purpose of automated crawling):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://digital.lib.buffalo.edu/files/original/0129bc17ea1d1c7c0be489d4edb888a3.jpg\"&gt;https://digital.lib.buffalo.edu/files/original/0129bc17ea1d1c7c0be489d4edb888a3.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate either an easy Windows or web-based tool that could crawl this, or if someone could crawl the files themselves and upload them here.&lt;/p&gt;\n\n&lt;p&gt;correction: these aren&amp;#39;t all Dore&amp;#39;s engravings (e.g. Inferno from the Divine Comedy is missing, though it&amp;#39;s available &lt;a href=\"https://archive.org/details/GustaveDoreIllustrationsForTheDivineComedyByDanteAlighieriHQ\"&gt;here&lt;/a&gt; in HD)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?auto=webp&amp;s=bb463010202981a47b2666124742323902bae38e", "width": 2500, "height": 3072}, "resolutions": [{"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=acb74b2090d03aae99b64186b62278e6238d3301", "width": 108, "height": 132}, {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2a251778fc8ae1d942b49b1f9d5725c3ce5928e9", "width": 216, "height": 265}, {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c0b4321fd10a701b048273ae99ef46c4bd9f990", "width": 320, "height": 393}, {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc2fb79754572cb868fda42753c42a0d0fbe363f", "width": 640, "height": 786}, {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79bf01aa1815b87de4c29f16b74cbbfcbec5fd75", "width": 960, "height": 1179}, {"url": "https://external-preview.redd.it/neamQMujbDrPl0NWZb3wJZLPOVmypWJpzcz2O7FL1d4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e69d629a464e631dae03ea9c1b39db905d6cbf7c", "width": 1080, "height": 1327}], "variants": {}, "id": "eeWb3D2Bpv9nLo8S9QWouJ83HlM7K1nG4rMEVGzDlIg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vplm1", "is_robot_indexable": true, "report_reasons": null, "author": "spermo_chuggins", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vplm1/trying_to_rip_this_collection_of_all_gustave/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vplm1/trying_to_rip_this_collection_of_all_gustave/", "subreddit_subscribers": 692075, "created_utc": 1688983143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hello,\n\nI'm considering building a high-capacity storage server, aiming eventually for 100 TB. I plan to purchase 20 TB SATA hard drives. Could you advise me on the type of RAID to use?\n\nI intend to install the hard drives in an 8-bay 3.5 server, using a Dell H730 RAID card.", "author_fullname": "t2_erwvt46e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning and Building a High-Capacity Storage Server: Seeking RAID Advice for a 100 TB Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vx65h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689002839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering building a high-capacity storage server, aiming eventually for 100 TB. I plan to purchase 20 TB SATA hard drives. Could you advise me on the type of RAID to use?&lt;/p&gt;\n\n&lt;p&gt;I intend to install the hard drives in an 8-bay 3.5 server, using a Dell H730 RAID card.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vx65h", "is_robot_indexable": true, "report_reasons": null, "author": "OniHanz", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vx65h/planning_and_building_a_highcapacity_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vx65h/planning_and_building_a_highcapacity_storage/", "subreddit_subscribers": 692075, "created_utc": 1689002839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know jackshit about this, but willing to learn. Looking for something with probably 4-6 bays. Capable of streaming 4k media if my TV, and otherwise just be a 'hold my pirated data and maybe some misc backup files' purposes. \n\nI don't know about brands or specifications or price points I should be looking at. Help?", "author_fullname": "t2_t9ykqpa6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Home NAS entry, need guidance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w8dva", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689027668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know jackshit about this, but willing to learn. Looking for something with probably 4-6 bays. Capable of streaming 4k media if my TV, and otherwise just be a &amp;#39;hold my pirated data and maybe some misc backup files&amp;#39; purposes. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know about brands or specifications or price points I should be looking at. Help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w8dva", "is_robot_indexable": true, "report_reasons": null, "author": "Ex-Fat", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w8dva/home_nas_entry_need_guidance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w8dva/home_nas_entry_need_guidance/", "subreddit_subscribers": 692075, "created_utc": 1689027668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All.\n\nI am looking for software to catalogue thousands of different media pieces, but more specifically video at this stage.\n\nTo give a bit of background, I am looking after an organizations AV and have currently over 13TB of media stored on a NAS with a online remote backup and an offline backup that gets checked regularly. The vast majority of the data is video and dates back decades. The plan is to also expand on this data and keep adding into the future.\n\nI am after a software that can be run locally (I have sufficient processing power) can stream the content to authorized people that can go through the footage, catalogue it and add metadata (ideally timestamp metadata). I have plenty of bandwidth for streaming so that's not an issue.\n\nI run my own Plex server and was hoping to utilize something similar, but I haven't found a suitable plugin for this and a lot of other software out there that do this type of activity are bloody expensive.\n\nThought I would try my luck and reach out and see if anyone else out there has something that would be suitable.\n\nThanks.\n\n&amp;#x200B;", "author_fullname": "t2_d11ek", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video Catalogue Software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w9scb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689031046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All.&lt;/p&gt;\n\n&lt;p&gt;I am looking for software to catalogue thousands of different media pieces, but more specifically video at this stage.&lt;/p&gt;\n\n&lt;p&gt;To give a bit of background, I am looking after an organizations AV and have currently over 13TB of media stored on a NAS with a online remote backup and an offline backup that gets checked regularly. The vast majority of the data is video and dates back decades. The plan is to also expand on this data and keep adding into the future.&lt;/p&gt;\n\n&lt;p&gt;I am after a software that can be run locally (I have sufficient processing power) can stream the content to authorized people that can go through the footage, catalogue it and add metadata (ideally timestamp metadata). I have plenty of bandwidth for streaming so that&amp;#39;s not an issue.&lt;/p&gt;\n\n&lt;p&gt;I run my own Plex server and was hoping to utilize something similar, but I haven&amp;#39;t found a suitable plugin for this and a lot of other software out there that do this type of activity are bloody expensive.&lt;/p&gt;\n\n&lt;p&gt;Thought I would try my luck and reach out and see if anyone else out there has something that would be suitable.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w9scb", "is_robot_indexable": true, "report_reasons": null, "author": "Deap99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w9scb/video_catalogue_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w9scb/video_catalogue_software/", "subreddit_subscribers": 692075, "created_utc": 1689031046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For the Imgur files that were stored, is there anyway to access them yet?\nWas any information about the original urls stored?", "author_fullname": "t2_1hspd4k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vxqip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689004117.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the Imgur files that were stored, is there anyway to access them yet?\nWas any information about the original urls stored?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vxqip", "is_robot_indexable": true, "report_reasons": null, "author": "Civil-Hypocrisy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vxqip/imgur_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vxqip/imgur_archive/", "subreddit_subscribers": 692075, "created_utc": 1689004117.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody,\n\nI just archived google photos. It's about 52.000 images (mostly photos, some screenshots / saved memes).\n\nThe files are on my NAS. So far, I started sorting them by year. One folder per year, all photos from that year in there. \n\nBut I still need to actually look through all of those images, delete what I don't need. Perhaps create subfolders.\n\nCan you recommend a tool for linux that allows me to\n\n*  display these images by folder\n* display these images by folder - including subfolders\n* display them as a grid, allowing me to select multiple images and copy/cut/delete/move them\n* work with the existing folder structure (so does \\_\\_not\\_\\_ require me to copy/move the images elsewhere)\n\nI am on Pop!\\_Os. Most tools running on Ubuntu should work. I am \\_\\_not\\_\\_ looking for a self-hosted tool, just a program that runs locally. I'll deal with serving these files on my local network later via some self-hosted tool, right now, I just want / need to organize them, sort them.\n\nThank you in advance for your ideas :)", "author_fullname": "t2_doh0lpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "52.000 images: sort, move, delete", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14wk1he", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689060067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;I just archived google photos. It&amp;#39;s about 52.000 images (mostly photos, some screenshots / saved memes).&lt;/p&gt;\n\n&lt;p&gt;The files are on my NAS. So far, I started sorting them by year. One folder per year, all photos from that year in there. &lt;/p&gt;\n\n&lt;p&gt;But I still need to actually look through all of those images, delete what I don&amp;#39;t need. Perhaps create subfolders.&lt;/p&gt;\n\n&lt;p&gt;Can you recommend a tool for linux that allows me to&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; display these images by folder&lt;/li&gt;\n&lt;li&gt;display these images by folder - including subfolders&lt;/li&gt;\n&lt;li&gt;display them as a grid, allowing me to select multiple images and copy/cut/delete/move them&lt;/li&gt;\n&lt;li&gt;work with the existing folder structure (so does __not__ require me to copy/move the images elsewhere)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am on Pop!_Os. Most tools running on Ubuntu should work. I am __not__ looking for a self-hosted tool, just a program that runs locally. I&amp;#39;ll deal with serving these files on my local network later via some self-hosted tool, right now, I just want / need to organize them, sort them.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your ideas :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.21 Gigawatts", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wk1he", "is_robot_indexable": true, "report_reasons": null, "author": "prankousky", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14wk1he/52000_images_sort_move_delete/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wk1he/52000_images_sort_move_delete/", "subreddit_subscribers": 692075, "created_utc": 1689060067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Was just starting to transition from Google to Dropbox and just got a notice that they will only increase by 10tb one a week. Signed up a few days ago and am stuck at 10tb until next week. Wish they would have been up front about that as I told the rep when I signed up that I would be transferring 40tb and they said it wouldn't be an issue.", "author_fullname": "t2_10m3pn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox advanced is now limiting increases.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w6djr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689035339.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689023206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was just starting to transition from Google to Dropbox and just got a notice that they will only increase by 10tb one a week. Signed up a few days ago and am stuck at 10tb until next week. Wish they would have been up front about that as I told the rep when I signed up that I would be transferring 40tb and they said it wouldn&amp;#39;t be an issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w6djr", "is_robot_indexable": true, "report_reasons": null, "author": "mr_frpdo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w6djr/dropbox_advanced_is_now_limiting_increases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w6djr/dropbox_advanced_is_now_limiting_increases/", "subreddit_subscribers": 692075, "created_utc": 1689023206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious to know if one exists. I want to tackle a no longer negligible pile of tapes whose content I want to preserve with archival quality. Would love something agnostic as those two projects as my current situation doesn't allow me to go for premium decks and whatnot.", "author_fullname": "t2_t9f6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any project like VHS-Decode or Doomsday Project but for cassette tapes (audio and possibly data)? A raw/flux capture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vwcva", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689001009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to know if one exists. I want to tackle a no longer negligible pile of tapes whose content I want to preserve with archival quality. Would love something agnostic as those two projects as my current situation doesn&amp;#39;t allow me to go for premium decks and whatnot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44 MB Floppy Disk", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vwcva", "is_robot_indexable": true, "report_reasons": null, "author": "dismalwasteland", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14vwcva/is_there_any_project_like_vhsdecode_or_doomsday/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vwcva/is_there_any_project_like_vhsdecode_or_doomsday/", "subreddit_subscribers": 692075, "created_utc": 1689001009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi experts. What\u2019s the fastest way to clone an external SSD to another when you only have a laptop. I\u2019m sure it only has one USB 3 bus. If I had access to a desktop machine would an internal pci usb 3.1 be the way to go to get maximum performance. Thanks guys.", "author_fullname": "t2_3223if4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the fastest way to clone an external SSD to another on the same laptop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14w8y0w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/W7Tb_a2s4wvlxpiaMFHaoj3N4C4UqalknJ5NDvOXfnc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689029033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi experts. What\u2019s the fastest way to clone an external SSD to another when you only have a laptop. I\u2019m sure it only has one USB 3 bus. If I had access to a desktop machine would an internal pci usb 3.1 be the way to go to get maximum performance. Thanks guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wl3esrmhu7bb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?auto=webp&amp;s=c7e6a553ae9ec93b5675b4859891ae28b870bd84", "width": 1170, "height": 1201}, "resolutions": [{"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e3137adf104a554e41b02c2dce61fd1e04f95cfa", "width": 108, "height": 110}, {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8c22c9ab69961212c239090489a480367709a41", "width": 216, "height": 221}, {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=845ad4e204af43aed7421931b7ec2b9b092149e0", "width": 320, "height": 328}, {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8985ad8b53fc55a0c70aa6f1b28a40ec8ef1bf9f", "width": 640, "height": 656}, {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e45835f3fe68799477441ace9fa98444f0e501cc", "width": 960, "height": 985}, {"url": "https://preview.redd.it/wl3esrmhu7bb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26b637ac6e5382d5f1010febaca0418de9cdd17d", "width": 1080, "height": 1108}], "variants": {}, "id": "__LURsjv7sgJ4VeT55uTY-4D58l3Q7hvIJh4lNCeC5A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w8y0w", "is_robot_indexable": true, "report_reasons": null, "author": "letstalkretro", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w8y0w/whats_the_fastest_way_to_clone_an_external_ssd_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wl3esrmhu7bb1.jpg", "subreddit_subscribers": 692075, "created_utc": 1689029033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm ripping Tv shows I have on DVD using Makemkv and Handbrake (to condense file sizes). I'm starting with Parks and Rec, and I noticed the titles are kinda nonsense when Makemkv outputs them onto my drive. \n\nIs there a program that can rename them automatically and add the meta data (episode name, description, etc.)?\n\nIf not, how do y'all do it? Just watch each episode and compare with another source?\n\nI'm new to this, and although I've seen threads pop up from Google searches, most lead to the locked Makemkv subreddit so I'm a bit stuck. \n\nAppreciate any help!", "author_fullname": "t2_9vaxg2a8d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ripping Tv series questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w1uak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689013303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m ripping Tv shows I have on DVD using Makemkv and Handbrake (to condense file sizes). I&amp;#39;m starting with Parks and Rec, and I noticed the titles are kinda nonsense when Makemkv outputs them onto my drive. &lt;/p&gt;\n\n&lt;p&gt;Is there a program that can rename them automatically and add the meta data (episode name, description, etc.)?&lt;/p&gt;\n\n&lt;p&gt;If not, how do y&amp;#39;all do it? Just watch each episode and compare with another source?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to this, and although I&amp;#39;ve seen threads pop up from Google searches, most lead to the locked Makemkv subreddit so I&amp;#39;m a bit stuck. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w1uak", "is_robot_indexable": true, "report_reasons": null, "author": "liger-rug", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w1uak/ripping_tv_series_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w1uak/ripping_tv_series_questions/", "subreddit_subscribers": 692075, "created_utc": 1689013303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm getting this error (as well as a few Code 31 errors on other files, if that's relevant) when trying to move some larger files to a local machine. \n\nI'm trying to use Teracopy to assist with this so I don't have to download a bunch of 20GB zip files and am running in to a few issues. \n\nWhen I navigate to one of these files (first one I picked) it appears its only 94MB.  \nWhat am I doing wrong here? Can anyone point me in the right direction to download these and larger files? there is about 55GB that errored out for the same reasons.  \n\n\nThank you!  \n\n\nSide note, I was trying my hand at the sharepoint powershell transfer but was met with many, MANY errors and finally gave up. If anyone is good with powershell scripting, I'd be happy to chat privately and share my ChatGPT ~~cry for help~~ transcript. ", "author_fullname": "t2_c5hsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Teracopy from Sharepoint O365 to local machine (Error) \"Error opening source file. The file size exceeds the limit allowed and cannot be saved. Code 223", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vttnp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688995073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting this error (as well as a few Code 31 errors on other files, if that&amp;#39;s relevant) when trying to move some larger files to a local machine. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to use Teracopy to assist with this so I don&amp;#39;t have to download a bunch of 20GB zip files and am running in to a few issues. &lt;/p&gt;\n\n&lt;p&gt;When I navigate to one of these files (first one I picked) it appears its only 94MB.&lt;br/&gt;\nWhat am I doing wrong here? Can anyone point me in the right direction to download these and larger files? there is about 55GB that errored out for the same reasons.  &lt;/p&gt;\n\n&lt;p&gt;Thank you!  &lt;/p&gt;\n\n&lt;p&gt;Side note, I was trying my hand at the sharepoint powershell transfer but was met with many, MANY errors and finally gave up. If anyone is good with powershell scripting, I&amp;#39;d be happy to chat privately and share my ChatGPT &lt;del&gt;cry for help&lt;/del&gt; transcript. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vttnp", "is_robot_indexable": true, "report_reasons": null, "author": "Squiggy_Pusterdump", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vttnp/using_teracopy_from_sharepoint_o365_to_local/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vttnp/using_teracopy_from_sharepoint_o365_to_local/", "subreddit_subscribers": 692075, "created_utc": 1688995073.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nMy Jurassic P8P67 PRO (Rev. 3.0) supports RAID 0, 1, 10, and 5, and I am focused first and foremost on data security and secondarily on speed. Given the attractive price of 1 TB Western Digital Black HDD (WDBMMA0010HNC-ERSN) where I live (44 USD apiece including taxes), does it make sense to buy four of them and set them up in a RAID 10 configuration? Or do you recommend something better at a similar cost that fulfils the stated goals?\n\nThank you.", "author_fullname": "t2_px8ps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID configuration with HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14wkjdh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689061716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;My Jurassic P8P67 PRO (Rev. 3.0) supports RAID 0, 1, 10, and 5, and I am focused first and foremost on data security and secondarily on speed. Given the attractive price of 1 TB Western Digital Black HDD (WDBMMA0010HNC-ERSN) where I live (44 USD apiece including taxes), does it make sense to buy four of them and set them up in a RAID 10 configuration? Or do you recommend something better at a similar cost that fulfils the stated goals?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wkjdh", "is_robot_indexable": true, "report_reasons": null, "author": "In_der_Tat", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wkjdh/raid_configuration_with_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wkjdh/raid_configuration_with_hdds/", "subreddit_subscribers": 692075, "created_utc": 1689061716.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_pv9ex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[$123] Western Digital 8TB WD Red Plus NAS Internal Hard Drive HDD - 5640 RPM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14wk55r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1689060425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.com/dp/B09QQX27GM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "250TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14wk55r", "is_robot_indexable": true, "report_reasons": null, "author": "Hamany99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14wk55r/123_western_digital_8tb_wd_red_plus_nas_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.com/dp/B09QQX27GM", "subreddit_subscribers": 692075, "created_utc": 1689060425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_pv9ex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[$240] Western Digital 18TB WD Red Pro NAS Internal Hard Drive HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14wk2qy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1689060190.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.com/Western-Digital-18TB-Internal-Drive/dp/B08K3TFM92/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "250TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "14wk2qy", "is_robot_indexable": true, "report_reasons": null, "author": "Hamany99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14wk2qy/240_western_digital_18tb_wd_red_pro_nas_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.com/Western-Digital-18TB-Internal-Drive/dp/B08K3TFM92/", "subreddit_subscribers": 692075, "created_utc": 1689060190.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nUntil now, I ran a script every day using [bulk-downloader-for-reddit](https://github.com/aliparlakci/bulk-downloader-for-reddit) to archive about ten subreddits. It usually took less than an hour each time, even when it had to download video files of a few hundred MB.\n\nFor the past few days, it has become extremely long and it takes more than an hour for a single subreddit.\n\nSince BDFR does not use an API key, I would like to know if some kind of limitation of posts has been introduced on Reddit? So it is not possible to fetch X posts per hour.\n\nThank you.\n\n**Edit:** By inspecting the logs, I notice that at irregular intervals (after 50 posts, after 115 posts...), there is a pause of about 10 minutes.\n\n**Edit 2:** I confirm that there is indeed a 10-minute break added and that it did not exist before. Can someone who is used to using this tool verify on their own with a simple scrape of 100 posts on a subreddit?", "author_fullname": "t2_vc7clehs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Limited Reddit access?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_14wjtgy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689061946.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689059346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;Until now, I ran a script every day using &lt;a href=\"https://github.com/aliparlakci/bulk-downloader-for-reddit\"&gt;bulk-downloader-for-reddit&lt;/a&gt; to archive about ten subreddits. It usually took less than an hour each time, even when it had to download video files of a few hundred MB.&lt;/p&gt;\n\n&lt;p&gt;For the past few days, it has become extremely long and it takes more than an hour for a single subreddit.&lt;/p&gt;\n\n&lt;p&gt;Since BDFR does not use an API key, I would like to know if some kind of limitation of posts has been introduced on Reddit? So it is not possible to fetch X posts per hour.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; By inspecting the logs, I notice that at irregular intervals (after 50 posts, after 115 posts...), there is a pause of about 10 minutes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt; I confirm that there is indeed a 10-minute break added and that it did not exist before. Can someone who is used to using this tool verify on their own with a simple scrape of 100 posts on a subreddit?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?auto=webp&amp;s=736012729fce4e2d2ec366a2cae6151ecde7d679", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d7c2504a370bb24eeccb9c381d3473424899201", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f77d2c588bdc43b4bbd572f5ab3ed81f82d9bbf", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ecd4cf78e7601184d8964b43aab58667d61f7fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=777ca7c11ca74698ec60f75acbe489d02a7424a2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7602bf8937eaee56d88bb29d5e7ce9719badcc86", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fSJRCaP4akvkiwChtVpjKUa54prihQMSxdGRwr95fWY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91225afdb929c5a37ed86dde04ec2c054e66dd9a", "width": 1080, "height": 540}], "variants": {}, "id": "EN5KeQdPAXMI46GDBO_gxx2eH2KOZeykxcJhAPo5_UA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wjtgy", "is_robot_indexable": true, "report_reasons": null, "author": "Feeling_Usual1541", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wjtgy/limited_reddit_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wjtgy/limited_reddit_access/", "subreddit_subscribers": 692075, "created_utc": 1689059346.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My job requires me to buy a external SSD for office work and I was wondering what SSD is best suited for multiple read and writes on a daily basis.", "author_fullname": "t2_w7dusgho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of external SSD has the best read and write speeds for office work adding and removing documents, photos, and videos on a daily basis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wirsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689056065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job requires me to buy a external SSD for office work and I was wondering what SSD is best suited for multiple read and writes on a daily basis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wirsg", "is_robot_indexable": true, "report_reasons": null, "author": "L3aking-Faucet", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wirsg/what_kind_of_external_ssd_has_the_best_read_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wirsg/what_kind_of_external_ssd_has_the_best_read_and/", "subreddit_subscribers": 692075, "created_utc": 1689056065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to do a daily cron task on a linux server that sends a tar file offsite in case of catastrophic failure. The tar file is fairly small: &lt;10mb. It must be executable from a linux bash file so it cannot include complicated apps or javascript transactions with a web server. What's a simple way to accomplish this?", "author_fullname": "t2_pruyzkp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "simple backup solution for small tar file (linux)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wfgh5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689045971.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to do a daily cron task on a linux server that sends a tar file offsite in case of catastrophic failure. The tar file is fairly small: &amp;lt;10mb. It must be executable from a linux bash file so it cannot include complicated apps or javascript transactions with a web server. What&amp;#39;s a simple way to accomplish this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wfgh5", "is_robot_indexable": true, "report_reasons": null, "author": "GetInHereStalker", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wfgh5/simple_backup_solution_for_small_tar_file_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wfgh5/simple_backup_solution_for_small_tar_file_linux/", "subreddit_subscribers": 692075, "created_utc": 1689045971.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just want to have an offline copy of all svg icons on svgrepo.com because I fear that the site may reach a tipping point in the future where access will be restricted further and further (like it happened with flaticon.com).\n\nHttrack and wget seem quite slow, I don't know how long the crawling is going to take and whether I'll be able to cleanly resume the mirroring after connection drops.\n\nI found an old thread where this has been done before but the archive is down, it would be quite outdated by now and sadly nobody shared instructions on their crawling: https://www.reddit.com/r/DataHoarder/comments/lj4ju6/browse_300000_svg_vectors_and_icons_free/\n\nCan anyone help me to hoard this data?\n\nThank you!", "author_fullname": "t2_5fib0w5l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "svgrepo.com - How to make an offline copy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14wb8rx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689034659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just want to have an offline copy of all svg icons on svgrepo.com because I fear that the site may reach a tipping point in the future where access will be restricted further and further (like it happened with flaticon.com).&lt;/p&gt;\n\n&lt;p&gt;Httrack and wget seem quite slow, I don&amp;#39;t know how long the crawling is going to take and whether I&amp;#39;ll be able to cleanly resume the mirroring after connection drops.&lt;/p&gt;\n\n&lt;p&gt;I found an old thread where this has been done before but the archive is down, it would be quite outdated by now and sadly nobody shared instructions on their crawling: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/lj4ju6/browse_300000_svg_vectors_and_icons_free/\"&gt;https://www.reddit.com/r/DataHoarder/comments/lj4ju6/browse_300000_svg_vectors_and_icons_free/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me to hoard this data?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14wb8rx", "is_robot_indexable": true, "report_reasons": null, "author": "abolish98", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14wb8rx/svgrepocom_how_to_make_an_offline_copy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14wb8rx/svgrepocom_how_to_make_an_offline_copy/", "subreddit_subscribers": 692075, "created_utc": 1689034659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My current unraid setup is repurposed from my old gaming PC from 2017. Has a Core i5-6500, 16GB ram, 3 HDDs (2x4TB and 1x1TB) and 2 SSDs (256GB &amp; 128GB). \n\nI recently acquired 2 6TB drives for the setup. However, I only have 6 SATA III ports in my PC. 5 of them already used in my existing setup. What would the best way be to add one additional SATA port to my Setup?\n\nI read that SATA cards like [these](https://www.amazon.ca/Port-Controller-Jmicro-Chipset-Devices/dp/B07Y92RDKH/ref=sr_1_7?crid=3M0OOCO2JJPR1) tend to be less reliable and fall short of HBA cards like [these](https://www.ebay.com/sch/i.html?_from=R40&amp;_trksid=p2047675.m570.l1313&amp;_nkw=hba+p20+it&amp;_sacat=0). Is that still true if I want to add only use 1 port on the PCI card?\n\nI'm not concerned about a future upgrade path. When the motherboard/CPU fails (7 years of almost 24x7 use) I'll be discarding most of the components and rebuilding the full thing with a \"proper\" server case/CPU/motherboard/HBA card setup. \n\nSide note: I would be powering the 2 new HDDs with [this](https://www.amazon.ca/Monoprice-108794-24-Inch-15-Pin-Female/dp/B009GULFJ0/ref=sr_1_7?crid=3FDEOU0DG0446). these look crimped and not molded. Thoughts or concerns would be appreciated. Thanks!", "author_fullname": "t2_ff3cdb0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding one more SATA port to my UNRAID setup. What's better: HBA card or 2 port PCI expansion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w45c5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689018388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current unraid setup is repurposed from my old gaming PC from 2017. Has a Core i5-6500, 16GB ram, 3 HDDs (2x4TB and 1x1TB) and 2 SSDs (256GB &amp;amp; 128GB). &lt;/p&gt;\n\n&lt;p&gt;I recently acquired 2 6TB drives for the setup. However, I only have 6 SATA III ports in my PC. 5 of them already used in my existing setup. What would the best way be to add one additional SATA port to my Setup?&lt;/p&gt;\n\n&lt;p&gt;I read that SATA cards like &lt;a href=\"https://www.amazon.ca/Port-Controller-Jmicro-Chipset-Devices/dp/B07Y92RDKH/ref=sr_1_7?crid=3M0OOCO2JJPR1\"&gt;these&lt;/a&gt; tend to be less reliable and fall short of HBA cards like &lt;a href=\"https://www.ebay.com/sch/i.html?_from=R40&amp;amp;_trksid=p2047675.m570.l1313&amp;amp;_nkw=hba+p20+it&amp;amp;_sacat=0\"&gt;these&lt;/a&gt;. Is that still true if I want to add only use 1 port on the PCI card?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not concerned about a future upgrade path. When the motherboard/CPU fails (7 years of almost 24x7 use) I&amp;#39;ll be discarding most of the components and rebuilding the full thing with a &amp;quot;proper&amp;quot; server case/CPU/motherboard/HBA card setup. &lt;/p&gt;\n\n&lt;p&gt;Side note: I would be powering the 2 new HDDs with &lt;a href=\"https://www.amazon.ca/Monoprice-108794-24-Inch-15-Pin-Female/dp/B009GULFJ0/ref=sr_1_7?crid=3FDEOU0DG0446\"&gt;this&lt;/a&gt;. these look crimped and not molded. Thoughts or concerns would be appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w45c5", "is_robot_indexable": true, "report_reasons": null, "author": "Hellohoware123", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w45c5/adding_one_more_sata_port_to_my_unraid_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w45c5/adding_one_more_sata_port_to_my_unraid_setup/", "subreddit_subscribers": 692075, "created_utc": 1689018388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently upgraded to 2 Adaptec 71605 raid controllers. Both controllers are set to HBA mode. I use crystaldiskinfo to view the smart statistics of my drives but ever since the upgrade the program no longer detects the drives. Is there a setting in the configuration program that enables smart statistics pass through? There's no mention of it in the manual but it is supposedly available through Adaptecs maxview storage manager software.", "author_fullname": "t2_zm7dz8n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adaptec 71605 SMART statistics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14w2qci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689015272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently upgraded to 2 Adaptec 71605 raid controllers. Both controllers are set to HBA mode. I use crystaldiskinfo to view the smart statistics of my drives but ever since the upgrade the program no longer detects the drives. Is there a setting in the configuration program that enables smart statistics pass through? There&amp;#39;s no mention of it in the manual but it is supposedly available through Adaptecs maxview storage manager software.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14w2qci", "is_robot_indexable": true, "report_reasons": null, "author": "smorgisborg1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14w2qci/adaptec_71605_smart_statistics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14w2qci/adaptec_71605_smart_statistics/", "subreddit_subscribers": 692075, "created_utc": 1689015272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys - looking for some advice on a NAS build. I'm fairly familiar with building PCs, but not servers. I'm mainly using it for bulk storage for media files (for a future Plex setup), raw photos and video for editing in Capture One and Photoshop, and general file/document storage. I'd like to be able to edit the photos and possibly video straight from the NAS, without having to transfer to an SSD on my editing rig, if possible. I'd also like to be able to watch movies on my TV (I'm assuming via Plex) at the highest/actual quality of the file, rather than transcoding it. Up to 4K at a good bitrate.\n\nI've always assumed I'd use TrueNAS Core, but I'm open to other suggestions if there's a better option. Unraid and things like that don't seem as \"professional\" as TrueNAS, but I don't know enough to have a strong opinion. I plan to use mirrored/RAID1 for ease of expanding future storage. I like the idea of just being able to put in two more drives, rather than worrying about parity drives or painting myself into a corner with a complex configuration of pools.\n\nRight now I plan to have at least 12 drives, but I'd like to leave myself the overhead of expanding up to 18-20. My main lack of knowledge is accomodating all of these drives in a way that doesn't bottleneck performance. What's the correct way to connect 12-18 drives to a motherboard? I've seen 9211-8i RAID controller cards mentioned here. Is this the best option and what kind of motherboard would I want to be able to accomodate enough of them? I also want to be able to use ECC memory.\n\nThe only components I'd say I've \"locked in\" are a Fractal Design Define 7 XL and a good EVGA PSU (unless you guys strongly recommend against either, of course). I want to keep it in a tower configurations, rather than a rack.\n\nSo to summarize, please let me know what you'd recommend for motherboard, ECC memory, RAID controller cards, networking, and if TrueNAS is my best bet.\n\nThank you!", "author_fullname": "t2_3zjtn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help With a NAS Build for Media Files &amp; Photo Editing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vvcy3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688998742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys - looking for some advice on a NAS build. I&amp;#39;m fairly familiar with building PCs, but not servers. I&amp;#39;m mainly using it for bulk storage for media files (for a future Plex setup), raw photos and video for editing in Capture One and Photoshop, and general file/document storage. I&amp;#39;d like to be able to edit the photos and possibly video straight from the NAS, without having to transfer to an SSD on my editing rig, if possible. I&amp;#39;d also like to be able to watch movies on my TV (I&amp;#39;m assuming via Plex) at the highest/actual quality of the file, rather than transcoding it. Up to 4K at a good bitrate.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always assumed I&amp;#39;d use TrueNAS Core, but I&amp;#39;m open to other suggestions if there&amp;#39;s a better option. Unraid and things like that don&amp;#39;t seem as &amp;quot;professional&amp;quot; as TrueNAS, but I don&amp;#39;t know enough to have a strong opinion. I plan to use mirrored/RAID1 for ease of expanding future storage. I like the idea of just being able to put in two more drives, rather than worrying about parity drives or painting myself into a corner with a complex configuration of pools.&lt;/p&gt;\n\n&lt;p&gt;Right now I plan to have at least 12 drives, but I&amp;#39;d like to leave myself the overhead of expanding up to 18-20. My main lack of knowledge is accomodating all of these drives in a way that doesn&amp;#39;t bottleneck performance. What&amp;#39;s the correct way to connect 12-18 drives to a motherboard? I&amp;#39;ve seen 9211-8i RAID controller cards mentioned here. Is this the best option and what kind of motherboard would I want to be able to accomodate enough of them? I also want to be able to use ECC memory.&lt;/p&gt;\n\n&lt;p&gt;The only components I&amp;#39;d say I&amp;#39;ve &amp;quot;locked in&amp;quot; are a Fractal Design Define 7 XL and a good EVGA PSU (unless you guys strongly recommend against either, of course). I want to keep it in a tower configurations, rather than a rack.&lt;/p&gt;\n\n&lt;p&gt;So to summarize, please let me know what you&amp;#39;d recommend for motherboard, ECC memory, RAID controller cards, networking, and if TrueNAS is my best bet.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vvcy3", "is_robot_indexable": true, "report_reasons": null, "author": "alphamini", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vvcy3/help_with_a_nas_build_for_media_files_photo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vvcy3/help_with_a_nas_build_for_media_files_photo/", "subreddit_subscribers": 692075, "created_utc": 1688998742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended drive size for RAID rebuild?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vsfpz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_m6gkgpmz", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "synology", "selftext": "Currently I have a Synology DS720+ (2 bays) with two 8TB drives in SHR. It\u2019s getting full, so I\u2019m thinking of upgrading to DS1522+ (5 bays), buying two 16TB drives, and then having 3 empty bays for future expansion. That way, I can expand from 16TB to 64TB in the future.\n\nI am aware of the 3-2-1 ideal backup strategy. The problem is the only backup I have now are external drives. While having another 5-bay NAS unit for backup is ideal, that would be too expensive for me. \n\nI\u2019m worried if I use 16TB drives, if one of them fails, the rebuild time would take too long, and the risk of a second drive failing during the rebuild would rise considerably. Some people recommend using 8TB drives instead to minimize rebuild time, but I feel that 8TB drives are just not big enough.\n\nWhat drive size do you guys think would be reasonable to balance between storage size vs raid rebuild time?", "author_fullname": "t2_m6gkgpmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended drive size for RAID rebuild?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/synology", "hidden": false, "pwls": null, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vr19s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "NAS hardware", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1688991278.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688987475.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have a Synology DS720+ (2 bays) with two 8TB drives in SHR. It\u2019s getting full, so I\u2019m thinking of upgrading to DS1522+ (5 bays), buying two 16TB drives, and then having 3 empty bays for future expansion. That way, I can expand from 16TB to 64TB in the future.&lt;/p&gt;\n\n&lt;p&gt;I am aware of the 3-2-1 ideal backup strategy. The problem is the only backup I have now are external drives. While having another 5-bay NAS unit for backup is ideal, that would be too expensive for me. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m worried if I use 16TB drives, if one of them fails, the rebuild time would take too long, and the risk of a second drive failing during the rebuild would rise considerably. Some people recommend using 8TB drives instead to minimize rebuild time, but I feel that 8TB drives are just not big enough.&lt;/p&gt;\n\n&lt;p&gt;What drive size do you guys think would be reasonable to balance between storage size vs raid rebuild time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b45c7c8-4b25-11ed-a1f3-5a29a1a8c4d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2s4co", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "14vr19s", "is_robot_indexable": true, "report_reasons": null, "author": "Unable_Rest6209", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/synology/comments/14vr19s/recommended_drive_size_for_raid_rebuild/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/synology/comments/14vr19s/recommended_drive_size_for_raid_rebuild/", "subreddit_subscribers": 125121, "created_utc": 1688987475.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1688991479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/synology/comments/14vr19s/recommended_drive_size_for_raid_rebuild/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vsfpz", "is_robot_indexable": true, "report_reasons": null, "author": "Unable_Rest6209", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_14vr19s", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/14vsfpz/recommended_drive_size_for_raid_rebuild/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/synology/comments/14vr19s/recommended_drive_size_for_raid_rebuild/", "subreddit_subscribers": 692075, "created_utc": 1688991479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, just completed learning selenium for work,   \nloved the process however, I've finished the course and now I want to use it for real some real world applications, any scripts you guys wish you had but don't have the time to write ?  \nThanks in advanced. ", "author_fullname": "t2_j36z7se0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automation script request", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vqrrn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688986747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, just completed learning selenium for work,&lt;br/&gt;\nloved the process however, I&amp;#39;ve finished the course and now I want to use it for real some real world applications, any scripts you guys wish you had but don&amp;#39;t have the time to write ?&lt;br/&gt;\nThanks in advanced. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vqrrn", "is_robot_indexable": true, "report_reasons": null, "author": "Which_Ad_3554", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vqrrn/automation_script_request/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vqrrn/automation_script_request/", "subreddit_subscribers": 692075, "created_utc": 1688986747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "anyone have a mirror, their site is broken with \"Product Not Found or is no longer active\"\n\nboth for windows and linux, before or after login\n\nhttps://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;softwareId=MTX_1e0b6903d237404a8501f04f73\n\nhttps://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;softwareId=MTX_2d9369cb0bf4495f8840a4a8a2", "author_fullname": "t2_7bbojdkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirror for HP StoreOpen 3.5", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14vq909", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1688985121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;anyone have a mirror, their site is broken with &amp;quot;Product Not Found or is no longer active&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;both for windows and linux, before or after login&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;amp;softwareId=MTX_1e0b6903d237404a8501f04f73\"&gt;https://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;amp;softwareId=MTX_1e0b6903d237404a8501f04f73&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;amp;softwareId=MTX_2d9369cb0bf4495f8840a4a8a2\"&gt;https://support.hpe.com/connect/s/softwaredetails?language=en_US&amp;amp;softwareId=MTX_2d9369cb0bf4495f8840a4a8a2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "14vq909", "is_robot_indexable": true, "report_reasons": null, "author": "kokizzu2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/14vq909/mirror_for_hp_storeopen_35/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/14vq909/mirror_for_hp_storeopen_35/", "subreddit_subscribers": 692075, "created_utc": 1688985121.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}