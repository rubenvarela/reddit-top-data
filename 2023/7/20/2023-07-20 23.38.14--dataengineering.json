{"kind": "Listing", "data": {"after": "t3_154zy6w", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just a quick rant. Thankfully I don\u2019t have to deal with this woman anymore since I\u2019m changing companies soon. \n\nWe had a legacy process of sql queries running on notebooks done by non-engineers. \n\nNow, the company decided that because this product brings a lot of money, it should be maintained by actual engineers. My team started working on this \u201cmigration\u201d and now that almost everything is done, this PO wants to have access to change code, etc. She is questioning every engineering decision Ive made like if she knew. Aside from not knowing what git is, I got a funny (not really funny) passive aggressive question \u201cwhy do you have everything starting with feature on github, better rename it to something we all know what it means\u201d. This is just the tip of the iceberg lol.\n\nI honestly feel sorry for my teammates that will have to deal with her after I\u2019m gone. \n\nHope no one in this sub is going through this lol.", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u201cTechnical\u201d PO driving me and my team nuts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154qdwf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689858267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a quick rant. Thankfully I don\u2019t have to deal with this woman anymore since I\u2019m changing companies soon. &lt;/p&gt;\n\n&lt;p&gt;We had a legacy process of sql queries running on notebooks done by non-engineers. &lt;/p&gt;\n\n&lt;p&gt;Now, the company decided that because this product brings a lot of money, it should be maintained by actual engineers. My team started working on this \u201cmigration\u201d and now that almost everything is done, this PO wants to have access to change code, etc. She is questioning every engineering decision Ive made like if she knew. Aside from not knowing what git is, I got a funny (not really funny) passive aggressive question \u201cwhy do you have everything starting with feature on github, better rename it to something we all know what it means\u201d. This is just the tip of the iceberg lol.&lt;/p&gt;\n\n&lt;p&gt;I honestly feel sorry for my teammates that will have to deal with her after I\u2019m gone. &lt;/p&gt;\n\n&lt;p&gt;Hope no one in this sub is going through this lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154qdwf", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154qdwf/technical_po_driving_me_and_my_team_nuts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154qdwf/technical_po_driving_me_and_my_team_nuts/", "subreddit_subscribers": 117051, "created_utc": 1689858267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Barbenheimer, Data Engineering edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1553rxb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qmBaGEarBcuyfamaOofxiVOMqmcO0vD2JO8ZA9tyeMo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689888296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/tw33gopht6db1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/tw33gopht6db1.png?auto=webp&amp;s=ae79237681aa888d1a8ebc7c5680ef348ddec49f", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/tw33gopht6db1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d39b23943c6c3a063f68042bf09634cf8a053a1f", "width": 108, "height": 108}, {"url": "https://preview.redd.it/tw33gopht6db1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8793d86819e73af3d1f639d9cbffc61df2e78e77", "width": 216, "height": 216}, {"url": "https://preview.redd.it/tw33gopht6db1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fa1cb25a71db0d6865edfd965386c735e60aa8b", "width": 320, "height": 320}, {"url": "https://preview.redd.it/tw33gopht6db1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7707b260d5c514c536636e65fc0698774d610dcb", "width": 640, "height": 640}, {"url": "https://preview.redd.it/tw33gopht6db1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e505a582baa0a10646a4d18b2fbced050d2c3daa", "width": 960, "height": 960}, {"url": "https://preview.redd.it/tw33gopht6db1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c55e82fecdec6867e0786e0eb4174628b34f1948", "width": 1080, "height": 1080}], "variants": {}, "id": "I43xkUU6eWx-3qfP-djCg0H5g0I_tgdJXmDMvmNqnGo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "1553rxb", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1553rxb/barbenheimer_data_engineering_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/tw33gopht6db1.png", "subreddit_subscribers": 117051, "created_utc": 1689888296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did you perhaps singlehanded do [data engineering task]?\n\nOr did you solve [difficult data engineering problem]?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most impressive thing you\u2019ve ever done as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154zbbu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689878337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you perhaps singlehanded do [data engineering task]?&lt;/p&gt;\n\n&lt;p&gt;Or did you solve [difficult data engineering problem]?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154zbbu", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154zbbu/what_is_the_most_impressive_thing_youve_ever_done/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154zbbu/what_is_the_most_impressive_thing_youve_ever_done/", "subreddit_subscribers": 117051, "created_utc": 1689878337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am (Data) engineer with near to 5 years of experience in Data. \n\nMy experience consists of working majorly on SQL (have got intermediate level expertise) and in past 1 year am working on Snowflake sql and ETL tools (Datastage &amp; Snaplogic - but not too deep).. Apart from these know some basics on Power BI &amp; recently started on Python).. \n\nI am STUCK as I feel like more of Support Engineer than Dev &amp; have no idea on what to do next as DE is becoming code heavy &amp; I don\u2019t like  advance BI tool work as it\u2019s slow, frustrating with those DAX &amp; M language. \n\nSo should I continue in DE (by learning any visual ETL tool like ADF) OR\n become Data Analyst (but BI tools are must here which I am not comfortable due to above)\nOR \nis becoming Data Scientist possible? \n\nREALLY NEED HELP \ud83d\ude4f\ud83c\udffb", "author_fullname": "t2_cw3fmfvos", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice in deciding DE Career path", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154jdep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689835969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am (Data) engineer with near to 5 years of experience in Data. &lt;/p&gt;\n\n&lt;p&gt;My experience consists of working majorly on SQL (have got intermediate level expertise) and in past 1 year am working on Snowflake sql and ETL tools (Datastage &amp;amp; Snaplogic - but not too deep).. Apart from these know some basics on Power BI &amp;amp; recently started on Python).. &lt;/p&gt;\n\n&lt;p&gt;I am STUCK as I feel like more of Support Engineer than Dev &amp;amp; have no idea on what to do next as DE is becoming code heavy &amp;amp; I don\u2019t like  advance BI tool work as it\u2019s slow, frustrating with those DAX &amp;amp; M language. &lt;/p&gt;\n\n&lt;p&gt;So should I continue in DE (by learning any visual ETL tool like ADF) OR\n become Data Analyst (but BI tools are must here which I am not comfortable due to above)\nOR \nis becoming Data Scientist possible? &lt;/p&gt;\n\n&lt;p&gt;REALLY NEED HELP \ud83d\ude4f\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "154jdep", "is_robot_indexable": true, "report_reasons": null, "author": "raj_gd", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154jdep/need_advice_in_deciding_de_career_path/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154jdep/need_advice_in_deciding_de_career_path/", "subreddit_subscribers": 117051, "created_utc": 1689835969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are working on a streaming spark data pipeline that reads events from multiple Kafka topics, perform some stateful operations on the data and outputs it to another Kafka topic.\nToday when we want to deploy a new version, we need to stop the currently running jobs, upload the jars to synapse (yes we are using synapse), and then run it again, picking up from where the checkpoint left off. checkpoint is crucial here btw because it stores data about state which helps with the stateful operations.\n\nHas anyone who's doing similar stuff achieved zero down time deployments without losing data and with minimum side effects?\nI would love to hear about your techniques.\n\nThanks!", "author_fullname": "t2_epwdhiqls", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you get zero down time in your streaming data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154v5vj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689869211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are working on a streaming spark data pipeline that reads events from multiple Kafka topics, perform some stateful operations on the data and outputs it to another Kafka topic.\nToday when we want to deploy a new version, we need to stop the currently running jobs, upload the jars to synapse (yes we are using synapse), and then run it again, picking up from where the checkpoint left off. checkpoint is crucial here btw because it stores data about state which helps with the stateful operations.&lt;/p&gt;\n\n&lt;p&gt;Has anyone who&amp;#39;s doing similar stuff achieved zero down time deployments without losing data and with minimum side effects?\nI would love to hear about your techniques.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "154v5vj", "is_robot_indexable": true, "report_reasons": null, "author": "data_is_great", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154v5vj/how_did_you_get_zero_down_time_in_your_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154v5vj/how_did_you_get_zero_down_time_in_your_streaming/", "subreddit_subscribers": 117051, "created_utc": 1689869211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I need some suggestions for projects involving Spark or Spark Streaming with Kafka and ELK, as I have searched but haven't found anything yet. I am just a little bit confused. Can anyone please help me? ", "author_fullname": "t2_feara4tb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I would like some project suggestions involving Spark.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154pq2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689856510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need some suggestions for projects involving Spark or Spark Streaming with Kafka and ELK, as I have searched but haven&amp;#39;t found anything yet. I am just a little bit confused. Can anyone please help me? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "154pq2q", "is_robot_indexable": true, "report_reasons": null, "author": "Kratos_1412", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154pq2q/i_would_like_some_project_suggestions_involving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154pq2q/i_would_like_some_project_suggestions_involving/", "subreddit_subscribers": 117051, "created_utc": 1689856510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had a systems design interview that I failed because I wasn't sure how to answer this question.\n\nMy naive ass said I would store it all on an in-mem db like redis and set the params there and just call the process that way.\n\nNot sure if there's a better way", "author_fullname": "t2_4jzrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have 100 different data sources and each one needs to have a different config file. What's the best way to design this process?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154g5w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689825254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had a systems design interview that I failed because I wasn&amp;#39;t sure how to answer this question.&lt;/p&gt;\n\n&lt;p&gt;My naive ass said I would store it all on an in-mem db like redis and set the params there and just call the process that way.&lt;/p&gt;\n\n&lt;p&gt;Not sure if there&amp;#39;s a better way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "154g5w3", "is_robot_indexable": true, "report_reasons": null, "author": "epictaco", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154g5w3/if_you_have_100_different_data_sources_and_each/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154g5w3/if_you_have_100_different_data_sources_and_each/", "subreddit_subscribers": 117051, "created_utc": 1689825254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow DEs!\n\nI've been exploring the idea of seeing tables as mathematical functions, where each column is a lookup function of the table's index:\n\nMath: `f(x)=y` Table: `column(index)=value`\n\nWhich means, theoretically, changes to the values in a table could be considered alterations to the table's \"code\". I come from a software engineering background, so my immediate reaction to this thought is that changes to code should always be tracked in a version control system so that:\n\n1. Differences between old and new code can be identified\n2. Code changes can be staged for testing compatibility\n3. We can track change metadata about who made the changes and when they were made\n4. Changes can be reverted to an old version\n\nI've seen hints here and there of people trying to solve these problems. For each of the above version control attributes, below are the current technologies that come to mind to solve the same problem (keep in mind I am fairly new to the industry and haven't used any of these)\n\n1. DataFold's data-diff tool\n2. dbt-tests\n3. change data capture\n4. database time travel\n\nSo I'm thinking that the data engineering field will eventually develop its own version of git for databases, but I would love to hear other people's opinions on whether there is a need or if it is even feasible.", "author_fullname": "t2_vaz49s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data as Code - Git like history for Databases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154wh3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689872108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow DEs!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring the idea of seeing tables as mathematical functions, where each column is a lookup function of the table&amp;#39;s index:&lt;/p&gt;\n\n&lt;p&gt;Math: &lt;code&gt;f(x)=y&lt;/code&gt; Table: &lt;code&gt;column(index)=value&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Which means, theoretically, changes to the values in a table could be considered alterations to the table&amp;#39;s &amp;quot;code&amp;quot;. I come from a software engineering background, so my immediate reaction to this thought is that changes to code should always be tracked in a version control system so that:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Differences between old and new code can be identified&lt;/li&gt;\n&lt;li&gt;Code changes can be staged for testing compatibility&lt;/li&gt;\n&lt;li&gt;We can track change metadata about who made the changes and when they were made&lt;/li&gt;\n&lt;li&gt;Changes can be reverted to an old version&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;ve seen hints here and there of people trying to solve these problems. For each of the above version control attributes, below are the current technologies that come to mind to solve the same problem (keep in mind I am fairly new to the industry and haven&amp;#39;t used any of these)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;DataFold&amp;#39;s data-diff tool&lt;/li&gt;\n&lt;li&gt;dbt-tests&lt;/li&gt;\n&lt;li&gt;change data capture&lt;/li&gt;\n&lt;li&gt;database time travel&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So I&amp;#39;m thinking that the data engineering field will eventually develop its own version of git for databases, but I would love to hear other people&amp;#39;s opinions on whether there is a need or if it is even feasible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154wh3y", "is_robot_indexable": true, "report_reasons": null, "author": "stringofsense", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154wh3y/data_as_code_git_like_history_for_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154wh3y/data_as_code_git_like_history_for_databases/", "subreddit_subscribers": 117051, "created_utc": 1689872108.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4tv0n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena Provisioned Capacity Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_154szgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/MlGe4emC5mxF-O-YiOCHrfM966nOfJamNFbQG2Q0ino.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689864471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bit.kevinslin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bit.kevinslin.com/p/athena-provisioned-capacity", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?auto=webp&amp;s=8eb0b7746d6fd8235530df42cb132be0c06ddea0", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e04bb3da92d14f7b71b816b0db79f91716706d2d", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ccf9a3792d196825affec568401488959f01eb3f", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6504618e8e79ab699c7df70d931108cdeea587c1", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=223c55c4a3a37a3bf354cba6f41b81e84beed303", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/PqA-zJUJEgaV5WIWYsRoP2SO-B3WRM89SdCwdK321bA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13238e9d11ef49a57b13aae1481e6b5da42f5e54", "width": 960, "height": 562}], "variants": {}, "id": "MAwhWfFlfZREUljbE5PsJPHjgIf3RbYYEyMz3G4plSU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "154szgo", "is_robot_indexable": true, "report_reasons": null, "author": "kevins8", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154szgo/athena_provisioned_capacity_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bit.kevinslin.com/p/athena-provisioned-capacity", "subreddit_subscribers": 117051, "created_utc": 1689864471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nI'm working on a project and need your guidance. I'm attempting to build a solution that daily downloads a file from a REST API, makes another call to the same API to indicate I want another measurement for the day (to be retrieved \"tomorrow\"), and then stores this file in a specific format: id-date.json. This process is repeated for 3 different IDs, resulting in 3 files per day.\n\nI have a background in Python and am using Azure. My current approach uses Azure Functions, with Python v2 selected for the runtime. The documentation suggested using the `func.write_blob` decorator, but I couldn't get it to work, so I switched to the `azure.storage` module. While I'm able to run it locally, I'm facing issues with deployment.\n\nMy questions for the community:\n\n1. Am I on the right track, or should I consider a different tool or methodology for this task?\n2. For those of you who've done something similar, how did you tackle this problem? \n\nThis seems like a common workflow, yet I've struggled to find a simple, straightforward example. Any tips, suggestions, or pointers to resources would be greatly appreciated!\n\nThanks in advance!", "author_fullname": "t2_r81va7is", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help with Automating Daily REST API Calls and File Storage using Python and Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154yptu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689877025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project and need your guidance. I&amp;#39;m attempting to build a solution that daily downloads a file from a REST API, makes another call to the same API to indicate I want another measurement for the day (to be retrieved &amp;quot;tomorrow&amp;quot;), and then stores this file in a specific format: id-date.json. This process is repeated for 3 different IDs, resulting in 3 files per day.&lt;/p&gt;\n\n&lt;p&gt;I have a background in Python and am using Azure. My current approach uses Azure Functions, with Python v2 selected for the runtime. The documentation suggested using the &lt;code&gt;func.write_blob&lt;/code&gt; decorator, but I couldn&amp;#39;t get it to work, so I switched to the &lt;code&gt;azure.storage&lt;/code&gt; module. While I&amp;#39;m able to run it locally, I&amp;#39;m facing issues with deployment.&lt;/p&gt;\n\n&lt;p&gt;My questions for the community:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Am I on the right track, or should I consider a different tool or methodology for this task?&lt;/li&gt;\n&lt;li&gt;For those of you who&amp;#39;ve done something similar, how did you tackle this problem? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This seems like a common workflow, yet I&amp;#39;ve struggled to find a simple, straightforward example. Any tips, suggestions, or pointers to resources would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "154yptu", "is_robot_indexable": true, "report_reasons": null, "author": "Next_Sink9778", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154yptu/need_help_with_automating_daily_rest_api_calls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154yptu/need_help_with_automating_daily_rest_api_calls/", "subreddit_subscribers": 117051, "created_utc": 1689877025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For audit purposes, how do you keep track of what database or other resources permissions have been granted, who has approved, etc.?\n\nAre you managing this with spreadsheets, or a dedicated tool?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does your permissions approval process look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154ufoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689867669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For audit purposes, how do you keep track of what database or other resources permissions have been granted, who has approved, etc.?&lt;/p&gt;\n\n&lt;p&gt;Are you managing this with spreadsheets, or a dedicated tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154ufoc", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154ufoc/what_does_your_permissions_approval_process_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154ufoc/what_does_your_permissions_approval_process_look/", "subreddit_subscribers": 117051, "created_utc": 1689867669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I get what the binary data type is, but am unsure on when I should be leaving a field as binary and when I should be storing it as a string.\n\nIs the sole difference that it's not being represented in a specific string format like UTF-8?\n\nIs there performance overhead on casting that to a string on reads that would make it beneficial to cast to string during an initial table write?\n\nAnother way to ask the question: Should only binary data be stored as binary or is there an advantage to storing text/string/json data as binary?", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta tables: Binary vs String data types", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154tym1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1689867895.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689866625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get what the binary data type is, but am unsure on when I should be leaving a field as binary and when I should be storing it as a string.&lt;/p&gt;\n\n&lt;p&gt;Is the sole difference that it&amp;#39;s not being represented in a specific string format like UTF-8?&lt;/p&gt;\n\n&lt;p&gt;Is there performance overhead on casting that to a string on reads that would make it beneficial to cast to string during an initial table write?&lt;/p&gt;\n\n&lt;p&gt;Another way to ask the question: Should only binary data be stored as binary or is there an advantage to storing text/string/json data as binary?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154tym1", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154tym1/delta_tables_binary_vs_string_data_types/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154tym1/delta_tables_binary_vs_string_data_types/", "subreddit_subscribers": 117051, "created_utc": 1689866625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nAs many of us are in the process of job hunting or preparing for interviews, it would be extremely helpful to gain insights into the types of questions being asked in recent system design interviews.\nPlease include\n\nCompany:\nTopic:\n\nYour contributions are much appreciated and I hope that we can learn a lot from each other's experiences.\r\n\r\n #Software #SystemDesign #System #Intervie", "author_fullname": "t2_mi42h86v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Share System Design interview topic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154i6yc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689831838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;As many of us are in the process of job hunting or preparing for interviews, it would be extremely helpful to gain insights into the types of questions being asked in recent system design interviews.\nPlease include&lt;/p&gt;\n\n&lt;p&gt;Company:\nTopic:&lt;/p&gt;\n\n&lt;p&gt;Your contributions are much appreciated and I hope that we can learn a lot from each other&amp;#39;s experiences.&lt;/p&gt;\n\n&lt;p&gt;#Software #SystemDesign #System #Intervie&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "154i6yc", "is_robot_indexable": true, "report_reasons": null, "author": "pauloj1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154i6yc/share_system_design_interview_topic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154i6yc/share_system_design_interview_topic/", "subreddit_subscribers": 117051, "created_utc": 1689831838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Came to Reddit to scroll for a bit. Scroll stopped at the first 2 posts on r/dataengineering.\n\nTop 2 posts are associated with skill gaps in product management and data engineering. Have an idea and I want to validate if it makes sense. So here goes...\n\nI am considering **offering my time for 3 free sessions of up to 60 minutes every week to discuss data careers, data architecture, data products, learning resources etc. with data practitioners**.\n\nI am also happy to record these conversations and share the recordings with you, or as podcast episodes to share and contribute to building your brand. (Flexible on this point)\n\nIf you are raising your eyebrows at 'free sessions'... Kudos to you for thinking critically. Here is my reasoning.\n\n**Why do I want to do this? What's in it for me?** *(Discovery and research as always)*\n\n* I want to prioritize learning from actual experiences over scrolling through social media and internet newsletters.\n* I work for a company building a composable data platform and iterating through a data product management mastermind with a handful of my past coworkers.\n* I am already talking to our current users and my existing network and I want to broaden the scope of my research beyond my echo chamber.\n\n**Is my time valuable to you?** *(Maybe... you decide)*\n\n* I am currently a Head of product at a sufficiently funded startup working to fix a few basic broken building blocks of data pipelines.\n* Prior to this I have led data and platform products in eCommerce, automotive manufacturing, surveillance, healthcare and life sciences.\n* I worked as a hands on data engineer from 2009 to 2014 and a software engineer from 2006 to 2009.\n* Through the last 10 years I have:\n   * Mentored several (100s) of data engineers, architects, product managers over the last 10 years.\n   * Managed teams of 12 to 60 people.\n   * Been in interview panels for data roles.\n   * Been in the instructor panel and advisory groups for technical training companies.\n\n**Who is this for?**\n\n* Senior data engineers/architects with experience in scalable data pipelines and data products.\n* Have experience with deploying data pipelines using tools like Kafka, Kinesis, Data Flow, Beam, Pulsar, K-Streams, Spark, Flink etc. (I am interested in diving deeper into this ecosystem right now)\n\nIf this resonates with you - **Book some time on** [**my calendar**](https://app.reclaim.ai/m/drc/high-priority-meeting)\n\nIf you have any questions or feedback, I will look for them in the comments.\n\nThat's it! My 1/10 \\*anonymous\\* self promotional content. Fingers crossed.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Purposeful networking sessions, *therapy sessions*, coaching/mentoring conversations with data folks [Free]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15532vo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689886740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Came to Reddit to scroll for a bit. Scroll stopped at the first 2 posts on &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Top 2 posts are associated with skill gaps in product management and data engineering. Have an idea and I want to validate if it makes sense. So here goes...&lt;/p&gt;\n\n&lt;p&gt;I am considering &lt;strong&gt;offering my time for 3 free sessions of up to 60 minutes every week to discuss data careers, data architecture, data products, learning resources etc. with data practitioners&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I am also happy to record these conversations and share the recordings with you, or as podcast episodes to share and contribute to building your brand. (Flexible on this point)&lt;/p&gt;\n\n&lt;p&gt;If you are raising your eyebrows at &amp;#39;free sessions&amp;#39;... Kudos to you for thinking critically. Here is my reasoning.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why do I want to do this? What&amp;#39;s in it for me?&lt;/strong&gt; &lt;em&gt;(Discovery and research as always)&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I want to prioritize learning from actual experiences over scrolling through social media and internet newsletters.&lt;/li&gt;\n&lt;li&gt;I work for a company building a composable data platform and iterating through a data product management mastermind with a handful of my past coworkers.&lt;/li&gt;\n&lt;li&gt;I am already talking to our current users and my existing network and I want to broaden the scope of my research beyond my echo chamber.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Is my time valuable to you?&lt;/strong&gt; &lt;em&gt;(Maybe... you decide)&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I am currently a Head of product at a sufficiently funded startup working to fix a few basic broken building blocks of data pipelines.&lt;/li&gt;\n&lt;li&gt;Prior to this I have led data and platform products in eCommerce, automotive manufacturing, surveillance, healthcare and life sciences.&lt;/li&gt;\n&lt;li&gt;I worked as a hands on data engineer from 2009 to 2014 and a software engineer from 2006 to 2009.&lt;/li&gt;\n&lt;li&gt;Through the last 10 years I have:\n\n&lt;ul&gt;\n&lt;li&gt;Mentored several (100s) of data engineers, architects, product managers over the last 10 years.&lt;/li&gt;\n&lt;li&gt;Managed teams of 12 to 60 people.&lt;/li&gt;\n&lt;li&gt;Been in interview panels for data roles.&lt;/li&gt;\n&lt;li&gt;Been in the instructor panel and advisory groups for technical training companies.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Who is this for?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Senior data engineers/architects with experience in scalable data pipelines and data products.&lt;/li&gt;\n&lt;li&gt;Have experience with deploying data pipelines using tools like Kafka, Kinesis, Data Flow, Beam, Pulsar, K-Streams, Spark, Flink etc. (I am interested in diving deeper into this ecosystem right now)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If this resonates with you - &lt;strong&gt;Book some time on&lt;/strong&gt; &lt;a href=\"https://app.reclaim.ai/m/drc/high-priority-meeting\"&gt;&lt;strong&gt;my calendar&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions or feedback, I will look for them in the comments.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s it! My 1/10 *anonymous* self promotional content. Fingers crossed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JJ8-l-skRdlFc0hTH4NZcPCCVg7s_cy4tnHGT6K7RtM.jpg?auto=webp&amp;s=2010e51e52ab26fe3a578b55469133f32a24685b", "width": 672, "height": 429}, "resolutions": [{"url": "https://external-preview.redd.it/JJ8-l-skRdlFc0hTH4NZcPCCVg7s_cy4tnHGT6K7RtM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=abe534a6d607969b5a65e4b64231cb8a98ba4752", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/JJ8-l-skRdlFc0hTH4NZcPCCVg7s_cy4tnHGT6K7RtM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62e488a23869a114885c5661772ab97704572a6a", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/JJ8-l-skRdlFc0hTH4NZcPCCVg7s_cy4tnHGT6K7RtM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=64b8078140860ea28cb02e0fb781357527599912", "width": 320, "height": 204}, {"url": "https://external-preview.redd.it/JJ8-l-skRdlFc0hTH4NZcPCCVg7s_cy4tnHGT6K7RtM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6fb8c3fb6cb193ea7111de34b09d79276a9b0b63", "width": 640, "height": 408}], "variants": {}, "id": "4rEzINWv8yA14zzwIXLze5cdnyTs_q30mV3yRfnVCz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15532vo", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15532vo/purposeful_networking_sessions_therapy_sessions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15532vo/purposeful_networking_sessions_therapy_sessions/", "subreddit_subscribers": 117051, "created_utc": 1689886740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI am doing a redesign of our datalake (Azure) structure to support incremental loads.\n\nIt needs to support easy deletion of data from specific users. User data is in jsonl format.\n\nWould it make sense to just dump new data into json files, one new file per patient with new data? And then tag or prefix the filename with user ids?\n\nOr does it make more sense to use parquet files and partition by date or similar? I am not aware of any easy way to remove specific rows from parquet files.", "author_fullname": "t2_lixpgvfy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datalake file structure design for easy deletion of specific user data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15510hy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689882120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I am doing a redesign of our datalake (Azure) structure to support incremental loads.&lt;/p&gt;\n\n&lt;p&gt;It needs to support easy deletion of data from specific users. User data is in jsonl format.&lt;/p&gt;\n\n&lt;p&gt;Would it make sense to just dump new data into json files, one new file per patient with new data? And then tag or prefix the filename with user ids?&lt;/p&gt;\n\n&lt;p&gt;Or does it make more sense to use parquet files and partition by date or similar? I am not aware of any easy way to remove specific rows from parquet files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15510hy", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping-Nail-250", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15510hy/datalake_file_structure_design_for_easy_deletion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15510hy/datalake_file_structure_design_for_easy_deletion/", "subreddit_subscribers": 117051, "created_utc": 1689882120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been exploring open-sourced tools to ensure data quality and implement an alerting system based on SQL-based validation rules. During my research, I came across Dataform. I would appreciate your opinion on whether it is the most suitable tool for my use case.\n\nI have tried Great Expectations but have come to the opinion that it is not well-suited for SQL-based validation.", "author_fullname": "t2_8vje3f0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Dataform relevant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154h3p2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689828170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been exploring open-sourced tools to ensure data quality and implement an alerting system based on SQL-based validation rules. During my research, I came across Dataform. I would appreciate your opinion on whether it is the most suitable tool for my use case.&lt;/p&gt;\n\n&lt;p&gt;I have tried Great Expectations but have come to the opinion that it is not well-suited for SQL-based validation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "154h3p2", "is_robot_indexable": true, "report_reasons": null, "author": "Itchy_Advantage_6267", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154h3p2/is_dataform_relevant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154h3p2/is_dataform_relevant/", "subreddit_subscribers": 117051, "created_utc": 1689828170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working as the sole DE on a non tech team. I am free to use whichever tool/tech I want. As I want to transition to backend dev, I want to learn and implement microservices, rest apis, lambdas. I am looking for suggestions on how and where I can use this in my role. P.S: We use GUI ETL tools to get data from external databases, use python to get data from external apis and push all to redshift. All jobs are on AWS instance. The data in redshift is the source for tableau dashboards", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Implementing microservices, API in DE context", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154exvt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689821595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working as the sole DE on a non tech team. I am free to use whichever tool/tech I want. As I want to transition to backend dev, I want to learn and implement microservices, rest apis, lambdas. I am looking for suggestions on how and where I can use this in my role. P.S: We use GUI ETL tools to get data from external databases, use python to get data from external apis and push all to redshift. All jobs are on AWS instance. The data in redshift is the source for tableau dashboards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "154exvt", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154exvt/implementing_microservices_api_in_de_context/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154exvt/implementing_microservices_api_in_de_context/", "subreddit_subscribers": 117051, "created_utc": 1689821595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " hey I had a question about how I should go about learning (upcoming senior undergrad)\n\nI would much rather have more fun building some data ops projects, and I see many junior roles wanting devops skills alongside DE\n\nthere's not many internships available now, and I have just been brushing up on python and SQL (can do medium LC SQL but barely easy python) \n\n\\- so should I keep learning and developing my SWE skills first or pivot into working on data ops projects (preferred)\n\nhere are my projects so you may have a grasp of what I have learned/worked on this year\n\n* Data Manipulation and ETL Processes Using Databricks and Spark SQL\n* Cloud-Based Data Pipeline with Azure Data Factory\n* Real-time Data Pipeline Development using Apache Pulsar and Python (Yelp API)\n\n[https://github.com/terrortad/pulsar\\_pipeline](https://github.com/terrortad/pulsar_pipeline)\n\nprojects I would rather do instead of learning deep dive python and DSA\n\n* Containerized Microservice Application with Logging and Monitoring\n* Data Warehousing and Analysis Pipeline\n* Cloud Infrastructure Automation with Terraform\n* Hybrid Cloud Data Processing w/ Azure and GCP and Disaster Recovery System\n\n\\*a pre thank-you to all insight that is able to be given, I will appreciate everyone's comment!", "author_fullname": "t2_pwk2f3iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best ROI to study?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1555qeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689892903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey I had a question about how I should go about learning (upcoming senior undergrad)&lt;/p&gt;\n\n&lt;p&gt;I would much rather have more fun building some data ops projects, and I see many junior roles wanting devops skills alongside DE&lt;/p&gt;\n\n&lt;p&gt;there&amp;#39;s not many internships available now, and I have just been brushing up on python and SQL (can do medium LC SQL but barely easy python) &lt;/p&gt;\n\n&lt;p&gt;- so should I keep learning and developing my SWE skills first or pivot into working on data ops projects (preferred)&lt;/p&gt;\n\n&lt;p&gt;here are my projects so you may have a grasp of what I have learned/worked on this year&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data Manipulation and ETL Processes Using Databricks and Spark SQL&lt;/li&gt;\n&lt;li&gt;Cloud-Based Data Pipeline with Azure Data Factory&lt;/li&gt;\n&lt;li&gt;Real-time Data Pipeline Development using Apache Pulsar and Python (Yelp API)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/terrortad/pulsar_pipeline\"&gt;https://github.com/terrortad/pulsar_pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;projects I would rather do instead of learning deep dive python and DSA&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Containerized Microservice Application with Logging and Monitoring&lt;/li&gt;\n&lt;li&gt;Data Warehousing and Analysis Pipeline&lt;/li&gt;\n&lt;li&gt;Cloud Infrastructure Automation with Terraform&lt;/li&gt;\n&lt;li&gt;Hybrid Cloud Data Processing w/ Azure and GCP and Disaster Recovery System&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;*a pre thank-you to all insight that is able to be given, I will appreciate everyone&amp;#39;s comment!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?auto=webp&amp;s=b0253344b1bbbab280807bed2a7de1db576fdef2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=958841e02cf5ffe94766bf2fc725dfbb255d1de9", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15e2697476af7fac3a8ab538d3089f01f044f5ae", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc833e8a2cc174458523e8e9acab14775eab228a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f869cc3e59894cfcdedc6fac424f947b9f6f10b6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9046e703e5270ce47ce860fbc5080961b5e9810e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/vYENY6pOsQGhNqdzpYLUXvnp3gDIEc-kHeXD6GJEqjk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f4a149a392e89021c8237a2b8090c659a78d1f2", "width": 1080, "height": 540}], "variants": {}, "id": "8jO7LHIlEpS5k06XiogE2Xv9EeYlgUzcOwreAaAn2rA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1555qeb", "is_robot_indexable": true, "report_reasons": null, "author": "CowUnfair4318", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1555qeb/best_roi_to_study/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1555qeb/best_roi_to_study/", "subreddit_subscribers": 117051, "created_utc": 1689892903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Problems i'm facing:\n\n1. Snowflake costs ()\n2. long pipeline runs pulling from SQL server\n\nIdeal state:\n\n1. modify or create a new generic pipeline that does incremental or full loads\n2. do as much lifting outside snowflake to help with compute costs\n\nI build a pipeline that's very generic and parametrized. It runs based on two inputs, Database table gives the pipeline the source database credentials and a table\\_list that tells the pipelines which tables to pull from based on schema and table names. So i kick off the pipeline, it populates the credentials, goes to the source and pulls the tables we specify in table\\_list. Nothing too special. \n\nFrom there i load into snowflake tables which is our consumption layer. Since ADF does not go directly from SQL server to Snowflake, it forces me to convert it into a flat file. As of now, the flat file is a text file (i am trying to write and consume as parquet or orc file but we have a few things running on the same server and JAVA has too many holes. However, i'm working on getting a specific server for Integration Runtime). Once the flat file is written, i call a stored procedure see below:\n\n    CREATE OR REPLACE PROCEDURE @{activity('LookupSchemaName').output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\n    RETURNS VARCHAR\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    \n    // Intialize parameters\n    var single_quote='\\'';\n    var SCEHMA=TBL_SCHEMA_NAME;\n    var Destination_Table=SF_TBL;\n    var Source_Table=\"@\"+STG_NAME+\"/\"+FILE_PATH+\" (file_format =&gt;\"+single_quote+FILE_FRMT+single_quote+\")  \";\n    \n    // First script to count number of columns and append $ to support quering external stage files\n    \n    var Blob_Columns_Query=\"SELECT CONCAT(\"+single_quote+\"$\"+single_quote\n                +\",LISTAGG(CONCAT(ORDINAL_POSITION,\"+single_quote+\" AS \"+single_quote+\",COLUMN_NAME),\"+single_quote+\",$\"+single_quote\n                +\") WITHIN GROUP (ORDER BY ORDINAL_POSITION) ) \\n FROM INFORMATION_SCHEMA.COLUMNS \\nWHERE TABLE_NAME=\"\n                +single_quote+Destination_Table+single_quote+\" AND TABLE_SCHEMA=\"+single_quote+SCEHMA+single_quote+\";\";\n    var Blob_Columns = snowflake.createStatement( {sqlText: Blob_Columns_Query}  ).execute();\n    Blob_Columns.next();\n    \n    //Final Code Preparation\n    var primary_key=[];\n    var update_columns=[];\n    var No_Update_Records=[];\n    var Columns_with_table_name=[];\n    var All_Columns=[];\n    var cmd = \"DESC TABLE \"+Destination_Table;\n    var stmt = snowflake.createStatement( {sqlText: cmd}  );\n    var result1 = stmt.execute();\n    while(result1.next() )\n    {\n      if(result1.getColumnValue(6)=='Y')\n      {\n            primary_key.push( 'A.'+result1.getColumnValue(1)+'=B.'+result1.getColumnValue(1) );\n      }\n      else\n      {\n            update_columns.push( 'A.'+result1.getColumnValue(1)+'=B.'+result1.getColumnValue(1) );\n            No_Update_Records.push( 'A.'+result1.getColumnValue(1)+'&lt;&gt;B.'+result1.getColumnValue(1) );\n      }\n      Columns_with_table_name.push( 'B.'+result1.getColumnValue(1) );\n      All_Columns.push( result1.getColumnValue(1) );\n    }\n    // combine\n    All_Columns=All_Columns.join(\",\");\n    Columns_with_table_name=Columns_with_table_name.join(\",\");\n    update_columns=update_columns.join(\",\");\n    primary_key=primary_key.join(\" AND \");\n    No_Update_Records=No_Update_Records.join(\" OR \");\n    \n    var Final_Command=  \"\\nMERGE INTO \"+Destination_Table+\" AS A USING (SELECT \"+Blob_Columns.getColumnValue(1)+\" FROM \"+Source_Table+\") AS B ON \"+primary_key+\n                        \"\\nWHEN MATCHED AND \"+No_Update_Records+\" THEN UPDATE SET \"+update_columns +\n                        \"\\nWHEN NOT MATCHED THEN INSERT ( \"+All_Columns+\" ) VALUES ( \"+ Columns_with_table_name +\" );\"\n    var Deleta_Load=snowflake.createStatement( {sqlText: Final_Command}  ).execute();   \n    Deleta_Load.next();\n    return \"Rows_Inserted:- \"+Deleta_Load.getColumnValue(1)+\"\\tRows_Updated:- \"+Deleta_Load.getColumnValue(2);\n    \n    \n    $$;\n\nOne of the problems i have now is that when i pull from the Source, i append a column called LOAD\\_DATE and populate with utc(now). It then takes the stored procedure above and loads it into snowflake. If the records are new based on the primary keys, it inserts it as new, but if the records are updated it updates the existing records (i'm doing incremental load). The problem is that since the LOAD\\_DATE is created at the run of the pipeline, it things that the records are updated and so does a merge and updates said records in snowflake, even though they are the same records. \n\nI removed the LOAD\\_DATE from the ADF pipeline and tried to modify the stored procedure to insert the LOAD\\_DATE as it's doing the load into snowflake with this updated code\n\n    CREATE OR REPLACE PROCEDURE @{activity('LookupSchemaName').output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\n    RETURNS VARCHAR\n    LANGUAGE JAVASCRIPT\n    AS\n    $$\n    \n    // Initialize parameters\n    var single_quote = '\\'';\n    var SCEHMA = TBL_SCHEMA_NAME;\n    var Destination_Table = SF_TBL;\n    var Source_Table = \"@\" + STG_NAME + \"/\" + FILE_PATH + \" (file_format =&gt; \" + single_quote + FILE_FRMT + single_quote + \")\";\n    \n    // First script to count number of columns and append $ to support querying external stage files\n    var Blob_Columns_Query = \"SELECT CONCAT(\" + single_quote + \"$\" + single_quote +\n        \", LISTAGG(CONCAT(ORDINAL_POSITION, \" + single_quote + \" AS \" + single_quote + \", COLUMN_NAME), \" + single_quote + \",$\" + single_quote +\n        \") WITHIN GROUP (ORDER BY ORDINAL_POSITION)) \\n FROM INFORMATION_SCHEMA.COLUMNS \\n WHERE TABLE_NAME = \" +\n        single_quote + Destination_Table + single_quote + \" AND TABLE_SCHEMA = \" + single_quote + SCEHMA + single_quote + \";\";\n    var Blob_Columns = snowflake.createStatement({sqlText: Blob_Columns_Query}).execute();\n    Blob_Columns.next();\n    \n    // Final Code Preparation\n    var primary_key = [];\n    var update_columns = [];\n    var No_Update_Records = [];\n    var Columns_with_table_name = [];\n    var All_Columns = [];\n    var cmd = \"DESC TABLE \" + Destination_Table;\n    var stmt = snowflake.createStatement({sqlText: cmd});\n    var result1 = stmt.execute();\n    while (result1.next()) {\n        if (result1.getColumnValue(6) == 'Y') {\n            primary_key.push('A.' + result1.getColumnValue(1) + '=B.' + result1.getColumnValue(1));\n        } else {\n            update_columns.push('A.' + result1.getColumnValue(1) + '=B.' + result1.getColumnValue(1));\n            No_Update_Records.push('A.' + result1.getColumnValue(1) + '&lt;&gt;B.' + result1.getColumnValue(1));\n        }\n        Columns_with_table_name.push('B.' + result1.getColumnValue(1));\n        All_Columns.push(result1.getColumnValue(1));\n    }\n    \n    // Check if LOAD_DATE column exists in All_Columns\n    var hasLoadDateColumn = All_Columns.includes('LOAD_DATE');\n    \n    // Append LOAD_DATE column if it doesn't exist\n    if (!hasLoadDateColumn) {\n        All_Columns.push(\"LOAD_DATE\");\n        Columns_with_table_name.push(\"CURRENT_TIMESTAMP() AS LOAD_DATE\");\n    }\n    \n    // Combine\n    All_Columns = All_Columns.join(\",\");\n    Columns_with_table_name = Columns_with_table_name.join(\",\");\n    update_columns = update_columns.join(\",\");\n    primary_key = primary_key.join(\" AND \");\n    No_Update_Records = No_Update_Records.join(\" OR \");\n    \n    var Final_Command = `\n    MERGE INTO ${Destination_Table} AS A \n    USING (\n        SELECT ${Blob_Columns.getColumnValue(1)}, CURRENT_TIMESTAMP() AS LOAD_DATE \n        FROM ${Source_Table}\n    ) AS B \n    ON ${primary_key}\n    WHEN MATCHED AND ${No_Update_Records} THEN UPDATE SET ${update_columns}\n    WHEN NOT MATCHED THEN INSERT (${All_Columns}) VALUES (${Columns_with_table_name});\n    `;\n    \n    var Deleta_Load = snowflake.createStatement({sqlText: Final_Command}).execute();\n    Deleta_Load.next();\n    \n    return \"Rows_Inserted:- \" + Deleta_Load.getColumnValue(1) + \"\\tRows_Updated:- \" + Deleta_Load.getColumnValue(2);\n    \n    $$;\n\nHowever, i get this error:\n\n    Operation on target ADL_Snowflake failed: Failure happened on 'Source' side. ErrorCode=UserErrorOdbcOperationFailed,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\n    SQL compilation error:\n    ambiguous column name 'LOAD_DATE'\n    At Statement.execute, line 64 position 70,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,''Type=System.Data.Odbc.OdbcException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\n    SQL compilation error:\n    ambiguous column name 'LOAD_DATE'\n    At Statement.execute, line 64 position 70,Source=SnowflakeODBC_sb64.dll,'\n\nmeaning it doesn't seem to know what do with the column. \n\nI also tried to give the snowflake a default value of  CURRENT\\_TIMESTAMP() so that if nothing is inserted it will give it the timestamp at insert. However that does not seem to be working either. The data loads fine, but the LOAD\\_DATE column is just null records. \n\nI'm not quite sure what i'm doing wrong. If there are other approaches to doing this process outside a stored procedure, i'm absolutely open to it since i have no experience in writing javascript besides what chatGPT helps me with. \n\nBonus question is this: when i add a new source or additional tables for consuption, i have to manually write the DDL. is there a way for me to automate this process and have incremental or full loads?", "author_fullname": "t2_k89p9s2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need some input on how to make my pipeline more efficient", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1555n7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689892688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Problems i&amp;#39;m facing:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Snowflake costs ()&lt;/li&gt;\n&lt;li&gt;long pipeline runs pulling from SQL server&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ideal state:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;modify or create a new generic pipeline that does incremental or full loads&lt;/li&gt;\n&lt;li&gt;do as much lifting outside snowflake to help with compute costs&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I build a pipeline that&amp;#39;s very generic and parametrized. It runs based on two inputs, Database table gives the pipeline the source database credentials and a table_list that tells the pipelines which tables to pull from based on schema and table names. So i kick off the pipeline, it populates the credentials, goes to the source and pulls the tables we specify in table_list. Nothing too special. &lt;/p&gt;\n\n&lt;p&gt;From there i load into snowflake tables which is our consumption layer. Since ADF does not go directly from SQL server to Snowflake, it forces me to convert it into a flat file. As of now, the flat file is a text file (i am trying to write and consume as parquet or orc file but we have a few things running on the same server and JAVA has too many holes. However, i&amp;#39;m working on getting a specific server for Integration Runtime). Once the flat file is written, i call a stored procedure see below:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE @{activity(&amp;#39;LookupSchemaName&amp;#39;).output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVASCRIPT\nAS\n$$\n\n// Intialize parameters\nvar single_quote=&amp;#39;\\&amp;#39;&amp;#39;;\nvar SCEHMA=TBL_SCHEMA_NAME;\nvar Destination_Table=SF_TBL;\nvar Source_Table=&amp;quot;@&amp;quot;+STG_NAME+&amp;quot;/&amp;quot;+FILE_PATH+&amp;quot; (file_format =&amp;gt;&amp;quot;+single_quote+FILE_FRMT+single_quote+&amp;quot;)  &amp;quot;;\n\n// First script to count number of columns and append $ to support quering external stage files\n\nvar Blob_Columns_Query=&amp;quot;SELECT CONCAT(&amp;quot;+single_quote+&amp;quot;$&amp;quot;+single_quote\n            +&amp;quot;,LISTAGG(CONCAT(ORDINAL_POSITION,&amp;quot;+single_quote+&amp;quot; AS &amp;quot;+single_quote+&amp;quot;,COLUMN_NAME),&amp;quot;+single_quote+&amp;quot;,$&amp;quot;+single_quote\n            +&amp;quot;) WITHIN GROUP (ORDER BY ORDINAL_POSITION) ) \\n FROM INFORMATION_SCHEMA.COLUMNS \\nWHERE TABLE_NAME=&amp;quot;\n            +single_quote+Destination_Table+single_quote+&amp;quot; AND TABLE_SCHEMA=&amp;quot;+single_quote+SCEHMA+single_quote+&amp;quot;;&amp;quot;;\nvar Blob_Columns = snowflake.createStatement( {sqlText: Blob_Columns_Query}  ).execute();\nBlob_Columns.next();\n\n//Final Code Preparation\nvar primary_key=[];\nvar update_columns=[];\nvar No_Update_Records=[];\nvar Columns_with_table_name=[];\nvar All_Columns=[];\nvar cmd = &amp;quot;DESC TABLE &amp;quot;+Destination_Table;\nvar stmt = snowflake.createStatement( {sqlText: cmd}  );\nvar result1 = stmt.execute();\nwhile(result1.next() )\n{\n  if(result1.getColumnValue(6)==&amp;#39;Y&amp;#39;)\n  {\n        primary_key.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;=B.&amp;#39;+result1.getColumnValue(1) );\n  }\n  else\n  {\n        update_columns.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;=B.&amp;#39;+result1.getColumnValue(1) );\n        No_Update_Records.push( &amp;#39;A.&amp;#39;+result1.getColumnValue(1)+&amp;#39;&amp;lt;&amp;gt;B.&amp;#39;+result1.getColumnValue(1) );\n  }\n  Columns_with_table_name.push( &amp;#39;B.&amp;#39;+result1.getColumnValue(1) );\n  All_Columns.push( result1.getColumnValue(1) );\n}\n// combine\nAll_Columns=All_Columns.join(&amp;quot;,&amp;quot;);\nColumns_with_table_name=Columns_with_table_name.join(&amp;quot;,&amp;quot;);\nupdate_columns=update_columns.join(&amp;quot;,&amp;quot;);\nprimary_key=primary_key.join(&amp;quot; AND &amp;quot;);\nNo_Update_Records=No_Update_Records.join(&amp;quot; OR &amp;quot;);\n\nvar Final_Command=  &amp;quot;\\nMERGE INTO &amp;quot;+Destination_Table+&amp;quot; AS A USING (SELECT &amp;quot;+Blob_Columns.getColumnValue(1)+&amp;quot; FROM &amp;quot;+Source_Table+&amp;quot;) AS B ON &amp;quot;+primary_key+\n                    &amp;quot;\\nWHEN MATCHED AND &amp;quot;+No_Update_Records+&amp;quot; THEN UPDATE SET &amp;quot;+update_columns +\n                    &amp;quot;\\nWHEN NOT MATCHED THEN INSERT ( &amp;quot;+All_Columns+&amp;quot; ) VALUES ( &amp;quot;+ Columns_with_table_name +&amp;quot; );&amp;quot;\nvar Deleta_Load=snowflake.createStatement( {sqlText: Final_Command}  ).execute();   \nDeleta_Load.next();\nreturn &amp;quot;Rows_Inserted:- &amp;quot;+Deleta_Load.getColumnValue(1)+&amp;quot;\\tRows_Updated:- &amp;quot;+Deleta_Load.getColumnValue(2);\n\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;One of the problems i have now is that when i pull from the Source, i append a column called LOAD_DATE and populate with utc(now). It then takes the stored procedure above and loads it into snowflake. If the records are new based on the primary keys, it inserts it as new, but if the records are updated it updates the existing records (i&amp;#39;m doing incremental load). The problem is that since the LOAD_DATE is created at the run of the pipeline, it things that the records are updated and so does a merge and updates said records in snowflake, even though they are the same records. &lt;/p&gt;\n\n&lt;p&gt;I removed the LOAD_DATE from the ADF pipeline and tried to modify the stored procedure to insert the LOAD_DATE as it&amp;#39;s doing the load into snowflake with this updated code&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE OR REPLACE PROCEDURE @{activity(&amp;#39;LookupSchemaName&amp;#39;).output.firstRow.TARGET_SCHEMA}.BLOBTOSNOWFLAKE(STG_NAME VARCHAR, FILE_PATH VARCHAR, FILE_FRMT VARCHAR, TBL_SCHEMA_NAME VARCHAR, SF_TBL VARCHAR)\nRETURNS VARCHAR\nLANGUAGE JAVASCRIPT\nAS\n$$\n\n// Initialize parameters\nvar single_quote = &amp;#39;\\&amp;#39;&amp;#39;;\nvar SCEHMA = TBL_SCHEMA_NAME;\nvar Destination_Table = SF_TBL;\nvar Source_Table = &amp;quot;@&amp;quot; + STG_NAME + &amp;quot;/&amp;quot; + FILE_PATH + &amp;quot; (file_format =&amp;gt; &amp;quot; + single_quote + FILE_FRMT + single_quote + &amp;quot;)&amp;quot;;\n\n// First script to count number of columns and append $ to support querying external stage files\nvar Blob_Columns_Query = &amp;quot;SELECT CONCAT(&amp;quot; + single_quote + &amp;quot;$&amp;quot; + single_quote +\n    &amp;quot;, LISTAGG(CONCAT(ORDINAL_POSITION, &amp;quot; + single_quote + &amp;quot; AS &amp;quot; + single_quote + &amp;quot;, COLUMN_NAME), &amp;quot; + single_quote + &amp;quot;,$&amp;quot; + single_quote +\n    &amp;quot;) WITHIN GROUP (ORDER BY ORDINAL_POSITION)) \\n FROM INFORMATION_SCHEMA.COLUMNS \\n WHERE TABLE_NAME = &amp;quot; +\n    single_quote + Destination_Table + single_quote + &amp;quot; AND TABLE_SCHEMA = &amp;quot; + single_quote + SCEHMA + single_quote + &amp;quot;;&amp;quot;;\nvar Blob_Columns = snowflake.createStatement({sqlText: Blob_Columns_Query}).execute();\nBlob_Columns.next();\n\n// Final Code Preparation\nvar primary_key = [];\nvar update_columns = [];\nvar No_Update_Records = [];\nvar Columns_with_table_name = [];\nvar All_Columns = [];\nvar cmd = &amp;quot;DESC TABLE &amp;quot; + Destination_Table;\nvar stmt = snowflake.createStatement({sqlText: cmd});\nvar result1 = stmt.execute();\nwhile (result1.next()) {\n    if (result1.getColumnValue(6) == &amp;#39;Y&amp;#39;) {\n        primary_key.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;=B.&amp;#39; + result1.getColumnValue(1));\n    } else {\n        update_columns.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;=B.&amp;#39; + result1.getColumnValue(1));\n        No_Update_Records.push(&amp;#39;A.&amp;#39; + result1.getColumnValue(1) + &amp;#39;&amp;lt;&amp;gt;B.&amp;#39; + result1.getColumnValue(1));\n    }\n    Columns_with_table_name.push(&amp;#39;B.&amp;#39; + result1.getColumnValue(1));\n    All_Columns.push(result1.getColumnValue(1));\n}\n\n// Check if LOAD_DATE column exists in All_Columns\nvar hasLoadDateColumn = All_Columns.includes(&amp;#39;LOAD_DATE&amp;#39;);\n\n// Append LOAD_DATE column if it doesn&amp;#39;t exist\nif (!hasLoadDateColumn) {\n    All_Columns.push(&amp;quot;LOAD_DATE&amp;quot;);\n    Columns_with_table_name.push(&amp;quot;CURRENT_TIMESTAMP() AS LOAD_DATE&amp;quot;);\n}\n\n// Combine\nAll_Columns = All_Columns.join(&amp;quot;,&amp;quot;);\nColumns_with_table_name = Columns_with_table_name.join(&amp;quot;,&amp;quot;);\nupdate_columns = update_columns.join(&amp;quot;,&amp;quot;);\nprimary_key = primary_key.join(&amp;quot; AND &amp;quot;);\nNo_Update_Records = No_Update_Records.join(&amp;quot; OR &amp;quot;);\n\nvar Final_Command = `\nMERGE INTO ${Destination_Table} AS A \nUSING (\n    SELECT ${Blob_Columns.getColumnValue(1)}, CURRENT_TIMESTAMP() AS LOAD_DATE \n    FROM ${Source_Table}\n) AS B \nON ${primary_key}\nWHEN MATCHED AND ${No_Update_Records} THEN UPDATE SET ${update_columns}\nWHEN NOT MATCHED THEN INSERT (${All_Columns}) VALUES (${Columns_with_table_name});\n`;\n\nvar Deleta_Load = snowflake.createStatement({sqlText: Final_Command}).execute();\nDeleta_Load.next();\n\nreturn &amp;quot;Rows_Inserted:- &amp;quot; + Deleta_Load.getColumnValue(1) + &amp;quot;\\tRows_Updated:- &amp;quot; + Deleta_Load.getColumnValue(2);\n\n$$;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, i get this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Operation on target ADL_Snowflake failed: Failure happened on &amp;#39;Source&amp;#39; side. ErrorCode=UserErrorOdbcOperationFailed,&amp;#39;Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\nSQL compilation error:\nambiguous column name &amp;#39;LOAD_DATE&amp;#39;\nAt Statement.execute, line 64 position 70,Source=Microsoft.DataTransfer.Runtime.GenericOdbcConnectors,&amp;#39;&amp;#39;Type=System.Data.Odbc.OdbcException,Message=ERROR [42601] Execution error in store procedure BLOBTOSNOWFLAKE:\nSQL compilation error:\nambiguous column name &amp;#39;LOAD_DATE&amp;#39;\nAt Statement.execute, line 64 position 70,Source=SnowflakeODBC_sb64.dll,&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;meaning it doesn&amp;#39;t seem to know what do with the column. &lt;/p&gt;\n\n&lt;p&gt;I also tried to give the snowflake a default value of  CURRENT_TIMESTAMP() so that if nothing is inserted it will give it the timestamp at insert. However that does not seem to be working either. The data loads fine, but the LOAD_DATE column is just null records. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not quite sure what i&amp;#39;m doing wrong. If there are other approaches to doing this process outside a stored procedure, i&amp;#39;m absolutely open to it since i have no experience in writing javascript besides what chatGPT helps me with. &lt;/p&gt;\n\n&lt;p&gt;Bonus question is this: when i add a new source or additional tables for consuption, i have to manually write the DDL. is there a way for me to automate this process and have incremental or full loads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1555n7i", "is_robot_indexable": true, "report_reasons": null, "author": "TheSnowWorm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1555n7i/i_need_some_input_on_how_to_make_my_pipeline_more/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1555n7i/i_need_some_input_on_how_to_make_my_pipeline_more/", "subreddit_subscribers": 117051, "created_utc": 1689892688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can anyone recommend any good crash course-style resources on JavaScript and basic web dev for someone with a background in data engineering?\n\nI\u2019m pretty familiar with sql, python and data modeling, but I\u2019m trying to get into web development as a hobby. I\u2019m a huge noob on that frontier.\n\nI\u2019d like to build a basic website with embedded videos, photos and a mailing list, but I\u2019ve never touched front end web design, apart from a very short html class in college. Is there any good primer that could outline essential concepts and tools that are fundament to web design?\n\nI thought I\u2019d ask here because maybe someone else found resources that were well suited for data engineers, as we probably don\u2019t need as much help with backend design.", "author_fullname": "t2_6hsp2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody in here experienced with web design and development?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1555dw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689892071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can anyone recommend any good crash course-style resources on JavaScript and basic web dev for someone with a background in data engineering?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m pretty familiar with sql, python and data modeling, but I\u2019m trying to get into web development as a hobby. I\u2019m a huge noob on that frontier.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d like to build a basic website with embedded videos, photos and a mailing list, but I\u2019ve never touched front end web design, apart from a very short html class in college. Is there any good primer that could outline essential concepts and tools that are fundament to web design?&lt;/p&gt;\n\n&lt;p&gt;I thought I\u2019d ask here because maybe someone else found resources that were well suited for data engineers, as we probably don\u2019t need as much help with backend design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1555dw9", "is_robot_indexable": true, "report_reasons": null, "author": "suitupyo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1555dw9/anybody_in_here_experienced_with_web_design_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1555dw9/anybody_in_here_experienced_with_web_design_and/", "subreddit_subscribers": 117051, "created_utc": 1689892071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "so i applied to a data engineer job and by far i have passed 3 rounds (phone screen, 1st vo, 2nd vo), now they invite me to attend on site next week, to go through another 4 rounds of interviews.....\n\ni feel like they want to kill me\n\nso by far i havent been tested a single question of python or sql, and its a very surprising thing to me\n\nthey tailing me about pipeline design, system design and api design, yes, you are seeing this right, api design, as if its interview for sde not de\n\nso now im trying to prepare for the next, and also the final round. i thought if anybody is interested or going through similar preparation process, maybe we can do a mock for each other. i mainly want to look for buddies who wants to do prep on design and not the sql/python codings.\n\nadd me on discord if you wanna team: Elaina#5305 ", "author_fullname": "t2_4ki6pfju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "i have encountered the weirdest process of de interviews ever, and is there anybody want to team up and mock?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1554o9y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689890397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i applied to a data engineer job and by far i have passed 3 rounds (phone screen, 1st vo, 2nd vo), now they invite me to attend on site next week, to go through another 4 rounds of interviews.....&lt;/p&gt;\n\n&lt;p&gt;i feel like they want to kill me&lt;/p&gt;\n\n&lt;p&gt;so by far i havent been tested a single question of python or sql, and its a very surprising thing to me&lt;/p&gt;\n\n&lt;p&gt;they tailing me about pipeline design, system design and api design, yes, you are seeing this right, api design, as if its interview for sde not de&lt;/p&gt;\n\n&lt;p&gt;so now im trying to prepare for the next, and also the final round. i thought if anybody is interested or going through similar preparation process, maybe we can do a mock for each other. i mainly want to look for buddies who wants to do prep on design and not the sql/python codings.&lt;/p&gt;\n\n&lt;p&gt;add me on discord if you wanna team: Elaina#5305 &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1554o9y", "is_robot_indexable": true, "report_reasons": null, "author": "ebink0010", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1554o9y/i_have_encountered_the_weirdest_process_of_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1554o9y/i_have_encountered_the_weirdest_process_of_de/", "subreddit_subscribers": 117051, "created_utc": 1689890397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently in some position between a DE and a Backend Developer wondering where I should go next? I saw coursera has an IBM Data Engineering course, but wanted to hear what resources people found helpful.\n\nMy current job has me writing a data pipeline (in Python) taking real time MQTT data and doing multiple stages of validation and processing and writing to a database.\n\nI'm also writing all the scripts and programs that monitor the data pipeline scripts and automatically restart them, logs issues, etc.\n\nI also wrote the API to expose the data, perform calculations using polars and pandas, and security.\n\nI wrote all the queries for the API as well as tested the queries and added indexes to make sure to reduce how much data needs to be scanned.\n\nI'm currently not making use of anything like Spark and have been doing processing locally with polars. \n\nAre there any good courses on coursera or somewhere else?", "author_fullname": "t2_5p10k8xk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Courses Should I Take and What Cloud Tools Should I be Learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15540pg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689888870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in some position between a DE and a Backend Developer wondering where I should go next? I saw coursera has an IBM Data Engineering course, but wanted to hear what resources people found helpful.&lt;/p&gt;\n\n&lt;p&gt;My current job has me writing a data pipeline (in Python) taking real time MQTT data and doing multiple stages of validation and processing and writing to a database.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also writing all the scripts and programs that monitor the data pipeline scripts and automatically restart them, logs issues, etc.&lt;/p&gt;\n\n&lt;p&gt;I also wrote the API to expose the data, perform calculations using polars and pandas, and security.&lt;/p&gt;\n\n&lt;p&gt;I wrote all the queries for the API as well as tested the queries and added indexes to make sure to reduce how much data needs to be scanned.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently not making use of anything like Spark and have been doing processing locally with polars. &lt;/p&gt;\n\n&lt;p&gt;Are there any good courses on coursera or somewhere else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15540pg", "is_robot_indexable": true, "report_reasons": null, "author": "sersherz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15540pg/what_courses_should_i_take_and_what_cloud_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15540pg/what_courses_should_i_take_and_what_cloud_tools/", "subreddit_subscribers": 117051, "created_utc": 1689888870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you have a source system that typically sends data into your data lake every hour, how can you handle delayed data or missing data altogether? what if the source system sends incomplete data?\n\nI'd imagine there are a few options:\n\n\\- your pipeline can be event-driven (lambda, CF) so lateness doesn't matter, whatever arrives is processed accordingly\n\n\\- your pipeline has sensors (a la airflow) that wait for data to arrive and fail or timeout after a given window\n\n\\- your pipeline has the ability to keep track of data that is \"supposed\" to be there and queues it up for a later data pull / streaming window.\n\n\\- your pipeline uses data quality checks (Deequ, GX, etc.) that determine if a batch of data fails to meet a certain requirement and then retries (?)\n\n&amp;#x200B;\n\nHow do you handle this scenario at your companies? What services do you use?", "author_fullname": "t2_ksjg7gi8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best approach to handle missing data or late data coming from source systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15520f1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689884382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you have a source system that typically sends data into your data lake every hour, how can you handle delayed data or missing data altogether? what if the source system sends incomplete data?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d imagine there are a few options:&lt;/p&gt;\n\n&lt;p&gt;- your pipeline can be event-driven (lambda, CF) so lateness doesn&amp;#39;t matter, whatever arrives is processed accordingly&lt;/p&gt;\n\n&lt;p&gt;- your pipeline has sensors (a la airflow) that wait for data to arrive and fail or timeout after a given window&lt;/p&gt;\n\n&lt;p&gt;- your pipeline has the ability to keep track of data that is &amp;quot;supposed&amp;quot; to be there and queues it up for a later data pull / streaming window.&lt;/p&gt;\n\n&lt;p&gt;- your pipeline uses data quality checks (Deequ, GX, etc.) that determine if a batch of data fails to meet a certain requirement and then retries (?)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How do you handle this scenario at your companies? What services do you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15520f1", "is_robot_indexable": true, "report_reasons": null, "author": "cyoogler", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15520f1/what_is_the_best_approach_to_handle_missing_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15520f1/what_is_the_best_approach_to_handle_missing_data/", "subreddit_subscribers": 117051, "created_utc": 1689884382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After university I got hired as the only engineer to a data team serving dozens of companies (within the company). Since then, Ive done everything from setting up data loaders,  pipelines, and self hosted airflow myself. Recently got a SQL/Python developer helping me do dbt migrations. Feeling pretty bummed out that I never had a full team around me to bounce ideas off of and I need to learn new things by myself. The people I help love me but I was wondering what size everybodys data engineering team is? Any advice for someone in my position to stay mentally strong?\n\n[View Poll](https://www.reddit.com/poll/1551x6d)", "author_fullname": "t2_kmq0e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many data engineers are on your team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1551x6d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689884169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After university I got hired as the only engineer to a data team serving dozens of companies (within the company). Since then, Ive done everything from setting up data loaders,  pipelines, and self hosted airflow myself. Recently got a SQL/Python developer helping me do dbt migrations. Feeling pretty bummed out that I never had a full team around me to bounce ideas off of and I need to learn new things by myself. The people I help love me but I was wondering what size everybodys data engineering team is? Any advice for someone in my position to stay mentally strong?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1551x6d\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1551x6d", "is_robot_indexable": true, "report_reasons": null, "author": "specificanaldolphin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1690056969639, "options": [{"text": "1-2", "id": "23982640"}, {"text": "3-5", "id": "23982641"}, {"text": "6-10", "id": "23982642"}, {"text": "10-15", "id": "23982643"}, {"text": "16-20", "id": "23982644"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 63, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1551x6d/how_many_data_engineers_are_on_your_team/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1551x6d/how_many_data_engineers_are_on_your_team/", "subreddit_subscribers": 117051, "created_utc": 1689884169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In terms of data engineering as a field, think of a seasoned data engineer as an ocean, and I'm just stepping into a plastic kiddie pool you get from Walmart. I've just begun to learn how to create an API in Python that creates a dataset and a table in BigQuery using an API key... What I want to know now is if it's important for my learning to understand how to move data from BigQuery to something like Snowflake? Perhaps I'm thinking about it inefficiently, and instead, I should be trying to now learn how to API into SF and replicate the creating of tables and importing data, not pipe from 1 data warehouse to another? ", "author_fullname": "t2_vnan4h8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it important to learn how to move data from one DW to another? i.e. BQ&gt;Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_154zy6w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689879733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In terms of data engineering as a field, think of a seasoned data engineer as an ocean, and I&amp;#39;m just stepping into a plastic kiddie pool you get from Walmart. I&amp;#39;ve just begun to learn how to create an API in Python that creates a dataset and a table in BigQuery using an API key... What I want to know now is if it&amp;#39;s important for my learning to understand how to move data from BigQuery to something like Snowflake? Perhaps I&amp;#39;m thinking about it inefficiently, and instead, I should be trying to now learn how to API into SF and replicate the creating of tables and importing data, not pipe from 1 data warehouse to another? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "154zy6w", "is_robot_indexable": true, "report_reasons": null, "author": "Analyst2163", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/154zy6w/is_it_important_to_learn_how_to_move_data_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/154zy6w/is_it_important_to_learn_how_to_move_data_from/", "subreddit_subscribers": 117051, "created_utc": 1689879733.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}