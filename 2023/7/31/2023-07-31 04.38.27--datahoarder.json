{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Yet another company just announced that they are going to shutdown their forums and move over to discord, this time it is wargaming / world of warships. \n\n[https://worldofwarships.eu/en/news/community/forum-shutdown/](https://worldofwarships.eu/en/news/community/forum-shutdown/)\n\nTheir forums are going to switch to read only mode within the next few days and are going to be shutdown for good on October 31st, 2023.\n\nThey contain years of conversations about the game and historic naval warfare in general and have together over 9 million post\n\nImportant!!   \nThere are two separate forums [forum.worldofwarships.eu](https://forum.worldofwarships.eu) and [forum.worldofwarships.com](https://forum.worldofwarships.com).   \nBoth are officially hosted by wargaming and afaik effected by the shutdown but have fully unique/independent post/content.\n\nAlso they are hosting several other forums for their other games like world of tanks &amp; world of warplanes (in total 8). As far as i know they havent announced anything regarding those forums yet but i wouldnt be surprised if they will succumb to a similar faith soon.\n\nHappy archiving!\n\nPs Currently preparing to archive both forums, if you want to work together feel free to dm me", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "World of Warships forums going offline October 31st | Millions of posts might get lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dle85", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 174, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 174, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690723017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another company just announced that they are going to shutdown their forums and move over to discord, this time it is wargaming / world of warships. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://worldofwarships.eu/en/news/community/forum-shutdown/\"&gt;https://worldofwarships.eu/en/news/community/forum-shutdown/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Their forums are going to switch to read only mode within the next few days and are going to be shutdown for good on October 31st, 2023.&lt;/p&gt;\n\n&lt;p&gt;They contain years of conversations about the game and historic naval warfare in general and have together over 9 million post&lt;/p&gt;\n\n&lt;p&gt;Important!!&lt;br/&gt;\nThere are two separate forums &lt;a href=\"https://forum.worldofwarships.eu\"&gt;forum.worldofwarships.eu&lt;/a&gt; and &lt;a href=\"https://forum.worldofwarships.com\"&gt;forum.worldofwarships.com&lt;/a&gt;.&lt;br/&gt;\nBoth are officially hosted by wargaming and afaik effected by the shutdown but have fully unique/independent post/content.&lt;/p&gt;\n\n&lt;p&gt;Also they are hosting several other forums for their other games like world of tanks &amp;amp; world of warplanes (in total 8). As far as i know they havent announced anything regarding those forums yet but i wouldnt be surprised if they will succumb to a similar faith soon.&lt;/p&gt;\n\n&lt;p&gt;Happy archiving!&lt;/p&gt;\n\n&lt;p&gt;Ps Currently preparing to archive both forums, if you want to work together feel free to dm me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?auto=webp&amp;s=c93c7558f4bbcd40e6e6b4ab1570780d93616ea4", "width": 1200, "height": 790}, "resolutions": [{"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f352825dfcda2e44bd2fdc29b8ce4c43e02654b", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=326f3db5e50549b26e928886d08a294232cfb72a", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=608ed9cfba21e392b021bc4f7d3614b71d923248", "width": 320, "height": 210}, {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=162ca32ff54454b5142c448eb24dfb2c7c0261fe", "width": 640, "height": 421}, {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c23f20e912fd63dfc7f96e76c30ebc57cb26dbde", "width": 960, "height": 632}, {"url": "https://external-preview.redd.it/bY5rV5R2FrQxZrACqyEhPlZgJtbkiHjCzAV8cnHriyQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfbfce0c27e0756216133b5ff705bdb8a6af8083", "width": 1080, "height": 711}], "variants": {}, "id": "AhbLGXBkvinVZY5PKuwjFtg0D5p8yvjJniic270ARYw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dle85", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 85, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dle85/world_of_warships_forums_going_offline_october/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dle85/world_of_warships_forums_going_offline_october/", "subreddit_subscribers": 695495, "created_utc": 1690723017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My main problem is organization.\n\nI'm constantly re-writing vocabulary (that's already been digitized) in my notebook and notecards because multiple copies of the same thing is great and I keep on finding more efficient ways to create a comprehensible and effective language class, which is exactly what's saving my language, but I feel like I'm drowning in destructible material.\n\nI have enough money for flash drives, and I already have a printer/scanner specifically for this purpose, so making copies of stuff isn't a problem.", "author_fullname": "t2_2p5226u2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have 30ish notecards, 5 half-filled notebooks, and literally hundreds of loose paper with my critically endangered language and cultural practices written down...and on my computer. How should I digitize it efficiently? How should I organize it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15e18mf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690763249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My main problem is organization.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m constantly re-writing vocabulary (that&amp;#39;s already been digitized) in my notebook and notecards because multiple copies of the same thing is great and I keep on finding more efficient ways to create a comprehensible and effective language class, which is exactly what&amp;#39;s saving my language, but I feel like I&amp;#39;m drowning in destructible material.&lt;/p&gt;\n\n&lt;p&gt;I have enough money for flash drives, and I already have a printer/scanner specifically for this purpose, so making copies of stuff isn&amp;#39;t a problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15e18mf", "is_robot_indexable": true, "report_reasons": null, "author": "Starfire-Galaxy", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15e18mf/i_have_30ish_notecards_5_halffilled_notebooks_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15e18mf/i_have_30ish_notecards_5_halffilled_notebooks_and/", "subreddit_subscribers": 695495, "created_utc": 1690763249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am in the process of scanning a large quantity of printed photos. Currently, I am manually naming the files based on the month and year stamped on the back. Is there a software that can visually scan for the date and automatically name the files by date?", "author_fullname": "t2_gkpijpe8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software that Automatically Names Scanned Photos by the Dates Stamped on the Back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dyjnf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690756120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of scanning a large quantity of printed photos. Currently, I am manually naming the files based on the month and year stamped on the back. Is there a software that can visually scan for the date and automatically name the files by date?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dyjnf", "is_robot_indexable": true, "report_reasons": null, "author": "QuietWanderingNerd", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dyjnf/software_that_automatically_names_scanned_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dyjnf/software_that_automatically_names_scanned_photos/", "subreddit_subscribers": 695495, "created_utc": 1690756120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The physical backup situation I'm facing involves the 3-2-1 method. Recently, one of my USB drives fell and stopped working. However, when I opened it up, removed the drive, and connected it to a dock, it worked fine. So, I started using this method to run my backups. Now, unfortunately, the dock has also stopped working, and after reading reviews of similar docks, I'm worried that using them might brick my drives. Given that I have new 20 TB drives times 4, I can't risk losing them.\n\nLooking for advice on what others use for their physical backups. One option I'm considering is putting together a bare-bone rig and connecting the drives directly to it, then copying the data over the network. I'm open to hearing any other suggestions or ideas you may have. What methods or setups have you found to be reliable for physical backups? Thanks", "author_fullname": "t2_143niv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Physical backup dilemma", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dhkct", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690710408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The physical backup situation I&amp;#39;m facing involves the 3-2-1 method. Recently, one of my USB drives fell and stopped working. However, when I opened it up, removed the drive, and connected it to a dock, it worked fine. So, I started using this method to run my backups. Now, unfortunately, the dock has also stopped working, and after reading reviews of similar docks, I&amp;#39;m worried that using them might brick my drives. Given that I have new 20 TB drives times 4, I can&amp;#39;t risk losing them.&lt;/p&gt;\n\n&lt;p&gt;Looking for advice on what others use for their physical backups. One option I&amp;#39;m considering is putting together a bare-bone rig and connecting the drives directly to it, then copying the data over the network. I&amp;#39;m open to hearing any other suggestions or ideas you may have. What methods or setups have you found to be reliable for physical backups? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dhkct", "is_robot_indexable": true, "report_reasons": null, "author": "Jbomb831", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15dhkct/physical_backup_dilemma/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dhkct/physical_backup_dilemma/", "subreddit_subscribers": 695495, "created_utc": 1690710408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello friends! I\u2019m a cinematographer, and generate a reasonable amount of data. I shoot advertising so most shoots only tend to be 1-4 days, and within that time I normally generate between 500gb - 4tb. I have 3 general needs for my data storage. \n\n1. Fast access to about 4tb of data of a time to edit projects. This is critical. \n2. Bulletproof back-up of working files until project is completed. This is critical. \n3. Long term storage of projects. The clients don\u2019t care about this (after approx. 6 months from delivery), but I like having an archive of my work, both personally (I made that!) and there is also a professional element (for pitching new jobs, showreel, etc).\n\nWhat I have currently been doing is using a QNAP NAS (TVS-1282T3) connected to a Mac Pro via 10gbe to edit off and store files. It\u2019s 84tb raw and about 50tb after raid stuff etc. I simultaneously back footage up to a couple of 3.5\u201d drives too, one of which gets kept in the office, and one comes home with me. I have a rolling system where I bump the oldest job off the nas to make room for the new job. \n\nI\u2019m worried about longevity of storage and general cost if I decide to upgrade my camera, which will generate a lot more data to store. I\u2019ve been spitballing:\n\n- some sort of SSD type of raid array to edit off. Approx 8tb, and I could have 2 or 3 for redundancy. Double points if I could get some sort of online back-up from an ssd at the office to an identical ssd at home.\n- LTO reader in the office, so once a project is completed, it goes on 3x tapes.\n\nThis would allow me to have quick storage to edit off, and cheaper, more stable storage to handle my archive which is rarely accessed by me or my clients. \n\nI\u2019d ideally want something mac-compatible. I\u2019ve got 2 MacBook pros, 2 iPad pros, a Mac Pro in the office, and a Mac mini at home. \n\nSorry for the wall of text. I\u2019m wondering if I\u2019m missing something or if anyone had an elegant solution. Thanks so much in advance!", "author_fullname": "t2_4o1koz5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A few questions from a lost cinematographer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15e4xf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690774153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends! I\u2019m a cinematographer, and generate a reasonable amount of data. I shoot advertising so most shoots only tend to be 1-4 days, and within that time I normally generate between 500gb - 4tb. I have 3 general needs for my data storage. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Fast access to about 4tb of data of a time to edit projects. This is critical. &lt;/li&gt;\n&lt;li&gt;Bulletproof back-up of working files until project is completed. This is critical. &lt;/li&gt;\n&lt;li&gt;Long term storage of projects. The clients don\u2019t care about this (after approx. 6 months from delivery), but I like having an archive of my work, both personally (I made that!) and there is also a professional element (for pitching new jobs, showreel, etc).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What I have currently been doing is using a QNAP NAS (TVS-1282T3) connected to a Mac Pro via 10gbe to edit off and store files. It\u2019s 84tb raw and about 50tb after raid stuff etc. I simultaneously back footage up to a couple of 3.5\u201d drives too, one of which gets kept in the office, and one comes home with me. I have a rolling system where I bump the oldest job off the nas to make room for the new job. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m worried about longevity of storage and general cost if I decide to upgrade my camera, which will generate a lot more data to store. I\u2019ve been spitballing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;some sort of SSD type of raid array to edit off. Approx 8tb, and I could have 2 or 3 for redundancy. Double points if I could get some sort of online back-up from an ssd at the office to an identical ssd at home.&lt;/li&gt;\n&lt;li&gt;LTO reader in the office, so once a project is completed, it goes on 3x tapes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This would allow me to have quick storage to edit off, and cheaper, more stable storage to handle my archive which is rarely accessed by me or my clients. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d ideally want something mac-compatible. I\u2019ve got 2 MacBook pros, 2 iPad pros, a Mac Pro in the office, and a Mac mini at home. &lt;/p&gt;\n\n&lt;p&gt;Sorry for the wall of text. I\u2019m wondering if I\u2019m missing something or if anyone had an elegant solution. Thanks so much in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15e4xf9", "is_robot_indexable": true, "report_reasons": null, "author": "LurkingAlter", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15e4xf9/a_few_questions_from_a_lost_cinematographer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15e4xf9/a_few_questions_from_a_lost_cinematographer/", "subreddit_subscribers": 695495, "created_utc": 1690774153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I downloaded some pretty large videos via yt-dlp. The video files were downloaded completely, but when yt-dlp tried to post-process them and add the metadata the process was aborted. Now the videos are no longer online and I'm sitting here with my .mp4 file and the .meta file (generated automatically by yt-dlp).\n\nIs there a command to tell yt-dlp to just take the video and metadata file and merge them? Or do I really have to write out all the data manually and add it via [metadata fields](https://github.com/yt-dlp/yt-dlp#modifying-metadata)?\n\nThe .meta file looks like this:\n\n    ;FFMETADATA1\n    [CHAPTER]\n    TIMEBASE=1/1000\n    START=0\n    END=1463000\n    title=Rick Rollin\n    [CHAPTER]\n    TIMEBASE=1/1000\n    START=1463000\n    END=9387000\n    title=OUTERPLANE\n    [CHAPTER]\n    TIMEBASE=1/1000\n    START=9387000\n    END=23529000\n    title=Rick Rollin\n\n&amp;#x200B;", "author_fullname": "t2_lgx1p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Add metadata from .meta files to already downloaded videos via yt-dlp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dyfv1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690755853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded some pretty large videos via yt-dlp. The video files were downloaded completely, but when yt-dlp tried to post-process them and add the metadata the process was aborted. Now the videos are no longer online and I&amp;#39;m sitting here with my .mp4 file and the .meta file (generated automatically by yt-dlp).&lt;/p&gt;\n\n&lt;p&gt;Is there a command to tell yt-dlp to just take the video and metadata file and merge them? Or do I really have to write out all the data manually and add it via &lt;a href=\"https://github.com/yt-dlp/yt-dlp#modifying-metadata\"&gt;metadata fields&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;The .meta file looks like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;;FFMETADATA1\n[CHAPTER]\nTIMEBASE=1/1000\nSTART=0\nEND=1463000\ntitle=Rick Rollin\n[CHAPTER]\nTIMEBASE=1/1000\nSTART=1463000\nEND=9387000\ntitle=OUTERPLANE\n[CHAPTER]\nTIMEBASE=1/1000\nSTART=9387000\nEND=23529000\ntitle=Rick Rollin\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZQ_uJX_Lp6oypSrjhuQOsnbGUpqrxS8YOvbhGDdWTOM.jpg?auto=webp&amp;s=4d74f75a5b0580200e8f00a056c89f07503ad6dc", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/ZQ_uJX_Lp6oypSrjhuQOsnbGUpqrxS8YOvbhGDdWTOM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8bac97db762ff29d0ded6665048b60ff0b9d29c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ZQ_uJX_Lp6oypSrjhuQOsnbGUpqrxS8YOvbhGDdWTOM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4667e7acb778cc1370932439b266f423bfaaa6e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ZQ_uJX_Lp6oypSrjhuQOsnbGUpqrxS8YOvbhGDdWTOM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b42a03cccbb3eff0dcaea3f42fce74b514bf9fdd", "width": 320, "height": 320}], "variants": {}, "id": "GSgbDGXRy8cTnzQmF08ew6ybIXeA8LP0f9wGVbbY2xc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dyfv1", "is_robot_indexable": true, "report_reasons": null, "author": "alex_roston", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dyfv1/add_metadata_from_meta_files_to_already/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dyfv1/add_metadata_from_meta_files_to_already/", "subreddit_subscribers": 695495, "created_utc": 1690755853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a few of 14TB Easystore drives that I plan on shucking. Coincidentally, I also need an external enclosure for 2.5 SSD (870 Evo) to use with a Raspberry Pi 4.Can it be done? Will the enclosure being powered be an issue since 2.5 SSDs need no external power supply?\n\nThanks\n\nUPDATE:\n\nI just went ahead and tried it - seems to be working fine in Windows. Gonna try Linux on RPi in a bit. Detects as Easystore. Some threads claim that you need to cut off one of the pins on the flash chip that locks the board to WD drives only, but my Samsung 870 EVO ssd worked without any modifications.\n\nNot sure if it's drive dependent, or maybe my batch of Easystores is less restrictive.\n\n&amp;#x200B;\n\nUPDATE 2:\n\nTried on RPi, and the thing is kinda slow:  \n\n\n&gt;sudo hdparm -tT /dev/sda  \n&gt;  \n&gt;/dev/sda:  \n&gt;  \n&gt;Timing cached reads:   1888 MB in  2.00 seconds = 945.07 MB/sec  \n&gt;  \n&gt;Timing buffered disk reads: 822 MB in  3.00 seconds = 273.69 MB/sec\n\nI wonder if the enclosure is bottlenecking it. The drive itself is capable of \\~560 MBs.", "author_fullname": "t2_2r0xql9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use Easystore enclosure for 2.5 SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dcnfo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690698830.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690692917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a few of 14TB Easystore drives that I plan on shucking. Coincidentally, I also need an external enclosure for 2.5 SSD (870 Evo) to use with a Raspberry Pi 4.Can it be done? Will the enclosure being powered be an issue since 2.5 SSDs need no external power supply?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;UPDATE:&lt;/p&gt;\n\n&lt;p&gt;I just went ahead and tried it - seems to be working fine in Windows. Gonna try Linux on RPi in a bit. Detects as Easystore. Some threads claim that you need to cut off one of the pins on the flash chip that locks the board to WD drives only, but my Samsung 870 EVO ssd worked without any modifications.&lt;/p&gt;\n\n&lt;p&gt;Not sure if it&amp;#39;s drive dependent, or maybe my batch of Easystores is less restrictive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;UPDATE 2:&lt;/p&gt;\n\n&lt;p&gt;Tried on RPi, and the thing is kinda slow:  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;sudo hdparm -tT /dev/sda  &lt;/p&gt;\n\n&lt;p&gt;/dev/sda:  &lt;/p&gt;\n\n&lt;p&gt;Timing cached reads:   1888 MB in  2.00 seconds = 945.07 MB/sec  &lt;/p&gt;\n\n&lt;p&gt;Timing buffered disk reads: 822 MB in  3.00 seconds = 273.69 MB/sec&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I wonder if the enclosure is bottlenecking it. The drive itself is capable of ~560 MBs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dcnfo", "is_robot_indexable": true, "report_reasons": null, "author": "Infinite100p", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dcnfo/use_easystore_enclosure_for_25_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dcnfo/use_easystore_enclosure_for_25_ssd/", "subreddit_subscribers": 695495, "created_utc": 1690692917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m trying to figure out the easiest way to transfer data from my fathers WD MyCloud on his home network to my synology on my network. I was hoping to be able to plug it into my DS220+ via USB but that doesn\u2019t appear to be an option. It is a fair amount of files", "author_fullname": "t2_biovz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring data from a WD MyCloud on another network to my Synology DS220+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dz2y5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690757502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to figure out the easiest way to transfer data from my fathers WD MyCloud on his home network to my synology on my network. I was hoping to be able to plug it into my DS220+ via USB but that doesn\u2019t appear to be an option. It is a fair amount of files&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dz2y5", "is_robot_indexable": true, "report_reasons": null, "author": "mmurp36", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dz2y5/transferring_data_from_a_wd_mycloud_on_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dz2y5/transferring_data_from_a_wd_mycloud_on_another/", "subreddit_subscribers": 695495, "created_utc": 1690757502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Long story short I've settled on using with tape on windows for my archiving needs (Not backup).\n\nI'd like to use tape like a spanning volume. I can keep adding files to folders as the number of tapes increase the software just tells me what tape to put in to get that particular file. I won't be accessing the files often, hence the tape among other reasons. It seems like Hedge Canister ([https://hedge.video/canister](https://hedge.video/canister)) does this, but it's macOS only right now. It can keep a browsable DB of the tapes and can do spanning.\n\nIs there any windows software that can do this? I think preroll-post ([https://www.imagineproducts.com/product/preroll-post/windows](https://www.imagineproducts.com/product/preroll-post/windows)) can do the DB part but not the spanning part. Maybe I'm wrong.\n\nAre there any other good (windows) tape software options out there?\n\nThanks for the help!", "author_fullname": "t2_46qoa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Spanning Archive over LTO tape and DB software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15duob5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690746885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;ve settled on using with tape on windows for my archiving needs (Not backup).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to use tape like a spanning volume. I can keep adding files to folders as the number of tapes increase the software just tells me what tape to put in to get that particular file. I won&amp;#39;t be accessing the files often, hence the tape among other reasons. It seems like Hedge Canister (&lt;a href=\"https://hedge.video/canister\"&gt;https://hedge.video/canister&lt;/a&gt;) does this, but it&amp;#39;s macOS only right now. It can keep a browsable DB of the tapes and can do spanning.&lt;/p&gt;\n\n&lt;p&gt;Is there any windows software that can do this? I think preroll-post (&lt;a href=\"https://www.imagineproducts.com/product/preroll-post/windows\"&gt;https://www.imagineproducts.com/product/preroll-post/windows&lt;/a&gt;) can do the DB part but not the spanning part. Maybe I&amp;#39;m wrong.&lt;/p&gt;\n\n&lt;p&gt;Are there any other good (windows) tape software options out there?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15duob5", "is_robot_indexable": true, "report_reasons": null, "author": "leprechaun7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15duob5/looking_for_spanning_archive_over_lto_tape_and_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15duob5/looking_for_spanning_archive_over_lto_tape_and_db/", "subreddit_subscribers": 695495, "created_utc": 1690746885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I started to digitize and organize my documents in paperless-ngx. Up until today I have been using SwiftScan (formerly ScanBot) to take a picture of the document, convert it to PDF and send it to my paperless-ngx consumption folder. This, in combination with a cardboard box of the right height and some lights, works very well. \n\nHowever, I'm annoyed by having to push the button everytime I'm ready to scan. There are ways around it but eventually I'd like to run a Rasperry Pi with its high quality camera to capture the pages (default european A4 or smaller), convert them and send them to further processing. \n\n&amp;#x200B;\n\nHowever, I was not able to find an application for Linux (let alone ARM) which allows me to do the same as SwiftScan (capture, cut to pagesize, apply filters, PDFize and OCR it). I have been searching for a long time but my search results keep revolving around the traditional SANE programs. I cannot believe no one ever has done such an application .. Do you know of any?", "author_fullname": "t2_6l9ep0bk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scanning application like 'SwiftScan' for Linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dq4fx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690735445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started to digitize and organize my documents in paperless-ngx. Up until today I have been using SwiftScan (formerly ScanBot) to take a picture of the document, convert it to PDF and send it to my paperless-ngx consumption folder. This, in combination with a cardboard box of the right height and some lights, works very well. &lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m annoyed by having to push the button everytime I&amp;#39;m ready to scan. There are ways around it but eventually I&amp;#39;d like to run a Rasperry Pi with its high quality camera to capture the pages (default european A4 or smaller), convert them and send them to further processing. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;However, I was not able to find an application for Linux (let alone ARM) which allows me to do the same as SwiftScan (capture, cut to pagesize, apply filters, PDFize and OCR it). I have been searching for a long time but my search results keep revolving around the traditional SANE programs. I cannot believe no one ever has done such an application .. Do you know of any?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dq4fx", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous-River-927", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dq4fx/scanning_application_like_swiftscan_for_linux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dq4fx/scanning_application_like_swiftscan_for_linux/", "subreddit_subscribers": 695495, "created_utc": 1690735445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a spare PC with a microATX motherboard that I'd like to repurpose as a NAS. I've looked in the wiki for the case recommendations but most of them are rather big. Ideally I'd want something that's as small as possible while having at least 4 3.5in HDD slots and that supports a mATX mobo ofc. I've searched a bit and the fractal design node 804 is what comes closer to what I want but it's still quite big. Thanks for your suggestions and help.", "author_fullname": "t2_4mnx1zf8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case suggestions for DIY NAS (microATX)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dpn41", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690734222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a spare PC with a microATX motherboard that I&amp;#39;d like to repurpose as a NAS. I&amp;#39;ve looked in the wiki for the case recommendations but most of them are rather big. Ideally I&amp;#39;d want something that&amp;#39;s as small as possible while having at least 4 3.5in HDD slots and that supports a mATX mobo ofc. I&amp;#39;ve searched a bit and the fractal design node 804 is what comes closer to what I want but it&amp;#39;s still quite big. Thanks for your suggestions and help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dpn41", "is_robot_indexable": true, "report_reasons": null, "author": "_therealERNESTO_", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dpn41/case_suggestions_for_diy_nas_microatx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dpn41/case_suggestions_for_diy_nas_microatx/", "subreddit_subscribers": 695495, "created_utc": 1690734222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have a folder with roughly 1,000 smartphone images. The unfortunate bit is the metadata is different for each phone. One phone set a \"Date Taken\" field, one set a \"Date Modified\" field, and one set has \"Date Created\". Is there a way for me to consolodate the metadata into just one tag for easy windows sorting? Sorting by \"date\" takes forever whereas simply doing date modified takes way shorter.", "author_fullname": "t2_dkxm61mwa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Folder containing photos from 3 different phones. One set is date taken, one sets metadata is date modified, and one set is date created. How can I sort these properly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dyfiu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690755831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a folder with roughly 1,000 smartphone images. The unfortunate bit is the metadata is different for each phone. One phone set a &amp;quot;Date Taken&amp;quot; field, one set a &amp;quot;Date Modified&amp;quot; field, and one set has &amp;quot;Date Created&amp;quot;. Is there a way for me to consolodate the metadata into just one tag for easy windows sorting? Sorting by &amp;quot;date&amp;quot; takes forever whereas simply doing date modified takes way shorter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dyfiu", "is_robot_indexable": true, "report_reasons": null, "author": "karmoin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dyfiu/folder_containing_photos_from_3_different_phones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dyfiu/folder_containing_photos_from_3_different_phones/", "subreddit_subscribers": 695495, "created_utc": 1690755831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have this old Dell Studio XPS 9100 with two 2TB ST32000641AS drives which are about 13 years old. I went to look at the smart data on the main drive, but it only shows 6205 power on hours. I think it must've wrapped around because it's been on 24/7 for about 3 years, plus however many hours it had from previous decade. I haven't used the second drive for a few years (and it was used as an image backup drive so it was rarely used before), and it reads 66162 hours.\n\nIs there a way to find out what the real power on hours value is, maybe if someone knows what value it rolls over at?", "author_fullname": "t2_jjyxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power on hours rollover on old Seagate Barracuda XT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dvj3s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690748891.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this old Dell Studio XPS 9100 with two 2TB ST32000641AS drives which are about 13 years old. I went to look at the smart data on the main drive, but it only shows 6205 power on hours. I think it must&amp;#39;ve wrapped around because it&amp;#39;s been on 24/7 for about 3 years, plus however many hours it had from previous decade. I haven&amp;#39;t used the second drive for a few years (and it was used as an image backup drive so it was rarely used before), and it reads 66162 hours.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to find out what the real power on hours value is, maybe if someone knows what value it rolls over at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dvj3s", "is_robot_indexable": true, "report_reasons": null, "author": "BBaoVanC", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15dvj3s/power_on_hours_rollover_on_old_seagate_barracuda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dvj3s/power_on_hours_rollover_on_old_seagate_barracuda/", "subreddit_subscribers": 695495, "created_utc": 1690748891.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Out of interest is anyone using MS distributed storage in their home datacenter?\nOnly alternative I've seen is truenas scale but not seen anyone using that either.\n\nCurious as to what options we have in the homelabs to aggregate/tier storage.\n\nCurrently running unraid but that drive limit is fast approaching and I really don't want to be managing multiple storage arrays independently.\n\nTempted to buy a few PS6610s but support long term would be a problem assuming the controllers stop being sold.", "author_fullname": "t2_73ith6gk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Home SAN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dnif0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690728751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Out of interest is anyone using MS distributed storage in their home datacenter?\nOnly alternative I&amp;#39;ve seen is truenas scale but not seen anyone using that either.&lt;/p&gt;\n\n&lt;p&gt;Curious as to what options we have in the homelabs to aggregate/tier storage.&lt;/p&gt;\n\n&lt;p&gt;Currently running unraid but that drive limit is fast approaching and I really don&amp;#39;t want to be managing multiple storage arrays independently.&lt;/p&gt;\n\n&lt;p&gt;Tempted to buy a few PS6610s but support long term would be a problem assuming the controllers stop being sold.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dnif0", "is_robot_indexable": true, "report_reasons": null, "author": "WraytheZ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dnif0/home_san/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dnif0/home_san/", "subreddit_subscribers": 695495, "created_utc": 1690728751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title said it all. Something like dell perc that can expose virtual disks to the host OS, but can keep up with u.2/3 speed.", "author_fullname": "t2_ratqygnj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "U.2 or U.3 raid controller that can create virtual disk", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dm0es", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690724738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title said it all. Something like dell perc that can expose virtual disks to the host OS, but can keep up with u.2/3 speed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15dm0es", "is_robot_indexable": true, "report_reasons": null, "author": "lmux", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dm0es/u2_or_u3_raid_controller_that_can_create_virtual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dm0es/u2_or_u3_raid_controller_that_can_create_virtual/", "subreddit_subscribers": 695495, "created_utc": 1690724738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Hi all. \n\nHas anyone used these cards? \n\n What is your impression ?\n\n&amp;#x200B;", "author_fullname": "t2_e3zlhbcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NVMe to Sata adapters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15e23nu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690765682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. &lt;/p&gt;\n\n&lt;p&gt;Has anyone used these cards? &lt;/p&gt;\n\n&lt;p&gt;What is your impression ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15e23nu", "is_robot_indexable": true, "report_reasons": null, "author": "rob4ik92", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15e23nu/nvme_to_sata_adapters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15e23nu/nvme_to_sata_adapters/", "subreddit_subscribers": 695495, "created_utc": 1690765682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use instaloader quite frequently to save posts from Instagram, but I realize a lot of what I need to save are already in my saved collections. I really don't want to download every post manually since I have well over 800 posts in my saved. Any apps like instaloader that can do separate saved collections?", "author_fullname": "t2_7ti1f2v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download saved collections from instagram", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dxncl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690753914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use instaloader quite frequently to save posts from Instagram, but I realize a lot of what I need to save are already in my saved collections. I really don&amp;#39;t want to download every post manually since I have well over 800 posts in my saved. Any apps like instaloader that can do separate saved collections?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dxncl", "is_robot_indexable": true, "report_reasons": null, "author": "FarWin3127", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dxncl/how_to_download_saved_collections_from_instagram/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dxncl/how_to_download_saved_collections_from_instagram/", "subreddit_subscribers": 695495, "created_utc": 1690753914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Okay,\n\nso i genuinly have no idea where to ask for help regarding my topic but i think this sub might comes the closest? (i looked for labeling, qr code, and stuff like that and didnt find a single freaking sub...)\n\n&amp;#x200B;\n\nBasically what i want: a local labeling system that works with JUST text. Basically i have a (several) label machines, but i want to make use of the ones that can just produce \"plain\" text.\n\nso my idea was to find an app or smth like that that would allow me to scan stings of text and then show info based on that text.\n\nexample:\n\n\\- Print \"3DP1-1\" (3DPrinter/Filament1/Color1)\n\n\\- take phone and scan\n\n\\- show Info with textbox (or small website) showing me further info (basically more text than whats practical to use on labels) and maybe a picture or smth\n\n&amp;#x200B;\n\nIs there anything that comes close to what im looking for, or is using \"advanced\" label makers my only option?\n\n&amp;#x200B;\n\nSorry if this is the wrong sub again. maybe if you know a better one, let me know :D", "author_fullname": "t2_15prbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Label System that works with text strings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dt4kg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690743019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay,&lt;/p&gt;\n\n&lt;p&gt;so i genuinly have no idea where to ask for help regarding my topic but i think this sub might comes the closest? (i looked for labeling, qr code, and stuff like that and didnt find a single freaking sub...)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically what i want: a local labeling system that works with JUST text. Basically i have a (several) label machines, but i want to make use of the ones that can just produce &amp;quot;plain&amp;quot; text.&lt;/p&gt;\n\n&lt;p&gt;so my idea was to find an app or smth like that that would allow me to scan stings of text and then show info based on that text.&lt;/p&gt;\n\n&lt;p&gt;example:&lt;/p&gt;\n\n&lt;p&gt;- Print &amp;quot;3DP1-1&amp;quot; (3DPrinter/Filament1/Color1)&lt;/p&gt;\n\n&lt;p&gt;- take phone and scan&lt;/p&gt;\n\n&lt;p&gt;- show Info with textbox (or small website) showing me further info (basically more text than whats practical to use on labels) and maybe a picture or smth&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there anything that comes close to what im looking for, or is using &amp;quot;advanced&amp;quot; label makers my only option?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is the wrong sub again. maybe if you know a better one, let me know :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dt4kg", "is_robot_indexable": true, "report_reasons": null, "author": "maxz-Reddit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dt4kg/label_system_that_works_with_text_strings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dt4kg/label_system_that_works_with_text_strings/", "subreddit_subscribers": 695495, "created_utc": 1690743019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Suddenly after a year of use, it got super laggy after bunch of write, the disk free is still 1.6TB out of 8TB, it feels like my old SMR harddisk that I used in the past that got super laggy after huge write.", "author_fullname": "t2_7bbojdkz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Seagate Backup+ Hub BK (D785) SMR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dx4ag", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690752648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suddenly after a year of use, it got super laggy after bunch of write, the disk free is still 1.6TB out of 8TB, it feels like my old SMR harddisk that I used in the past that got super laggy after huge write.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dx4ag", "is_robot_indexable": true, "report_reasons": null, "author": "kokizzu2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dx4ag/is_seagate_backup_hub_bk_d785_smr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dx4ag/is_seagate_backup_hub_bk_d785_smr/", "subreddit_subscribers": 695495, "created_utc": 1690752648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys &amp; gals, \n\nI hope this is the correct subreddit for this question.\n\nI have 100Gb of data backups on idrive (both desktop and mobile device backups) and currently a capped &amp; expensive Internet connection.\n\nLooking to move to Google Drive.\n\nI see services like multcloud can handle this migration without me needing to either\n\n1. Redo and reupload all my backups.\n2. Download from idrive and reupload to Google drive.\n\nThough seems like idrive is not supported on multcloud.\n\nAny other options? Of course, I'd be willing to pay for the service.\n\nThank you,", "author_fullname": "t2_7bg4cfrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to migrate from idrive to Google One without downloading &amp; reuploading all my backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dg9av", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690705501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys &amp;amp; gals, &lt;/p&gt;\n\n&lt;p&gt;I hope this is the correct subreddit for this question.&lt;/p&gt;\n\n&lt;p&gt;I have 100Gb of data backups on idrive (both desktop and mobile device backups) and currently a capped &amp;amp; expensive Internet connection.&lt;/p&gt;\n\n&lt;p&gt;Looking to move to Google Drive.&lt;/p&gt;\n\n&lt;p&gt;I see services like multcloud can handle this migration without me needing to either&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Redo and reupload all my backups.&lt;/li&gt;\n&lt;li&gt;Download from idrive and reupload to Google drive.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Though seems like idrive is not supported on multcloud.&lt;/p&gt;\n\n&lt;p&gt;Any other options? Of course, I&amp;#39;d be willing to pay for the service.&lt;/p&gt;\n\n&lt;p&gt;Thank you,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dg9av", "is_robot_indexable": true, "report_reasons": null, "author": "AnyNumber2881", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dg9av/looking_to_migrate_from_idrive_to_google_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dg9av/looking_to_migrate_from_idrive_to_google_one/", "subreddit_subscribers": 695495, "created_utc": 1690705501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to set up a cheap NAS. I will be away from it for long stretches so I want HA. I will have 2 old office PCs each with 1 dedicated data storage drive. I also have a cheap used thin client for quorum. \n\nI want bitrot protection. I know I can use Ceph for 3 drives/ 3 nodes. I know that I can use BTRFS/ZFS RAID1 for 2 drives/ 1 node (then I lose HA). \n\nHow can I stay safe from bitrot with 2 drives across 2 nodes (and one quorum node if needed)?", "author_fullname": "t2_3wz9ifntc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bitrot Protection 2 Drives 2 Nodes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15e0xoe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690762408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to set up a cheap NAS. I will be away from it for long stretches so I want HA. I will have 2 old office PCs each with 1 dedicated data storage drive. I also have a cheap used thin client for quorum. &lt;/p&gt;\n\n&lt;p&gt;I want bitrot protection. I know I can use Ceph for 3 drives/ 3 nodes. I know that I can use BTRFS/ZFS RAID1 for 2 drives/ 1 node (then I lose HA). &lt;/p&gt;\n\n&lt;p&gt;How can I stay safe from bitrot with 2 drives across 2 nodes (and one quorum node if needed)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15e0xoe", "is_robot_indexable": true, "report_reasons": null, "author": "Independent-Park9987", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15e0xoe/bitrot_protection_2_drives_2_nodes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15e0xoe/bitrot_protection_2_drives_2_nodes/", "subreddit_subscribers": 695495, "created_utc": 1690762408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Or convert chords to a progression", "author_fullname": "t2_4jurunac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Music] Where to get Chord Progressions metadata ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dlhp2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690723281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Or convert chords to a progression&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dlhp2", "is_robot_indexable": true, "report_reasons": null, "author": "RedditNoobie777", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dlhp2/music_where_to_get_chord_progressions_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dlhp2/music_where_to_get_chord_progressions_metadata/", "subreddit_subscribers": 695495, "created_utc": 1690723281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had it in a little side panel for a headphone case, no zipper.  I got it out of the car not thinking and from 4 feet high, must have hurtled 9 feet backwards into the gutter.  Chkdsk shows the 600gb already on it to be ok.  Try and fill it up to see if there are physical errors?", "author_fullname": "t2_fznnjop2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I sent a 1tb laptop drive flying. How do i check if the disk is physically ok?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15dl0hi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690721937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had it in a little side panel for a headphone case, no zipper.  I got it out of the car not thinking and from 4 feet high, must have hurtled 9 feet backwards into the gutter.  Chkdsk shows the 600gb already on it to be ok.  Try and fill it up to see if there are physical errors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15dl0hi", "is_robot_indexable": true, "report_reasons": null, "author": "cheetocat2021", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15dl0hi/i_sent_a_1tb_laptop_drive_flying_how_do_i_check/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15dl0hi/i_sent_a_1tb_laptop_drive_flying_how_do_i_check/", "subreddit_subscribers": 695495, "created_utc": 1690721937.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}