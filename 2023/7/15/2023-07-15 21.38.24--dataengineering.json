{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I legit don't care for the hustle and bustle of promotions and improving shit anymore. I'm super content just being a worker bee.\n\nAny industries or employers where DEs can just coast that pay well?", "author_fullname": "t2_58ehd9fn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jobs to just coast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzmfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I legit don&amp;#39;t care for the hustle and bustle of promotions and improving shit anymore. I&amp;#39;m super content just being a worker bee.&lt;/p&gt;\n\n&lt;p&gt;Any industries or employers where DEs can just coast that pay well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14zzmfq", "is_robot_indexable": true, "report_reasons": null, "author": "TAno15", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzmfq/jobs_to_just_coast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzmfq/jobs_to_just_coast/", "subreddit_subscribers": 115964, "created_utc": 1689388368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.\n\nI have good knowledge of SQL.\n\nSince I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.\n\nIs it worth to learning SSIS ? What are the pros and cons ?\n\nThank you.", "author_fullname": "t2_dtr7r94xd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark VS SSIS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1509lyr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689420528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.&lt;/p&gt;\n\n&lt;p&gt;I have good knowledge of SQL.&lt;/p&gt;\n\n&lt;p&gt;Since I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.&lt;/p&gt;\n\n&lt;p&gt;Is it worth to learning SSIS ? What are the pros and cons ?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1509lyr", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate_Finish673", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1509lyr/pyspark_vs_ssis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1509lyr/pyspark_vs_ssis/", "subreddit_subscribers": 115964, "created_utc": 1689420528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I used SQL my whole life and I don't have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should a DE really know in SQL to succeed in an entry level job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150e59x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689433102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used SQL my whole life and I don&amp;#39;t have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150e59x", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150e59x/what_should_a_de_really_know_in_sql_to_succeed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150e59x/what_should_a_de_really_know_in_sql_to_succeed_in/", "subreddit_subscribers": 115964, "created_utc": 1689433102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently learning data engineering and taking cs50, should I just watch all the videos? Or should I go through all the practice problems? I'm currently on week 4 and have been going through the practice problems, but that's moving pretty slowly since I'm also reading a textbook on data warehousing. Would it be better to just watch all the videos on cs50 so I can move through it quicker?\n\nFor some background info, I'm a data analyst with no degree. I no longer do any type of data analysis, instead I'm doing ETL, OLAP cube building, and automating for my departments reports. I'll also start focusing on pre-processing data on clients where our current processes can't handle the data quantity, and I'm helping come up with some data pipelines to automate some manual processes. None of this is focused on super high volume data.\n\nI want to build a strong foundation in computer science since I have no degree, but I also want to learn at a decent rate since I'm already doing some data engineering to an extent.\n\ntl;dr - If you were learning data engineering again and taking cs50, would you just watch the lectures or do all the work?", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I skim cs50 or do all the work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150gzsj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689440176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently learning data engineering and taking cs50, should I just watch all the videos? Or should I go through all the practice problems? I&amp;#39;m currently on week 4 and have been going through the practice problems, but that&amp;#39;s moving pretty slowly since I&amp;#39;m also reading a textbook on data warehousing. Would it be better to just watch all the videos on cs50 so I can move through it quicker?&lt;/p&gt;\n\n&lt;p&gt;For some background info, I&amp;#39;m a data analyst with no degree. I no longer do any type of data analysis, instead I&amp;#39;m doing ETL, OLAP cube building, and automating for my departments reports. I&amp;#39;ll also start focusing on pre-processing data on clients where our current processes can&amp;#39;t handle the data quantity, and I&amp;#39;m helping come up with some data pipelines to automate some manual processes. None of this is focused on super high volume data.&lt;/p&gt;\n\n&lt;p&gt;I want to build a strong foundation in computer science since I have no degree, but I also want to learn at a decent rate since I&amp;#39;m already doing some data engineering to an extent.&lt;/p&gt;\n\n&lt;p&gt;tl;dr - If you were learning data engineering again and taking cs50, would you just watch the lectures or do all the work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "150gzsj", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150gzsj/should_i_skim_cs50_or_do_all_the_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150gzsj/should_i_skim_cs50_or_do_all_the_work/", "subreddit_subscribers": 115964, "created_utc": 1689440176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI recently joined a company where I am given the role of a data engineer. They have plans of converting it to a full-stack data engineer role (not really sure if that's a thing). This decision was made entirely by them without any prior discussion. Was eventually let to know but I wasn't told this at the time of joining. I have mainly worked in backend development and data engineering space before. I am worried they would move me completely to frontend which I neither have prior experience in nor the inclination. Their explanation is that it would give me an end-to-end picture of the product and also reduce dependency on FE team for backend features we have worked on. But I am not particularly interested in adding on that skill set and instead focus on learning what else there is in the area I have been working so far. But the following thoughts are worrying me:  \n1. Will saying no be taken as \"I don't wish to grow and learn\"   \n2. I am okay with maybe ramping up to resolve bugs and add small features but being part of a complete design story or overhaul is not something I am interested in but the discussion indicated it might be the case in the future if it comes to that.  \n\n\nIf given a choice I would avoid frontend work. But wanted to know what the general opinion is around this and what would be suggested. \n\nTIA", "author_fullname": "t2_a40fenez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "full stack DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150cc7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689428407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I recently joined a company where I am given the role of a data engineer. They have plans of converting it to a full-stack data engineer role (not really sure if that&amp;#39;s a thing). This decision was made entirely by them without any prior discussion. Was eventually let to know but I wasn&amp;#39;t told this at the time of joining. I have mainly worked in backend development and data engineering space before. I am worried they would move me completely to frontend which I neither have prior experience in nor the inclination. Their explanation is that it would give me an end-to-end picture of the product and also reduce dependency on FE team for backend features we have worked on. But I am not particularly interested in adding on that skill set and instead focus on learning what else there is in the area I have been working so far. But the following thoughts are worrying me:&lt;br/&gt;\n1. Will saying no be taken as &amp;quot;I don&amp;#39;t wish to grow and learn&amp;quot;&lt;br/&gt;\n2. I am okay with maybe ramping up to resolve bugs and add small features but being part of a complete design story or overhaul is not something I am interested in but the discussion indicated it might be the case in the future if it comes to that.  &lt;/p&gt;\n\n&lt;p&gt;If given a choice I would avoid frontend work. But wanted to know what the general opinion is around this and what would be suggested. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150cc7j", "is_robot_indexable": true, "report_reasons": null, "author": "puzzled-cognition", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150cc7j/full_stack_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150cc7j/full_stack_de/", "subreddit_subscribers": 115964, "created_utc": 1689428407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team have some large workloads which we are currently running on Azure Databricks. On an ad-hoc hasis, we occasionally do some large simulations for analytics purposes over all of our historic data which require large costly clusters. It works ok, but since we use cheap Spot instances, 80% of the resulting bill is for the Databricks DBU cost rather than the underlying infrastructure. \n\nSince the jobs don't rely on any of the Databricks closed-source tech, and they're ad-hoc so don't form part of any other integrated workflows, I'd like to see if there's a good option for running these jobs on self managed clusters so that I can cut down the bill.\n\nHaving done some research, there was historically a library called [AZTK](https://github.com/Azure/aztk) (Azure Distributed Data Engineering Toolkit) which provided a simple CLI for creating Spark Clusters on Azure Batch. However, it's no longer being maintained.\n\nDoes anyone have any other recommendations?", "author_fullname": "t2_173s1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplest solution for self managed spark cluster on Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150budt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689427048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team have some large workloads which we are currently running on Azure Databricks. On an ad-hoc hasis, we occasionally do some large simulations for analytics purposes over all of our historic data which require large costly clusters. It works ok, but since we use cheap Spot instances, 80% of the resulting bill is for the Databricks DBU cost rather than the underlying infrastructure. &lt;/p&gt;\n\n&lt;p&gt;Since the jobs don&amp;#39;t rely on any of the Databricks closed-source tech, and they&amp;#39;re ad-hoc so don&amp;#39;t form part of any other integrated workflows, I&amp;#39;d like to see if there&amp;#39;s a good option for running these jobs on self managed clusters so that I can cut down the bill.&lt;/p&gt;\n\n&lt;p&gt;Having done some research, there was historically a library called &lt;a href=\"https://github.com/Azure/aztk\"&gt;AZTK&lt;/a&gt; (Azure Distributed Data Engineering Toolkit) which provided a simple CLI for creating Spark Clusters on Azure Batch. However, it&amp;#39;s no longer being maintained.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?auto=webp&amp;s=b06c012cb41ecc749eb5f90c904274d46a26840d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adbb3b707613aca52feaf697def8a8a2b342215b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd8add2cbde77c9734804c7207c81c1563a7673d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba3432a3dc98d43482ee02b50d7bd61c03caa6ee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86aea326065a814d88ab3c561665790878990f58", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bca8ae50018d5ace9308ece2d856c6e9cfe52b70", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9bee3ef5f4fbc546675378d5e70503fdc99b8fa", "width": 1080, "height": 540}], "variants": {}, "id": "q67RFzude0ccMbhzCzeXcdZFMywLc15tAjithvMoja4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150budt", "is_robot_indexable": true, "report_reasons": null, "author": "Pancakeman123000", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150budt/simplest_solution_for_self_managed_spark_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150budt/simplest_solution_for_self_managed_spark_cluster/", "subreddit_subscribers": 115964, "created_utc": 1689427048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90z29170", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to increase my reading: Any books to add/take-away from this list?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_14ztv2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "author_name": "Seattle Data Guy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fIdp1zrXhF4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SeattleDataGuy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/14ztv2d", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6HhnRhgByA8O01yHHpqRK2MkMptgjYwGyPZgLpJnhY8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689372812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=fIdp1zrXhF4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?auto=webp&amp;s=032db8ea491d25171e232fb3d80d8b599c097b35", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3950eb7cff00d2f0b9b81917107dbc2490e7f610", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1830a44e95aa46309c282008d8f6741e296e4f2c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd3417f411f23e2fad4a4d215c1001c45bf5504", "width": 320, "height": 240}], "variants": {}, "id": "8QTuBfki2AvUagIUD-Qvh-jLmyvk1cj4sE_QaBnYeck"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14ztv2d", "is_robot_indexable": true, "report_reasons": null, "author": "No-Platypus4021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ztv2d/trying_to_increase_my_reading_any_books_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=fIdp1zrXhF4", "subreddit_subscribers": 115964, "created_utc": 1689372812.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "author_name": "Seattle Data Guy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fIdp1zrXhF4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SeattleDataGuy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, In the company I work for we use PySpark on EMR or Glue Jobs for Big Data processing.\n\nFor small data cases we use AWS SDK for Pandas (awswrangler) on Lambda.\n\nIs there anyway to use polars in this manner while also reading and writing to Glue Catalog tables?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Data processing with Polars @AWS lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150fgvc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689436394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, In the company I work for we use PySpark on EMR or Glue Jobs for Big Data processing.&lt;/p&gt;\n\n&lt;p&gt;For small data cases we use AWS SDK for Pandas (awswrangler) on Lambda.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to use polars in this manner while also reading and writing to Glue Catalog tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150fgvc", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150fgvc/small_data_processing_with_polars_aws_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150fgvc/small_data_processing_with_polars_aws_lambda/", "subreddit_subscribers": 115964, "created_utc": 1689436394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a student preparing to get a job as a data engineer.\n\n1. linux, Python, SQL, JAVA or Scala\n2. Cloud Knowledge\n3. Docker, Kubernetes\n4. Server security + data security/quality\n5. Database (\"Cassandra\", \"Mongo\", \"Mysql\", \"postgres\", \"redis\")\n6. ELK or (fluentd, Opensearch)\n7. Kafka\n8. Spark Stream or Flink\n9. Spark or Trino\n10. table format ( Iceberg/deltalake/ Hudi)\n11. snowflake/ Redshfit / Bigquery   12 . OLAP Data Modeling  13.Airflow  \nI only wrote down the essentials.There are many good tools like dbt, lakeFS, etcBut in additionCompanies may still require Hadoop Echo System (Hive/HDFS/Hbase).They might ask us to build a dashboard with javascript or python.They can also ask us to create the web.From number 1 to 13, each one has a very difficult and difficult concept to master.What I'm really curious about is how much I need to know and how much I need to master to apply for a company.I'm sorry. Actually, I was whining because I got hit with reality while studying.", "author_fullname": "t2_6pkq7jrch", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why do data engineers have so much to learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_150mfrd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689453652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a student preparing to get a job as a data engineer.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;linux, Python, SQL, JAVA or Scala&lt;/li&gt;\n&lt;li&gt;Cloud Knowledge&lt;/li&gt;\n&lt;li&gt;Docker, Kubernetes&lt;/li&gt;\n&lt;li&gt;Server security + data security/quality&lt;/li&gt;\n&lt;li&gt;Database (&amp;quot;Cassandra&amp;quot;, &amp;quot;Mongo&amp;quot;, &amp;quot;Mysql&amp;quot;, &amp;quot;postgres&amp;quot;, &amp;quot;redis&amp;quot;)&lt;/li&gt;\n&lt;li&gt;ELK or (fluentd, Opensearch)&lt;/li&gt;\n&lt;li&gt;Kafka&lt;/li&gt;\n&lt;li&gt;Spark Stream or Flink&lt;/li&gt;\n&lt;li&gt;Spark or Trino&lt;/li&gt;\n&lt;li&gt;table format ( Iceberg/deltalake/ Hudi)&lt;/li&gt;\n&lt;li&gt;snowflake/ Redshfit / Bigquery   12 . OLAP Data Modeling  13.Airflow&lt;br/&gt;\nI only wrote down the essentials.There are many good tools like dbt, lakeFS, etcBut in additionCompanies may still require Hadoop Echo System (Hive/HDFS/Hbase).They might ask us to build a dashboard with javascript or python.They can also ask us to create the web.From number 1 to 13, each one has a very difficult and difficult concept to master.What I&amp;#39;m really curious about is how much I need to know and how much I need to master to apply for a company.I&amp;#39;m sorry. Actually, I was whining because I got hit with reality while studying.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150mfrd", "is_robot_indexable": true, "report_reasons": null, "author": "Hankaul", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150mfrd/why_do_data_engineers_have_so_much_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150mfrd/why_do_data_engineers_have_so_much_to_learn/", "subreddit_subscribers": 115964, "created_utc": 1689453652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is something. I'm feeling bad since it happened yesterday.\n\nSomehow I got promoted from a Sr Data Engineer role to a Sr Data Analyst with a salary increase. \n\nI mean it's good but since I do both roles it's awkward since the title should be something like analytics engineer, which is more attractive for any job seeking in the future.\n\nI have a lot of confidence in my boss. I plan to tell him to reconsider the title based on all my functions.\n\nWhat do you think about\u00a0this\u00a0guys? am I wrong?", "author_fullname": "t2_2doz54hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got \"promoted\" from Sr DE to Sr DA \u00bf?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150knwq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689449186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is something. I&amp;#39;m feeling bad since it happened yesterday.&lt;/p&gt;\n\n&lt;p&gt;Somehow I got promoted from a Sr Data Engineer role to a Sr Data Analyst with a salary increase. &lt;/p&gt;\n\n&lt;p&gt;I mean it&amp;#39;s good but since I do both roles it&amp;#39;s awkward since the title should be something like analytics engineer, which is more attractive for any job seeking in the future.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of confidence in my boss. I plan to tell him to reconsider the title based on all my functions.&lt;/p&gt;\n\n&lt;p&gt;What do you think about\u00a0this\u00a0guys? am I wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150knwq", "is_robot_indexable": true, "report_reasons": null, "author": "erwingm10", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/150knwq/just_got_promoted_from_sr_de_to_sr_da/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150knwq/just_got_promoted_from_sr_de_to_sr_da/", "subreddit_subscribers": 115964, "created_utc": 1689449186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently trying to learn Apache Airflow and I have read and heard many times that Apache Airflow is an orchestration tool and should not be used to process data.\n\nHowever, some things are not clear to me:\n\n1. Does this mean I should not use the `PythonOperator` to transform my data but instead use something like the `SparkOperator`?\n2. If I can use the `PythonOperator` to transform my data, where exactly is the process done? On a worker (if I use Celery Executor) or on a POD (if I use Kubernetes Executor)? How is that different from using Apache Airflow as a processing tool?\n3. Also, If I can use the `PythonOperator` to transform my data, then how exactly does one use Apache Airflow to process data? \n\n&amp;#x200B;", "author_fullname": "t2_c8f4gnokr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where data is processed when using PythonOperator?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150j1ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689445198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently trying to learn Apache Airflow and I have read and heard many times that Apache Airflow is an orchestration tool and should not be used to process data.&lt;/p&gt;\n\n&lt;p&gt;However, some things are not clear to me:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Does this mean I should not use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data but instead use something like the &lt;code&gt;SparkOperator&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;If I can use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data, where exactly is the process done? On a worker (if I use Celery Executor) or on a POD (if I use Kubernetes Executor)? How is that different from using Apache Airflow as a processing tool?&lt;/li&gt;\n&lt;li&gt;Also, If I can use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data, then how exactly does one use Apache Airflow to process data? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150j1ge", "is_robot_indexable": true, "report_reasons": null, "author": "NoobAllTheWay", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150j1ge/where_data_is_processed_when_using_pythonoperator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150j1ge/where_data_is_processed_when_using_pythonoperator/", "subreddit_subscribers": 115964, "created_utc": 1689445198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was curious if there are examples of projects for pulling in machine sensor data from different locations via api? Looking to do a project and specifically how to handle outages and incremental jobs for a location or certain machines that may go offline while the rest are running smoothly.", "author_fullname": "t2_b5za7mst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Example of Incremental Sensor Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150i6w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689443087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious if there are examples of projects for pulling in machine sensor data from different locations via api? Looking to do a project and specifically how to handle outages and incremental jobs for a location or certain machines that may go offline while the rest are running smoothly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150i6w3", "is_robot_indexable": true, "report_reasons": null, "author": "No_Cover_Undercover", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150i6w3/example_of_incremental_sensor_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150i6w3/example_of_incremental_sensor_project/", "subreddit_subscribers": 115964, "created_utc": 1689443087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to Snowflake and I was trying to read about different types of tables. I understood how fail-safe is an internal mechanism in Snowflake that is kept for operational purposes.\n\n But the fail-safe is for 7 days beyond the retention period is what Snowflake docs mention. So in that case, if my retention period is 7 days, then fail-safe will be for another 7 days, which means I have to pay storage costs for 14 days of data for each table right? \n\nWhy can't fail-safe be just as retention period and not on top of retention period? This will save costs considerably if it is a large table. \n\nPS: I know about fail-safe not being a option that can be leveraged by developers. In case of operational failures, Snowflake support team can help recovering the data and that's the reason for fail-safe. ", "author_fullname": "t2_2xxs9nne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding fail-safe in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15052dq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689405440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to Snowflake and I was trying to read about different types of tables. I understood how fail-safe is an internal mechanism in Snowflake that is kept for operational purposes.&lt;/p&gt;\n\n&lt;p&gt;But the fail-safe is for 7 days beyond the retention period is what Snowflake docs mention. So in that case, if my retention period is 7 days, then fail-safe will be for another 7 days, which means I have to pay storage costs for 14 days of data for each table right? &lt;/p&gt;\n\n&lt;p&gt;Why can&amp;#39;t fail-safe be just as retention period and not on top of retention period? This will save costs considerably if it is a large table. &lt;/p&gt;\n\n&lt;p&gt;PS: I know about fail-safe not being a option that can be leveraged by developers. In case of operational failures, Snowflake support team can help recovering the data and that&amp;#39;s the reason for fail-safe. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15052dq", "is_robot_indexable": true, "report_reasons": null, "author": "inglocines", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15052dq/understanding_failsafe_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15052dq/understanding_failsafe_in_snowflake/", "subreddit_subscribers": 115964, "created_utc": 1689405440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019ve been seeing quite often how much variety Data Engineer responsibilities can be. Some can be more of an Analytics Engineer who is focused on metrics and dashboards, while also building pipelines. While others are more of a SWE focused on data. \n\nMy question comes with the SWE Data Engineer. Obviously SQL and Python are very important, but are Data Engineers who have more SWE responsibilities, more Python heavy, or SQL heavy, or both equally? I understand this varies by company, but I\u2019m curious.", "author_fullname": "t2_9jq8g6tr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Title Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzq93", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been seeing quite often how much variety Data Engineer responsibilities can be. Some can be more of an Analytics Engineer who is focused on metrics and dashboards, while also building pipelines. While others are more of a SWE focused on data. &lt;/p&gt;\n\n&lt;p&gt;My question comes with the SWE Data Engineer. Obviously SQL and Python are very important, but are Data Engineers who have more SWE responsibilities, more Python heavy, or SQL heavy, or both equally? I understand this varies by company, but I\u2019m curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14zzq93", "is_robot_indexable": true, "report_reasons": null, "author": "WorldlyDirt5024", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzq93/job_title_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzq93/job_title_question/", "subreddit_subscribers": 115964, "created_utc": 1689388679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm kinda new to data-engineering\n\nI'm currently working on revamping our data warehouse setup in Snowflake and would like to implement DBT into our workflow. I'm seeking guidance and recommendations from those experienced in Snowflake and DBT on how to optimize our current approach. Here's a summary of our existing setup:\n\n1. We create views on our stage database, which act as the source for our data warehouse.\n2. To merge the data into our data warehouse, we rely on a stored procedure that incorporates these views.\n\nWhile our current setup is functional, we believe incorporating DBT could help improve performance, maintainability, and scalability. We are particularly interested in addressing the following questions:\n\n1. What are the recommended steps for transitioning to DBT for data warehouse management in Snowflake?\n2. How can we extract data from our stage database efficiently using DBT?\n3. Are there any best practices for optimizing data loading using DBT in Snowflake?\n\nWe are excited to explore the potential of DBT and would appreciate any insights, resources, or personal experiences related to implementing DBT in a Snowflake data warehouse setup. Whether it's tutorials, case studies, or practical tips, your expertise will be invaluable in our transition.\n\nSpecifically, we would like to know:\n\n1. How can DBT models be leveraged to streamline our workflow and reduce complexity?\n2. Are there any features or techniques in DBT that can enhance the performance of our data loading process?\n3. Are there any recommended strategies for managing transformations and data quality checks using DBT?\n\nThank you in advance for your time and expertise! I look forward to learning from your experiences and integrating DBT seamlessly into our Snowflake data warehouse setup.", "author_fullname": "t2_2yutl8h5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on implementing DBT for Snowflake data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_150mmbt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689454099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m kinda new to data-engineering&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on revamping our data warehouse setup in Snowflake and would like to implement DBT into our workflow. I&amp;#39;m seeking guidance and recommendations from those experienced in Snowflake and DBT on how to optimize our current approach. Here&amp;#39;s a summary of our existing setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We create views on our stage database, which act as the source for our data warehouse.&lt;/li&gt;\n&lt;li&gt;To merge the data into our data warehouse, we rely on a stored procedure that incorporates these views.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;While our current setup is functional, we believe incorporating DBT could help improve performance, maintainability, and scalability. We are particularly interested in addressing the following questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the recommended steps for transitioning to DBT for data warehouse management in Snowflake?&lt;/li&gt;\n&lt;li&gt;How can we extract data from our stage database efficiently using DBT?&lt;/li&gt;\n&lt;li&gt;Are there any best practices for optimizing data loading using DBT in Snowflake?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We are excited to explore the potential of DBT and would appreciate any insights, resources, or personal experiences related to implementing DBT in a Snowflake data warehouse setup. Whether it&amp;#39;s tutorials, case studies, or practical tips, your expertise will be invaluable in our transition.&lt;/p&gt;\n\n&lt;p&gt;Specifically, we would like to know:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How can DBT models be leveraged to streamline our workflow and reduce complexity?&lt;/li&gt;\n&lt;li&gt;Are there any features or techniques in DBT that can enhance the performance of our data loading process?&lt;/li&gt;\n&lt;li&gt;Are there any recommended strategies for managing transformations and data quality checks using DBT?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you in advance for your time and expertise! I look forward to learning from your experiences and integrating DBT seamlessly into our Snowflake data warehouse setup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150mmbt", "is_robot_indexable": true, "report_reasons": null, "author": "vgowthamvk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/150mmbt/seeking_advice_on_implementing_dbt_for_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150mmbt/seeking_advice_on_implementing_dbt_for_snowflake/", "subreddit_subscribers": 115964, "created_utc": 1689454099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm going through some of the dbt training courses on their website. Across multiple videos and presenters, they seem to use the syntax \"GROUP BY 1\" in their SQL code. I honestly had to Google wtf that meant lol.\n\nPlease correct me if I'm overgeneralizing, but it seems like in almost every case, you should just use the column name in the group by clause. \n\nI'm very new to dbt, so please let me know if there's a good reason to use GROUP BY 1 rather than the column name.", "author_fullname": "t2_vw2sv4u4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why use \"GROUP BY 1\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150korq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689449247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going through some of the dbt training courses on their website. Across multiple videos and presenters, they seem to use the syntax &amp;quot;GROUP BY 1&amp;quot; in their SQL code. I honestly had to Google wtf that meant lol.&lt;/p&gt;\n\n&lt;p&gt;Please correct me if I&amp;#39;m overgeneralizing, but it seems like in almost every case, you should just use the column name in the group by clause. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very new to dbt, so please let me know if there&amp;#39;s a good reason to use GROUP BY 1 rather than the column name.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150korq", "is_robot_indexable": true, "report_reasons": null, "author": "aria_____51", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150korq/why_use_group_by_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150korq/why_use_group_by_1/", "subreddit_subscribers": 115964, "created_utc": 1689449247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nSo I work at a small clinic and have recently been charged with consolidating our various data sources into one single database so that analytics can be performed in a fast, easier way. Our sources include excel sheets and csv files created in-house, and reports downloaded in these formats from medical web-portals. \n\nWhat would be the best way (and technology) to go about creating a single database system where I can eventually create automated systems for all these different data sources to be consolidated in one database system? \n\n&amp;#x200B;\n\nThank you\n\n(P.S. I do have a computer science degree, but I haven't worked too much with architecting database systems/infrastructure, mostly running SQL queries, and ML algorithms with existing datasets)", "author_fullname": "t2_7a3r9mqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBMS options for a small clinic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150jm2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689446581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;So I work at a small clinic and have recently been charged with consolidating our various data sources into one single database so that analytics can be performed in a fast, easier way. Our sources include excel sheets and csv files created in-house, and reports downloaded in these formats from medical web-portals. &lt;/p&gt;\n\n&lt;p&gt;What would be the best way (and technology) to go about creating a single database system where I can eventually create automated systems for all these different data sources to be consolidated in one database system? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n\n&lt;p&gt;(P.S. I do have a computer science degree, but I haven&amp;#39;t worked too much with architecting database systems/infrastructure, mostly running SQL queries, and ML algorithms with existing datasets)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150jm2q", "is_robot_indexable": true, "report_reasons": null, "author": "juiceleft88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150jm2q/dbms_options_for_a_small_clinic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150jm2q/dbms_options_for_a_small_clinic/", "subreddit_subscribers": 115964, "created_utc": 1689446581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here familiar working with Syndigo PXM or MDM solution? \n\nI\u2019m starting thinking if is this really the future career? \n\nI need your advise what are the trends for DE role. \ud83d\ude0a", "author_fullname": "t2_7cswt26k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Syndigo MDM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150f1yp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689435350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here familiar working with Syndigo PXM or MDM solution? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting thinking if is this really the future career? &lt;/p&gt;\n\n&lt;p&gt;I need your advise what are the trends for DE role. \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "150f1yp", "is_robot_indexable": true, "report_reasons": null, "author": "Federal_Ad179", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150f1yp/syndigo_mdm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150f1yp/syndigo_mdm/", "subreddit_subscribers": 115964, "created_utc": 1689435350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on a personal project to ETL fin data and dashboard it. However, I'm having confusion about whether I should ingest API -&gt; split into different tables using a data model in BQ -&gt; transformation -&gt; dashboard\n\nOR\n\nAPI ingestion -&gt; extract the required columns using pandas &amp; transformation using duck db -&gt; load df directly to BQ -&gt; dashboard\n\nI'm currently in progress with the second implementation as I decided to go on with the light &amp; quick way to get the insights.\n\nWhat do you guys think, should it be the other way infact?", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Doubt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150ez1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689435147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a personal project to ETL fin data and dashboard it. However, I&amp;#39;m having confusion about whether I should ingest API -&amp;gt; split into different tables using a data model in BQ -&amp;gt; transformation -&amp;gt; dashboard&lt;/p&gt;\n\n&lt;p&gt;OR&lt;/p&gt;\n\n&lt;p&gt;API ingestion -&amp;gt; extract the required columns using pandas &amp;amp; transformation using duck db -&amp;gt; load df directly to BQ -&amp;gt; dashboard&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently in progress with the second implementation as I decided to go on with the light &amp;amp; quick way to get the insights.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think, should it be the other way infact?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150ez1j", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/150ez1j/project_doubt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150ez1j/project_doubt/", "subreddit_subscribers": 115964, "created_utc": 1689435147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.\n\nCurrently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.\n\nThis I am posting because I want to get to know the experience from the personal user instead.\n\nI am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.", "author_fullname": "t2_kmi1m0w3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "M1 vs M2 MBA for tasks relating to PySpark or Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15082zu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689415630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.&lt;/p&gt;\n\n&lt;p&gt;This I am posting because I want to get to know the experience from the personal user instead.&lt;/p&gt;\n\n&lt;p&gt;I am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15082zu", "is_robot_indexable": true, "report_reasons": null, "author": "Prudent-Writing-5724", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15082zu/m1_vs_m2_mba_for_tasks_relating_to_pyspark_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15082zu/m1_vs_m2_mba_for_tasks_relating_to_pyspark_or/", "subreddit_subscribers": 115964, "created_utc": 1689415630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This doesn't seem like out of the box functionality (and since dbt is so new even github copilot is confused on what to do), so I'm wondering if I have to define my own macro or something\n\nI'm new to dbt so I'm wondering is there is a way to select columns based on a pattern like startswith or contains or something like that. This is in postgres btw.", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In dbt postgres, how would I create a model that only selects columns that start with 'c_' ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzm50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This doesn&amp;#39;t seem like out of the box functionality (and since dbt is so new even github copilot is confused on what to do), so I&amp;#39;m wondering if I have to define my own macro or something&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to dbt so I&amp;#39;m wondering is there is a way to select columns based on a pattern like startswith or contains or something like that. This is in postgres btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14zzm50", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzm50/in_dbt_postgres_how_would_i_create_a_model_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzm50/in_dbt_postgres_how_would_i_create_a_model_that/", "subreddit_subscribers": 115964, "created_utc": 1689388342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ftuqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_150dzb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.45, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JeG6AEPTyH8uzy3n39-MUYvqhB5xnPdGBpLIW8W9o_Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689432670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?auto=webp&amp;s=c51efa238a37f2c964e17c669be93c6b29f2fd9f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e811f98c026cb8f9131ff033c9c32d0f24c7f2aa", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=404960819ee2fd33af872657472c95c26ece4a4f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82d740b30ba7f26b5c6dcefe1783d5bf59a90e7f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d7c5e19e213c8e0547788dc2a8f2a8c32eecf28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f41758a992ddd35178cd2f501f4fc7f14b8e9542", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de802c91aa8ab836fdcf1869220eb01cc40a7b09", "width": 1080, "height": 540}], "variants": {}, "id": "APy3ZRoodILM2d2MRzQ4YrvSvSJa9Z19Xya3CDQkmVQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "150dzb6", "is_robot_indexable": true, "report_reasons": null, "author": "philonoist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150dzb6/unit_testing_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers", "subreddit_subscribers": 115964, "created_utc": 1689432670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ftuqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chat-GPT plugins and Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_150dwlj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/67Z-n6kzGpks-2jD6sMBkFlrxkeUtZ7IEghhHW3r2oE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689432477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "juhache.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://juhache.substack.com/p/chat-gpt-plugins-and-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?auto=webp&amp;s=13a1917ae36d972b4368bcf804defb913747c46f", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ffc3de90b908118fbd7f037aabe80c74b51a2ed", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a87dce3e137c9af896cc581e449f12eaee419088", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca960a8a642de15ca86b4e8f973b56ec72008182", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6463906b55c177dfdfc2c7f0c7acdaada0509d7b", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a675d07ab79df5ef91cee1025ab32069fc4ef7d", "width": 960, "height": 562}], "variants": {}, "id": "9DsW5dB33_z07FSvheGgB0Z9t7wSr8pHJ8z5z_o0muA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "150dwlj", "is_robot_indexable": true, "report_reasons": null, "author": "philonoist", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150dwlj/chatgpt_plugins_and_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://juhache.substack.com/p/chat-gpt-plugins-and-data-engineering", "subreddit_subscribers": 115964, "created_utc": 1689432477.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}