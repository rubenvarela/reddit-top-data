{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_khw2bm36", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you backup your S3 data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_14zpfq8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 90, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 90, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K8JCZ5TGzHI_3mps2dcQZpSL_73HoKmA7jXSwX5Ta0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689362383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/om6ea88jdzbb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?auto=webp&amp;s=3fb2e515fcefee9891e6d56acb31fbaafc9d55cc", "width": 996, "height": 1134}, "resolutions": [{"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a321fb84ceefca374565845a55395bfc50eff6a5", "width": 108, "height": 122}, {"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77dde8c7aa2699529eaa125172797f968ca92e08", "width": 216, "height": 245}, {"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b54de664c6c169fa18697ff9c838bd79a5d9e4a", "width": 320, "height": 364}, {"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c13777fa3f8cbbdab074cbfa0785039b66e1d6fe", "width": 640, "height": 728}, {"url": "https://preview.redd.it/om6ea88jdzbb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=284646d8744b39811dcd96e2d24dd9e4cebf9d27", "width": 960, "height": 1093}], "variants": {}, "id": "ePehiGT5MnUpeJor52Hjcc1-VWazaGM3Sp_wwzU0ViM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "14zpfq8", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate-Day-240", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zpfq8/do_you_backup_your_s3_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/om6ea88jdzbb1.jpg", "subreddit_subscribers": 115954, "created_utc": 1689362383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I legit don't care for the hustle and bustle of promotions and improving shit anymore. I'm super content just being a worker bee.\n\nAny industries or employers where DEs can just coast that pay well?", "author_fullname": "t2_58ehd9fn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jobs to just coast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzmfq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I legit don&amp;#39;t care for the hustle and bustle of promotions and improving shit anymore. I&amp;#39;m super content just being a worker bee.&lt;/p&gt;\n\n&lt;p&gt;Any industries or employers where DEs can just coast that pay well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14zzmfq", "is_robot_indexable": true, "report_reasons": null, "author": "TAno15", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzmfq/jobs_to_just_coast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzmfq/jobs_to_just_coast/", "subreddit_subscribers": 115954, "created_utc": 1689388368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.\n\nI have good knowledge of SQL.\n\nSince I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.\n\nIs it worth to learning SSIS ? What are the pros and cons ?\n\nThank you.", "author_fullname": "t2_dtr7r94xd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark VS SSIS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1509lyr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689420528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I currently do my etl task with spark. It is simple ETL jobs. The amount of data is small to medium. I essentially work with on-prem data, but some may be stored in S3 buckets or Redshift.&lt;/p&gt;\n\n&lt;p&gt;I have good knowledge of SQL.&lt;/p&gt;\n\n&lt;p&gt;Since I am looking for a new job, I wonder if I should learn SSIS to do ETL, since a lot of companies use it, or is AWS Glue and Pyspark is sufficient to tackle ETL tasks.&lt;/p&gt;\n\n&lt;p&gt;Is it worth to learning SSIS ? What are the pros and cons ?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1509lyr", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate_Finish673", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1509lyr/pyspark_vs_ssis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1509lyr/pyspark_vs_ssis/", "subreddit_subscribers": 115954, "created_utc": 1689420528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I used SQL my whole life and I don't have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should a DE really know in SQL to succeed in an entry level job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150e59x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689433102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used SQL my whole life and I don&amp;#39;t have issues with data modeling or querying in general. But when I see jobs asking for a good level in SQL, I wonder what does good mean ? What are the items that I should really know to qualify as ready for an SQL DE job ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150e59x", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150e59x/what_should_a_de_really_know_in_sql_to_succeed_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150e59x/what_should_a_de_really_know_in_sql_to_succeed_in/", "subreddit_subscribers": 115954, "created_utc": 1689433102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI recently joined a company where I am given the role of a data engineer. They have plans of converting it to a full-stack data engineer role (not really sure if that's a thing). This decision was made entirely by them without any prior discussion. Was eventually let to know but I wasn't told this at the time of joining. I have mainly worked in backend development and data engineering space before. I am worried they would move me completely to frontend which I neither have prior experience in nor the inclination. Their explanation is that it would give me an end-to-end picture of the product and also reduce dependency on FE team for backend features we have worked on. But I am not particularly interested in adding on that skill set and instead focus on learning what else there is in the area I have been working so far. But the following thoughts are worrying me:  \n1. Will saying no be taken as \"I don't wish to grow and learn\"   \n2. I am okay with maybe ramping up to resolve bugs and add small features but being part of a complete design story or overhaul is not something I am interested in but the discussion indicated it might be the case in the future if it comes to that.  \n\n\nIf given a choice I would avoid frontend work. But wanted to know what the general opinion is around this and what would be suggested. \n\nTIA", "author_fullname": "t2_a40fenez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "full stack DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150cc7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689428407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I recently joined a company where I am given the role of a data engineer. They have plans of converting it to a full-stack data engineer role (not really sure if that&amp;#39;s a thing). This decision was made entirely by them without any prior discussion. Was eventually let to know but I wasn&amp;#39;t told this at the time of joining. I have mainly worked in backend development and data engineering space before. I am worried they would move me completely to frontend which I neither have prior experience in nor the inclination. Their explanation is that it would give me an end-to-end picture of the product and also reduce dependency on FE team for backend features we have worked on. But I am not particularly interested in adding on that skill set and instead focus on learning what else there is in the area I have been working so far. But the following thoughts are worrying me:&lt;br/&gt;\n1. Will saying no be taken as &amp;quot;I don&amp;#39;t wish to grow and learn&amp;quot;&lt;br/&gt;\n2. I am okay with maybe ramping up to resolve bugs and add small features but being part of a complete design story or overhaul is not something I am interested in but the discussion indicated it might be the case in the future if it comes to that.  &lt;/p&gt;\n\n&lt;p&gt;If given a choice I would avoid frontend work. But wanted to know what the general opinion is around this and what would be suggested. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150cc7j", "is_robot_indexable": true, "report_reasons": null, "author": "puzzled-cognition", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150cc7j/full_stack_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150cc7j/full_stack_de/", "subreddit_subscribers": 115954, "created_utc": 1689428407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90z29170", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to increase my reading: Any books to add/take-away from this list?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_14ztv2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "author_name": "Seattle Data Guy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fIdp1zrXhF4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SeattleDataGuy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/14ztv2d", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6HhnRhgByA8O01yHHpqRK2MkMptgjYwGyPZgLpJnhY8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689372812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=fIdp1zrXhF4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?auto=webp&amp;s=032db8ea491d25171e232fb3d80d8b599c097b35", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3950eb7cff00d2f0b9b81917107dbc2490e7f610", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1830a44e95aa46309c282008d8f6741e296e4f2c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/pJlxo7-6B4RVsUdH55lqkvpqUxvS5BevbnloVWxxyQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd3417f411f23e2fad4a4d215c1001c45bf5504", "width": 320, "height": 240}], "variants": {}, "id": "8QTuBfki2AvUagIUD-Qvh-jLmyvk1cj4sE_QaBnYeck"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14ztv2d", "is_robot_indexable": true, "report_reasons": null, "author": "No-Platypus4021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14ztv2d/trying_to_increase_my_reading_any_books_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=fIdp1zrXhF4", "subreddit_subscribers": 115954, "created_utc": 1689372812.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fIdp1zrXhF4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"9 MUST Read Books For Data Engineers In 2023 - From Beginner To Advanced\"&gt;&lt;/iframe&gt;", "author_name": "Seattle Data Guy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fIdp1zrXhF4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@SeattleDataGuy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently learning data engineering and taking cs50, should I just watch all the videos? Or should I go through all the practice problems? I'm currently on week 4 and have been going through the practice problems, but that's moving pretty slowly since I'm also reading a textbook on data warehousing. Would it be better to just watch all the videos on cs50 so I can move through it quicker?\n\nFor some background info, I'm a data analyst with no degree. I no longer do any type of data analysis, instead I'm doing ETL, OLAP cube building, and automating for my departments reports. I'll also start focusing on pre-processing data on clients where our current processes can't handle the data quantity, and I'm helping come up with some data pipelines to automate some manual processes. None of this is focused on super high volume data.\n\nI want to build a strong foundation in computer science since I have no degree, but I also want to learn at a decent rate since I'm already doing some data engineering to an extent.\n\ntl;dr - If you were learning data engineering again and taking cs50, would you just watch the lectures or do all the work?", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I skim cs50 or do all the work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150gzsj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689440176.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently learning data engineering and taking cs50, should I just watch all the videos? Or should I go through all the practice problems? I&amp;#39;m currently on week 4 and have been going through the practice problems, but that&amp;#39;s moving pretty slowly since I&amp;#39;m also reading a textbook on data warehousing. Would it be better to just watch all the videos on cs50 so I can move through it quicker?&lt;/p&gt;\n\n&lt;p&gt;For some background info, I&amp;#39;m a data analyst with no degree. I no longer do any type of data analysis, instead I&amp;#39;m doing ETL, OLAP cube building, and automating for my departments reports. I&amp;#39;ll also start focusing on pre-processing data on clients where our current processes can&amp;#39;t handle the data quantity, and I&amp;#39;m helping come up with some data pipelines to automate some manual processes. None of this is focused on super high volume data.&lt;/p&gt;\n\n&lt;p&gt;I want to build a strong foundation in computer science since I have no degree, but I also want to learn at a decent rate since I&amp;#39;m already doing some data engineering to an extent.&lt;/p&gt;\n\n&lt;p&gt;tl;dr - If you were learning data engineering again and taking cs50, would you just watch the lectures or do all the work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "150gzsj", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150gzsj/should_i_skim_cs50_or_do_all_the_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150gzsj/should_i_skim_cs50_or_do_all_the_work/", "subreddit_subscribers": 115954, "created_utc": 1689440176.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was curious if there are examples of projects for pulling in machine sensor data from different locations via api? Looking to do a project and specifically how to handle outages and incremental jobs for a location or certain machines that may go offline while the rest are running smoothly.", "author_fullname": "t2_b5za7mst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Example of Incremental Sensor Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_150i6w3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689443087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was curious if there are examples of projects for pulling in machine sensor data from different locations via api? Looking to do a project and specifically how to handle outages and incremental jobs for a location or certain machines that may go offline while the rest are running smoothly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150i6w3", "is_robot_indexable": true, "report_reasons": null, "author": "No_Cover_Undercover", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150i6w3/example_of_incremental_sensor_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150i6w3/example_of_incremental_sensor_project/", "subreddit_subscribers": 115954, "created_utc": 1689443087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team have some large workloads which we are currently running on Azure Databricks. On an ad-hoc hasis, we occasionally do some large simulations for analytics purposes over all of our historic data which require large costly clusters. It works ok, but since we use cheap Spot instances, 80% of the resulting bill is for the Databricks DBU cost rather than the underlying infrastructure. \n\nSince the jobs don't rely on any of the Databricks closed-source tech, and they're ad-hoc so don't form part of any other integrated workflows, I'd like to see if there's a good option for running these jobs on self managed clusters so that I can cut down the bill.\n\nHaving done some research, there was historically a library called [AZTK](https://github.com/Azure/aztk) (Azure Distributed Data Engineering Toolkit) which provided a simple CLI for creating Spark Clusters on Azure Batch. However, it's no longer being maintained.\n\nDoes anyone have any other recommendations?", "author_fullname": "t2_173s1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simplest solution for self managed spark cluster on Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150budt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1689427048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team have some large workloads which we are currently running on Azure Databricks. On an ad-hoc hasis, we occasionally do some large simulations for analytics purposes over all of our historic data which require large costly clusters. It works ok, but since we use cheap Spot instances, 80% of the resulting bill is for the Databricks DBU cost rather than the underlying infrastructure. &lt;/p&gt;\n\n&lt;p&gt;Since the jobs don&amp;#39;t rely on any of the Databricks closed-source tech, and they&amp;#39;re ad-hoc so don&amp;#39;t form part of any other integrated workflows, I&amp;#39;d like to see if there&amp;#39;s a good option for running these jobs on self managed clusters so that I can cut down the bill.&lt;/p&gt;\n\n&lt;p&gt;Having done some research, there was historically a library called &lt;a href=\"https://github.com/Azure/aztk\"&gt;AZTK&lt;/a&gt; (Azure Distributed Data Engineering Toolkit) which provided a simple CLI for creating Spark Clusters on Azure Batch. However, it&amp;#39;s no longer being maintained.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?auto=webp&amp;s=b06c012cb41ecc749eb5f90c904274d46a26840d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=adbb3b707613aca52feaf697def8a8a2b342215b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd8add2cbde77c9734804c7207c81c1563a7673d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba3432a3dc98d43482ee02b50d7bd61c03caa6ee", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=86aea326065a814d88ab3c561665790878990f58", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bca8ae50018d5ace9308ece2d856c6e9cfe52b70", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YfCCaKDU6xNxdB0_jSf3seyilHcwFVgMouFFowBqioE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9bee3ef5f4fbc546675378d5e70503fdc99b8fa", "width": 1080, "height": 540}], "variants": {}, "id": "q67RFzude0ccMbhzCzeXcdZFMywLc15tAjithvMoja4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150budt", "is_robot_indexable": true, "report_reasons": null, "author": "Pancakeman123000", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150budt/simplest_solution_for_self_managed_spark_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150budt/simplest_solution_for_self_managed_spark_cluster/", "subreddit_subscribers": 115954, "created_utc": 1689427048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently trying to learn Apache Airflow and I have read and heard many times that Apache Airflow is an orchestration tool and should not be used to process data.\n\nHowever, some things are not clear to me:\n\n1. Does this mean I should not use the `PythonOperator` to transform my data but instead use something like the `SparkOperator`?\n2. If I can use the `PythonOperator` to transform my data, where exactly is the process done? On a worker (if I use Celery Executor) or on a POD (if I use Kubernetes Executor)? How is that different from using Apache Airflow as a processing tool?\n3. Also, If I can use the `PythonOperator` to transform my data, then how exactly does one use Apache Airflow to process data? \n\n&amp;#x200B;", "author_fullname": "t2_c8f4gnokr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where data is processed when using PythonOperator?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_150j1ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689445198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently trying to learn Apache Airflow and I have read and heard many times that Apache Airflow is an orchestration tool and should not be used to process data.&lt;/p&gt;\n\n&lt;p&gt;However, some things are not clear to me:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Does this mean I should not use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data but instead use something like the &lt;code&gt;SparkOperator&lt;/code&gt;?&lt;/li&gt;\n&lt;li&gt;If I can use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data, where exactly is the process done? On a worker (if I use Celery Executor) or on a POD (if I use Kubernetes Executor)? How is that different from using Apache Airflow as a processing tool?&lt;/li&gt;\n&lt;li&gt;Also, If I can use the &lt;code&gt;PythonOperator&lt;/code&gt; to transform my data, then how exactly does one use Apache Airflow to process data? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150j1ge", "is_robot_indexable": true, "report_reasons": null, "author": "NoobAllTheWay", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150j1ge/where_data_is_processed_when_using_pythonoperator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150j1ge/where_data_is_processed_when_using_pythonoperator/", "subreddit_subscribers": 115954, "created_utc": 1689445198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, In the company I work for we use PySpark on EMR or Glue Jobs for Big Data processing.\n\nFor small data cases we use AWS SDK for Pandas (awswrangler) on Lambda.\n\nIs there anyway to use polars in this manner while also reading and writing to Glue Catalog tables?", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Data processing with Polars @AWS lambda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150fgvc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689436394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, In the company I work for we use PySpark on EMR or Glue Jobs for Big Data processing.&lt;/p&gt;\n\n&lt;p&gt;For small data cases we use AWS SDK for Pandas (awswrangler) on Lambda.&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to use polars in this manner while also reading and writing to Glue Catalog tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "150fgvc", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150fgvc/small_data_processing_with_polars_aws_lambda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150fgvc/small_data_processing_with_polars_aws_lambda/", "subreddit_subscribers": 115954, "created_utc": 1689436394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to Snowflake and I was trying to read about different types of tables. I understood how fail-safe is an internal mechanism in Snowflake that is kept for operational purposes.\n\n But the fail-safe is for 7 days beyond the retention period is what Snowflake docs mention. So in that case, if my retention period is 7 days, then fail-safe will be for another 7 days, which means I have to pay storage costs for 14 days of data for each table right? \n\nWhy can't fail-safe be just as retention period and not on top of retention period? This will save costs considerably if it is a large table. \n\nPS: I know about fail-safe not being a option that can be leveraged by developers. In case of operational failures, Snowflake support team can help recovering the data and that's the reason for fail-safe. ", "author_fullname": "t2_2xxs9nne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding fail-safe in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15052dq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689405440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to Snowflake and I was trying to read about different types of tables. I understood how fail-safe is an internal mechanism in Snowflake that is kept for operational purposes.&lt;/p&gt;\n\n&lt;p&gt;But the fail-safe is for 7 days beyond the retention period is what Snowflake docs mention. So in that case, if my retention period is 7 days, then fail-safe will be for another 7 days, which means I have to pay storage costs for 14 days of data for each table right? &lt;/p&gt;\n\n&lt;p&gt;Why can&amp;#39;t fail-safe be just as retention period and not on top of retention period? This will save costs considerably if it is a large table. &lt;/p&gt;\n\n&lt;p&gt;PS: I know about fail-safe not being a option that can be leveraged by developers. In case of operational failures, Snowflake support team can help recovering the data and that&amp;#39;s the reason for fail-safe. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15052dq", "is_robot_indexable": true, "report_reasons": null, "author": "inglocines", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15052dq/understanding_failsafe_in_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15052dq/understanding_failsafe_in_snowflake/", "subreddit_subscribers": 115954, "created_utc": 1689405440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI\u2019ve been seeing quite often how much variety Data Engineer responsibilities can be. Some can be more of an Analytics Engineer who is focused on metrics and dashboards, while also building pipelines. While others are more of a SWE focused on data. \n\nMy question comes with the SWE Data Engineer. Obviously SQL and Python are very important, but are Data Engineers who have more SWE responsibilities, more Python heavy, or SQL heavy, or both equally? I understand this varies by company, but I\u2019m curious.", "author_fullname": "t2_9jq8g6tr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Title Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzq93", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been seeing quite often how much variety Data Engineer responsibilities can be. Some can be more of an Analytics Engineer who is focused on metrics and dashboards, while also building pipelines. While others are more of a SWE focused on data. &lt;/p&gt;\n\n&lt;p&gt;My question comes with the SWE Data Engineer. Obviously SQL and Python are very important, but are Data Engineers who have more SWE responsibilities, more Python heavy, or SQL heavy, or both equally? I understand this varies by company, but I\u2019m curious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "14zzq93", "is_robot_indexable": true, "report_reasons": null, "author": "WorldlyDirt5024", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzq93/job_title_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzq93/job_title_question/", "subreddit_subscribers": 115954, "created_utc": 1689388679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nSo I work at a small clinic and have recently been charged with consolidating our various data sources into one single database so that analytics can be performed in a fast, easier way. Our sources include excel sheets and csv files created in-house, and reports downloaded in these formats from medical web-portals. \n\nWhat would be the best way (and technology) to go about creating a single database system where I can eventually create automated systems for all these different data sources to be consolidated in one database system? \n\n&amp;#x200B;\n\nThank you\n\n(P.S. I do have a computer science degree, but I haven't worked too much with architecting database systems/infrastructure, mostly running SQL queries, and ML algorithms with existing datasets)", "author_fullname": "t2_7a3r9mqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBMS options for a small clinic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_150jm2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689446581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;So I work at a small clinic and have recently been charged with consolidating our various data sources into one single database so that analytics can be performed in a fast, easier way. Our sources include excel sheets and csv files created in-house, and reports downloaded in these formats from medical web-portals. &lt;/p&gt;\n\n&lt;p&gt;What would be the best way (and technology) to go about creating a single database system where I can eventually create automated systems for all these different data sources to be consolidated in one database system? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n\n&lt;p&gt;(P.S. I do have a computer science degree, but I haven&amp;#39;t worked too much with architecting database systems/infrastructure, mostly running SQL queries, and ML algorithms with existing datasets)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150jm2q", "is_robot_indexable": true, "report_reasons": null, "author": "juiceleft88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150jm2q/dbms_options_for_a_small_clinic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150jm2q/dbms_options_for_a_small_clinic/", "subreddit_subscribers": 115954, "created_utc": 1689446581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here familiar working with Syndigo PXM or MDM solution? \n\nI\u2019m starting thinking if is this really the future career? \n\nI need your advise what are the trends for DE role. \ud83d\ude0a", "author_fullname": "t2_7cswt26k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Syndigo MDM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150f1yp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689435350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here familiar working with Syndigo PXM or MDM solution? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m starting thinking if is this really the future career? &lt;/p&gt;\n\n&lt;p&gt;I need your advise what are the trends for DE role. \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "150f1yp", "is_robot_indexable": true, "report_reasons": null, "author": "Federal_Ad179", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150f1yp/syndigo_mdm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150f1yp/syndigo_mdm/", "subreddit_subscribers": 115954, "created_utc": 1689435350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working on a personal project to ETL fin data and dashboard it. However, I'm having confusion about whether I should ingest API -&gt; split into different tables using a data model in BQ -&gt; transformation -&gt; dashboard\n\nOR\n\nAPI ingestion -&gt; extract the required columns using pandas &amp; transformation using duck db -&gt; load df directly to BQ -&gt; dashboard\n\nI'm currently in progress with the second implementation as I decided to go on with the light &amp; quick way to get the insights.\n\nWhat do you guys think, should it be the other way infact?", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Doubt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_150ez1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689435147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a personal project to ETL fin data and dashboard it. However, I&amp;#39;m having confusion about whether I should ingest API -&amp;gt; split into different tables using a data model in BQ -&amp;gt; transformation -&amp;gt; dashboard&lt;/p&gt;\n\n&lt;p&gt;OR&lt;/p&gt;\n\n&lt;p&gt;API ingestion -&amp;gt; extract the required columns using pandas &amp;amp; transformation using duck db -&amp;gt; load df directly to BQ -&amp;gt; dashboard&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently in progress with the second implementation as I decided to go on with the light &amp;amp; quick way to get the insights.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think, should it be the other way infact?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "150ez1j", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/150ez1j/project_doubt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/150ez1j/project_doubt/", "subreddit_subscribers": 115954, "created_utc": 1689435147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.\n\nCurrently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.\n\nThis I am posting because I want to get to know the experience from the personal user instead.\n\nI am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.", "author_fullname": "t2_kmi1m0w3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "M1 vs M2 MBA for tasks relating to PySpark or Apache Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15082zu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689415630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to purchase a MacBook for my basic tasks relating to data engineering. For instance, running Apache Spark Server and executing PySpark jobs and/or running Apache Airflow Server and executing the code for workflow orchestration.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am using a virtual machine inside my Windows Laptop. So, I am sure that any MacBook Air will outperform deadly with respect to the setup that I am currently using. Plus, I have had enough of reading reviews and watching comparisons between M1 Air and M2 Air.&lt;/p&gt;\n\n&lt;p&gt;This I am posting because I want to get to know the experience from the personal user instead.&lt;/p&gt;\n\n&lt;p&gt;I am confused about purchasing which MacBook Air only for personal use considering I will not be replacing it in a year or two.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15082zu", "is_robot_indexable": true, "report_reasons": null, "author": "Prudent-Writing-5724", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15082zu/m1_vs_m2_mba_for_tasks_relating_to_pyspark_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15082zu/m1_vs_m2_mba_for_tasks_relating_to_pyspark_or/", "subreddit_subscribers": 115954, "created_utc": 1689415630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This doesn't seem like out of the box functionality (and since dbt is so new even github copilot is confused on what to do), so I'm wondering if I have to define my own macro or something\n\nI'm new to dbt so I'm wondering is there is a way to select columns based on a pattern like startswith or contains or something like that. This is in postgres btw.", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In dbt postgres, how would I create a model that only selects columns that start with 'c_' ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zzm50", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689388342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This doesn&amp;#39;t seem like out of the box functionality (and since dbt is so new even github copilot is confused on what to do), so I&amp;#39;m wondering if I have to define my own macro or something&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to dbt so I&amp;#39;m wondering is there is a way to select columns based on a pattern like startswith or contains or something like that. This is in postgres btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "14zzm50", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zzm50/in_dbt_postgres_how_would_i_create_a_model_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zzm50/in_dbt_postgres_how_would_i_create_a_model_that/", "subreddit_subscribers": 115954, "created_utc": 1689388342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are a lot of options now in terms of data pipelines, tools, and paradigms. Databricks, airflow, trino, dbt, snowflake, medallion , lake house, mesh, fabric\u2026. \n\nI\u2019m interested in a discussion around how people have set up their environment to do effective development. I am especially interested in ways to enable local development that can then be promoted to a prod data pipeline. \n\nFor instance, let\u2019s say I have airflow executing tasks on a simple job or spark cluster cluster. These jobs live in a VPC that is not accessible to the internet. I want to develop my jobs in some test environment, preferably locally, then push these changes to a CI/CD pipeline to hook them in to prod. How do you go about solving this challenge? Do you replicate any source or target destinations locally? \n\nAnother example is if I have databricks running my pipeline, I can set up a duplicate test environment or database to development or test my code. Although it is easy to have a dev/test pipeline then promote these jobs with a global variable, this can get quite costly if one has a duplicate everything, not to mention devising unit tests in databricks can be a hassle. \n\nAnother hassle I have run across is using lambda functions as part of a data processing pipeline. If one uses SNS to trigger the lambda function, do you use localstack to recreate the AWS infra?\n\nSorry for sort of rambling off for a while, but TLDR;\n\nHow do you set up your environment to develop data pipelines locally and what challenges have you overcome to accomplish this?", "author_fullname": "t2_q87mqvk0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you build an effective development experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zrhee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689367241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of options now in terms of data pipelines, tools, and paradigms. Databricks, airflow, trino, dbt, snowflake, medallion , lake house, mesh, fabric\u2026. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m interested in a discussion around how people have set up their environment to do effective development. I am especially interested in ways to enable local development that can then be promoted to a prod data pipeline. &lt;/p&gt;\n\n&lt;p&gt;For instance, let\u2019s say I have airflow executing tasks on a simple job or spark cluster cluster. These jobs live in a VPC that is not accessible to the internet. I want to develop my jobs in some test environment, preferably locally, then push these changes to a CI/CD pipeline to hook them in to prod. How do you go about solving this challenge? Do you replicate any source or target destinations locally? &lt;/p&gt;\n\n&lt;p&gt;Another example is if I have databricks running my pipeline, I can set up a duplicate test environment or database to development or test my code. Although it is easy to have a dev/test pipeline then promote these jobs with a global variable, this can get quite costly if one has a duplicate everything, not to mention devising unit tests in databricks can be a hassle. &lt;/p&gt;\n\n&lt;p&gt;Another hassle I have run across is using lambda functions as part of a data processing pipeline. If one uses SNS to trigger the lambda function, do you use localstack to recreate the AWS infra?&lt;/p&gt;\n\n&lt;p&gt;Sorry for sort of rambling off for a while, but TLDR;&lt;/p&gt;\n\n&lt;p&gt;How do you set up your environment to develop data pipelines locally and what challenges have you overcome to accomplish this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14zrhee", "is_robot_indexable": true, "report_reasons": null, "author": "drrednirgskizif", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zrhee/how_do_you_build_an_effective_development/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zrhee/how_do_you_build_an_effective_development/", "subreddit_subscribers": 115954, "created_utc": 1689367241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just set up my web app with error/performance analytics for engineering and it got me thinking about what marketing/user data I should be collecting. I was listening to an episode of software daily (Making Data Driven Decisions with Soumyadeb Mitra) and one thing the guest (Soumyadeb) mentioned was that many companies he\u2019d worked with didn\u2019t have clean data, so they couldn\u2019t do much with it.\n\nI am a software engineer, so I do understand some aspects of clean data, for instance, typing and null extraction. \n\nBut, is there anything else I should consider?", "author_fullname": "t2_3ar5501p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting a new venture. What steps would you all suggest I take to make sure my data is \u201cclean\u201d? What would the first data you would look to collect?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_14zpqdw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1689363082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just set up my web app with error/performance analytics for engineering and it got me thinking about what marketing/user data I should be collecting. I was listening to an episode of software daily (Making Data Driven Decisions with Soumyadeb Mitra) and one thing the guest (Soumyadeb) mentioned was that many companies he\u2019d worked with didn\u2019t have clean data, so they couldn\u2019t do much with it.&lt;/p&gt;\n\n&lt;p&gt;I am a software engineer, so I do understand some aspects of clean data, for instance, typing and null extraction. &lt;/p&gt;\n\n&lt;p&gt;But, is there anything else I should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "14zpqdw", "is_robot_indexable": true, "report_reasons": null, "author": "5olArchitect", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/14zpqdw/starting_a_new_venture_what_steps_would_you_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/14zpqdw/starting_a_new_venture_what_steps_would_you_all/", "subreddit_subscribers": 115954, "created_utc": 1689363082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ftuqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chat-GPT plugins and Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_150dwlj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/67Z-n6kzGpks-2jD6sMBkFlrxkeUtZ7IEghhHW3r2oE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689432477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "juhache.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://juhache.substack.com/p/chat-gpt-plugins-and-data-engineering", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?auto=webp&amp;s=13a1917ae36d972b4368bcf804defb913747c46f", "width": 1024, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ffc3de90b908118fbd7f037aabe80c74b51a2ed", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a87dce3e137c9af896cc581e449f12eaee419088", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca960a8a642de15ca86b4e8f973b56ec72008182", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6463906b55c177dfdfc2c7f0c7acdaada0509d7b", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/01GMlJes8670cvCY8gSaOU-U6g_UPO7l1w4wCeD-UPY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a675d07ab79df5ef91cee1025ab32069fc4ef7d", "width": 960, "height": 562}], "variants": {}, "id": "9DsW5dB33_z07FSvheGgB0Z9t7wSr8pHJ8z5z_o0muA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "150dwlj", "is_robot_indexable": true, "report_reasons": null, "author": "philonoist", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150dwlj/chatgpt_plugins_and_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://juhache.substack.com/p/chat-gpt-plugins-and-data-engineering", "subreddit_subscribers": 115954, "created_utc": 1689432477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ftuqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unit Testing for data engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_150dzb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JeG6AEPTyH8uzy3n39-MUYvqhB5xnPdGBpLIW8W9o_Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1689432670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?auto=webp&amp;s=c51efa238a37f2c964e17c669be93c6b29f2fd9f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e811f98c026cb8f9131ff033c9c32d0f24c7f2aa", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=404960819ee2fd33af872657472c95c26ece4a4f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82d740b30ba7f26b5c6dcefe1783d5bf59a90e7f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d7c5e19e213c8e0547788dc2a8f2a8c32eecf28", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f41758a992ddd35178cd2f501f4fc7f14b8e9542", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hvYyopF-cZX2WKFRSrbTx_PslHjwqnrG0kQHfuNLsJk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de802c91aa8ab836fdcf1869220eb01cc40a7b09", "width": 1080, "height": 540}], "variants": {}, "id": "APy3ZRoodILM2d2MRzQ4YrvSvSJa9Z19Xya3CDQkmVQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "150dzb6", "is_robot_indexable": true, "report_reasons": null, "author": "philonoist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/150dzb6/unit_testing_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/unit-testing-for-data-engineers", "subreddit_subscribers": 115954, "created_utc": 1689432670.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}