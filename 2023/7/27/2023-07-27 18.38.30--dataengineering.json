{"kind": "Listing", "data": {"after": "t3_15b7zx7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Turns out databases are \"relational\" or something", "author_fullname": "t2_qlmkijv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The data engineer came to me... tears in his eyes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 107, "top_awarded_type": null, "hide_score": false, "name": "t3_15ae6kp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 673, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 673, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w5lBiBLnqC1xzIdkywBWCqB0YGYQeRWrqr08EMByD2s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690397313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Turns out databases are &amp;quot;relational&amp;quot; or something&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/1wm37l33vceb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/1wm37l33vceb1.png?auto=webp&amp;s=8eb68cc0feab067c94c5d88ca54a7eda2cbfcfd2", "width": 1080, "height": 830}, "resolutions": [{"url": "https://preview.redd.it/1wm37l33vceb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=48652b42e4fe71b9c48eac0b2a2c6a0eaaad045b", "width": 108, "height": 83}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=79d5b3781803dce41f5534196cb74c8f84aa1caa", "width": 216, "height": 166}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afdafe3ea62eae50680c1797cac89dd8c6010a0f", "width": 320, "height": 245}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=992489262d93f2834aba10c908b504e4f58e328a", "width": 640, "height": 491}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=97be176695471f82f58a8349a936a5d2c170eaf9", "width": 960, "height": 737}, {"url": "https://preview.redd.it/1wm37l33vceb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b45cda7bf48386140f347a041f725b62cbbe33b", "width": 1080, "height": 830}], "variants": {}, "id": "UwqiG5dVs8n80MCeWPA1--klG-ySKne2YEq3LyGRi8k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "15ae6kp", "is_robot_indexable": true, "report_reasons": null, "author": "shed_antlers", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ae6kp/the_data_engineer_came_to_me_tears_in_his_eyes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/1wm37l33vceb1.png", "subreddit_subscribers": 118496, "created_utc": 1690397313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nWe recently ran an experiment at [Dozer](https://github.com/getdozer/dozer) that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:\n\n[https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce](https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce)\n\nWould love to get your feedback.  \n\n\nThanks  \nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing 160m of records (with 4 JOINs and 1 aggregation) in real-time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15axba2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1690450727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;We recently ran an experiment at &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; that I think worth sharing. We processed in real-time 160m records spread across 4 tables and created APIs from the result. The operation consisted of 4 JOINs and 1 aggregation. Here is a link to the experiment:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce\"&gt;https://github.com/getdozer/dozer-samples/tree/main/usecases/scaling-ecommerce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get your feedback.  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;br/&gt;\nMatteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?auto=webp&amp;s=205a7ce9ef4cd41ab708aeb4c8b2f411ade2b5c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4b2137c62de69a82f790eca2cf323b4e9fa49a42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e9e76b368d9bd2737828569495d8f080ea1ca66", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecc4927d891da4391fe409958ef35c9d5e83965e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19b42c63769cc4774b26d17ac2f1dbbcc10fd21a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ba1086ce1186414717e41899ed3393ff00818f4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/s612CZA7VZM8OY-3Gnn7qwyEW65qB0LXv68Ev2-JnvQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f22dfc70fa64869063b5eedee17d4f7b841f7dc", "width": 1080, "height": 540}], "variants": {}, "id": "bsBU-V26yp9Cn213aetzEQxsmD1y-nrva8jMGPRMp5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15axba2", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15axba2/processing_160m_of_records_with_4_joins_and_1/", "subreddit_subscribers": 118496, "created_utc": 1690450727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used Microsoft Fabric yet? If your a Microsoft shop are you planning on adopting it? \n\nNot coming from any angle I just haven\u2019t talked with anyone who has been using it a lot.", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Fabric - have you used it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15am3g2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690415941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used Microsoft Fabric yet? If your a Microsoft shop are you planning on adopting it? &lt;/p&gt;\n\n&lt;p&gt;Not coming from any angle I just haven\u2019t talked with anyone who has been using it a lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15am3g2", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15am3g2/microsoft_fabric_have_you_used_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15am3g2/microsoft_fabric_have_you_used_it/", "subreddit_subscribers": 118496, "created_utc": 1690415941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a cs software manager at a public company. I\u2019m not currently in a data engineering role, but I want to make a pivot into DE. I\u2019ve had a DE ticket open for several months for them just to scope and there\u2019s no end in sight. It could be several more months before they can even think about looking at our ticket. Suffice it to say our DE team is under resourced. \n\nThis is where I feel that maybe I\u2019m being naive: I feel like the request is simple. We want to create a daily job for some API reports and bring it into our data warehouse. \n\nHow many hoops does one need to jump through in a mid-level organization to write a script then push it to the warehouse? \n\nWhat level of difficulty is this normally? \n\nWould it be rude to write the script and hand it over to the DE team?\n\nLooking for advice to navigate the situation. Thanks!", "author_fullname": "t2_b7jxy4i2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I being naive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ad17i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690394712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a cs software manager at a public company. I\u2019m not currently in a data engineering role, but I want to make a pivot into DE. I\u2019ve had a DE ticket open for several months for them just to scope and there\u2019s no end in sight. It could be several more months before they can even think about looking at our ticket. Suffice it to say our DE team is under resourced. &lt;/p&gt;\n\n&lt;p&gt;This is where I feel that maybe I\u2019m being naive: I feel like the request is simple. We want to create a daily job for some API reports and bring it into our data warehouse. &lt;/p&gt;\n\n&lt;p&gt;How many hoops does one need to jump through in a mid-level organization to write a script then push it to the warehouse? &lt;/p&gt;\n\n&lt;p&gt;What level of difficulty is this normally? &lt;/p&gt;\n\n&lt;p&gt;Would it be rude to write the script and hand it over to the DE team?&lt;/p&gt;\n\n&lt;p&gt;Looking for advice to navigate the situation. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ad17i", "is_robot_indexable": true, "report_reasons": null, "author": "Marble_Kween", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ad17i/am_i_being_naive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ad17i/am_i_being_naive/", "subreddit_subscribers": 118496, "created_utc": 1690394712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If  you were to start over in the Data Engineering or would have to mentor someone, how would you do.   \nTaking into account all these new tools in today's tech stack.  \nI've researched all this, but TBH I don't have idea about 90% of these:  \n  \n\n**\u00b7 ETL and Scheduling or Orchestration or Jobs =**\n\n1. Open source Tools (Airflow(most used), Dagster, Argo, Prefect, Luigi)\n\n2. Traditional UI Tool (SSIS, Informatica, Talend, FiveTran)\n\n3. Cloud Tool (Azure Data Factory, Google Dataflow, AWS Glue)\n\n4. Modern Proprietary Tool (Databricks, Trifacta)\n\n5. Fivetran (just usually mostly used but a little old school) | MWAA (cost effective) | Apache Beam\n\n**\u00b7 Data Warehouse =** Snowflake or BigQuery | For Cloud = Amazon S3\n\n**\u00b7 Data Lakes =** DataBricks, Redshift\n\n**\u00b7 Data Transformation or Data Quality Control testing or Parallel Processing Tools =** dbt, Pandas, Apache Spark, \n\n**\u00b7 BI =** Power BI\n\n**\u00b7 Exploratory Data Analysis =** Snowflake, Jupyter, Alteryx, Snowflake Snowsight\n\n**\u00b7 Scaling Workers =** aws lambdas, kubernetes\n\n**\u00b7 Devops Methods =** Observability, alerting, incident management, ci/cd and good pr hygiene apply as much to DE as regular backend SE\n\n**\u00b7 Stream Processing =** Apache Kafka,\n\n**\u00b7 AWS Services =** RDS (to setup and scale db in cloud)", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you learn DE if you had to start over ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15adu9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690396515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If  you were to start over in the Data Engineering or would have to mentor someone, how would you do.&lt;br/&gt;\nTaking into account all these new tools in today&amp;#39;s tech stack.&lt;br/&gt;\nI&amp;#39;ve researched all this, but TBH I don&amp;#39;t have idea about 90% of these:  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 ETL and Scheduling or Orchestration or Jobs =&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Open source Tools (Airflow(most used), Dagster, Argo, Prefect, Luigi)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Traditional UI Tool (SSIS, Informatica, Talend, FiveTran)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Cloud Tool (Azure Data Factory, Google Dataflow, AWS Glue)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Modern Proprietary Tool (Databricks, Trifacta)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fivetran (just usually mostly used but a little old school) | MWAA (cost effective) | Apache Beam&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Warehouse =&lt;/strong&gt; Snowflake or BigQuery | For Cloud = Amazon S3&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Lakes =&lt;/strong&gt; DataBricks, Redshift&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Data Transformation or Data Quality Control testing or Parallel Processing Tools =&lt;/strong&gt; dbt, Pandas, Apache Spark, &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 BI =&lt;/strong&gt; Power BI&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Exploratory Data Analysis =&lt;/strong&gt; Snowflake, Jupyter, Alteryx, Snowflake Snowsight&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Scaling Workers =&lt;/strong&gt; aws lambdas, kubernetes&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Devops Methods =&lt;/strong&gt; Observability, alerting, incident management, ci/cd and good pr hygiene apply as much to DE as regular backend SE&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 Stream Processing =&lt;/strong&gt; Apache Kafka,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;\u00b7 AWS Services =&lt;/strong&gt; RDS (to setup and scale db in cloud)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15adu9z", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15adu9z/how_would_you_learn_de_if_you_had_to_start_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15adu9z/how_would_you_learn_de_if_you_had_to_start_over/", "subreddit_subscribers": 118496, "created_utc": 1690396515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We got a FastAPI server at work that that's used for calculating machine learning features. It receives a big json blob, calculates features and responds with another big json blob. We run these complex CPU operations in a process pool so that we don't block the main event loop. These operations are mostly Pandas operations that can be written in Polars nowadays. So what I'm thinking is that if Polars completely released the GIL we would be able to use a thread pool inside this service rather than a process pool, incurring in less overhead and resource consumption. ", "author_fullname": "t2_v0xlvj6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Python Polars completely release the GIL ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1lnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We got a FastAPI server at work that that&amp;#39;s used for calculating machine learning features. It receives a big json blob, calculates features and responds with another big json blob. We run these complex CPU operations in a process pool so that we don&amp;#39;t block the main event loop. These operations are mostly Pandas operations that can be written in Polars nowadays. So what I&amp;#39;m thinking is that if Polars completely released the GIL we would be able to use a thread pool inside this service rather than a process pool, incurring in less overhead and resource consumption. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b1lnf", "is_robot_indexable": true, "report_reasons": null, "author": "nightwolfomar", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1lnf/does_python_polars_completely_release_the_gil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1lnf/does_python_polars_completely_release_the_gil/", "subreddit_subscribers": 118496, "created_utc": 1690463652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we obviously have classic batch pipelines and now evaluating whether we really need/want realtime pipelines. The problem we have with the realtime pipelines is there is no idempotency, if something fails, no retries etc. What we are thinking is to utilize something like micro-batch architecture where we would run jobs e.g. every minute.. Or every x seconds. Until the pipeline triggers, messages would buffer in some message queue (Kafka?). Then e.g. Airflow would spawn a job that would take all messages from T-1min and process them. \n\nWhat do you think of such approach? How common is it? How good/bad idea is it? Do you use it as well? Are you aware of some do's/dont\u2019s? Some references?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micro-batch architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1l0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we obviously have classic batch pipelines and now evaluating whether we really need/want realtime pipelines. The problem we have with the realtime pipelines is there is no idempotency, if something fails, no retries etc. What we are thinking is to utilize something like micro-batch architecture where we would run jobs e.g. every minute.. Or every x seconds. Until the pipeline triggers, messages would buffer in some message queue (Kafka?). Then e.g. Airflow would spawn a job that would take all messages from T-1min and process them. &lt;/p&gt;\n\n&lt;p&gt;What do you think of such approach? How common is it? How good/bad idea is it? Do you use it as well? Are you aware of some do&amp;#39;s/dont\u2019s? Some references?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b1l0m", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1l0m/microbatch_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1l0m/microbatch_architecture/", "subreddit_subscribers": 118496, "created_utc": 1690463603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "50-60GB database on aurora, everythings fine on writes/updates and transactional operations. We hooked the db to be used as analytics dashboard pulling a fair amount of data but disk i/o is extremely slow from query plan around 5-15mb/s. Bumping up instance ram and loading all working database into shared_buffers cache everything works fine. So I came up to the following conclusions:\n\n- aws aurora is basically unusable for olap workloads as i/o timings are terribly slow (and couldn't find an official figure from aws)\n- aws aurora storage layer uses a storage attached network observed disk i/o is very low and there is no OS level cache so they recommend loading up all databse in memory\n- does aws rds postgres with provisioned ssd suffer from the same problem? iops and throughput are 100x better on paper\n- anyone had similar experiences with aurora and working with relatively large datasets?\n- considering moving to redshift", "author_fullname": "t2_b5vlra5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Aurora not usable for OLAP workloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15agi5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690402621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;50-60GB database on aurora, everythings fine on writes/updates and transactional operations. We hooked the db to be used as analytics dashboard pulling a fair amount of data but disk i/o is extremely slow from query plan around 5-15mb/s. Bumping up instance ram and loading all working database into shared_buffers cache everything works fine. So I came up to the following conclusions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;aws aurora is basically unusable for olap workloads as i/o timings are terribly slow (and couldn&amp;#39;t find an official figure from aws)&lt;/li&gt;\n&lt;li&gt;aws aurora storage layer uses a storage attached network observed disk i/o is very low and there is no OS level cache so they recommend loading up all databse in memory&lt;/li&gt;\n&lt;li&gt;does aws rds postgres with provisioned ssd suffer from the same problem? iops and throughput are 100x better on paper&lt;/li&gt;\n&lt;li&gt;anyone had similar experiences with aurora and working with relatively large datasets?&lt;/li&gt;\n&lt;li&gt;considering moving to redshift&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15agi5q", "is_robot_indexable": true, "report_reasons": null, "author": "golangcafe", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15agi5q/aws_aurora_not_usable_for_olap_workloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15agi5q/aws_aurora_not_usable_for_olap_workloads/", "subreddit_subscribers": 118496, "created_utc": 1690402621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As someone who works in Snowflake day to day but has had little to no exposure to Databricks I'm curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?   \nYes, I know it's not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.   \nThis isn't meant to be a comparison of which you think is necessarily *BETTER*... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who has worked with both Snowflake and Databricks and what do you enjoy/dislike about each?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15b85up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690479473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As someone who works in Snowflake day to day but has had little to no exposure to Databricks I&amp;#39;m curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?&lt;br/&gt;\nYes, I know it&amp;#39;s not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.&lt;br/&gt;\nThis isn&amp;#39;t meant to be a comparison of which you think is necessarily &lt;em&gt;BETTER&lt;/em&gt;... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b85up", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/", "subreddit_subscribers": 118496, "created_utc": 1690479473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "3.5 years as a data analyst. Learning SSIS ETL to try to become my company's data engineer. So far, for data profiling I have just been using custom SQL queries to find basic information about the columns (length, count of blank cells, precision of numerics, etc), but it's very manual work. \n\nIs there a good piece of software that can answer these basic questions for me? Are there tutorials out there that would teach more efficient methods?", "author_fullname": "t2_4xvmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software and learning material to improve my data profiling -&gt; data cleansing work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b1i8y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690463403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;3.5 years as a data analyst. Learning SSIS ETL to try to become my company&amp;#39;s data engineer. So far, for data profiling I have just been using custom SQL queries to find basic information about the columns (length, count of blank cells, precision of numerics, etc), but it&amp;#39;s very manual work. &lt;/p&gt;\n\n&lt;p&gt;Is there a good piece of software that can answer these basic questions for me? Are there tutorials out there that would teach more efficient methods?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b1i8y", "is_robot_indexable": true, "report_reasons": null, "author": "funkyman50", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b1i8y/software_and_learning_material_to_improve_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b1i8y/software_and_learning_material_to_improve_my_data/", "subreddit_subscribers": 118496, "created_utc": 1690463403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit community!\r  \n\r  \nI came across this insightful blog post on SG Analytics about \"Data Trust: How to Build Data Trust.\" I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.\r  \n\r  \n**URL:** [***Data Trust: How to Build Data Trust***](https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/)\r  \n\r  \nIn this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.\r  \n\r  \nI found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.\r  \n\r  \nLet's dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!", "author_fullname": "t2_t1lpdh2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust the Data: A Guide to Building Data Trust in Your Organization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b0869", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690459849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit community!&lt;/p&gt;\n\n&lt;p&gt;I came across this insightful blog post on SG Analytics about &amp;quot;Data Trust: How to Build Data Trust.&amp;quot; I thought it might be of great interest to all the data enthusiasts and professionals here, so I wanted to share it with you all.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;URL:&lt;/strong&gt; &lt;a href=\"https://www.sganalytics.com/blog/data-trust-how-to-build-data-trust/\"&gt;&lt;strong&gt;&lt;em&gt;Data Trust: How to Build Data Trust&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this article, SG Analytics discusses the essential steps and strategies for establishing data trust in any organization. With the increasing reliance on data-driven decision-making, ensuring data integrity and trustworthiness is crucial. The post covers topics like data governance, security measures, data quality, and more, making it a valuable resource for anyone dealing with data management.&lt;/p&gt;\n\n&lt;p&gt;I found the insights in this blog post to be quite helpful, and I believe it can spark interesting discussions within our community. So take a look, and feel free to share your thoughts or experiences related to data trust and how you or your organization ensures the credibility of your data.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s dive into the world of data trust and explore ways to harness the power of data while maintaining its accuracy and reliability! Happy reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b0869", "is_robot_indexable": true, "report_reasons": null, "author": "David_starc150", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b0869/trust_the_data_a_guide_to_building_data_trust_in/", "subreddit_subscribers": 118496, "created_utc": 1690459849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific", "author_fullname": "t2_2fcmcq98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are good tools and references to learn design and architecting data engineering solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ay13l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690453100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of learning and improvising designing systems and solutions around data engineering usecases. Would like to know better tools and references for learning well-architected solutions (just like AWS Solution Architect - Learning Path), but generalizer and data engineering specific&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ay13l", "is_robot_indexable": true, "report_reasons": null, "author": "rockeyjam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ay13l/which_are_good_tools_and_references_to_learn/", "subreddit_subscribers": 118496, "created_utc": 1690453100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_15ar04s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/15ar04s", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UTHqpTlFHrrrFREZFZTlXAqR4y13Ig3P1bFS37PHAVQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690429699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Q3g8NFhvddk", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?auto=webp&amp;s=58f6eb9c6da7e629e8d7d9552a11c526550b142f", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=093b1485febcece41dbb6ab0d6c431d65c795dd9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c3d43d042ab2f713542f19a17bdfa229b03d4a0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/YEosjDQSPs6x5ZEcqj1prjh4vJ4qmRJDPfsj24Cv3cQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88609eb050a6bf25a93e32b82ce82244d89e5855", "width": 320, "height": 240}], "variants": {}, "id": "8_alXOI9EvUESL8dnHyGGGOsh0IN9L4WwQtx6q-hGZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15ar04s", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ar04s/ep26_versioning_data_in_the_data_lakehouse_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Q3g8NFhvddk", "subreddit_subscribers": 118496, "created_utc": 1690429699.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Q3g8NFhvddk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"EP26 - Versioning Data in the Data Lakehouse (File, Table and Catalog Versioning)\"&gt;&lt;/iframe&gt;", "author_name": "Dremio", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Q3g8NFhvddk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Dremio"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org uses kafka to ingest data from source teams into snowflake, but then uses dbt + airflow to do nearly every model that happens downstream of raw data. I had a recent conversation with a coworker about how it's kindof a shame that we have real-time data, but don't actually serve it on our data platform except for once or a few times per day. \n\nIs it generally recommended to push everything to stream processing (if it can be)? \n\nFor example, suppose you're a retail company and a typical use case could be serving data for downstream about the price of products. You might maintain: \n\n* the latest price of the product in a dimensional table - `dim_product_current`\n* the historical prices of the product in a type 2 scd history table - `dim_product_history`\n* a daily snapshot of the prices for all of your products on a given day\n\nOf the three, the current use-case seems most appropriate (and easiest to implement) for real-time. Does it even make sense to enable maintaining scd type 2 for streaming data?\n\nCurious how other teams/orgs approach these problems", "author_fullname": "t2_5e0ue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming vs Batch modeling discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ak8si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690411292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org uses kafka to ingest data from source teams into snowflake, but then uses dbt + airflow to do nearly every model that happens downstream of raw data. I had a recent conversation with a coworker about how it&amp;#39;s kindof a shame that we have real-time data, but don&amp;#39;t actually serve it on our data platform except for once or a few times per day. &lt;/p&gt;\n\n&lt;p&gt;Is it generally recommended to push everything to stream processing (if it can be)? &lt;/p&gt;\n\n&lt;p&gt;For example, suppose you&amp;#39;re a retail company and a typical use case could be serving data for downstream about the price of products. You might maintain: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the latest price of the product in a dimensional table - &lt;code&gt;dim_product_current&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;the historical prices of the product in a type 2 scd history table - &lt;code&gt;dim_product_history&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;a daily snapshot of the prices for all of your products on a given day&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Of the three, the current use-case seems most appropriate (and easiest to implement) for real-time. Does it even make sense to enable maintaining scd type 2 for streaming data?&lt;/p&gt;\n\n&lt;p&gt;Curious how other teams/orgs approach these problems&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ak8si", "is_robot_indexable": true, "report_reasons": null, "author": "harrytrumanprimate", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ak8si/streaming_vs_batch_modeling_discussion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ak8si/streaming_vs_batch_modeling_discussion/", "subreddit_subscribers": 118496, "created_utc": 1690411292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know if anyone else was confused about the multitude of ETL tools out there.  With the help of ChatGPT I came up with this table.  It really helped turn a light bulb on for me.  It would be great if there could be a public list like this that we could keep updated.\n\nEdit: updated table to add Airbyte\n\n&amp;#x200B;\n\n|Product Name|Description|Specialization|\n|:-|:-|:-|\n|dbt|dbt (data build tool) is a popular open-source data transformation tool that enables data analysts and engineers to transform, analyze, and document data.|Transform|\n|Apache Airflow|Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It is commonly used for orchestrating data pipelines.|Orchestrator|\n|Apache Spark|Apache Spark is an open-source distributed data processing engine that provides fast and general-purpose cluster computing for big data processing.|Transform|\n|Apache Kafka|Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.|Extract, Transform|\n|Talend|Talend is a popular data integration and data quality platform that supports data engineering, data integration, data profiling, and more.|Extract, Transform, Load|\n|Stitch|Stitch is a cloud-based data integration service that replicates data from various sources into data warehouses for analytics purposes.|Extract, Load|\n|Fivetran|Fivetran is a fully managed data integration service that continuously syncs data from source systems to data warehouses, making it easy to centralize data.|Extract, Load|\n|AWS Glue|AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services, making it easy to prepare and load data for analytics.|Extract, Transform, Load|\n|Google Cloud Dataflow|Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It supports both batch and stream processing on Google Cloud Platform.|Transform|\n|Databricks|Databricks is a unified data analytics platform that provides a collaborative workspace for data engineering and data science tasks, built on Apache Spark.\u00a0\u00a0|Transform|\n|Matillion|Matillion is a cloud-native ETL and data integration platform that simplifies the process of loading, transforming, and joining data on cloud data warehouses.|Extract, Transform, Load|\n|Airbyte|Open source ELT tool.   Full table and incremental via change data capture. Integrate deeply with Kubernetes, Airflow, Dagster, Prefect, and dbt.\u00a0 |ELT|\n\n^(Table) ^(formatting) ^(brought) ^(to) ^(you) ^(by) [^(ExcelToReddit)](https://xl2reddit.github.io/)", "author_fullname": "t2_j15uu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List of ETL tools, short description, and their primary purpose", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15aeh6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690404884.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690398006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if anyone else was confused about the multitude of ETL tools out there.  With the help of ChatGPT I came up with this table.  It really helped turn a light bulb on for me.  It would be great if there could be a public list like this that we could keep updated.&lt;/p&gt;\n\n&lt;p&gt;Edit: updated table to add Airbyte&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Product Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Description&lt;/th&gt;\n&lt;th align=\"left\"&gt;Specialization&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dbt&lt;/td&gt;\n&lt;td align=\"left\"&gt;dbt (data build tool) is a popular open-source data transformation tool that enables data analysts and engineers to transform, analyze, and document data.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Airflow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It is commonly used for orchestrating data pipelines.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Orchestrator&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Spark&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Spark is an open-source distributed data processing engine that provides fast and general-purpose cluster computing for big data processing.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Apache Kafka&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Talend&lt;/td&gt;\n&lt;td align=\"left\"&gt;Talend is a popular data integration and data quality platform that supports data engineering, data integration, data profiling, and more.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Stitch&lt;/td&gt;\n&lt;td align=\"left\"&gt;Stitch is a cloud-based data integration service that replicates data from various sources into data warehouses for analytics purposes.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Fivetran&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fivetran is a fully managed data integration service that continuously syncs data from source systems to data warehouses, making it easy to centralize data.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AWS Glue&lt;/td&gt;\n&lt;td align=\"left\"&gt;AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services, making it easy to prepare and load data for analytics.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Google Cloud Dataflow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines. It supports both batch and stream processing on Google Cloud Platform.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Databricks&lt;/td&gt;\n&lt;td align=\"left\"&gt;Databricks is a unified data analytics platform that provides a collaborative workspace for data engineering and data science tasks, built on Apache Spark.\u00a0\u00a0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Transform&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Matillion&lt;/td&gt;\n&lt;td align=\"left\"&gt;Matillion is a cloud-native ETL and data integration platform that simplifies the process of loading, transforming, and joining data on cloud data warehouses.&lt;/td&gt;\n&lt;td align=\"left\"&gt;Extract, Transform, Load&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Airbyte&lt;/td&gt;\n&lt;td align=\"left\"&gt;Open source ELT tool.   Full table and incremental via change data capture. Integrate deeply with Kubernetes, Airflow, Dagster, Prefect, and dbt.\u00a0&lt;/td&gt;\n&lt;td align=\"left\"&gt;ELT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;sup&gt;Table&lt;/sup&gt; &lt;sup&gt;formatting&lt;/sup&gt; &lt;sup&gt;brought&lt;/sup&gt; &lt;sup&gt;to&lt;/sup&gt; &lt;sup&gt;you&lt;/sup&gt; &lt;sup&gt;by&lt;/sup&gt; &lt;a href=\"https://xl2reddit.github.io/\"&gt;&lt;sup&gt;ExcelToReddit&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15aeh6s", "is_robot_indexable": true, "report_reasons": null, "author": "jbrune", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15aeh6s/list_of_etl_tools_short_description_and_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15aeh6s/list_of_etl_tools_short_description_and_their/", "subreddit_subscribers": 118496, "created_utc": 1690398006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**DISCLAIMER**\n\nI am no data engineer and I don't know python or any programming language. I can put together stuff into VScode but thats about the extent of my programming abilities. \n\n&amp;#x200B;\n\n**INTRO &amp; CONTEXT**\n\nI\u2019m working on a personal project that takes a PDF containing medical lab results, extracts the data and processes the data into a new centralized schema. From that schema, data is imported into an Excel or Sheets table, which is used as a data source for a no-code website builder. (like Noloco)\n\nI designed the centralized database schema both as a JSON and as an Excel.\n\nI\u2019ve managed to extract some data successfully using AWS Textract GUI console demo. The outputs that AWS gives are JSONs and CSVs.\n\nI now need to transform the data from the AWS output, to match the centralized schema so I can map the data. And here is where im stuck..\n\n**PROBLEM**\n\nMy table has a hierarchical layout with nested rows under one column. I need to transform the data, split the first column into 2 columns, do some parsing etc.\n\nBut in order to transform the data I need to programmatically identify what data needs to be manipulated and somehow mark it. Theoretically, my table has 2 types of entries / records. Type 1 entries are singleTests and type 2 entries are collectionTests.\n\n[The table output sample and entries types.](https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2)\n\nI need to think of a parsing logic or a logic to identify the data.\n\n* I first thought about **identification by keywords** and indeed there is an \u201cLC\u201d at the beginning of each record. That LC stands for Central Lab and its the location where the tests have been processed. The problem with this that LC appears irregularly and its not reliable.\n* The second method I thought was **identification by pattern**. For a singleTest there are always 2 rows, the first one having empty values (besides the first column). For a collectionTest, the first 2 rows are always empty (besides the first column). A collectionTest has a variable number of sub-tests.\n\nHere is a draft of the centralized schema database:\n\nhttps://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;format=png&amp;auto=webp&amp;s=c06740e81178ea509d56aefa1849db26e50796cd\n\nIn this first iteration, the original TestName values that were spanning multiple rows have been split into CollectionName and CollectionMethod according to the entry type.\n\n&amp;#x200B;\n\n**NEED**\n\nI need a parsing logic implemented in python (pandas) that can identify entries types and mark them accordingly. Im not sure if a script can do this. Suggest alternatives if you know.\n\n&amp;#x200B;\n\n**PROGRESS**\n\nI\u2019ve tried about 25 iterations of threads with GPT-4 and Code Interpreter. The problem here is I don\u2019t know exactly what to ask since I don\u2019t know data science specific terms and what technique or collection of techniques should be used. Even so, I\u2019ve managed to get very close to marking the entries correctly with Code Interpreter but it seems that, even though it understands the logic, It can\u2019t write a working script.\n\n&amp;#x200B;\n\n**RESOURCES**\n\n* A Notion with my progress on the data identification logic with my prompts [LINK](https://dennis-kampien.notion.site/Parsing-logic-c70e92b49a8b433398a85d49a0d06343?pvs=4)\n* Links to the sheets containing the tables [LINK](https://docs.google.com/spreadsheets/d/1U7bbTX7qPVFb9FeSGzxxWZyDVk1q48lx/edit?usp=sharing&amp;ouid=107105976591677253200&amp;rtpof=true&amp;sd=true)\n* A link with a partial successful GPT thread [LINK](https://chat.openai.com/share/fbac4366-5b57-4dc4-82d0-f1cf946fa58f)\n\n&amp;#x200B;\n\n If you have ideas where else I can get help, would be appreciated. Thank you.  ", "author_fullname": "t2_14ss6y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on Python (pandas) script for data identification logic. Working to extract and transform data from PDF\u2019s into Excel. (Long post)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ft5c4hn88jeb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ab8840e242592775ac67637541564549cd8a8d5"}, {"y": 84, "x": 216, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e8c30e3c03c79b8fc2b9f54a6590914e8339292"}, {"y": 124, "x": 320, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=288d2bab0d4a2de8a2cd1708b231b51882b2ac25"}, {"y": 249, "x": 640, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=caff806277a3bcb4200b33eee3a52bd2f1ba5026"}, {"y": 374, "x": 960, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fca6fddfbcb2814fb6bdbce3f3c02d336f47169f"}, {"y": 421, "x": 1080, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33a4e65b84aaef814225f3ef6664b5b88b342b1e"}], "s": {"y": 624, "x": 1600, "u": "https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2"}, "id": "ft5c4hn88jeb1"}, "fzj0thjd8jeb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 20, "x": 108, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2ad1cff2f1410be46cb494df6b9e050a5464a0d"}, {"y": 40, "x": 216, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=862a962acc049d806c34236588d3fdb48e8f4511"}, {"y": 59, "x": 320, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8de591af43390b23b44c89554fc03600bb1a06c3"}, {"y": 118, "x": 640, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84ae66308adeee36d66da091225fde18955c785b"}, {"y": 178, "x": 960, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cdbe062dc0bda64c6ba2917e89ff60b6005d39c"}, {"y": 200, "x": 1080, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17c41336da949d82933fa4d19cfaf729430ce3f5"}], "s": {"y": 265, "x": 1428, "u": "https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;format=png&amp;auto=webp&amp;s=c06740e81178ea509d56aefa1849db26e50796cd"}, "id": "fzj0thjd8jeb1"}}, "name": "t3_15b68s8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/b-fDRzF5urH9nJsyANLJage8vwRiW7cKtTDc42UNRzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690474956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am no data engineer and I don&amp;#39;t know python or any programming language. I can put together stuff into VScode but thats about the extent of my programming abilities. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;INTRO &amp;amp; CONTEXT&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019m working on a personal project that takes a PDF containing medical lab results, extracts the data and processes the data into a new centralized schema. From that schema, data is imported into an Excel or Sheets table, which is used as a data source for a no-code website builder. (like Noloco)&lt;/p&gt;\n\n&lt;p&gt;I designed the centralized database schema both as a JSON and as an Excel.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve managed to extract some data successfully using AWS Textract GUI console demo. The outputs that AWS gives are JSONs and CSVs.&lt;/p&gt;\n\n&lt;p&gt;I now need to transform the data from the AWS output, to match the centralized schema so I can map the data. And here is where im stuck..&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;PROBLEM&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My table has a hierarchical layout with nested rows under one column. I need to transform the data, split the first column into 2 columns, do some parsing etc.&lt;/p&gt;\n\n&lt;p&gt;But in order to transform the data I need to programmatically identify what data needs to be manipulated and somehow mark it. Theoretically, my table has 2 types of entries / records. Type 1 entries are singleTests and type 2 entries are collectionTests.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ft5c4hn88jeb1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e4b0b1fe7054739ed1fb65ffbeff6cf1aa1169e2\"&gt;The table output sample and entries types.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I need to think of a parsing logic or a logic to identify the data.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I first thought about &lt;strong&gt;identification by keywords&lt;/strong&gt; and indeed there is an \u201cLC\u201d at the beginning of each record. That LC stands for Central Lab and its the location where the tests have been processed. The problem with this that LC appears irregularly and its not reliable.&lt;/li&gt;\n&lt;li&gt;The second method I thought was &lt;strong&gt;identification by pattern&lt;/strong&gt;. For a singleTest there are always 2 rows, the first one having empty values (besides the first column). For a collectionTest, the first 2 rows are always empty (besides the first column). A collectionTest has a variable number of sub-tests.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here is a draft of the centralized schema database:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c06740e81178ea509d56aefa1849db26e50796cd\"&gt;https://preview.redd.it/fzj0thjd8jeb1.png?width=1428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c06740e81178ea509d56aefa1849db26e50796cd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this first iteration, the original TestName values that were spanning multiple rows have been split into CollectionName and CollectionMethod according to the entry type.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;NEED&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I need a parsing logic implemented in python (pandas) that can identify entries types and mark them accordingly. Im not sure if a script can do this. Suggest alternatives if you know.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;PROGRESS&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried about 25 iterations of threads with GPT-4 and Code Interpreter. The problem here is I don\u2019t know exactly what to ask since I don\u2019t know data science specific terms and what technique or collection of techniques should be used. Even so, I\u2019ve managed to get very close to marking the entries correctly with Code Interpreter but it seems that, even though it understands the logic, It can\u2019t write a working script.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RESOURCES&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Notion with my progress on the data identification logic with my prompts &lt;a href=\"https://dennis-kampien.notion.site/Parsing-logic-c70e92b49a8b433398a85d49a0d06343?pvs=4\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Links to the sheets containing the tables &lt;a href=\"https://docs.google.com/spreadsheets/d/1U7bbTX7qPVFb9FeSGzxxWZyDVk1q48lx/edit?usp=sharing&amp;amp;ouid=107105976591677253200&amp;amp;rtpof=true&amp;amp;sd=true\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;A link with a partial successful GPT thread &lt;a href=\"https://chat.openai.com/share/fbac4366-5b57-4dc4-82d0-f1cf946fa58f\"&gt;LINK&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have ideas where else I can get help, would be appreciated. Thank you.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b68s8", "is_robot_indexable": true, "report_reasons": null, "author": "dkampien", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b68s8/help_on_python_pandas_script_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b68s8/help_on_python_pandas_script_for_data/", "subreddit_subscribers": 118496, "created_utc": 1690474956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI work as the main data person for a fintech startup running a Buy Now Pay Later scheme. My tasks involve business intelligence and data engineering, where my final output is generally dashboards on Power BI for different teams. As we are planning to scale up, I am tasked with ensuring our data tasks follow good data engineering practices.\n\nWhile I'm experienced with Power BI, Python, SQL, and Python, I'm not a seasoned data engineer. Hence, I am seeking advice on how to optimize our data engineering further.\n\nOur current setup uses AWS RDS and DynamoDB as primary data sources. I create MySQL views for AWS RDS to transform tables, add auxiliary columns, and then connect to Power BI with a Mysql connector. For DynamoDB, I use AWS Glue to catalog data into S3, clean up necessary parts with Athena views, and connect to Power Bi with an Amazon Athena connector.\n\nThe tables in AWS RDS are transactions, users, and payment links with thousands of rows and the table of the DynamoDB is the biggest one with millions of rows.\n\nWe also use Google Sheets for additional data like exchange rates (converted manually once a month), and some user information provided by our sales team. I have considered automating the exchange rates by using a Python script with AWS Lambda, and perhaps storing them in RDS or somewhere better than Google Sheets. All these tables have just hundreds of rows.\n\nTransformations are mostly done in MySQL views and some in DAX.\n\nMy question is, should I consider incorporating data engineering tools and concepts like pyspark, scala, airflow, dbt, git, etc., that I have been learning but not yet using? Any advice or recommendations will be appreciated.\n\nThank you!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Optimize Data Transformations in a Buy Now Pay Later Fintech Startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b2gh2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690465916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I work as the main data person for a fintech startup running a Buy Now Pay Later scheme. My tasks involve business intelligence and data engineering, where my final output is generally dashboards on Power BI for different teams. As we are planning to scale up, I am tasked with ensuring our data tasks follow good data engineering practices.&lt;/p&gt;\n\n&lt;p&gt;While I&amp;#39;m experienced with Power BI, Python, SQL, and Python, I&amp;#39;m not a seasoned data engineer. Hence, I am seeking advice on how to optimize our data engineering further.&lt;/p&gt;\n\n&lt;p&gt;Our current setup uses AWS RDS and DynamoDB as primary data sources. I create MySQL views for AWS RDS to transform tables, add auxiliary columns, and then connect to Power BI with a Mysql connector. For DynamoDB, I use AWS Glue to catalog data into S3, clean up necessary parts with Athena views, and connect to Power Bi with an Amazon Athena connector.&lt;/p&gt;\n\n&lt;p&gt;The tables in AWS RDS are transactions, users, and payment links with thousands of rows and the table of the DynamoDB is the biggest one with millions of rows.&lt;/p&gt;\n\n&lt;p&gt;We also use Google Sheets for additional data like exchange rates (converted manually once a month), and some user information provided by our sales team. I have considered automating the exchange rates by using a Python script with AWS Lambda, and perhaps storing them in RDS or somewhere better than Google Sheets. All these tables have just hundreds of rows.&lt;/p&gt;\n\n&lt;p&gt;Transformations are mostly done in MySQL views and some in DAX.&lt;/p&gt;\n\n&lt;p&gt;My question is, should I consider incorporating data engineering tools and concepts like pyspark, scala, airflow, dbt, git, etc., that I have been learning but not yet using? Any advice or recommendations will be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b2gh2", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b2gh2/how_to_optimize_data_transformations_in_a_buy_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b2gh2/how_to_optimize_data_transformations_in_a_buy_now/", "subreddit_subscribers": 118496, "created_utc": 1690465916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe are doing a POC for DB right now and it's going pretty well.  One thing though is we don't have access to the photon engine for querying, only as the executor for Spark SQL.  It's been performant compared to what we have now, but I guess I get paranoid when someone tells me I can't see something they want me to buy.  \n\nMy concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?\n\nAppreciate any feedback.", "author_fullname": "t2_117fpt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Photon engine question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15b159i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690462447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We are doing a POC for DB right now and it&amp;#39;s going pretty well.  One thing though is we don&amp;#39;t have access to the photon engine for querying, only as the executor for Spark SQL.  It&amp;#39;s been performant compared to what we have now, but I guess I get paranoid when someone tells me I can&amp;#39;t see something they want me to buy.  &lt;/p&gt;\n\n&lt;p&gt;My concern is more around whether there is lag or hanging at all when doing SQL or the BI layer based on some of the behavior I saw with the Spark side.  About 80% of our usage is through the BI tools so that is significant.  Has anyone had any issues with that or any other issues with Photon you think I should be aware of?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15b159i", "is_robot_indexable": true, "report_reasons": null, "author": "Gators1992", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b159i/databricks_photon_engine_question/", "subreddit_subscribers": 118496, "created_utc": 1690462447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How A Database Get Rid of OOM Crashes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 24, "top_awarded_type": null, "hide_score": false, "name": "t3_15b03up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8p0me-lxGI0ye38ln9SAqjHamG7fEJnx56anle2Onzw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690459507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/d3e91fb39ea5", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?auto=webp&amp;s=c6b255ae65df4e7d855e996be1a18b3913b3c16b", "width": 1200, "height": 209}, "resolutions": [{"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6cc6b77f5b127ce5b4abb15d72981db560ebbffe", "width": 108, "height": 18}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b715ad9378ef133ba8e99571b574cb8c07fc2d3", "width": 216, "height": 37}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4df7804d72f0502bcb05ec9aa64513ae94c3310a", "width": 320, "height": 55}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bb89db914e3a13bc2dd9ce16225c0763d1c3b03", "width": 640, "height": 111}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13e7a15f1e837741b7e5bfbd5962ea9fb98b8ca1", "width": 960, "height": 167}, {"url": "https://external-preview.redd.it/6VJh7J9W3e9a7ERW2YvPOxM2coTCBZSJ257ScBceJP4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb8f46c3e3028df7b04e8f1b8d393a66bf6c2c91", "width": 1080, "height": 188}], "variants": {}, "id": "e-9zVxHoiwpkzfG7uqCmLN2rjt6CjbVDqkj47Avtxoo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15b03up", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b03up/how_a_database_get_rid_of_oom_crashes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/d3e91fb39ea5", "subreddit_subscribers": 118496, "created_utc": 1690459507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "dbt on Spark vs dbt on Trino\n\nHi, we're running a on-prem data platform solution with stacks of **Iceberg** (parquet), **Hive**, **Spark**, **MINIO**, etc. and planning to use **dbt** for our **SQL transformation** of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch\n\n* Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n   * I don't have much experience with Trino, so I'm not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I've seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it's true.\n* For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n   * Spark Thrift: Spark Thrift Server seems to have many issues mentioned here ([https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi\\_vs\\_thriftserver.html#kyuubi-vs-spark-thrift-server](https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server)), which make us think Apache Kyuubi would be a good choice. However, we couldn't find any document/post related to running dbt on Spark Apache Kyuubi\n   * dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn't have much update recently, so I'm afraid we will go into another dead end with this approach\n\n&amp;#x200B;", "author_fullname": "t2_4j9omvnso", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Spark vs dbt Trino", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15av4jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690443143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;dbt on Spark vs dbt on Trino&lt;/p&gt;\n\n&lt;p&gt;Hi, we&amp;#39;re running a on-prem data platform solution with stacks of &lt;strong&gt;Iceberg&lt;/strong&gt; (parquet), &lt;strong&gt;Hive&lt;/strong&gt;, &lt;strong&gt;Spark&lt;/strong&gt;, &lt;strong&gt;MINIO&lt;/strong&gt;, etc. and planning to use &lt;strong&gt;dbt&lt;/strong&gt; for our &lt;strong&gt;SQL transformation&lt;/strong&gt; of tabular data (Dim Fact modeling, Data Mart, etc.), after the data has been processed by Spark (Ingestion, parsing, etc.). The processing data size is usually around 10-100s million records per batch&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should we use dbt on Spark or dbt on Trino. Our use cases are ETL, so what is the difference between the twos, regarding the performance, scalability, feature, etc.? \n\n&lt;ul&gt;\n&lt;li&gt;I don&amp;#39;t have much experience with Trino, so I&amp;#39;m not sure if we need to process heavy jobs, will Trino perform as good as Spark, given the same resources. Has anyone use Trino as the processing tool for ETL yet, rather than just a query engine for adhoc/interactive query? I&amp;#39;ve seen many blogs from Starburst of using Trino as a processing engine for ETL, but not sure if it&amp;#39;s true.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;For dbt on Spark, what should be the setup? Spark Thrift (default Spark Thrift Server or Apache Kyuubi?), Spark HTTP (using Apache Livy) or any other options? \n\n&lt;ul&gt;\n&lt;li&gt;Spark Thrift: Spark Thrift Server seems to have many issues mentioned here (&lt;a href=\"https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server\"&gt;https://kyuubi.readthedocs.io/en/v1.5.2-incubating/overview/kyuubi_vs_thriftserver.html#kyuubi-vs-spark-thrift-server&lt;/a&gt;), which make us think Apache Kyuubi would be a good choice. However, we couldn&amp;#39;t find any document/post related to running dbt on Spark Apache Kyuubi&lt;/li&gt;\n&lt;li&gt;dbt livy: provided by Cloudera, with very few updates. The Apache Livy project doesn&amp;#39;t have much update recently, so I&amp;#39;m afraid we will go into another dead end with this approach&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15av4jc", "is_robot_indexable": true, "report_reasons": null, "author": "chuqbach", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15av4jc/dbt_spark_vs_dbt_trino/", "subreddit_subscribers": 118496, "created_utc": 1690443143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. i want to build a concurrent api that can be used for bulk hitting. I just want to know what are some industry standard practices that I can put in the process. I have previously used flask with asynchronous process but just curious about how the process get performed in the industry.", "author_fullname": "t2_270isr53", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Concurrent API with bulk hit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15atkim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690437849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. i want to build a concurrent api that can be used for bulk hitting. I just want to know what are some industry standard practices that I can put in the process. I have previously used flask with asynchronous process but just curious about how the process get performed in the industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15atkim", "is_robot_indexable": true, "report_reasons": null, "author": "rishabhdev_rd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15atkim/concurrent_api_with_bulk_hit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15atkim/concurrent_api_with_bulk_hit/", "subreddit_subscribers": 118496, "created_utc": 1690437849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.\n\nCurrently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&gt; performing transformations -&gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the \"origin\" for specific IDs. \n\nI'm treating it like a standard application architecture, with the ETL process acting as the \"front end,\" and the database holding the persisted data serving as the \"back end.\" The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. \n\n Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task \"front end\" container needs to execute on the PostgreSQL database backend, given that the database doesn't exist until after the application is up and running. I can't just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can't seem to wrap my head around this one. \n\nHope this makes sense, I can clarify and makes edits if needed. Thanks! ", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-Container Docker Application for ETL Task", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15arvak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690432327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I have an okay amount of knowledge in containerization and docker, but certainly still learning.&lt;/p&gt;\n\n&lt;p&gt;Currently, I am developing an ETL job as a Docker application that reads parquet data from a data lake, performs transformations, and then loads the processed data back into the data lake. This process is basically read silver layer -&amp;gt; performing transformations -&amp;gt; load to gold layer. The ETL task handles new data that has been ingested within the last minute, where a message queue tells the ETL task which parquet file needs processed. To facilitate the transformations, some data needs to be persisted in an application database, enabling lookups during the transformation process. For instance, the database is used to join previously stored header data with new process data or to perform recursive lookups through parent-child relationships, seeking the &amp;quot;origin&amp;quot; for specific IDs. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m treating it like a standard application architecture, with the ETL process acting as the &amp;quot;front end,&amp;quot; and the database holding the persisted data serving as the &amp;quot;back end.&amp;quot; The ETL front end, which is implemented in Python, reads the new data and executes multiple SQL queries against the back-end database (while also writing to it). These queries are needed to perform the necessary transformations. For the deployment, I intend to utilize Docker Compose and manage it as a multi-container application in production, incorporating a PostgreSQL database volume mount. &lt;/p&gt;\n\n&lt;p&gt;Now, my primary concern is how to effectively test and develop the SQL queries that the ETL task &amp;quot;front end&amp;quot; container needs to execute on the PostgreSQL database backend, given that the database doesn&amp;#39;t exist until after the application is up and running. I can&amp;#39;t just randomly write the SQL queries throughout the python application and hope they will all work when I actually build and run the container. Just can&amp;#39;t seem to wrap my head around this one. &lt;/p&gt;\n\n&lt;p&gt;Hope this makes sense, I can clarify and makes edits if needed. Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15arvak", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15arvak/multicontainer_docker_application_for_etl_task/", "subreddit_subscribers": 118496, "created_utc": 1690432327.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an API endpoint that writes to a postgres database on each call. The writes are insert only, rows are not mutated, and can be partitioned by their write time.\n\nUltimately I want to replicate the database in Snowflake via S3, and I don't want to use something like Fivetran.\n\nS3 to Snowflake is straightforward enough. We would do this on a 30 minute schedule running a COPY INTO statement.\n\nThe decision I am stuck on is how to do Postgres to S3.\n\nAs far as I can tell, I have two options:\n\n- Copy directly from Postgres to s3 with the aws_s3 postgres extension as part of the batch job\n- Write to s3 after each API call as a background task in the app\n\nI'm unclear on the best option to go with.\n\nArguments in favor of aws_s3 extension: Copying directly from Postgres is safe for data integrity since the app code's transactions are atomic and reliable, whereas the background job can potentially fail even if a request and transaction succeeds. It also leads to simpler infrastructure. (I'd rather push more logic into batch jobs than the app code.)\n\nArguments in favor of app dumping directly to S3 from the app: Mainly I'm worried about having to manage performance of the Postgres instance. Performance wise, I'd rather just scale up the number of web app instances than have the Postgres database take a hit copying 30 minutes of data at a time.\n\nAny thoughts? What's the best way to go about this? I'm leaning toward the aws_s3 Postgres extension, but I'm not 100% sure.", "author_fullname": "t2_umwzpce8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API -&gt; Postgres -&gt; S3 -- what is the best design pattern?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15amy4f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690418191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an API endpoint that writes to a postgres database on each call. The writes are insert only, rows are not mutated, and can be partitioned by their write time.&lt;/p&gt;\n\n&lt;p&gt;Ultimately I want to replicate the database in Snowflake via S3, and I don&amp;#39;t want to use something like Fivetran.&lt;/p&gt;\n\n&lt;p&gt;S3 to Snowflake is straightforward enough. We would do this on a 30 minute schedule running a COPY INTO statement.&lt;/p&gt;\n\n&lt;p&gt;The decision I am stuck on is how to do Postgres to S3.&lt;/p&gt;\n\n&lt;p&gt;As far as I can tell, I have two options:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Copy directly from Postgres to s3 with the aws_s3 postgres extension as part of the batch job&lt;/li&gt;\n&lt;li&gt;Write to s3 after each API call as a background task in the app&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m unclear on the best option to go with.&lt;/p&gt;\n\n&lt;p&gt;Arguments in favor of aws_s3 extension: Copying directly from Postgres is safe for data integrity since the app code&amp;#39;s transactions are atomic and reliable, whereas the background job can potentially fail even if a request and transaction succeeds. It also leads to simpler infrastructure. (I&amp;#39;d rather push more logic into batch jobs than the app code.)&lt;/p&gt;\n\n&lt;p&gt;Arguments in favor of app dumping directly to S3 from the app: Mainly I&amp;#39;m worried about having to manage performance of the Postgres instance. Performance wise, I&amp;#39;d rather just scale up the number of web app instances than have the Postgres database take a hit copying 30 minutes of data at a time.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts? What&amp;#39;s the best way to go about this? I&amp;#39;m leaning toward the aws_s3 Postgres extension, but I&amp;#39;m not 100% sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15amy4f", "is_robot_indexable": true, "report_reasons": null, "author": "riv3rtrip", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15amy4f/api_postgres_s3_what_is_the_best_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15amy4f/api_postgres_s3_what_is_the_best_design_pattern/", "subreddit_subscribers": 118496, "created_utc": 1690418191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is streaming SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15al69d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Re0FrSKMKH7YzlSDIlhfvarfaaPUaILjQFPTJNoxxhQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690413596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/streaming-sql-explained", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?auto=webp&amp;s=60cfb566bad70b9f2446d482dd485a7ce91f2f6a", "width": 4000, "height": 2250}, "resolutions": [{"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8eecfb2808009fd734c4760b8b50c68a6d1aaa2", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1775b73296b89801e089b6019bae3538a3513fc1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=25901a5d50ac17e2eb59f6d17798ef81011aec78", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f31c8d445f2be0eec7b36cbc3f7c1d8aa6fdc1f5", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d643256c055bda777359846c4365c23c6a18a6c", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/hlUyH04htRFHDmPXMfa2ShAi-5orpk5NucVNicnuep8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=feab0d107309110d3ed7f808565aa6c0ce73add5", "width": 1080, "height": 607}], "variants": {}, "id": "6IvzkDLHQGPnq8tOrsS9svvlhoQh6aVrBV8elvIdwPc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15al69d", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15al69d/what_is_streaming_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/streaming-sql-explained", "subreddit_subscribers": 118496, "created_utc": 1690413596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is the best certification for data engineering that can be done in 30-60 days? I have intermediate knowledge of SQL, python, and theoretical knowledge of data warehousing and data pipelines. Azure data engineering associate and IBM data warehouse engineer professional both seem good, but i am leaning more towards azure. Can anyone who has taken the certification help me make a decision? Thanks for any help.", "author_fullname": "t2_8z6u9wxo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15b7zx7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690479093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is the best certification for data engineering that can be done in 30-60 days? I have intermediate knowledge of SQL, python, and theoretical knowledge of data warehousing and data pipelines. Azure data engineering associate and IBM data warehouse engineer professional both seem good, but i am leaning more towards azure. Can anyone who has taken the certification help me make a decision? Thanks for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15b7zx7", "is_robot_indexable": true, "report_reasons": null, "author": "AlwaysAsad", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15b7zx7/best_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15b7zx7/best_certification/", "subreddit_subscribers": 118496, "created_utc": 1690479093.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}